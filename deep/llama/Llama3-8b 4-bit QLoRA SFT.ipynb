{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec54eca4-7ac9-440d-8065-6d0f7f2aec62",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/paultimothymooney/popular-kaggle-notebooks-using-llms\n",
    "\n",
    "https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91415d21-ee71-41dc-ae91-ff8cb2c0b3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.42.3 in /home/kotech/venv-llama/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/kotech/venv-llama/lib/python3.11/site-packages (0.45.3)\n",
      "Requirement already satisfied: accelerate in /home/kotech/venv-llama/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: peft in /home/kotech/venv-llama/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/kotech/venv-llama/lib/python3.11/site-packages (from transformers>=4.42.3) (4.67.1)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /home/kotech/venv-llama/lib/python3.11/site-packages (from bitsandbytes) (2.6.0)\n",
      "Requirement already satisfied: psutil in /home/kotech/venv-llama/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/kotech/venv-llama/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.42.3) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kotech/venv-llama/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers>=4.42.3) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/kotech/venv-llama/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kotech/venv-llama/lib/python3.11/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kotech/venv-llama/lib/python3.11/site-packages (from requests->transformers>=4.42.3) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kotech/venv-llama/lib/python3.11/site-packages (from requests->transformers>=4.42.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kotech/venv-llama/lib/python3.11/site-packages (from requests->transformers>=4.42.3) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kotech/venv-llama/lib/python3.11/site-packages (from requests->transformers>=4.42.3) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kotech/venv-llama/lib/python3.11/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf460759-d0ed-4ba0-ac44-682869dacfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/kotech/venv-llama/lib/python3.11/site-packages (from scikit-learn) (2.2.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf77b08a-09c4-4d3a-a3b9-5c254c80df83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd2cb9f-cbda-4ee8-ba2f-7968469611cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecb13bd-18c6-4e37-af50-8f3df61c730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"lmsys-chatbot-arena/train.csv\"\n",
    "model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train = train.head(100)\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aedee80c-cc7a-47ce-984d-bb642e4bfc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'????'\n",
      "????\n",
      "b'????'\n"
     ]
    }
   ],
   "source": [
    "s = '\\ud83c\\udf4d\\ud83c\\udf55'\n",
    "#s2 = \"\\ud83d\\ude04\".encode('utf-8','surrogatepass')\n",
    "s2 = s.encode('utf-8', errors='replace')\n",
    "print(s2)\n",
    "s3 = s2.decode()\n",
    "print(s3)\n",
    "print(s3.encode('utf-8'))\n",
    "#print(s.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d39cc75-7ad3-49ea-b5a0-3ae052fcc5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def filter(s):\n",
    "    #print(s)\n",
    "    arr = eval(s, {\"null\": \"\"})\n",
    "    arr_ret = []\n",
    "    for s2 in arr:\n",
    "        s3 = s2.encode('utf-8', errors='replace').decode()\n",
    "        arr_ret.append(json.dumps(s3))\n",
    "    s_ret =  '['+','.join(arr_ret)+']'\n",
    "    #if s != s_ret:\n",
    "        #print('###')\n",
    "        #print(s)\n",
    "        #print(s_ret)\n",
    "    return s_ret\n",
    "    \n",
    "for i, row in train.iterrows():\n",
    "    #s = row['prompt']\n",
    "    #s = row['response_a']\n",
    "    #print(filter(s))\n",
    "    train.loc[i, 'prompt'] = filter(row['prompt'])\n",
    "    train.loc[i, 'response_a'] = filter(row['response_a'])\n",
    "    train.loc[i, 'response_b'] = filter(row['response_b'])\n",
    "    #print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7faaa4ff-e91d-4d17-81ac-9d08fcf0316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    #if example['response_a'].startswith('[\"The question of whether it is'):\n",
    "        #print('#### check')\n",
    "        #example['response_a'] = '[\"The question of whether it is morally\"]'\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(prompt+response_a+response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:256]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids)*[1]\n",
    "    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e95b1636-5d5b-4870-8d61-7784d6f6e263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'response_a', 'response_b', 'label'],\n",
      "    num_rows: 100\n",
      "})\n",
      "['prompt', 'response_a', 'response_b', 'label']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b326910b576c4df2bc3aabe2b7a65d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    print(raw_datasets)\n",
    "    print(raw_datasets.column_names)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    label_tokens_ids = np.array(LABEL_IDS)\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    log_loss_ = log_loss(labels, probs)\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}\n",
    "\n",
    "n_splits = 5\n",
    "fold_idx = 0\n",
    "ds = load_data(train, tokenizer)\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f25f130-8892-467c-97ff-f43fdbda174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Llama3ForSFT(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096, padding_idx=128255)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 9,437,184 || all params: 8,039,698,432 || trainable%: 0.1174\n"
     ]
    }
   ],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]\n",
    "            loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")\n",
    "\n",
    "model = Llama3ForSFT.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16, \n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "350a91a4-66f4-45e3-85e6-fa267037f17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotech/venv-llama/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir = True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=20,\n",
    "    optim=\"adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better = False,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "607390a0-bd1a-4acc-a5f4-60697b887289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/kotech/venv-llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.515600</td>\n",
       "      <td>1.319517</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.319518</td>\n",
       "      <td>6.553900</td>\n",
       "      <td>3.052000</td>\n",
       "      <td>0.763000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=1.7958526134490966, metrics={'train_runtime': 75.3382, 'train_samples_per_second': 1.062, 'train_steps_per_second': 0.265, 'total_flos': 2344210382045184.0, 'train_loss': 1.7958526134490966, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de026e-cb98-47fa-906c-d856852560aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
