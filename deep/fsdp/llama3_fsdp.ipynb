{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50498796-d324-4b30-ba8e-8ae833055f68",
   "metadata": {},
   "source": [
    "https://www.philschmid.de/fsdp-qlora-llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "040769a7-bc61-4619-a3a3-a4c6b7196ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1,2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a23a3b-a704-4075-b08b-832cd763cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is original versions\n",
    "#!pip install numpy==1.26.4\n",
    "#!pip install \"torch==2.2.2\" tensorboard\n",
    "#!pip install --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \n",
    "#!pip install --upgrade \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2183e187-d54c-4f8b-a3d1-b50b4fb683b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate                0.33.0\n",
      "bitsandbytes              0.43.3\n",
      "datasets                  2.20.0\n",
      "evaluate                  0.4.2\n",
      "huggingface-hub           0.24.5\n",
      "peft                      0.12.0\n",
      "torch                     2.4.0\n",
      "torchvision               0.19.0\n",
      "transformers              4.44.0\n",
      "trl                       0.9.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade huggingface_hub\n",
    "#!pip install torch transformers datasets\n",
    "#!pip install accelerate evaluate bitsandbytes trl peft tensorboardx\n",
    "!pip list | grep \"torch\\|transformers\\|huggingface-hub\\|datasets\\|accelerate\\|evaluate\\|trl\\|peft\\|bitsandbytes\\|tensorboardx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc08866f-de1d-4a7f-b6f6-828c3d1a197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/kotech/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#!huggingface-cli login --token \"your_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c6433ff-418f-4310-af20-1364730f115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    " \n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    " \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    " \n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove,batched=False)\n",
    " \n",
    "# Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    " \n",
    "# save datasets to disk\n",
    "#dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "#dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5dfbca7-85fa-433e-ad54-f529388ebaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 9485\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20f3fc0f-82ec-49c1-9a9b-cee939c845cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llama_3_70b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama_3_70b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id: \"meta-llama/Meta-Llama-3-70b\" # Hugging Face model id\n",
    "dataset_path: \".\"                      # path to dataset\n",
    "max_seq_length:  3072 # 2048              # max sequence length for model and packing of the dataset\n",
    "# training parameters\n",
    "output_dir: \"./llama-3-70b-hf-no-robot\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 3                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "eval_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "#gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  activation_checkpointing: True\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\"\n",
    "  cpu_ram_efficient_loading: \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a3f97-3ec5-4ebe-9459-66226cf3bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id: \"meta-llama/Meta-Llama-3-8b\" # Hugging Face model id\n",
    "dataset_path: \".\"                      # path to dataset\n",
    "max_seq_length:  3072 # 2048              # max sequence length for model and packing of the dataset\n",
    "# training parameters\n",
    "output_dir: \"./llama-3-8b-hf-no-robot\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 3                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "eval_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "#gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  activation_checkpointing: True\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\"\n",
    "  cpu_ram_efficient_loading: \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b496678d-a82a-47f2-b323-83b468c8ce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-09 14:27:50--  https://github.com/philschmid/deep-learning-pytorch-huggingface/raw/main/training/scripts/run_fsdp_qlora.py\n",
      "github.com (github.com) 해석 중... 20.200.245.247\n",
      "다음으로 연결 중: github.com (github.com)|20.200.245.247|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 302 Found\n",
      "위치: https://raw.githubusercontent.com/philschmid/deep-learning-pytorch-huggingface/main/training/scripts/run_fsdp_qlora.py [따라감]\n",
      "--2024-08-09 14:27:50--  https://raw.githubusercontent.com/philschmid/deep-learning-pytorch-huggingface/main/training/scripts/run_fsdp_qlora.py\n",
      "raw.githubusercontent.com (raw.githubusercontent.com) 해석 중... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "다음으로 연결 중: raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 6302 (6.2K) [text/plain]\n",
      "저장 위치: ‘run_fsdp_qlora.py’\n",
      "\n",
      "run_fsdp_qlora.py   100%[===================>]   6.15K  --.-KB/s    / 0s       \n",
      "\n",
      "2024-08-09 14:27:51 (39.1 MB/s) - ‘run_fsdp_qlora.py’ 저장함 [6302/6302]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/philschmid/deep-learning-pytorch-huggingface/raw/main/training/scripts/run_fsdp_qlora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69e6e9da-8936-4c8b-aafd-29698692723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0809 14:28:01.735000 140132300277568 torch/distributed/run.py:779] \n",
      "W0809 14:28:01.735000 140132300277568 torch/distributed/run.py:779] *****************************************\n",
      "W0809 14:28:01.735000 140132300277568 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0809 14:28:01.735000 140132300277568 torch/distributed/run.py:779] *****************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kotech/venv-torch/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/run.py\", line 901, in main\n",
      "    run(args)\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/run.py\", line 892, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 133, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 255, in launch_agent\n",
      "    result = agent.run()\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/elastic/metrics/api.py\", line 124, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py\", line 680, in run\n",
      "    result = self._invoke_run(role)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py\", line 829, in _invoke_run\n",
      "    self._initialize_workers(self._worker_group)\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/elastic/metrics/api.py\", line 124, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py\", line 652, in _initialize_workers\n",
      "    self._rendezvous(worker_group)\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/elastic/metrics/api.py\", line 124, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/elastic/agent/server/api.py\", line 489, in _rendezvous\n",
      "    rdzv_info = spec.rdzv_handler.next_rendezvous()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kotech/venv-torch/lib/python3.11/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 66, in next_rendezvous\n",
      "    self._store = TCPStore(  # type: ignore[call-arg]\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: The server socket has failed to listen on any local network address. useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1,2\n",
    "#!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 ./scripts/run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml\n",
    "!OMP_NUM_THREADS=1 ACCELERATE_USE_FSDP=1 torchrun --nproc_per_node=2 --master_port=25678 ./run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97273b8c-bff2-4c91-aaaf-6aad01653a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
