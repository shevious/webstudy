{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a9ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c18ce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딥러닝과 컴퓨터 비전(2022년도, 2학기, AI0106, 01, U)-4주차 과제-46551.zip: Zip\n",
      "  안수빈_45998_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  안수빈_45998_assignsubmission_file_/202130989.ipynb  (74852 B)... OK.\n",
      "  백찬영_46013_assignsubmission_onlinetext_/온라인문서.html  (190 B)... OK.\n",
      "  백찬영_46013_assignsubmission_file_/202132251.ipynb.ipynb  (71211 B)... OK.\n",
      "  백근화_46025_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  백근화_46025_assignsubmission_file_/202132161.ipynb  (73152 B)... OK.\n",
      "  권범수_46040_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  권범수_46040_assignsubmission_file_/202230453.ipynb  (75113 B)... OK.\n",
      "  이창묵_46042_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  이창묵_46042_assignsubmission_file_/202130645_이창묵.ipynb  (73506 B)... OK.\n",
      "  조민지_46076_assignsubmission_onlinetext_/온라인문서.html  (150 B)... OK.\n",
      "  조민지_46076_assignsubmission_file_/202232212.ipynb  (72238 B)... OK.\n",
      "  최치헌_46097_assignsubmission_onlinetext_/온라인문서.html  (95 B)... OK.\n",
      "  최치헌_46097_assignsubmission_file_/202120137.ipynb  (70963 B)... OK.\n",
      "  최원석_46047_assignsubmission_onlinetext_/온라인문서.html  (215 B)... OK.\n",
      "  최원석_46047_assignsubmission_file_/202132463.ipynb  (76480 B)... OK.\n",
      "  최요한_46089_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  최요한_46089_assignsubmission_file_/202010687.ipynb  (670232 B)... OK.\n",
      "  천동암_46080_assignsubmission_onlinetext_/온라인문서.html  (106 B)... OK.\n",
      "  천동암_46080_assignsubmission_file_/천동암.ipynb  (6762770 B)... OK.\n",
      "  정다훈_46072_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  정다훈_46072_assignsubmission_file_/04_report_22_(정다훈).ipynb  (350860 B)... OK.\n",
      "  CUIGUANGZHI_46005_assignsubmission_onlinetext_/온라인문서.html  (195 B)... OK.\n",
      "  CUIGUANGZHI_46005_assignsubmission_file_/202132190.ipynb  (752129 B)... OK.\n",
      "  안다솜_46061_assignsubmission_onlinetext_/온라인문서.html  (233 B)... OK.\n",
      "  안다솜_46061_assignsubmission_file_/202132374.ipynb  (77142 B)... OK.\n",
      "  김동현_46027_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김동현_46027_assignsubmission_file_/202011067.ipynb  (74370 B)... OK.\n",
      "  윤두환_46057_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  윤두환_46057_assignsubmission_file_/202132624.ipynb  (73251 B)... OK.\n",
      "  강은혜_46073_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  강은혜_46073_assignsubmission_file_/202132383.ipynb  (75248 B)... OK.\n",
      "  강길모_46096_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  강길모_46096_assignsubmission_file_/202230963.ipynb  (73352 B)... OK.\n",
      "  박해찬_46070_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  박해찬_46070_assignsubmission_file_/202010102.ipynb  (72998 B)... OK.\n",
      "  함현석_46003_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  함현석_46003_assignsubmission_file_/202132575.ipynb.ipynb  (72005 B)... OK.\n",
      "  한성우_46032_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  한성우_46032_assignsubmission_file_/202230692.ipynb  (73936 B)... OK.\n",
      "  홍준의_46087_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  홍준의_46087_assignsubmission_file_/202011079.ipynb  (72861 B)... OK.\n",
      "  허인회_46067_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  허인회_46067_assignsubmission_file_/202031861.ipynb  (69172 B)... OK.\n",
      "  황주훈_46062_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  황주훈_46062_assignsubmission_file_/202230187.ipynb  (72430 B)... OK.\n",
      "  황창현_46085_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  황창현_46085_assignsubmission_file_/202110237.ipynb  (72098 B)... OK.\n",
      "  조형래_46016_assignsubmission_onlinetext_/온라인문서.html  (103 B)... OK.\n",
      "  조형래_46016_assignsubmission_file_/202111064.ipynb  (71227 B)... OK.\n",
      "  박현준_46018_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  박현준_46018_assignsubmission_file_/202230500.ipynb  (72301 B)... OK.\n",
      "  정일영_45999_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  정일영_45999_assignsubmission_file_/202230711.ipynb  (72378 B)... OK.\n",
      "  이재인_46024_assignsubmission_onlinetext_/온라인문서.html  (98 B)... OK.\n",
      "  이재인_46024_assignsubmission_file_/202231861.ipynb  (73339 B)... OK.\n",
      "  장철_45997_assignsubmission_onlinetext_/온라인문서.html  (276 B)... OK.\n",
      "  장철_45997_assignsubmission_file_/202230236.ipynb  (1054327 B)... OK.\n",
      "  장경찬_46049_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  장경찬_46049_assignsubmission_file_/202110271.ipynb  (1071368 B)... OK.\n",
      "  전수인_46015_assignsubmission_onlinetext_/온라인문서.html  (142 B)... OK.\n",
      "  전수인_46015_assignsubmission_file_/202131490.ipynb  (76818 B)... OK.\n",
      "  장전인_46037_assignsubmission_onlinetext_/온라인문서.html  (207 B)... OK.\n",
      "  장전인_46037_assignsubmission_file_/202231878.ipynb  (56614 B)... OK.\n",
      "  정영욱_46059_assignsubmission_onlinetext_/온라인문서.html  (131 B)... OK.\n",
      "  정영욱_46059_assignsubmission_file_/202230620.ipynb  (77565 B)... OK.\n",
      "  강연실_46034_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  강연실_46034_assignsubmission_file_/202011473.ipynb  (77353 B)... OK.\n",
      "  김창훈_46030_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김창훈_46030_assignsubmission_file_/202131244.ipynb  (73031 B)... OK.\n",
      "  김홍섭_46007_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김홍섭_46007_assignsubmission_file_/202230896.ipynb  (71625 B)... OK.\n",
      "  김현빈_45995_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김현빈_45995_assignsubmission_file_/202132418_김현빈.ipynb  (61048 B)... OK.\n",
      "  김지은_46012_assignsubmission_onlinetext_/온라인문서.html  (211 B)... OK.\n",
      "  김지은_46012_assignsubmission_file_/202132681.ipynb  (73820 B)... OK.\n",
      "  김진미_46023_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김진미_46023_assignsubmission_file_/202231382.ipynb  (998481 B)... OK.\n",
      "  김미현_46026_assignsubmission_onlinetext_/온라인문서.html  (110 B)... OK.\n",
      "  김미현_46026_assignsubmission_file_/202032080_김미현.ipynb  (74303 B)... OK.\n",
      "  김선호_46079_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김선호_46079_assignsubmission_file_/04_report_22_ipynb의_사본.ipynb  (845902 B)... OK.\n",
      "  김선형_46009_assignsubmission_onlinetext_/온라인문서.html  (211 B)... OK.\n",
      "  김선형_46009_assignsubmission_file_/04_report_김선형(202010386).ipynb  (635831 B)... OK.\n",
      "  김원경_46078_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김원경_46078_assignsubmission_file_/202011413.ipynb  (71977 B)... OK.\n",
      "  김기찬_46058_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김기찬_46058_assignsubmission_file_/202131539.ipynb  (72154 B)... OK.\n",
      "  김태권_46033_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김태권_46033_assignsubmission_file_/202010022.ipynb  (73660 B)... OK.\n",
      "  고동일_46028_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  고동일_46028_assignsubmission_file_/201910860.ipynb  (73851 B)... OK.\n",
      "  구태홍_45996_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  구태홍_45996_assignsubmission_file_/202132532.ipynb  (1043266 B)... OK.\n",
      "  권대천_46052_assignsubmission_onlinetext_/온라인문서.html  (107 B)... OK.\n",
      "  권대천_46052_assignsubmission_file_/202230347.ipynb  (76090 B)... OK.\n",
      "  이동훈_46044_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  이동훈_46044_assignsubmission_file_/201911267.ipynb  (61739 B)... OK.\n",
      "  이동주_46069_assignsubmission_onlinetext_/온라인문서.html  (116 B)... OK.\n",
      "  이동주_46069_assignsubmission_file_/202230485.ipynb  (59327 B)... OK.\n",
      "  이정휘_46029_assignsubmission_onlinetext_/온라인문서.html  (113 B)... OK.\n",
      "  이정휘_46029_assignsubmission_file_/202131486.ipynb  (560576 B)... OK.\n",
      "  이민기_46048_assignsubmission_onlinetext_/온라인문서.html  (276 B)... OK.\n",
      "  이민기_46048_assignsubmission_file_/202132184.ipynb  (74588 B)... OK.\n",
      "  이민기_46093_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  이민기_46093_assignsubmission_file_/202132519.ipynb  (57662 B)... OK.\n",
      "  이루오_46050_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  이루오_46050_assignsubmission_file_/202031279.ipynb  (73454 B)... OK.\n",
      "  이순광_46084_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  이순광_46084_assignsubmission_file_/202130290.ipynb  (1389856 B)... OK.\n",
      "  이수정_46098_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  이수정_46098_assignsubmission_file_/202231381.ipynb  (73308 B)... OK.\n",
      "  이우근_46090_assignsubmission_onlinetext_/온라인문서.html  (266 B)... OK.\n",
      "  이우근_46090_assignsubmission_file_/202230470.ipynb  (74528 B)... OK.\n",
      "  이범윤_46045_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  이범윤_46045_assignsubmission_file_/202130686.ipynb  (644072 B)... OK.\n",
      "  이지수_46068_assignsubmission_onlinetext_/온라인문서.html  (206 B)... OK.\n",
      "  이지수_46068_assignsubmission_file_/202230345.ipynb  (75034 B)... OK.\n",
      "  이강희_46077_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  이강희_46077_assignsubmission_file_/202020034.ipynb  (1328916 B)... OK.\n",
      "  문예지_46065_assignsubmission_onlinetext_/온라인문서.html  (213 B)... OK.\n",
      "  문예지_46065_assignsubmission_file_/202130816.ipynb  (72455 B)... OK.\n",
      "  남정표_46051_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  남정표_46051_assignsubmission_file_/202031930.ipynb  (71667 B)... OK.\n",
      "  김남영_46063_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  김남영_46063_assignsubmission_file_/202132229.ipynb  (74403 B)... OK.\n",
      "  박찬우_46075_assignsubmission_onlinetext_/온라인문서.html  (99 B)... OK.\n",
      "  박찬우_46075_assignsubmission_file_/202032322.ipynb  (70770 B)... OK.\n",
      "  박준하_46046_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  박준하_46046_assignsubmission_file_/202011231.ipynb  (73413 B)... OK.\n",
      "  박사일_46055_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  박사일_46055_assignsubmission_file_/202032505.ipynb  (71708 B)... OK.\n",
      "  박시현_46031_assignsubmission_onlinetext_/온라인문서.html  (161 B)... OK.\n",
      "  박시현_46031_assignsubmission_file_/202020097.ipynb  (73663 B)... OK.\n",
      "  서정훈_46066_assignsubmission_onlinetext_/온라인문서.html  (110 B)... OK.\n",
      "  서정훈_46066_assignsubmission_file_/202130538.ipynb  (72340 B)... OK.\n",
      "  서정민_46091_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  서정민_46091_assignsubmission_file_/201911343.ipynb  (71404 B)... OK.\n",
      "  석병득_46022_assignsubmission_onlinetext_/온라인문서.html  (246 B)... OK.\n",
      "  석병득_46022_assignsubmission_file_/202131318.ipynb  (441453 B)... OK.\n",
      "  조성백_46071_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  조성백_46071_assignsubmission_file_/202110934.ipynb  (72878 B)... OK.\n",
      "  공선호_46039_assignsubmission_onlinetext_/온라인문서.html  (143 B)... OK.\n",
      "  공선호_46039_assignsubmission_file_/202132466.ipynb  (73989 B)... OK.\n",
      "  소재훈_46082_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  소재훈_46082_assignsubmission_file_/202232225.ipynb  (72204 B)... OK.\n",
      "  송영석_46064_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  송영석_46064_assignsubmission_file_/202010657.ipynb  (70681 B)... OK.\n",
      "  황수연_46036_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  황수연_46036_assignsubmission_file_/202120138.ipynb  (71447 B)... OK.\n",
      "  윤울암_46060_assignsubmission_onlinetext_/온라인문서.html  (289 B)... OK.\n",
      "  윤울암_46060_assignsubmission_file_/202131020.ipynb  (725476 B)... OK.\n",
      "  우형욱_46053_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  우형욱_46053_assignsubmission_file_/202131300.ipynb  (73380 B)... OK.\n",
      "  양해영_46086_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  양해영_46086_assignsubmission_file_/202130056.ipynb  (446669 B)... OK.\n",
      "  유승진_46021_assignsubmission_onlinetext_/온라인문서.html  (92 B)... OK.\n",
      "  유승진_46021_assignsubmission_file_/04_report_22_딥러닝과_컴퓨터비전_유승진.ipynb  (1128098 B)... OK.\n",
      "  윤찬혁_46088_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  윤찬혁_46088_assignsubmission_file_/202010219.ipynb  (251267 B)... OK.\n",
      "  윤경일_46035_assignsubmission_onlinetext_/온라인문서.html  (76 B)... OK.\n",
      "  윤경일_46035_assignsubmission_file_/202232010.ipynb  (73486 B)... OK.\n",
      "  윤성훈_46010_assignsubmission_onlinetext_/온라인문서.html  (481 B)... OK.\n",
      "  윤현기_46095_assignsubmission_onlinetext_/온라인문서.html  (114 B)... OK.\n",
      "  윤현기_46095_assignsubmission_file_/202230795.ipynb  (70779 B)... OK.\n",
      "  유하영_46019_assignsubmission_onlinetext_/온라인문서.html  (446 B)... OK.\n",
      "  유하영_46019_assignsubmission_file_/202132658_유하영.ipynb  (63863 B)... OK.\n",
      "  이형우_46056_assignsubmission_onlinetext_/온라인문서.html  (101 B)... OK.\n",
      "  이형우_46056_assignsubmission_file_/202132228.ipynb  (75326 B)... OK.\n",
      "Successfully extracted to \"report1\".\n"
     ]
    }
   ],
   "source": [
    "!unar -f -e EUC-KR '딥러닝과 컴퓨터 비전(2022년도, 2학기, AI0106, 01, U)-4주차 과제-46551.zip' -D -o report1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b3166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654d392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_folders = glob('report1/*_file_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82464122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: 'repor1_fix/*.ipynb'를 설명할 수 없음: 그런 파일이나 디렉터리가 없습니다\r\n"
     ]
    }
   ],
   "source": [
    "py_list = []\n",
    "for folder in report_folders:\n",
    "    ipynb_name = folder.replace('_assignsubmission_file_', '.ipynb')\n",
    "    py_list.append(ipynb_name)\n",
    "    #!echo cp $folder/*.ipynb $ipynb_name | wc\n",
    "    !cp $folder/*.ipynb $ipynb_name \n",
    "!cp report1_fix/*.ipynb report1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc459240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7591606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"report1/안수빈_45998.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/안수빈_45998.ipynb to python\n",
      "[NbConvertApp] Writing 36580 bytes to report1/안수빈_45998.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 292561.22it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:30:39.788392: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:30:40.169492: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:30:40.169529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:30:40.416452: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:30:41.083258: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 8ms/step - loss: 240.9341 - pos_accuracy: 0.0000e+00 - val_loss: 212.3778 - val_pos_accuracy: 0.0045\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 222.2086 - pos_accuracy: 6.2500e-04 - val_loss: 195.9663 - val_pos_accuracy: 0.0000e+00\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 107221.84it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,329,666\n",
      "Trainable params: 1,329,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 140.2492 - pos_accuracy: 0.0037 - val_loss: 9.7524 - val_pos_accuracy: 0.0361\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.1237 - pos_accuracy: 0.0487 - val_loss: 2.2532 - val_pos_accuracy: 0.1250\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.2769 - pos_accuracy: 0.1312 - val_loss: 2.4046 - val_pos_accuracy: 0.1923\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6319 - pos_accuracy: 0.3744 - val_loss: 0.8459 - val_pos_accuracy: 0.2236\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5342 - pos_accuracy: 0.4425 - val_loss: 0.3454 - val_pos_accuracy: 0.5192\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3811 - pos_accuracy: 0.5412 - val_loss: 0.3441 - val_pos_accuracy: 0.5625\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3160 - pos_accuracy: 0.5850 - val_loss: 0.2011 - val_pos_accuracy: 0.7572\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1221 - pos_accuracy: 0.8031 - val_loss: 0.1589 - val_pos_accuracy: 0.8053\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1354 - pos_accuracy: 0.7594 - val_loss: 0.1633 - val_pos_accuracy: 0.8029\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2194 - pos_accuracy: 0.6931 - val_loss: 0.1411 - val_pos_accuracy: 0.8486\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1928 - pos_accuracy: 0.7369 - val_loss: 0.1942 - val_pos_accuracy: 0.7716\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0754 - pos_accuracy: 0.8838 - val_loss: 0.1027 - val_pos_accuracy: 0.8894\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0626 - pos_accuracy: 0.9144 - val_loss: 0.1163 - val_pos_accuracy: 0.8990\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0474 - pos_accuracy: 0.9456 - val_loss: 0.1405 - val_pos_accuracy: 0.8510\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0586 - pos_accuracy: 0.9137 - val_loss: 0.1119 - val_pos_accuracy: 0.8990\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0344 - pos_accuracy: 0.9638 - val_loss: 0.1626 - val_pos_accuracy: 0.7404\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0307 - pos_accuracy: 0.9725 - val_loss: 0.1005 - val_pos_accuracy: 0.9062\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0268 - pos_accuracy: 0.9812 - val_loss: 0.0836 - val_pos_accuracy: 0.9207\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0508 - pos_accuracy: 0.9287 - val_loss: 0.1138 - val_pos_accuracy: 0.8630\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0353 - pos_accuracy: 0.9681 - val_loss: 0.0795 - val_pos_accuracy: 0.9207\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0213 - pos_accuracy: 0.9931 - val_loss: 0.0713 - val_pos_accuracy: 0.9327\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0187 - pos_accuracy: 0.9881 - val_loss: 0.0843 - val_pos_accuracy: 0.9279\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0208 - pos_accuracy: 0.9894 - val_loss: 0.0675 - val_pos_accuracy: 0.9447\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0293 - pos_accuracy: 0.9812 - val_loss: 0.0690 - val_pos_accuracy: 0.9375\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0152 - pos_accuracy: 0.9925 - val_loss: 0.0637 - val_pos_accuracy: 0.9351\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9962 - val_loss: 0.0735 - val_pos_accuracy: 0.9375\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0208 - pos_accuracy: 0.9856 - val_loss: 0.0787 - val_pos_accuracy: 0.9231\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9969 - val_loss: 0.0635 - val_pos_accuracy: 0.9399\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0137 - pos_accuracy: 0.9962 - val_loss: 0.0606 - val_pos_accuracy: 0.9495\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0072 - pos_accuracy: 0.9981 - val_loss: 0.0605 - val_pos_accuracy: 0.9615\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0503 - pos_accuracy: 0.9212 - val_loss: 0.0904 - val_pos_accuracy: 0.8894\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0114 - pos_accuracy: 0.9962 - val_loss: 0.0600 - val_pos_accuracy: 0.9399\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0136 - pos_accuracy: 0.9944 - val_loss: 0.0611 - val_pos_accuracy: 0.9327\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0125 - pos_accuracy: 0.9925 - val_loss: 0.0586 - val_pos_accuracy: 0.9639\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0099 - pos_accuracy: 0.9987 - val_loss: 0.0558 - val_pos_accuracy: 0.9423\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9987 - val_loss: 0.0764 - val_pos_accuracy: 0.9279\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9937 - val_loss: 0.0564 - val_pos_accuracy: 0.9471\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9987 - val_loss: 0.0573 - val_pos_accuracy: 0.9543\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0080 - pos_accuracy: 0.9994 - val_loss: 0.0523 - val_pos_accuracy: 0.9519\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9987 - val_loss: 0.0533 - val_pos_accuracy: 0.9567\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9994 - val_loss: 0.0520 - val_pos_accuracy: 0.9543\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9994 - val_loss: 0.0531 - val_pos_accuracy: 0.9351\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9994 - val_loss: 0.0514 - val_pos_accuracy: 0.9447\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9994 - val_loss: 0.0581 - val_pos_accuracy: 0.9471\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9994 - val_loss: 0.0521 - val_pos_accuracy: 0.9423\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9994 - val_loss: 0.0512 - val_pos_accuracy: 0.9567\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9994 - val_loss: 0.0508 - val_pos_accuracy: 0.9471\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9994 - val_loss: 0.0516 - val_pos_accuracy: 0.9543\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.0505 - val_pos_accuracy: 0.9567\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.0503 - val_pos_accuracy: 0.9567\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.0499 - val_pos_accuracy: 0.9567\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9994 - val_loss: 0.0506 - val_pos_accuracy: 0.9543\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9994 - val_loss: 0.0537 - val_pos_accuracy: 0.9519\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9994 - val_loss: 0.0508 - val_pos_accuracy: 0.9567\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.0537 - val_pos_accuracy: 0.9543\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.0503 - val_pos_accuracy: 0.9567\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.0492 - val_pos_accuracy: 0.9543\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.0491 - val_pos_accuracy: 0.9543\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9994 - val_loss: 0.0525 - val_pos_accuracy: 0.9567\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9994 - val_loss: 0.0490 - val_pos_accuracy: 0.9543\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.0487 - val_pos_accuracy: 0.9543\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.0487 - val_pos_accuracy: 0.9543\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.0492 - val_pos_accuracy: 0.9543\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.0485 - val_pos_accuracy: 0.9543\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.2725e-04 - pos_accuracy: 0.9994 - val_loss: 0.0487 - val_pos_accuracy: 0.9543\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.0488 - val_pos_accuracy: 0.9519\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9994 - val_loss: 0.0501 - val_pos_accuracy: 0.9567\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.0503 - val_pos_accuracy: 0.9519\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.0487 - val_pos_accuracy: 0.9519\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.6712e-04 - pos_accuracy: 0.9994 - val_loss: 0.0482 - val_pos_accuracy: 0.9519\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.8175e-04 - pos_accuracy: 0.9994 - val_loss: 0.0487 - val_pos_accuracy: 0.9543\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.6564e-04 - pos_accuracy: 0.9994 - val_loss: 0.0481 - val_pos_accuracy: 0.9543\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.7953e-04 - pos_accuracy: 0.9994 - val_loss: 0.0479 - val_pos_accuracy: 0.9543\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.0608e-04 - pos_accuracy: 0.9994 - val_loss: 0.0478 - val_pos_accuracy: 0.9519\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.6864e-04 - pos_accuracy: 0.9994 - val_loss: 0.0474 - val_pos_accuracy: 0.9543\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 6.9833e-04 - pos_accuracy: 0.9994 - val_loss: 0.0477 - val_pos_accuracy: 0.9543\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.2985e-04 - pos_accuracy: 0.9994 - val_loss: 0.0478 - val_pos_accuracy: 0.9543\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.0478 - val_pos_accuracy: 0.9543\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.5350e-04 - pos_accuracy: 0.9994 - val_loss: 0.0473 - val_pos_accuracy: 0.9543\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.0516e-04 - pos_accuracy: 0.9994 - val_loss: 0.0473 - val_pos_accuracy: 0.9543\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.4429e-04 - pos_accuracy: 0.9994 - val_loss: 0.0476 - val_pos_accuracy: 0.9543\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.3593e-04 - pos_accuracy: 0.9994 - val_loss: 0.0476 - val_pos_accuracy: 0.9543\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.0007e-04 - pos_accuracy: 0.9994 - val_loss: 0.0477 - val_pos_accuracy: 0.9543\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.2447e-04 - pos_accuracy: 0.9994 - val_loss: 0.0474 - val_pos_accuracy: 0.9543\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.2552e-04 - pos_accuracy: 0.9994 - val_loss: 0.0473 - val_pos_accuracy: 0.9543\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.8670e-04 - pos_accuracy: 0.9994 - val_loss: 0.0480 - val_pos_accuracy: 0.9543\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.7603e-04 - pos_accuracy: 0.9994 - val_loss: 0.0474 - val_pos_accuracy: 0.9567\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.6156e-04 - pos_accuracy: 0.9994 - val_loss: 0.0477 - val_pos_accuracy: 0.9543\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2061e-04 - pos_accuracy: 0.9994 - val_loss: 0.0475 - val_pos_accuracy: 0.9519\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.6568e-04 - pos_accuracy: 0.9994 - val_loss: 0.0473 - val_pos_accuracy: 0.9543\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.0495e-04 - pos_accuracy: 0.9994 - val_loss: 0.0473 - val_pos_accuracy: 0.9519\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.3980e-04 - pos_accuracy: 0.9994 - val_loss: 0.0472 - val_pos_accuracy: 0.9567\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9994 - val_loss: 0.0476 - val_pos_accuracy: 0.9495\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.2167e-04 - pos_accuracy: 0.9994 - val_loss: 0.0475 - val_pos_accuracy: 0.9543\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.4766e-04 - pos_accuracy: 0.9994 - val_loss: 0.0470 - val_pos_accuracy: 0.9543\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.0317e-04 - pos_accuracy: 0.9994 - val_loss: 0.0474 - val_pos_accuracy: 0.9543\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4998e-04 - pos_accuracy: 0.9994 - val_loss: 0.0470 - val_pos_accuracy: 0.9519\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.0471 - val_pos_accuracy: 0.9543\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.7988e-04 - pos_accuracy: 0.9994 - val_loss: 0.0471 - val_pos_accuracy: 0.9543\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.8174e-04 - pos_accuracy: 0.9994 - val_loss: 0.0469 - val_pos_accuracy: 0.9543\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:31:03.571210: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "0.6013071895424837\n",
      "0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1716: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate은 배열을 합치는 함수다.\\n배열이 2차원 이상일 경우 axis에 따라 세로축, 가로축으로 합칠 축을 정할 수 있는데,\\naxis가 1인 경우에는 가로축으로 배열을 합친다.\\n\",\n",
      "        \"\\n121, 122는 각각 subplot의 행, 열, 인덱스를 뜻하는 정수를 입력한 숫자이다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntensorflow의 reduce 함수는 차원을 축소하기 위해 사용되는데, reduce에 _all이 붙으면 AND연산을 통해 차원을 축소한다는 의미이다.\\n이때 axis를 1로 설정하면 행방향으로 축소시킨다.\\n\",\n",
      "        \"\\nreduce_mean 함수가 평균을 통해 차원을 축소시켜 결과값(정확도)를 도출한다. \\n\",\n",
      "        \"\\n결과값(정확도)가 custom metric을 통해 reduce함수를 사용해서 축소된 결과값이 나오기 때문에 활성화 함수를 사용하지 않는다. \\n\",\n",
      "        \"\\n랜덤으로 생성시킨 난수가 계속해서 변경되면 실행을 시킬때마다 다른 결과가 나올 수 있기 때문에\\nrandom seed를 사용하여 난수를 고정시켜 항상 같은 결과값을 얻을 수 있도록 한다. \\n\",\n",
      "        true,\n",
      "        \"\\n어려웠지만 직접 레이어나 파라미터를 추가하고 변경하면서 진행하니\\n이해가 더 잘 되는 것 같습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.9663\n",
      "}\"report1/최치헌_46097.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/최치헌_46097.ipynb to python\n",
      "[NbConvertApp] Writing 34458 bytes to report1/최치헌_46097.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:209: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:240: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 294823.32it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:31:10.917158: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:31:11.301647: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:31:11.301687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:31:11.526290: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:31:12.152582: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 113394.81it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:31:14.394746: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1639: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할은 행렬을 좌우나 위아래로 같다 붙여주는 기능을 갖고 있습니다.\\naxis=1의 의미는 가로로 붙이기의 의미이며, 기본적으로는 세로로 붙고, 이는 axis=0 을 넣어줍니다.\\n\",\n",
      "        \"\\nsubplot에서 121은 1행과 2열로 이루어진 칸 1번째 칸에 그림을 넣는 의미이며, \\n122는 2번째 칸에 그림을 넣으라는 의미입니다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_mean은 코드에서 특정차원을 제거하고 평균을 구한다. \\n\",\n",
      "        false,\n",
      "        \"\\nrandom seed의 사용 이유는 계속 실행하더라도 동일한 결과 갖기 위해서 이다.\\n\",\n",
      "        false,\n",
      "        \"\\n너무 친절하게 잘 알려주셔서 감사합니다. 제가 군인인지라 한달짜리 훈련을 다녀오니\\n과제를 모두 수행하지 못하였습니다. 정성껏 만들어주셨는데 그만한 결과물을 내지 못해서\\n죄송할 따름입니다. 아직 따라가기 힘들지만 포기하지 않고 끝까지 해보겠습니다.\\n감사합니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 59.09090909090909,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/김진미_46023.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김진미_46023.ipynb to python\n",
      "[NbConvertApp] Writing 35345 bytes to report1/김진미_46023.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:243: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -15. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 297700.62it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:31:20.824314: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:31:21.204097: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:31:21.204137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:31:21.430160: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/20\n",
      "2022-10-14 10:31:22.047263: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 42.3551 - val_loss: 5.8140\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.6661 - val_loss: 3.2219\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.3149 - val_loss: 2.4054\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7912 - val_loss: 1.9737\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4885 - val_loss: 1.6795\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2890 - val_loss: 1.4952\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1464 - val_loss: 1.3469\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0364 - val_loss: 1.2472\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9537 - val_loss: 1.1494\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8855 - val_loss: 1.0697\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8289 - val_loss: 1.0123\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7822 - val_loss: 0.9534\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7407 - val_loss: 0.9165\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7054 - val_loss: 0.8783\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6725 - val_loss: 0.8516\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6460 - val_loss: 0.8204\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6200 - val_loss: 0.7880\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5993 - val_loss: 0.7710\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5804 - val_loss: 0.7463\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5633 - val_loss: 0.7287\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 100164.88it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 42.3551 - val_loss: 5.8140\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 3.6661 - val_loss: 3.2219\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.3149 - val_loss: 2.4054\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7912 - val_loss: 1.9737\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4885 - val_loss: 1.6795\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2890 - val_loss: 1.4952\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1464 - val_loss: 1.3469\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0364 - val_loss: 1.2472\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9537 - val_loss: 1.1494\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8855 - val_loss: 1.0697\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8289 - val_loss: 1.0123\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7822 - val_loss: 0.9534\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7407 - val_loss: 0.9165\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7054 - val_loss: 0.8783\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6725 - val_loss: 0.8516\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6460 - val_loss: 0.8204\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6200 - val_loss: 0.7880\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5993 - val_loss: 0.7710\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5804 - val_loss: 0.7463\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5633 - val_loss: 0.7287\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:31:27.533602: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1686: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과 axis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concatenate는 두 배열을 나란히 붙인다는 의미.\\naxis의 값이 0이면 두 배열이 가로로 나란히 같은 행에 붙여지고,\\naxis의 값이 1이면 두 배열이 세로로 나란히 다른 행에 붙여진다는 의미.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\nsubplot 그리드 인자를 정수 하나에 다 모아서 표현한 것.\\n121은 1x2그리드에 첫 번째를, 122는 1x2그리드에 두 번째를 나타냄.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntf.reduce_all은 AND연산\\naxis는 행렬에서 연산 할 방향을 나타냄. 1이면 같은 행의 데이터끼리 참/거짓 비교하기\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n평균 구하기\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\ny=wx+b의 형태가 되도록 \\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n예측 가능한 난수를 만들기 위해\\n\",\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 76.36363636363636,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/서정훈_46066.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/서정훈_46066.ipynb to python\n",
      "[NbConvertApp] Writing 35270 bytes to report1/서정훈_46066.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:243: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 296312.54it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:31:33.928662: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:31:34.303642: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:31:34.303686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:31:34.526426: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:31:35.169189: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 113377.95it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 665,602\n",
      "Trainable params: 665,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 35.7872 - pos_accuracy: 0.0375 - val_loss: 3.9052 - val_pos_accuracy: 0.2225\n",
      "Epoch 2/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 1.5563 - pos_accuracy: 0.2606 - val_loss: 0.7591 - val_pos_accuracy: 0.3750\n",
      "Epoch 3/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.5452 - pos_accuracy: 0.4888 - val_loss: 0.4357 - val_pos_accuracy: 0.5000\n",
      "Epoch 4/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.2699 - pos_accuracy: 0.6650 - val_loss: 0.1654 - val_pos_accuracy: 0.7575\n",
      "Epoch 5/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.1341 - pos_accuracy: 0.8100 - val_loss: 0.1411 - val_pos_accuracy: 0.7900\n",
      "Epoch 6/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0811 - pos_accuracy: 0.8744 - val_loss: 0.1360 - val_pos_accuracy: 0.8050\n",
      "Epoch 7/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0463 - pos_accuracy: 0.9400 - val_loss: 0.0688 - val_pos_accuracy: 0.9200\n",
      "Epoch 8/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0326 - pos_accuracy: 0.9575 - val_loss: 0.0776 - val_pos_accuracy: 0.8850\n",
      "Epoch 9/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0183 - pos_accuracy: 0.9844 - val_loss: 0.0562 - val_pos_accuracy: 0.9325\n",
      "Epoch 10/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0135 - pos_accuracy: 0.9894 - val_loss: 0.0686 - val_pos_accuracy: 0.9025\n",
      "Epoch 11/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0122 - pos_accuracy: 0.9962 - val_loss: 0.0524 - val_pos_accuracy: 0.9325\n",
      "Epoch 12/12\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0092 - pos_accuracy: 0.9962 - val_loss: 0.0475 - val_pos_accuracy: 0.9225\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:32:08.265779: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1671: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\nnrows, ncols, index를 의미하는것으로 121의 경우 1행 2열의 첫번째, 122의 경우 1행 2열의 두번째 위치를 의미합니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntf.reduce_all은 tensor가 지정한 축 방향의 각 요소의 논리 and 연산을 하는 함수로 \\n정답과 예측값이 동일한 값을 추출하는 역할을 하며 axis=1은 수평축으로 연산을 하라는 의미입니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다.\\n정답의 평균을 계산하는 역할을 합니다. \\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\ny=wx+b의 형태로 linear한 결과를 출력하기 위한 목적입니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유\\n동일한 결과가 나올수 있도록  재현 가능한(고정된) 난수를 생성하기 위한 목적입니다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.965\n",
      "}\"report1/이동주_46069.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이동주_46069.ipynb to python\n",
      "[NbConvertApp] Writing 27478 bytes to report1/이동주_46069.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:241: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "1000 10000\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 250107.57it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:32:16.431201: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:32:16.813551: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:32:16.813592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:32:16.950301: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:32:17.550140: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 10ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:32:19.318574: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\n열기준으로 배열 합치기.\\n\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\n121은 최대1행2열 중 1번째 의미\\n122는 최대1행2열 중 2번째 의미\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\n열에 해당하는 모든부분을 축소시킴을 의미함.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n\\nfloat32형식의 is_correct값의 평군을 구하고, score로 정의하는것을 의미함.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유\\n\\n몇개의 input가 들어올지 알 수 없기 때문에,\\n활성화 함수를 none로 처리.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n\\n무작위 하게 난수를 계속 발생시키고 학습을 하게 된다면, 매번 다른값의 난수를 가지게 되고,\\n결과또한 매번 다르게 나오게됨.\\nrandom seed는 랜덤하게 나오는 난수를 고정으로 잡아주여, 매번 같은 값의 난수를 가지게 되어\\n결과 또한 매번 같게 나오게 설정 할 수 있다.\\n\\n\",\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\\n안녕하세요 교수님.\\n최근에 인공지능 구축 사업을 진행하는 바이오 회사에 입사를 하게되었습니다.\\n일이 너무 많아서 수업에 대해서 신경을 많이 쓰지 못하였습니다.\\n다음주까지 과제에 대해서 스스로 풀어보고, 모르는것과 문제에 대해서 질문을 드려도 될까요?\\n문제를 성실히 풀지 못하여 죄송합니다.\\n다음번 부턴 열심히 참여하겠습니다.\\n감사합니다.\\n\\n\"\n",
      "    ],\n",
      "    \"score\": 40.72727272727273,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/박해찬_46070.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/박해찬_46070.ipynb to python\n",
      "[NbConvertApp] Writing 35857 bytes to report1/박해찬_46070.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "x value\n",
      " [[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]]\n",
      "x.T value\n",
      " [[5. 7. 9.]\n",
      " [6. 8. 0.]]\n",
      "y value\n",
      " [[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 308302.69it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:32:25.691142: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:32:26.073094: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:32:26.073132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:32:26.294463: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:32:26.924617: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 123381.84it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(2, 2)\n",
      "(2,)\n",
      "[ True False]\n",
      "[1. 0.]\n",
      "0.5\n",
      "Epoch 1/20\n",
      "400/400 [==============================] - 2s 3ms/step - loss: 17.7026 - pos_accuracy: 0.0569 - val_loss: 2.6103 - val_pos_accuracy: 0.1175\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 1.8386 - pos_accuracy: 0.2169 - val_loss: 1.2870 - val_pos_accuracy: 0.2775\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.8505 - pos_accuracy: 0.3562 - val_loss: 0.6999 - val_pos_accuracy: 0.4075\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.5372 - pos_accuracy: 0.5225 - val_loss: 0.2767 - val_pos_accuracy: 0.6350\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3072 - pos_accuracy: 0.6538 - val_loss: 0.2444 - val_pos_accuracy: 0.6425\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1848 - pos_accuracy: 0.7344 - val_loss: 0.2193 - val_pos_accuracy: 0.7300\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1146 - pos_accuracy: 0.8350 - val_loss: 0.1631 - val_pos_accuracy: 0.8050\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0801 - pos_accuracy: 0.8831 - val_loss: 0.1288 - val_pos_accuracy: 0.8225\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0577 - pos_accuracy: 0.9219 - val_loss: 0.1097 - val_pos_accuracy: 0.8600\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0440 - pos_accuracy: 0.9400 - val_loss: 0.0959 - val_pos_accuracy: 0.8800\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0351 - pos_accuracy: 0.9613 - val_loss: 0.0869 - val_pos_accuracy: 0.9025\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0262 - pos_accuracy: 0.9737 - val_loss: 0.0850 - val_pos_accuracy: 0.8950\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0235 - pos_accuracy: 0.9800 - val_loss: 0.0808 - val_pos_accuracy: 0.9050\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0181 - pos_accuracy: 0.9862 - val_loss: 0.0759 - val_pos_accuracy: 0.9125\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0143 - pos_accuracy: 0.9931 - val_loss: 0.0821 - val_pos_accuracy: 0.9050\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0133 - pos_accuracy: 0.9912 - val_loss: 0.0691 - val_pos_accuracy: 0.9125\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0098 - pos_accuracy: 0.9944 - val_loss: 0.0637 - val_pos_accuracy: 0.9200\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0116 - pos_accuracy: 0.9906 - val_loss: 0.0646 - val_pos_accuracy: 0.9175\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0083 - pos_accuracy: 0.9969 - val_loss: 0.0680 - val_pos_accuracy: 0.9175\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0066 - pos_accuracy: 0.9962 - val_loss: 0.0604 - val_pos_accuracy: 0.9175\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:32:55.099130: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1683: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할은 두 배열을 합치는 것이다.\\naxis 값에 따라 합치는 방향이 달라지며, axis=1은 가로방향으로 합치는 것을 의미한다.\\n\",\n",
      "        \"\\nsubplot의 121과 122는 행, 열, 인덱스 값을 나열한 것이다.\\n121은 subplot의 행이 1개, 열이 1개, 인덱스가 1임을 의미하며,\\n122는 subplot의 행이 1개, 열이 1개, 인덱스가 2임을 의미한다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all은 텐서상의 AND 논리연산을 담당하며, axis=1일 때, label_trued와 label_pred를 세로축(열) 단위로 비교한다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 is_correct가 가진 전체 원소의 평균을 계산한다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유는, 값을 분류하지 않고 그대로 출력해야 하기 때문이다.\\n\",\n",
      "        \"\\nrandom seed를 설정하면, 프로그램을 여러번 돌려도 매번 같은 결과가 나타난다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.9225\n",
      "}\"report1/최요한_46089.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/최요한_46089.ipynb to python\n",
      "[NbConvertApp] Writing 35469 bytes to report1/최요한_46089.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "---------------------------\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "--------------------------\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "--------------------------\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:217: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:256: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "0\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  0.1]\n",
      " [0.  1.2 0.1]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.  0.  0.3]\n",
      " [0.  1.  0.3]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 300731.63it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:33:02.518887: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:33:02.900457: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:33:02.900496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:33:03.122329: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:33:03.776225: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 116686.72it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:33:06.003747: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "0.6013071895424837\n",
      "0.6013072\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1708: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\nAND 연산으로 계산\\naxis =1은 줄인 차수일 뜻함\\n\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n열단위로 평균을 낼대 사용 - 정확도를 높이기 위해\\n\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n활성화 함수 사용에서 none-linear 하게 변환\\n\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\nseed()를 사용하면 우리는 '무작위'의 결과를 특정한 값으로 고정할수 있음\\n\",\n",
      "        true,\n",
      "        \"\\n\\n너무 어렵습니다. ㅠㅠ\\n\\n\"\n",
      "    ],\n",
      "    \"score\": 80.72727272727272,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/김남영_46063.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김남영_46063.ipynb to python\n",
      "[NbConvertApp] Writing 36051 bytes to report1/김남영_46063.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:212: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -12. ]\n",
      " [  0.    1.2 -24. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 305740.72it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:33:14.171792: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:33:14.546311: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:33:14.546349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:33:14.770689: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:33:15.403090: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 96793.49it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 85.4358 - accuracy: 0.8181 - val_loss: 13.4132 - val_accuracy: 0.9050\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.6213 - accuracy: 0.9475 - val_loss: 3.6751 - val_accuracy: 0.9500\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5199 - accuracy: 0.9681 - val_loss: 2.3678 - val_accuracy: 0.9675\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7292 - accuracy: 0.9775 - val_loss: 1.8243 - val_accuracy: 0.9675\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3475 - accuracy: 0.9750 - val_loss: 1.4871 - val_accuracy: 0.9675\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1205 - accuracy: 0.9769 - val_loss: 1.3060 - val_accuracy: 0.9725\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9733 - accuracy: 0.9800 - val_loss: 1.1445 - val_accuracy: 0.9725\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8583 - accuracy: 0.9787 - val_loss: 1.0718 - val_accuracy: 0.9675\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.7781 - accuracy: 0.9800 - val_loss: 0.9637 - val_accuracy: 0.9725\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7132 - accuracy: 0.9812 - val_loss: 0.8881 - val_accuracy: 0.9725\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6544 - accuracy: 0.9800 - val_loss: 0.8267 - val_accuracy: 0.9675\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6108 - accuracy: 0.9819 - val_loss: 0.7701 - val_accuracy: 0.9750\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5687 - accuracy: 0.9787 - val_loss: 0.7433 - val_accuracy: 0.9725\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5331 - accuracy: 0.9775 - val_loss: 0.7020 - val_accuracy: 0.9725\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4985 - accuracy: 0.9819 - val_loss: 0.7103 - val_accuracy: 0.9725\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4744 - accuracy: 0.9806 - val_loss: 0.6590 - val_accuracy: 0.9725\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4488 - accuracy: 0.9787 - val_loss: 0.6156 - val_accuracy: 0.9675\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4262 - accuracy: 0.9775 - val_loss: 0.6149 - val_accuracy: 0.9725\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4055 - accuracy: 0.9781 - val_loss: 0.5768 - val_accuracy: 0.9700\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3908 - accuracy: 0.9806 - val_loss: 0.5666 - val_accuracy: 0.9675\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3728 - accuracy: 0.9806 - val_loss: 0.5503 - val_accuracy: 0.9725\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3517 - accuracy: 0.9806 - val_loss: 0.5293 - val_accuracy: 0.9675\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3417 - accuracy: 0.9794 - val_loss: 0.5120 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3262 - accuracy: 0.9806 - val_loss: 0.4961 - val_accuracy: 0.9650\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.9794 - val_loss: 0.4857 - val_accuracy: 0.9700\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3020 - accuracy: 0.9812 - val_loss: 0.4758 - val_accuracy: 0.9700\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2929 - accuracy: 0.9819 - val_loss: 0.4631 - val_accuracy: 0.9700\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2829 - accuracy: 0.9794 - val_loss: 0.4517 - val_accuracy: 0.9700\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2709 - accuracy: 0.9825 - val_loss: 0.4439 - val_accuracy: 0.9700\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2610 - accuracy: 0.9806 - val_loss: 0.4378 - val_accuracy: 0.9750\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2527 - accuracy: 0.9831 - val_loss: 0.4253 - val_accuracy: 0.9700\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2446 - accuracy: 0.9837 - val_loss: 0.4203 - val_accuracy: 0.9675\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2354 - accuracy: 0.9800 - val_loss: 0.4102 - val_accuracy: 0.9650\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2271 - accuracy: 0.9812 - val_loss: 0.4032 - val_accuracy: 0.9725\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2223 - accuracy: 0.9831 - val_loss: 0.3920 - val_accuracy: 0.9625\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2148 - accuracy: 0.9787 - val_loss: 0.3865 - val_accuracy: 0.9725\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2076 - accuracy: 0.9844 - val_loss: 0.3789 - val_accuracy: 0.9700\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2001 - accuracy: 0.9825 - val_loss: 0.3732 - val_accuracy: 0.9650\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1959 - accuracy: 0.9800 - val_loss: 0.3701 - val_accuracy: 0.9725\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1906 - accuracy: 0.9837 - val_loss: 0.3656 - val_accuracy: 0.9675\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9825 - val_loss: 0.3612 - val_accuracy: 0.9725\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1803 - accuracy: 0.9806 - val_loss: 0.3620 - val_accuracy: 0.9675\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1759 - accuracy: 0.9819 - val_loss: 0.3497 - val_accuracy: 0.9675\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1686 - accuracy: 0.9806 - val_loss: 0.3452 - val_accuracy: 0.9625\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1661 - accuracy: 0.9812 - val_loss: 0.3423 - val_accuracy: 0.9675\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.9812 - val_loss: 0.3386 - val_accuracy: 0.9650\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9806 - val_loss: 0.3340 - val_accuracy: 0.9775\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9825 - val_loss: 0.3299 - val_accuracy: 0.9675\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1481 - accuracy: 0.9775 - val_loss: 0.3282 - val_accuracy: 0.9675\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1445 - accuracy: 0.9787 - val_loss: 0.3190 - val_accuracy: 0.9675\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1407 - accuracy: 0.9819 - val_loss: 0.3189 - val_accuracy: 0.9650\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1367 - accuracy: 0.9787 - val_loss: 0.3123 - val_accuracy: 0.9650\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1338 - accuracy: 0.9775 - val_loss: 0.3118 - val_accuracy: 0.9625\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9794 - val_loss: 0.3098 - val_accuracy: 0.9625\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1270 - accuracy: 0.9787 - val_loss: 0.3027 - val_accuracy: 0.9625\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1237 - accuracy: 0.9800 - val_loss: 0.3000 - val_accuracy: 0.9625\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9825 - val_loss: 0.2978 - val_accuracy: 0.9625\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9775 - val_loss: 0.2953 - val_accuracy: 0.9625\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1161 - accuracy: 0.9806 - val_loss: 0.2902 - val_accuracy: 0.9650\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9787 - val_loss: 0.2932 - val_accuracy: 0.9625\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1103 - accuracy: 0.9787 - val_loss: 0.2871 - val_accuracy: 0.9625\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1081 - accuracy: 0.9775 - val_loss: 0.2829 - val_accuracy: 0.9625\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1057 - accuracy: 0.9787 - val_loss: 0.2797 - val_accuracy: 0.9625\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1021 - accuracy: 0.9775 - val_loss: 0.2799 - val_accuracy: 0.9575\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0999 - accuracy: 0.9781 - val_loss: 0.2770 - val_accuracy: 0.9625\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.9775 - val_loss: 0.2785 - val_accuracy: 0.9625\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9762 - val_loss: 0.2717 - val_accuracy: 0.9675\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0935 - accuracy: 0.9794 - val_loss: 0.2700 - val_accuracy: 0.9625\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9744 - val_loss: 0.2654 - val_accuracy: 0.9625\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0899 - accuracy: 0.9775 - val_loss: 0.2679 - val_accuracy: 0.9625\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9762 - val_loss: 0.2614 - val_accuracy: 0.9625\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9731 - val_loss: 0.2603 - val_accuracy: 0.9625\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0841 - accuracy: 0.9775 - val_loss: 0.2578 - val_accuracy: 0.9625\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9762 - val_loss: 0.2546 - val_accuracy: 0.9625\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0811 - accuracy: 0.9737 - val_loss: 0.2576 - val_accuracy: 0.9575\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.9787 - val_loss: 0.2532 - val_accuracy: 0.9625\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0771 - accuracy: 0.9756 - val_loss: 0.2515 - val_accuracy: 0.9625\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0757 - accuracy: 0.9762 - val_loss: 0.2503 - val_accuracy: 0.9600\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9769 - val_loss: 0.2501 - val_accuracy: 0.9575\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0727 - accuracy: 0.9737 - val_loss: 0.2477 - val_accuracy: 0.9575\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9737 - val_loss: 0.2443 - val_accuracy: 0.9625\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0692 - accuracy: 0.9744 - val_loss: 0.2439 - val_accuracy: 0.9625\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0680 - accuracy: 0.9750 - val_loss: 0.2430 - val_accuracy: 0.9625\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.9750 - val_loss: 0.2434 - val_accuracy: 0.9625\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0657 - accuracy: 0.9744 - val_loss: 0.2371 - val_accuracy: 0.9600\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0644 - accuracy: 0.9750 - val_loss: 0.2387 - val_accuracy: 0.9625\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0629 - accuracy: 0.9781 - val_loss: 0.2335 - val_accuracy: 0.9625\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0616 - accuracy: 0.9744 - val_loss: 0.2351 - val_accuracy: 0.9675\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0608 - accuracy: 0.9750 - val_loss: 0.2342 - val_accuracy: 0.9700\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9750 - val_loss: 0.2310 - val_accuracy: 0.9675\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.9744 - val_loss: 0.2307 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 0.9787 - val_loss: 0.2305 - val_accuracy: 0.9675\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0556 - accuracy: 0.9731 - val_loss: 0.2278 - val_accuracy: 0.9675\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9756 - val_loss: 0.2254 - val_accuracy: 0.9625\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0536 - accuracy: 0.9731 - val_loss: 0.2255 - val_accuracy: 0.9625\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0529 - accuracy: 0.9756 - val_loss: 0.2220 - val_accuracy: 0.9625\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0515 - accuracy: 0.9750 - val_loss: 0.2226 - val_accuracy: 0.9625\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0504 - accuracy: 0.9719 - val_loss: 0.2221 - val_accuracy: 0.9625\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0495 - accuracy: 0.9725 - val_loss: 0.2202 - val_accuracy: 0.9625\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0490 - accuracy: 0.9762 - val_loss: 0.2194 - val_accuracy: 0.9625\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.2194 - accuracy: 0.9625\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:33:28.355306: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1695: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concatenate는 선택한 축의 방향으로 배열을 하나로 합치게 합니다. \\nAxis=1은 2D 어레이 이상에서 사용하는 값입니다. 1D 어레이에서 axis=1을 하면 에러가 뜹니다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\nsubplot : 여러가지 그래프를 그리고 싶을 경우 사용\\n1 : subplot의 행 수\\n2 : subplot의 열 수\\n1/2 : 인덱스 (1번은 original, 2번은 affine)\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\ntf.reduce_all : 논리 AND 연산\\naxis=1 : 가로 축 방향\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n측정 차원을 줄이면서 평균을 구합니다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\ninput에서 먼저 none을 했기 때문입니다.\\ninput에서 설정하지 않으면 출력층도 none으로 나옵니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n랜덤으로 만든 숫자를 나중에도 같은 숫자로 계속 써야 하기 때문입니다. \\n\",\n",
      "        false,\n",
      "        \"\\n1,2,3은 강의와 실습을 따라하면 수월하게 진행을 할 수 있었는데 4번과 5번은 강의와 실습을 여러번 복습했는데도 과제가 어려웠습니다. 특히 5번의 제일 처음 문제인 이미지 2장을 불러오는 것 부터가 모르겠습니다. 해본다고 열심히 해봤는데도 잘 안됩니다. \\n(원래 저는 끝까지 해보는데 이번 만큼은 너무 안되어 자괴감이 크네요.특히 뭔가 조금만 더 손보면 될 것 같은데 안되니 힘듭니다.)\\n또한 강의 내용도 생각보다 더 전문적이라 두세번 들어야지만 이해가 갔습니다. 3주차 까지는 그래도 이해했습니다만 (그래도 지금처럼 이미지 2장을 사용하라는 응용문제가 나오면 힘이 듭니다), 4주차 강의에서는 여러번 들어도 70%이해한 듯 합니다. \\n뭔가 미래학부 기초 수업은 정말 기초만을 가르치는데 3~4 수업 부터는 중간이 없이 바로 완성작을 만들어 내야하는 수준인 것 같아 따라가기가 벅찹니다.  \\n그래도 교수님께서 과제 페이지에 설명을 많이 해주시어 과제 하면서 도움이 정말 많이 되었습니다. 감사합니다.\\n어떻게 하면 더 쉽게 이해할 수 있는지 조언을 주신다면 감사히 받아 열심히 더 공부해 보겠습니다. (더 기초적인 내용을 따로 공부해야 한다든지 등)\\n\"\n",
      "    ],\n",
      "    \"score\": 80.0,\n",
      "    \"accuracy\": 0.9625\n",
      "}\"report1/조형래_46016.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/조형래_46016.ipynb to python\n",
      "[NbConvertApp] Writing 34577 bytes to report1/조형래_46016.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.  -1. 200.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 294823.32it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:33:35.734938: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:33:36.121671: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:33:36.121725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:33:36.346289: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:33:36.963883: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 249.4044 - val_loss: 228.4966\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 247.3796 - val_loss: 226.6412\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 112256.72it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 175.8720 - val_loss: 105.9368\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 84.0050 - val_loss: 54.7166\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 44.8447 - val_loss: 32.0637\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 26.8500 - val_loss: 21.1127\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 17.8418 - val_loss: 15.2303\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 12.8726 - val_loss: 11.7606\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.9017 - val_loss: 9.5434\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.9870 - val_loss: 8.0327\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.6852 - val_loss: 6.9600\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.7609 - val_loss: 6.1654\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.0794 - val_loss: 5.5618\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.5630 - val_loss: 5.0907\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.1612 - val_loss: 4.7129\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.8392 - val_loss: 4.4034\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.5750 - val_loss: 4.1433\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.3547 - val_loss: 3.9222\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.1664 - val_loss: 3.7304\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.0051 - val_loss: 3.5631\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8646 - val_loss: 3.4153\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.7401 - val_loss: 3.2829\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6293 - val_loss: 3.1627\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.5285 - val_loss: 3.0540\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4392 - val_loss: 2.9545\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.3564 - val_loss: 2.8634\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2802 - val_loss: 2.7787\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2114 - val_loss: 2.7004\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1469 - val_loss: 2.6269\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0871 - val_loss: 2.5584\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0316 - val_loss: 2.4944\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9791 - val_loss: 2.4343\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9301 - val_loss: 2.3777\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8840 - val_loss: 2.3249\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8404 - val_loss: 2.2747\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7995 - val_loss: 2.2274\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7610 - val_loss: 2.1821\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7245 - val_loss: 2.1385\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6900 - val_loss: 2.0971\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6568 - val_loss: 2.0586\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6262 - val_loss: 2.0213\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5956 - val_loss: 1.9860\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5669 - val_loss: 1.9519\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5394 - val_loss: 1.9196\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5133 - val_loss: 1.8883\n",
      "Epoch 44/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4877 - val_loss: 1.8586\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4639 - val_loss: 1.8296\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4407 - val_loss: 1.8019\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4183 - val_loss: 1.7750\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3970 - val_loss: 1.7492\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3764 - val_loss: 1.7250\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3566 - val_loss: 1.7010\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3375 - val_loss: 1.6777\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3189 - val_loss: 1.6555\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3013 - val_loss: 1.6339\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2841 - val_loss: 1.6133\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2671 - val_loss: 1.5935\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2515 - val_loss: 1.5739\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2357 - val_loss: 1.5547\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2205 - val_loss: 1.5368\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2062 - val_loss: 1.5192\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1918 - val_loss: 1.5022\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1783 - val_loss: 1.4854\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1649 - val_loss: 1.4695\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1519 - val_loss: 1.4534\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1389 - val_loss: 1.4381\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1267 - val_loss: 1.4235\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1145 - val_loss: 1.4091\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1029 - val_loss: 1.3951\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0918 - val_loss: 1.3813\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0808 - val_loss: 1.3676\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0706 - val_loss: 1.3547\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0599 - val_loss: 1.3419\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0495 - val_loss: 1.3298\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0397 - val_loss: 1.3174\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0299 - val_loss: 1.3062\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0206 - val_loss: 1.2949\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0115 - val_loss: 1.2835\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0025 - val_loss: 1.2727\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9933 - val_loss: 1.2618\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9847 - val_loss: 1.2516\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9765 - val_loss: 1.2413\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9681 - val_loss: 1.2314\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9604 - val_loss: 1.2216\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9523 - val_loss: 1.2123\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9446 - val_loss: 1.2030\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9373 - val_loss: 1.1937\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9296 - val_loss: 1.1846\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9225 - val_loss: 1.1759\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9154 - val_loss: 1.1677\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9087 - val_loss: 1.1592\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9018 - val_loss: 1.1511\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8948 - val_loss: 1.1428\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8881 - val_loss: 1.1349\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8820 - val_loss: 1.1274\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8756 - val_loss: 1.1198\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8692 - val_loss: 1.1125\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8636 - val_loss: 1.1054\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8576 - val_loss: 1.0982\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8518 - val_loss: 1.0912\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8462 - val_loss: 1.0847\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8407 - val_loss: 1.0779\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8346 - val_loss: 1.0712\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8293 - val_loss: 1.0645\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8239 - val_loss: 1.0584\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8189 - val_loss: 1.0519\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8136 - val_loss: 1.0456\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8087 - val_loss: 1.0395\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8036 - val_loss: 1.0336\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7985 - val_loss: 1.0276\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7939 - val_loss: 1.0219\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7891 - val_loss: 1.0162\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7842 - val_loss: 1.0106\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7798 - val_loss: 1.0052\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7751 - val_loss: 0.9997\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7708 - val_loss: 0.9947\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7665 - val_loss: 0.9894\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7622 - val_loss: 0.9844\n",
      "Epoch 117/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7580 - val_loss: 0.9791\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7539 - val_loss: 0.9743\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7492 - val_loss: 0.9691\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7456 - val_loss: 0.9645\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7416 - val_loss: 0.9602\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7375 - val_loss: 0.9553\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7338 - val_loss: 0.9508\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7298 - val_loss: 0.9461\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7260 - val_loss: 0.9416\n",
      "Epoch 126/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7224 - val_loss: 0.9372\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7185 - val_loss: 0.9330\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7151 - val_loss: 0.9287\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7115 - val_loss: 0.9243\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7079 - val_loss: 0.9203\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7044 - val_loss: 0.9164\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7009 - val_loss: 0.9124\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6977 - val_loss: 0.9084\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6946 - val_loss: 0.9045\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6908 - val_loss: 0.9007\n",
      "Epoch 136/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6878 - val_loss: 0.8971\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6845 - val_loss: 0.8933\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6814 - val_loss: 0.8897\n",
      "Epoch 139/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6783 - val_loss: 0.8860\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6749 - val_loss: 0.8823\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6724 - val_loss: 0.8789\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6690 - val_loss: 0.8755\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6661 - val_loss: 0.8720\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6634 - val_loss: 0.8683\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6605 - val_loss: 0.8651\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6573 - val_loss: 0.8618\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6549 - val_loss: 0.8585\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6519 - val_loss: 0.8552\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6491 - val_loss: 0.8521\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6463 - val_loss: 0.8488\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6435 - val_loss: 0.8458\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6411 - val_loss: 0.8428\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6387 - val_loss: 0.8398\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6359 - val_loss: 0.8367\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6333 - val_loss: 0.8338\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6306 - val_loss: 0.8309\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6282 - val_loss: 0.8282\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6256 - val_loss: 0.8253\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6232 - val_loss: 0.8226\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6208 - val_loss: 0.8198\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6183 - val_loss: 0.8170\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6161 - val_loss: 0.8143\n",
      "Epoch 163/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6136 - val_loss: 0.8114\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6113 - val_loss: 0.8087\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6091 - val_loss: 0.8062\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6069 - val_loss: 0.8036\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6043 - val_loss: 0.8009\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6024 - val_loss: 0.7984\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6002 - val_loss: 0.7959\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5979 - val_loss: 0.7936\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5956 - val_loss: 0.7913\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5936 - val_loss: 0.7888\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5916 - val_loss: 0.7864\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.7840\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5875 - val_loss: 0.7818\n",
      "Epoch 176/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5853 - val_loss: 0.7795\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5833 - val_loss: 0.7773\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5814 - val_loss: 0.7753\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5791 - val_loss: 0.7730\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5772 - val_loss: 0.7708\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5751 - val_loss: 0.7687\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5734 - val_loss: 0.7664\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5714 - val_loss: 0.7644\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5696 - val_loss: 0.7622\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5678 - val_loss: 0.7604\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5659 - val_loss: 0.7583\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5640 - val_loss: 0.7561\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5623 - val_loss: 0.7541\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5603 - val_loss: 0.7521\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5585 - val_loss: 0.7503\n",
      "Epoch 191/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5567 - val_loss: 0.7483\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5548 - val_loss: 0.7464\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5533 - val_loss: 0.7445\n",
      "Epoch 194/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5516 - val_loss: 0.7425\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5500 - val_loss: 0.7406\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5482 - val_loss: 0.7388\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5465 - val_loss: 0.7370\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5449 - val_loss: 0.7350\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5432 - val_loss: 0.7333\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5413 - val_loss: 0.7316\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5399 - val_loss: 0.7299\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5383 - val_loss: 0.7281\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5369 - val_loss: 0.7262\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5351 - val_loss: 0.7246\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5336 - val_loss: 0.7229\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5318 - val_loss: 0.7213\n",
      "Epoch 207/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5304 - val_loss: 0.7197\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5289 - val_loss: 0.7181\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5274 - val_loss: 0.7164\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5261 - val_loss: 0.7149\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5244 - val_loss: 0.7132\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5231 - val_loss: 0.7117\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5216 - val_loss: 0.7101\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5202 - val_loss: 0.7085\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5187 - val_loss: 0.7070\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5171 - val_loss: 0.7054\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5157 - val_loss: 0.7039\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5143 - val_loss: 0.7025\n",
      "Epoch 219/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5129 - val_loss: 0.7011\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5117 - val_loss: 0.6995\n",
      "Epoch 221/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5102 - val_loss: 0.6980\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5088 - val_loss: 0.6966\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5076 - val_loss: 0.6952\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5060 - val_loss: 0.6938\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5048 - val_loss: 0.6924\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5035 - val_loss: 0.6910\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5021 - val_loss: 0.6896\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5008 - val_loss: 0.6884\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4995 - val_loss: 0.6869\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4983 - val_loss: 0.6857\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4969 - val_loss: 0.6843\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4959 - val_loss: 0.6831\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4945 - val_loss: 0.6818\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4932 - val_loss: 0.6806\n",
      "Epoch 235/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4920 - val_loss: 0.6795\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4907 - val_loss: 0.6782\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4895 - val_loss: 0.6768\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4884 - val_loss: 0.6756\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4872 - val_loss: 0.6743\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4860 - val_loss: 0.6731\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4848 - val_loss: 0.6718\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4835 - val_loss: 0.6706\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4825 - val_loss: 0.6694\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4812 - val_loss: 0.6684\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4800 - val_loss: 0.6671\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4788 - val_loss: 0.6659\n",
      "Epoch 247/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4777 - val_loss: 0.6648\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4769 - val_loss: 0.6637\n",
      "Epoch 249/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4756 - val_loss: 0.6624\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4745 - val_loss: 0.6613\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4733 - val_loss: 0.6602\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4723 - val_loss: 0.6590\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4710 - val_loss: 0.6579\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4700 - val_loss: 0.6569\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4689 - val_loss: 0.6557\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4680 - val_loss: 0.6547\n",
      "Epoch 257/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4670 - val_loss: 0.6536\n",
      "Epoch 258/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4657 - val_loss: 0.6525\n",
      "Epoch 259/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4647 - val_loss: 0.6514\n",
      "Epoch 260/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4636 - val_loss: 0.6505\n",
      "Epoch 261/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4629 - val_loss: 0.6494\n",
      "Epoch 262/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4617 - val_loss: 0.6483\n",
      "Epoch 263/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4605 - val_loss: 0.6474\n",
      "Epoch 264/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4597 - val_loss: 0.6464\n",
      "Epoch 265/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4585 - val_loss: 0.6454\n",
      "Epoch 266/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4575 - val_loss: 0.6444\n",
      "Epoch 267/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4566 - val_loss: 0.6434\n",
      "Epoch 268/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4556 - val_loss: 0.6425\n",
      "Epoch 269/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4546 - val_loss: 0.6415\n",
      "Epoch 270/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4536 - val_loss: 0.6406\n",
      "Epoch 271/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4526 - val_loss: 0.6397\n",
      "Epoch 272/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4517 - val_loss: 0.6388\n",
      "Epoch 273/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4507 - val_loss: 0.6378\n",
      "Epoch 274/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4498 - val_loss: 0.6370\n",
      "Epoch 275/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4490 - val_loss: 0.6359\n",
      "Epoch 276/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4479 - val_loss: 0.6349\n",
      "Epoch 277/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4471 - val_loss: 0.6339\n",
      "Epoch 278/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4461 - val_loss: 0.6331\n",
      "Epoch 279/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4451 - val_loss: 0.6322\n",
      "Epoch 280/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4441 - val_loss: 0.6314\n",
      "Epoch 281/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4432 - val_loss: 0.6306\n",
      "Epoch 282/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4423 - val_loss: 0.6295\n",
      "Epoch 283/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4416 - val_loss: 0.6287\n",
      "Epoch 284/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4405 - val_loss: 0.6279\n",
      "Epoch 285/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4396 - val_loss: 0.6269\n",
      "Epoch 286/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4387 - val_loss: 0.6262\n",
      "Epoch 287/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4378 - val_loss: 0.6252\n",
      "Epoch 288/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4370 - val_loss: 0.6244\n",
      "Epoch 289/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4360 - val_loss: 0.6235\n",
      "Epoch 290/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4354 - val_loss: 0.6227\n",
      "Epoch 291/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4343 - val_loss: 0.6219\n",
      "Epoch 292/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4335 - val_loss: 0.6210\n",
      "Epoch 293/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4328 - val_loss: 0.6203\n",
      "Epoch 294/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4317 - val_loss: 0.6194\n",
      "Epoch 295/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4309 - val_loss: 0.6187\n",
      "Epoch 296/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4300 - val_loss: 0.6179\n",
      "Epoch 297/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4294 - val_loss: 0.6171\n",
      "Epoch 298/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4283 - val_loss: 0.6164\n",
      "Epoch 299/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4277 - val_loss: 0.6155\n",
      "Epoch 300/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4268 - val_loss: 0.6148\n",
      "Epoch 301/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4259 - val_loss: 0.6140\n",
      "Epoch 302/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4252 - val_loss: 0.6133\n",
      "Epoch 303/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4243 - val_loss: 0.6124\n",
      "Epoch 304/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4236 - val_loss: 0.6119\n",
      "Epoch 305/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4227 - val_loss: 0.6110\n",
      "Epoch 306/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4220 - val_loss: 0.6103\n",
      "Epoch 307/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4211 - val_loss: 0.6094\n",
      "Epoch 308/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4204 - val_loss: 0.6088\n",
      "Epoch 309/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4197 - val_loss: 0.6081\n",
      "Epoch 310/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4189 - val_loss: 0.6074\n",
      "Epoch 311/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4180 - val_loss: 0.6067\n",
      "Epoch 312/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4171 - val_loss: 0.6059\n",
      "Epoch 313/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4165 - val_loss: 0.6052\n",
      "Epoch 314/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4158 - val_loss: 0.6045\n",
      "Epoch 315/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4150 - val_loss: 0.6038\n",
      "Epoch 316/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4143 - val_loss: 0.6030\n",
      "Epoch 317/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4135 - val_loss: 0.6024\n",
      "Epoch 318/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4126 - val_loss: 0.6017\n",
      "Epoch 319/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4120 - val_loss: 0.6011\n",
      "Epoch 320/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4112 - val_loss: 0.6004\n",
      "Epoch 321/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4105 - val_loss: 0.5997\n",
      "Epoch 322/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4097 - val_loss: 0.5990\n",
      "Epoch 323/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4091 - val_loss: 0.5983\n",
      "Epoch 324/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4084 - val_loss: 0.5978\n",
      "Epoch 325/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4077 - val_loss: 0.5970\n",
      "Epoch 326/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4069 - val_loss: 0.5963\n",
      "Epoch 327/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4061 - val_loss: 0.5956\n",
      "Epoch 328/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4056 - val_loss: 0.5950\n",
      "Epoch 329/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4048 - val_loss: 0.5943\n",
      "Epoch 330/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4041 - val_loss: 0.5936\n",
      "Epoch 331/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4033 - val_loss: 0.5931\n",
      "Epoch 332/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4026 - val_loss: 0.5924\n",
      "Epoch 333/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4019 - val_loss: 0.5917\n",
      "Epoch 334/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4013 - val_loss: 0.5912\n",
      "Epoch 335/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4006 - val_loss: 0.5905\n",
      "Epoch 336/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3999 - val_loss: 0.5899\n",
      "Epoch 337/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3992 - val_loss: 0.5893\n",
      "Epoch 338/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3984 - val_loss: 0.5887\n",
      "Epoch 339/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3978 - val_loss: 0.5882\n",
      "Epoch 340/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3971 - val_loss: 0.5876\n",
      "Epoch 341/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3965 - val_loss: 0.5869\n",
      "Epoch 342/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3958 - val_loss: 0.5864\n",
      "Epoch 343/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3951 - val_loss: 0.5858\n",
      "Epoch 344/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3946 - val_loss: 0.5853\n",
      "Epoch 345/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3938 - val_loss: 0.5846\n",
      "Epoch 346/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3932 - val_loss: 0.5841\n",
      "Epoch 347/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3925 - val_loss: 0.5834\n",
      "Epoch 348/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3918 - val_loss: 0.5829\n",
      "Epoch 349/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3913 - val_loss: 0.5823\n",
      "Epoch 350/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3907 - val_loss: 0.5818\n",
      "Epoch 351/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3899 - val_loss: 0.5812\n",
      "Epoch 352/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3895 - val_loss: 0.5806\n",
      "Epoch 353/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3886 - val_loss: 0.5801\n",
      "Epoch 354/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3881 - val_loss: 0.5795\n",
      "Epoch 355/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3874 - val_loss: 0.5789\n",
      "Epoch 356/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3868 - val_loss: 0.5784\n",
      "Epoch 357/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3862 - val_loss: 0.5779\n",
      "Epoch 358/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3856 - val_loss: 0.5773\n",
      "Epoch 359/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3849 - val_loss: 0.5769\n",
      "Epoch 360/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3843 - val_loss: 0.5763\n",
      "Epoch 361/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3836 - val_loss: 0.5757\n",
      "Epoch 362/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3832 - val_loss: 0.5753\n",
      "Epoch 363/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3825 - val_loss: 0.5747\n",
      "Epoch 364/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3819 - val_loss: 0.5742\n",
      "Epoch 365/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3813 - val_loss: 0.5737\n",
      "Epoch 366/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3807 - val_loss: 0.5732\n",
      "Epoch 367/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3801 - val_loss: 0.5727\n",
      "Epoch 368/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3795 - val_loss: 0.5721\n",
      "Epoch 369/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3789 - val_loss: 0.5717\n",
      "Epoch 370/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3783 - val_loss: 0.5713\n",
      "Epoch 371/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3777 - val_loss: 0.5707\n",
      "Epoch 372/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3772 - val_loss: 0.5702\n",
      "Epoch 373/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3766 - val_loss: 0.5697\n",
      "Epoch 374/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3760 - val_loss: 0.5692\n",
      "Epoch 375/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3754 - val_loss: 0.5686\n",
      "Epoch 376/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3749 - val_loss: 0.5682\n",
      "Epoch 377/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3743 - val_loss: 0.5678\n",
      "Epoch 378/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3737 - val_loss: 0.5673\n",
      "Epoch 379/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3731 - val_loss: 0.5668\n",
      "Epoch 380/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3726 - val_loss: 0.5664\n",
      "Epoch 381/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3721 - val_loss: 0.5659\n",
      "Epoch 382/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3714 - val_loss: 0.5655\n",
      "Epoch 383/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3710 - val_loss: 0.5650\n",
      "Epoch 384/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3703 - val_loss: 0.5646\n",
      "Epoch 385/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3698 - val_loss: 0.5641\n",
      "Epoch 386/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3693 - val_loss: 0.5636\n",
      "Epoch 387/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3686 - val_loss: 0.5631\n",
      "Epoch 388/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3683 - val_loss: 0.5626\n",
      "Epoch 389/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3677 - val_loss: 0.5622\n",
      "Epoch 390/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3671 - val_loss: 0.5617\n",
      "Epoch 391/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3665 - val_loss: 0.5612\n",
      "Epoch 392/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3660 - val_loss: 0.5609\n",
      "Epoch 393/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3655 - val_loss: 0.5603\n",
      "Epoch 394/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3649 - val_loss: 0.5599\n",
      "Epoch 395/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3644 - val_loss: 0.5595\n",
      "Epoch 396/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3638 - val_loss: 0.5591\n",
      "Epoch 397/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3633 - val_loss: 0.5586\n",
      "Epoch 398/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3628 - val_loss: 0.5582\n",
      "Epoch 399/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3624 - val_loss: 0.5579\n",
      "Epoch 400/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3617 - val_loss: 0.5573\n",
      "Epoch 401/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3613 - val_loss: 0.5569\n",
      "Epoch 402/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3608 - val_loss: 0.5565\n",
      "Epoch 403/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3602 - val_loss: 0.5561\n",
      "Epoch 404/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3597 - val_loss: 0.5556\n",
      "Epoch 405/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3593 - val_loss: 0.5553\n",
      "Epoch 406/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3587 - val_loss: 0.5549\n",
      "Epoch 407/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3582 - val_loss: 0.5545\n",
      "Epoch 408/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3576 - val_loss: 0.5542\n",
      "Epoch 409/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3570 - val_loss: 0.5537\n",
      "Epoch 410/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3566 - val_loss: 0.5532\n",
      "Epoch 411/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3561 - val_loss: 0.5528\n",
      "Epoch 412/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3557 - val_loss: 0.5524\n",
      "Epoch 413/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3552 - val_loss: 0.5521\n",
      "Epoch 414/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3547 - val_loss: 0.5517\n",
      "Epoch 415/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3541 - val_loss: 0.5513\n",
      "Epoch 416/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3536 - val_loss: 0.5509\n",
      "Epoch 417/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3533 - val_loss: 0.5505\n",
      "Epoch 418/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3527 - val_loss: 0.5501\n",
      "Epoch 419/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3523 - val_loss: 0.5497\n",
      "Epoch 420/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3518 - val_loss: 0.5494\n",
      "Epoch 421/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3512 - val_loss: 0.5489\n",
      "Epoch 422/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3508 - val_loss: 0.5486\n",
      "Epoch 423/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3503 - val_loss: 0.5482\n",
      "Epoch 424/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3498 - val_loss: 0.5478\n",
      "Epoch 425/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3493 - val_loss: 0.5475\n",
      "Epoch 426/500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3489 - val_loss: 0.5471\n",
      "Epoch 427/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3484 - val_loss: 0.5467\n",
      "Epoch 428/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3479 - val_loss: 0.5462\n",
      "Epoch 429/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3475 - val_loss: 0.5460\n",
      "Epoch 430/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3470 - val_loss: 0.5456\n",
      "Epoch 431/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3466 - val_loss: 0.5452\n",
      "Epoch 432/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3462 - val_loss: 0.5449\n",
      "Epoch 433/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3456 - val_loss: 0.5446\n",
      "Epoch 434/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3451 - val_loss: 0.5442\n",
      "Epoch 435/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3447 - val_loss: 0.5438\n",
      "Epoch 436/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3442 - val_loss: 0.5435\n",
      "Epoch 437/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3439 - val_loss: 0.5431\n",
      "Epoch 438/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3433 - val_loss: 0.5428\n",
      "Epoch 439/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3429 - val_loss: 0.5424\n",
      "Epoch 440/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3425 - val_loss: 0.5421\n",
      "Epoch 441/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3420 - val_loss: 0.5418\n",
      "Epoch 442/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3415 - val_loss: 0.5414\n",
      "Epoch 443/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3411 - val_loss: 0.5411\n",
      "Epoch 444/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3408 - val_loss: 0.5406\n",
      "Epoch 445/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3402 - val_loss: 0.5403\n",
      "Epoch 446/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3398 - val_loss: 0.5399\n",
      "Epoch 447/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3393 - val_loss: 0.5397\n",
      "Epoch 448/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3389 - val_loss: 0.5392\n",
      "Epoch 449/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3385 - val_loss: 0.5389\n",
      "Epoch 450/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3381 - val_loss: 0.5386\n",
      "Epoch 451/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3376 - val_loss: 0.5383\n",
      "Epoch 452/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3371 - val_loss: 0.5380\n",
      "Epoch 453/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3367 - val_loss: 0.5376\n",
      "Epoch 454/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3363 - val_loss: 0.5373\n",
      "Epoch 455/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3359 - val_loss: 0.5370\n",
      "Epoch 456/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3354 - val_loss: 0.5366\n",
      "Epoch 457/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3352 - val_loss: 0.5363\n",
      "Epoch 458/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3345 - val_loss: 0.5360\n",
      "Epoch 459/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3342 - val_loss: 0.5357\n",
      "Epoch 460/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3338 - val_loss: 0.5354\n",
      "Epoch 461/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3334 - val_loss: 0.5351\n",
      "Epoch 462/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3330 - val_loss: 0.5347\n",
      "Epoch 463/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3325 - val_loss: 0.5344\n",
      "Epoch 464/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3321 - val_loss: 0.5341\n",
      "Epoch 465/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3318 - val_loss: 0.5338\n",
      "Epoch 466/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3312 - val_loss: 0.5335\n",
      "Epoch 467/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3309 - val_loss: 0.5332\n",
      "Epoch 468/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3305 - val_loss: 0.5330\n",
      "Epoch 469/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3302 - val_loss: 0.5325\n",
      "Epoch 470/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3296 - val_loss: 0.5322\n",
      "Epoch 471/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3292 - val_loss: 0.5320\n",
      "Epoch 472/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3289 - val_loss: 0.5317\n",
      "Epoch 473/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3284 - val_loss: 0.5314\n",
      "Epoch 474/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3281 - val_loss: 0.5312\n",
      "Epoch 475/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3277 - val_loss: 0.5309\n",
      "Epoch 476/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3272 - val_loss: 0.5306\n",
      "Epoch 477/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3268 - val_loss: 0.5303\n",
      "Epoch 478/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3265 - val_loss: 0.5299\n",
      "Epoch 479/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3261 - val_loss: 0.5297\n",
      "Epoch 480/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3256 - val_loss: 0.5293\n",
      "Epoch 481/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3254 - val_loss: 0.5290\n",
      "Epoch 482/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3249 - val_loss: 0.5288\n",
      "Epoch 483/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3245 - val_loss: 0.5285\n",
      "Epoch 484/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3241 - val_loss: 0.5282\n",
      "Epoch 485/500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3237 - val_loss: 0.5279\n",
      "Epoch 486/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3234 - val_loss: 0.5276\n",
      "Epoch 487/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3229 - val_loss: 0.5274\n",
      "Epoch 488/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3226 - val_loss: 0.5270\n",
      "Epoch 489/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3222 - val_loss: 0.5267\n",
      "Epoch 490/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3218 - val_loss: 0.5264\n",
      "Epoch 491/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3215 - val_loss: 0.5262\n",
      "Epoch 492/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3211 - val_loss: 0.5260\n",
      "Epoch 493/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3208 - val_loss: 0.5257\n",
      "Epoch 494/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3203 - val_loss: 0.5254\n",
      "Epoch 495/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3199 - val_loss: 0.5252\n",
      "Epoch 496/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3195 - val_loss: 0.5249\n",
      "Epoch 497/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3192 - val_loss: 0.5247\n",
      "Epoch 498/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3188 - val_loss: 0.5244\n",
      "Epoch 499/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3185 - val_loss: 0.5242\n",
      "Epoch 500/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3181 - val_loss: 0.5239\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:34:22.326614: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1653: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nNumpy 배열들을 하나로 합치는데 이용 / axis 값을 조절하여 어떤 축을 기준으로 배열을 합칠지 정함\\n\",\n",
      "        \"\\n1x2 행렬 첫번째 칸에 plot를 그리겠다는 의미\\n1x2 행렬 두번째 칸에 plot를 그리겠다는 의미\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n가로세로값을 줄임 / 세로값을 줄임\\n\",\n",
      "        \"\\n평균을 계산함\\n\",\n",
      "        \"\\nlinear,sigmoid,softmax,relu를 넣고 실행 해도 값이 동일하게 나옴\\n\",\n",
      "        \"\\n예측가능한 난수 / 동일한셋트 난수\\n\",\n",
      "        false,\n",
      "        \"\\n감사합니다. 열심히 이해해 보도록 노력 하겠습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 56.54545454545455,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/이동훈_46044.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이동훈_46044.ipynb to python\n",
      "[NbConvertApp] Writing 29264 bytes to report1/이동훈_46044.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:247: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[inf inf]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -15. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 298399.54it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:34:28.708505: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:34:29.092930: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:34:29.092976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:34:29.318601: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:34:29.984161: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:34:31.635523: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1440: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 57.45454545454545,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/김원경_46078.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김원경_46078.ipynb to python\n",
      "[NbConvertApp] Writing 35210 bytes to report1/김원경_46078.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(2, 2)\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -30. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 304763.23it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:34:37.946507: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:34:38.320861: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:34:38.320901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 50,370\n",
      "Trainable params: 50,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:34:38.547266: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:34:39.205151: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 238.4722 - pos_accuracy: 0.0012 - val_loss: 205.8856 - val_pos_accuracy: 0.0022\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 205.7819 - pos_accuracy: 0.0019 - val_loss: 166.7985 - val_pos_accuracy: 0.0000e+00\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 111578.83it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 51,314\n",
      "Trainable params: 51,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 7ms/step - loss: 109.5058 - pos_accuracy: 0.0025 - val_loss: 68.1418 - val_pos_accuracy: 0.0156\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 12.9113 - pos_accuracy: 0.0594 - val_loss: 2.4485 - val_pos_accuracy: 0.1384\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.9456 - pos_accuracy: 0.1119 - val_loss: 2.8145 - val_pos_accuracy: 0.0759\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.6916 - pos_accuracy: 0.2050 - val_loss: 0.6920 - val_pos_accuracy: 0.3527\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.1479 - pos_accuracy: 0.2225 - val_loss: 0.5336 - val_pos_accuracy: 0.4196\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.5820 - pos_accuracy: 0.3562 - val_loss: 0.5231 - val_pos_accuracy: 0.4777\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1720 - pos_accuracy: 0.2744 - val_loss: 0.7296 - val_pos_accuracy: 0.2366\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3446 - pos_accuracy: 0.5300 - val_loss: 0.3729 - val_pos_accuracy: 0.5424\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3907 - pos_accuracy: 0.4456 - val_loss: 0.3193 - val_pos_accuracy: 0.6205\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1894 - pos_accuracy: 0.7225 - val_loss: 0.2898 - val_pos_accuracy: 0.6429\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1831 - pos_accuracy: 0.7181 - val_loss: 0.3479 - val_pos_accuracy: 0.5022\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2066 - pos_accuracy: 0.6881 - val_loss: 0.2685 - val_pos_accuracy: 0.6652\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1538 - pos_accuracy: 0.7556 - val_loss: 0.2254 - val_pos_accuracy: 0.7277\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1391 - pos_accuracy: 0.7663 - val_loss: 0.2284 - val_pos_accuracy: 0.6830\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1395 - pos_accuracy: 0.7663 - val_loss: 0.2286 - val_pos_accuracy: 0.7098\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1289 - pos_accuracy: 0.7825 - val_loss: 0.2520 - val_pos_accuracy: 0.6049\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0975 - pos_accuracy: 0.8512 - val_loss: 0.1920 - val_pos_accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0947 - pos_accuracy: 0.8506 - val_loss: 0.1794 - val_pos_accuracy: 0.7567\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0859 - pos_accuracy: 0.8819 - val_loss: 0.2171 - val_pos_accuracy: 0.6987\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0904 - pos_accuracy: 0.8606 - val_loss: 0.1686 - val_pos_accuracy: 0.8036\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0747 - pos_accuracy: 0.9031 - val_loss: 0.1581 - val_pos_accuracy: 0.7969\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0998 - pos_accuracy: 0.8456 - val_loss: 0.1736 - val_pos_accuracy: 0.7701\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0604 - pos_accuracy: 0.9212 - val_loss: 0.1501 - val_pos_accuracy: 0.8058\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0917 - pos_accuracy: 0.8512 - val_loss: 0.1584 - val_pos_accuracy: 0.7946\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0597 - pos_accuracy: 0.9119 - val_loss: 0.1365 - val_pos_accuracy: 0.8170\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0548 - pos_accuracy: 0.9325 - val_loss: 0.1728 - val_pos_accuracy: 0.7768\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0581 - pos_accuracy: 0.9144 - val_loss: 0.1363 - val_pos_accuracy: 0.8170\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0588 - pos_accuracy: 0.9025 - val_loss: 0.1280 - val_pos_accuracy: 0.8192\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0403 - pos_accuracy: 0.9425 - val_loss: 0.1355 - val_pos_accuracy: 0.8214\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0352 - pos_accuracy: 0.9463 - val_loss: 0.1508 - val_pos_accuracy: 0.8080\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0625 - pos_accuracy: 0.9144 - val_loss: 0.1287 - val_pos_accuracy: 0.8326\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0480 - pos_accuracy: 0.9356 - val_loss: 0.1263 - val_pos_accuracy: 0.8348\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0495 - pos_accuracy: 0.9300 - val_loss: 0.1354 - val_pos_accuracy: 0.8259\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0441 - pos_accuracy: 0.9469 - val_loss: 0.1150 - val_pos_accuracy: 0.8527\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0370 - pos_accuracy: 0.9519 - val_loss: 0.1195 - val_pos_accuracy: 0.8460\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0259 - pos_accuracy: 0.9650 - val_loss: 0.1137 - val_pos_accuracy: 0.8415\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0375 - pos_accuracy: 0.9538 - val_loss: 0.1132 - val_pos_accuracy: 0.8460\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0356 - pos_accuracy: 0.9525 - val_loss: 0.1083 - val_pos_accuracy: 0.8460\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0224 - pos_accuracy: 0.9688 - val_loss: 0.1074 - val_pos_accuracy: 0.8594\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0225 - pos_accuracy: 0.9688 - val_loss: 0.1088 - val_pos_accuracy: 0.8504\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0248 - pos_accuracy: 0.9712 - val_loss: 0.1074 - val_pos_accuracy: 0.8460\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0410 - pos_accuracy: 0.9481 - val_loss: 0.1539 - val_pos_accuracy: 0.7143\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0290 - pos_accuracy: 0.9500 - val_loss: 0.1042 - val_pos_accuracy: 0.8549\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0183 - pos_accuracy: 0.9725 - val_loss: 0.1037 - val_pos_accuracy: 0.8482\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0208 - pos_accuracy: 0.9731 - val_loss: 0.1173 - val_pos_accuracy: 0.8527\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0180 - pos_accuracy: 0.9762 - val_loss: 0.0969 - val_pos_accuracy: 0.8638\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0147 - pos_accuracy: 0.9769 - val_loss: 0.1022 - val_pos_accuracy: 0.8661\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0188 - pos_accuracy: 0.9762 - val_loss: 0.0992 - val_pos_accuracy: 0.8638\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0153 - pos_accuracy: 0.9787 - val_loss: 0.1085 - val_pos_accuracy: 0.8504\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0206 - pos_accuracy: 0.9787 - val_loss: 0.0963 - val_pos_accuracy: 0.8661\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0135 - pos_accuracy: 0.9819 - val_loss: 0.0969 - val_pos_accuracy: 0.8683\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0140 - pos_accuracy: 0.9831 - val_loss: 0.0944 - val_pos_accuracy: 0.8616\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0233 - pos_accuracy: 0.9756 - val_loss: 0.1013 - val_pos_accuracy: 0.8661\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0161 - pos_accuracy: 0.9844 - val_loss: 0.0957 - val_pos_accuracy: 0.8616\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0186 - pos_accuracy: 0.9869 - val_loss: 0.0983 - val_pos_accuracy: 0.8772\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0127 - pos_accuracy: 0.9869 - val_loss: 0.0901 - val_pos_accuracy: 0.8705\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0091 - pos_accuracy: 0.9881 - val_loss: 0.0886 - val_pos_accuracy: 0.8728\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0086 - pos_accuracy: 0.9875 - val_loss: 0.0882 - val_pos_accuracy: 0.8795\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0083 - pos_accuracy: 0.9887 - val_loss: 0.0884 - val_pos_accuracy: 0.8728\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0078 - pos_accuracy: 0.9906 - val_loss: 0.0925 - val_pos_accuracy: 0.8795\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0108 - pos_accuracy: 0.9919 - val_loss: 0.0872 - val_pos_accuracy: 0.8795\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9925 - val_loss: 0.0877 - val_pos_accuracy: 0.8705\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0072 - pos_accuracy: 0.9919 - val_loss: 0.0859 - val_pos_accuracy: 0.8728\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0080 - pos_accuracy: 0.9931 - val_loss: 0.0955 - val_pos_accuracy: 0.8772\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0340 - pos_accuracy: 0.9525 - val_loss: 0.0993 - val_pos_accuracy: 0.8750\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0093 - pos_accuracy: 0.9931 - val_loss: 0.0856 - val_pos_accuracy: 0.8772\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0063 - pos_accuracy: 0.9931 - val_loss: 0.0854 - val_pos_accuracy: 0.8772\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0098 - pos_accuracy: 0.9925 - val_loss: 0.0842 - val_pos_accuracy: 0.8772\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9931 - val_loss: 0.0864 - val_pos_accuracy: 0.8683\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0202 - pos_accuracy: 0.9869 - val_loss: 0.0850 - val_pos_accuracy: 0.8772\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0068 - pos_accuracy: 0.9931 - val_loss: 0.0836 - val_pos_accuracy: 0.8929\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9937 - val_loss: 0.0865 - val_pos_accuracy: 0.8772\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0062 - pos_accuracy: 0.9944 - val_loss: 0.0844 - val_pos_accuracy: 0.8750\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0045 - pos_accuracy: 0.9950 - val_loss: 0.0815 - val_pos_accuracy: 0.8772\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9956 - val_loss: 0.0816 - val_pos_accuracy: 0.8750\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0046 - pos_accuracy: 0.9962 - val_loss: 0.0816 - val_pos_accuracy: 0.8795\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0057 - pos_accuracy: 0.9956 - val_loss: 0.0836 - val_pos_accuracy: 0.8750\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0045 - pos_accuracy: 0.9956 - val_loss: 0.0809 - val_pos_accuracy: 0.8772\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9962 - val_loss: 0.0801 - val_pos_accuracy: 0.8795\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0051 - pos_accuracy: 0.9962 - val_loss: 0.0828 - val_pos_accuracy: 0.8750\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0093 - pos_accuracy: 0.9962 - val_loss: 0.0797 - val_pos_accuracy: 0.8795\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9962 - val_loss: 0.0802 - val_pos_accuracy: 0.8795\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9962 - val_loss: 0.0803 - val_pos_accuracy: 0.8772\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9969 - val_loss: 0.0815 - val_pos_accuracy: 0.8906\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0033 - pos_accuracy: 0.9962 - val_loss: 0.0793 - val_pos_accuracy: 0.8772\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9969 - val_loss: 0.0819 - val_pos_accuracy: 0.8884\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0038 - pos_accuracy: 0.9962 - val_loss: 0.0794 - val_pos_accuracy: 0.8750\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0028 - pos_accuracy: 0.9969 - val_loss: 0.0804 - val_pos_accuracy: 0.8750\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0036 - pos_accuracy: 0.9975 - val_loss: 0.0789 - val_pos_accuracy: 0.8929\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9975 - val_loss: 0.0790 - val_pos_accuracy: 0.8795\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0043 - pos_accuracy: 0.9987 - val_loss: 0.0785 - val_pos_accuracy: 0.8906\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0061 - pos_accuracy: 0.9981 - val_loss: 0.0777 - val_pos_accuracy: 0.8728\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0039 - pos_accuracy: 0.9987 - val_loss: 0.0785 - val_pos_accuracy: 0.8906\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0027 - pos_accuracy: 0.9987 - val_loss: 0.0797 - val_pos_accuracy: 0.8750\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0041 - pos_accuracy: 0.9987 - val_loss: 0.0778 - val_pos_accuracy: 0.8772\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.0774 - val_pos_accuracy: 0.8772\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.0775 - val_pos_accuracy: 0.8884\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.0777 - val_pos_accuracy: 0.8772\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0029 - pos_accuracy: 0.9987 - val_loss: 0.0786 - val_pos_accuracy: 0.8884\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.0779 - val_pos_accuracy: 0.8884\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:34:52.823004: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1658: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate은 기준축에 따라 배열을 결합하는 역할을 합니다.\\naxis가 0이면 행기준, 1이면 열기준으로 병합입니다.\\n\",\n",
      "        \"\\n121은 1x2그리드에 첫 번째 subplot, 122는 두 번째 subplot을 말합니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all은 bool형식의 tensor의 차수를 줄이는 역할을 합니다.\\naxis=1은 열에 해당하는 차수를 줄인다는 뜻입니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 tf.cast를 통해 True면 1, False면 0으로 반환된 값들을 평균내어 정확도로 변환하는 역할을 합니다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유는 마지막 출력값이 0, 1로 두개로 분리되기 때문입니다.\\n\",\n",
      "        \"\\nrandom seed는 실행할때 마다 같은 random 값이 생성되도록 하는 역할을 합니다.\\n\",\n",
      "        false,\n",
      "        \"\\n과제를 통해 강의로만 들었던 내용을 실습할 수 있게 해주셔서 감사합니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 80.0,\n",
      "    \"accuracy\": 0.9085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\"report1/강은혜_46073.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/강은혜_46073.ipynb to python\n",
      "[NbConvertApp] Writing 37064 bytes to report1/강은혜_46073.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "w.shape: (2, 2)\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "x.shape: (3, 2)\n",
      "[[5 6]\n",
      " [7 8]\n",
      " [9 0]]\n",
      "x.T.shape: (2, 3)\n",
      "[[5 7 9]\n",
      " [6 8 0]]\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "y.shape: (2, 3)\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 307072.55it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:34:59.363332: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:34:59.737139: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:34:59.737181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:34:59.987591: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:35:00.615128: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "y_true.shape: (4, 2)\n",
      "label_true : tf.Tensor(\n",
      "[[10. 10.]\n",
      " [20. 20.]\n",
      " [ 2. 19.]\n",
      " [28.  2.]], shape=(4, 2), dtype=float32)\n",
      "label_pred : tf.Tensor(\n",
      "[[10. 10.]\n",
      " [20. 36.]\n",
      " [ 2. 19.]\n",
      " [28.  2.]], shape=(4, 2), dtype=float32)\n",
      "is_correct.shape: (4,)\n",
      "is_correct = [ True False  True  True]\n",
      "is_correct_after_cast= [1. 0. 1. 1.]\n",
      "score: tf.Tensor(0.75, shape=(), dtype=float32)\n",
      "score.numpy(): 0.75\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 135265.22it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,066,754\n",
      "Trainable params: 1,066,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 1s 4ms/step - loss: 48.4132 - pos_accuracy: 0.0225 - val_loss: 4.8896 - val_pos_accuracy: 0.0250\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 1.8177 - pos_accuracy: 0.1894 - val_loss: 2.0902 - val_pos_accuracy: 0.2600\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.0360 - pos_accuracy: 0.3119 - val_loss: 0.4448 - val_pos_accuracy: 0.5225\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3268 - pos_accuracy: 0.5756 - val_loss: 0.2490 - val_pos_accuracy: 0.7200\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.2110 - pos_accuracy: 0.7219 - val_loss: 0.1621 - val_pos_accuracy: 0.8150\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.1571 - pos_accuracy: 0.7819 - val_loss: 0.1873 - val_pos_accuracy: 0.7650\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0709 - pos_accuracy: 0.9013 - val_loss: 0.1173 - val_pos_accuracy: 0.8850\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0556 - pos_accuracy: 0.9275 - val_loss: 0.0944 - val_pos_accuracy: 0.8975\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0419 - pos_accuracy: 0.9613 - val_loss: 0.1747 - val_pos_accuracy: 0.7650\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0364 - pos_accuracy: 0.9594 - val_loss: 0.0949 - val_pos_accuracy: 0.8900\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0251 - pos_accuracy: 0.9781 - val_loss: 0.0872 - val_pos_accuracy: 0.9125\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0200 - pos_accuracy: 0.9819 - val_loss: 0.0702 - val_pos_accuracy: 0.9250\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0158 - pos_accuracy: 0.9906 - val_loss: 0.0780 - val_pos_accuracy: 0.9225\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0117 - pos_accuracy: 0.9919 - val_loss: 0.0689 - val_pos_accuracy: 0.9450\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9962 - val_loss: 0.0670 - val_pos_accuracy: 0.9425\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0075 - pos_accuracy: 0.9962 - val_loss: 0.0622 - val_pos_accuracy: 0.9425\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9975 - val_loss: 0.0586 - val_pos_accuracy: 0.9475\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9975 - val_loss: 0.0576 - val_pos_accuracy: 0.9575\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0037 - pos_accuracy: 0.9975 - val_loss: 0.0580 - val_pos_accuracy: 0.9425\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0035 - pos_accuracy: 0.9987 - val_loss: 0.0559 - val_pos_accuracy: 0.9425\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:35:09.872524: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "out_images.shape: (2, 80, 80, 3)\n",
      "out_image_1.shape: (80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixels average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1710: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할: numpy array를 axis 설정 방향으로 병합\\naxis=1의 의미: 2차원 배열에서 axis=1은 열 방향 병합을 의미한다. 반대로 axis=0은 행 방향 병합을 뜻한다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미: 121은 서브플롯을 1행 2열 구조로 배치할 때 그 중 첫번째 플롯이라는 뜻이고 122는 1행 2열 배치 중 두번째 플롯이라는 의미이다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할: 주어진 axis 방향에 대해 AND가 성립할 경우 True, 아니면 False를 반환. \\ninput으로 label_true == label_pred를 주었으므로 tf.Tensor 기준 행 (axis=0) 또는 열 (axis=1) 방향으로 모든 숫자가 같은지 판별한다. \\n\\naxis=1 의 의미: 결과물이 [x,y]의 좌표를 수직방향으로 나열한 Tensor이므로 axis=1로 하여 열 방향으로 모든 숫자가 일치할 경우, 즉 true와 pred의 각 [x,y] 가 같을 경우 True를 반환한다. \\n(axis=0일 경우 true-pred에 대해 행 방향으로 숫자가 모두 일치해야, 즉 x끼리 모두 같거나 y끼리 모두 같아야 True가 됨)\\n\",\n",
      "        \"\\ntf.reduce_mean: true와 pred가 같은 경우 1, 같지 않은 경우 (예측 실패) 0으로 표기한 tensor의 전체 평균이므로 예측이 성공한 비율을 의미한다. \\nex. 100 개 test set에 대해 87번 true==pred일 경우 1이 87개, 0이 13개로 이루어진 tensor를 결과물로 얻게 된다. (True-False를 float로 type 변환함)\\n이 tensor의 평균값은 (87/100) = 0.87 이며 87%의 예측 성공률을 의미한다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유: [x,y] 좌표는 특정 카테고리로의 분류가 아닌 연속성을 갖는 데이터이므로 선형회귀로 예측이 수행되어야한다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유: 본 과제는 랜덤생성 데이터로 모델을 학습시키는 것이므로, 데이터를 고정하여 모델 최적화 과정에서 불필요한 변동성을 피할 수 있다.\\n파라미터 최적화가 이루어지지 않은 상태에서 데이터가 계속 변화하면 run 마다 다른 정확도를 도출하여 모델 개선 시간이 더 걸릴 수 있다.\\n\\n\",\n",
      "        true,\n",
      "        \"\\n그 동안 정성적으로 배우던 딥러닝을 실제 파이썬을 이용해서 실습할 수 있어서 매우 재미있습니다.\\n과제 템플릿 코드가 공부에 도움이 많이 되고, 과제에서 핵심적으로 익혀야 하는 부분을 적절히 설정해주셔서 학습 성취도가 높다고 생각됩니다.\\n코드 구조 이해 및 다양한 함수 습득이 가능하였습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.9625\n",
      "}\"report1/김창훈_46030.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김창훈_46030.ipynb to python\n",
      "[NbConvertApp] Writing 35644 bytes to report1/김창훈_46030.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(3, 2)\n",
      "[[5. 7. 9.]\n",
      " [6. 8. 0.]]\n",
      "(2, 3)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 285676.61it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:35:18.328047: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:35:18.725549: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:35:18.725587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:35:18.958895: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:35:19.581267: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 104921.86it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 27,362\n",
      "Trainable params: 27,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0.5\n",
      "Epoch 1/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 6ms/step - loss: 100.6692 - pos_accuracy: 0.0063 - val_loss: 13.2929 - val_pos_accuracy: 0.0201\n",
      "Epoch 2/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 15.1807 - pos_accuracy: 0.0437 - val_loss: 7.6002 - val_pos_accuracy: 0.0335\n",
      "Epoch 3/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.0071 - pos_accuracy: 0.1063 - val_loss: 3.9934 - val_pos_accuracy: 0.0446\n",
      "Epoch 4/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1694 - pos_accuracy: 0.2331 - val_loss: 1.1137 - val_pos_accuracy: 0.2210\n",
      "Epoch 5/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1639 - pos_accuracy: 0.2387 - val_loss: 1.1369 - val_pos_accuracy: 0.1875\n",
      "Epoch 6/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2854 - pos_accuracy: 0.2125 - val_loss: 0.6503 - val_pos_accuracy: 0.4129\n",
      "Epoch 7/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2.5062 - pos_accuracy: 0.1781 - val_loss: 0.8359 - val_pos_accuracy: 0.2701\n",
      "Epoch 8/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4674 - pos_accuracy: 0.4050 - val_loss: 0.5531 - val_pos_accuracy: 0.3795\n",
      "Epoch 9/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2671 - pos_accuracy: 0.5806 - val_loss: 0.4459 - val_pos_accuracy: 0.4933\n",
      "Epoch 10/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2197 - pos_accuracy: 0.6556 - val_loss: 0.3471 - val_pos_accuracy: 0.6004\n",
      "Epoch 11/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2550 - pos_accuracy: 0.5881 - val_loss: 0.5259 - val_pos_accuracy: 0.4219\n",
      "Epoch 12/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2520 - pos_accuracy: 0.6100 - val_loss: 0.3049 - val_pos_accuracy: 0.6339\n",
      "Epoch 13/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1470 - pos_accuracy: 0.7706 - val_loss: 0.2734 - val_pos_accuracy: 0.6652\n",
      "Epoch 14/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1436 - pos_accuracy: 0.7594 - val_loss: 0.2670 - val_pos_accuracy: 0.6406\n",
      "Epoch 15/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1696 - pos_accuracy: 0.6994 - val_loss: 0.3653 - val_pos_accuracy: 0.5737\n",
      "Epoch 16/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3168 - pos_accuracy: 0.5119 - val_loss: 0.3483 - val_pos_accuracy: 0.4375\n",
      "Epoch 17/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1168 - pos_accuracy: 0.8100 - val_loss: 0.2392 - val_pos_accuracy: 0.6942\n",
      "Epoch 18/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1057 - pos_accuracy: 0.8275 - val_loss: 0.2063 - val_pos_accuracy: 0.7589\n",
      "Epoch 19/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0940 - pos_accuracy: 0.8581 - val_loss: 0.1845 - val_pos_accuracy: 0.7812\n",
      "Epoch 20/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1271 - pos_accuracy: 0.8006 - val_loss: 0.2202 - val_pos_accuracy: 0.7634\n",
      "Epoch 21/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1390 - pos_accuracy: 0.7588 - val_loss: 0.2170 - val_pos_accuracy: 0.7054\n",
      "Epoch 22/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1543 - pos_accuracy: 0.7481 - val_loss: 0.1880 - val_pos_accuracy: 0.7723\n",
      "Epoch 23/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0663 - pos_accuracy: 0.9025 - val_loss: 0.1523 - val_pos_accuracy: 0.8147\n",
      "Epoch 24/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0666 - pos_accuracy: 0.8988 - val_loss: 0.1492 - val_pos_accuracy: 0.8371\n",
      "Epoch 25/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0611 - pos_accuracy: 0.9137 - val_loss: 0.1469 - val_pos_accuracy: 0.8281\n",
      "Epoch 26/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0533 - pos_accuracy: 0.9244 - val_loss: 0.1408 - val_pos_accuracy: 0.8281\n",
      "Epoch 27/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0529 - pos_accuracy: 0.9262 - val_loss: 0.1395 - val_pos_accuracy: 0.8326\n",
      "Epoch 28/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0620 - pos_accuracy: 0.9119 - val_loss: 0.1346 - val_pos_accuracy: 0.8326\n",
      "Epoch 29/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0510 - pos_accuracy: 0.9237 - val_loss: 0.1387 - val_pos_accuracy: 0.8304\n",
      "Epoch 30/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0434 - pos_accuracy: 0.9394 - val_loss: 0.1957 - val_pos_accuracy: 0.7254\n",
      "Epoch 31/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1081 - pos_accuracy: 0.8219 - val_loss: 0.1289 - val_pos_accuracy: 0.8527\n",
      "Epoch 32/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0404 - pos_accuracy: 0.9406 - val_loss: 0.1215 - val_pos_accuracy: 0.8683\n",
      "Epoch 33/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0382 - pos_accuracy: 0.9431 - val_loss: 0.1310 - val_pos_accuracy: 0.8616\n",
      "Epoch 34/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0537 - pos_accuracy: 0.9187 - val_loss: 0.1130 - val_pos_accuracy: 0.8683\n",
      "Epoch 35/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0378 - pos_accuracy: 0.9469 - val_loss: 0.1265 - val_pos_accuracy: 0.8683\n",
      "Epoch 36/68\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0300 - pos_accuracy: 0.9575 - val_loss: 0.1115 - val_pos_accuracy: 0.8750\n",
      "Epoch 37/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0293 - pos_accuracy: 0.9613 - val_loss: 0.1067 - val_pos_accuracy: 0.8929\n",
      "Epoch 38/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0263 - pos_accuracy: 0.9625 - val_loss: 0.1047 - val_pos_accuracy: 0.8862\n",
      "Epoch 39/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0239 - pos_accuracy: 0.9650 - val_loss: 0.1017 - val_pos_accuracy: 0.8862\n",
      "Epoch 40/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0242 - pos_accuracy: 0.9700 - val_loss: 0.1023 - val_pos_accuracy: 0.8862\n",
      "Epoch 41/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0239 - pos_accuracy: 0.9706 - val_loss: 0.0994 - val_pos_accuracy: 0.8996\n",
      "Epoch 42/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0249 - pos_accuracy: 0.9694 - val_loss: 0.1163 - val_pos_accuracy: 0.8862\n",
      "Epoch 43/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0234 - pos_accuracy: 0.9781 - val_loss: 0.0981 - val_pos_accuracy: 0.9062\n",
      "Epoch 44/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0203 - pos_accuracy: 0.9775 - val_loss: 0.0963 - val_pos_accuracy: 0.8973\n",
      "Epoch 45/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0258 - pos_accuracy: 0.9744 - val_loss: 0.1107 - val_pos_accuracy: 0.8795\n",
      "Epoch 46/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0202 - pos_accuracy: 0.9819 - val_loss: 0.0974 - val_pos_accuracy: 0.9040\n",
      "Epoch 47/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0174 - pos_accuracy: 0.9837 - val_loss: 0.0932 - val_pos_accuracy: 0.9085\n",
      "Epoch 48/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0167 - pos_accuracy: 0.9837 - val_loss: 0.0972 - val_pos_accuracy: 0.9018\n",
      "Epoch 49/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0186 - pos_accuracy: 0.9837 - val_loss: 0.0949 - val_pos_accuracy: 0.9085\n",
      "Epoch 50/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0181 - pos_accuracy: 0.9844 - val_loss: 0.0945 - val_pos_accuracy: 0.8862\n",
      "Epoch 51/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0183 - pos_accuracy: 0.9862 - val_loss: 0.0893 - val_pos_accuracy: 0.9107\n",
      "Epoch 52/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0161 - pos_accuracy: 0.9856 - val_loss: 0.0903 - val_pos_accuracy: 0.9085\n",
      "Epoch 53/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0153 - pos_accuracy: 0.9875 - val_loss: 0.0887 - val_pos_accuracy: 0.9040\n",
      "Epoch 54/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0145 - pos_accuracy: 0.9881 - val_loss: 0.0885 - val_pos_accuracy: 0.9062\n",
      "Epoch 55/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0162 - pos_accuracy: 0.9894 - val_loss: 0.0909 - val_pos_accuracy: 0.9129\n",
      "Epoch 56/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0179 - pos_accuracy: 0.9862 - val_loss: 0.0902 - val_pos_accuracy: 0.8929\n",
      "Epoch 57/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0187 - pos_accuracy: 0.9869 - val_loss: 0.0847 - val_pos_accuracy: 0.9107\n",
      "Epoch 58/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0120 - pos_accuracy: 0.9912 - val_loss: 0.0838 - val_pos_accuracy: 0.9129\n",
      "Epoch 59/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0104 - pos_accuracy: 0.9900 - val_loss: 0.0853 - val_pos_accuracy: 0.9085\n",
      "Epoch 60/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9894 - val_loss: 0.1111 - val_pos_accuracy: 0.9018\n",
      "Epoch 61/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0543 - pos_accuracy: 0.9375 - val_loss: 0.0912 - val_pos_accuracy: 0.8996\n",
      "Epoch 62/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0168 - pos_accuracy: 0.9900 - val_loss: 0.0852 - val_pos_accuracy: 0.9152\n",
      "Epoch 63/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0205 - pos_accuracy: 0.9894 - val_loss: 0.0812 - val_pos_accuracy: 0.9152\n",
      "Epoch 64/68\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0105 - pos_accuracy: 0.9925 - val_loss: 0.0879 - val_pos_accuracy: 0.9152\n",
      "Epoch 65/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0177 - pos_accuracy: 0.9912 - val_loss: 0.0824 - val_pos_accuracy: 0.9018\n",
      "Epoch 66/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0093 - pos_accuracy: 0.9912 - val_loss: 0.0803 - val_pos_accuracy: 0.9129\n",
      "Epoch 67/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0084 - pos_accuracy: 0.9919 - val_loss: 0.0799 - val_pos_accuracy: 0.9129\n",
      "Epoch 68/68\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0089 - pos_accuracy: 0.9925 - val_loss: 0.0790 - val_pos_accuracy: 0.9129\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:35:29.255660: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1694: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할은 선택한 축방향으로 배열을 연결해주는 메소드이다.\\naxis=1의 의미는 2차원 배열에서 열방향으로 선택한 배열을 연결해주는 것을 의미한다.\\n\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미는 subplot 함수의 row, column, index를 순서대로 나열한것을 의미한다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n지정된 축방향의 각 요소의 논리 and 연산을 하는데 사용된다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\ntensor의 평균값을 계산하는데 사용된다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n출력값이 2차원 선형으로 표현되기 때문이다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n재현가능한 난수를 구현하기 위해 random seed[0]을 사용한다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.9174\n",
      "}\"report1/정영욱_46059.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/정영욱_46059.ipynb to python\n",
      "[NbConvertApp] Writing 37953 bytes to report1/정영욱_46059.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -18. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 300011.02it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:35:35.687792: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:35:36.077659: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:35:36.077697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:35:36.300204: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:35:36.928058: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 208547.33it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 26,242\n",
      "Trainable params: 26,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 40.6631 - pos_accuracy: 0.0275 - val_loss: 3.6616 - val_pos_accuracy: 0.1130\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.2156 - pos_accuracy: 0.1256 - val_loss: 1.4164 - val_pos_accuracy: 0.2188\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.2002 - pos_accuracy: 0.1469 - val_loss: 3.2508 - val_pos_accuracy: 0.0673\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.0432 - pos_accuracy: 0.2537 - val_loss: 0.8619 - val_pos_accuracy: 0.3149\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.7982 - pos_accuracy: 0.3131 - val_loss: 0.8931 - val_pos_accuracy: 0.3125\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.7271 - pos_accuracy: 0.3525 - val_loss: 0.6427 - val_pos_accuracy: 0.4327\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6740 - pos_accuracy: 0.3688 - val_loss: 0.6545 - val_pos_accuracy: 0.3942\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5517 - pos_accuracy: 0.3406 - val_loss: 0.7907 - val_pos_accuracy: 0.2933\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4399 - pos_accuracy: 0.4550 - val_loss: 0.5033 - val_pos_accuracy: 0.4784\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3618 - pos_accuracy: 0.5031 - val_loss: 0.5120 - val_pos_accuracy: 0.4519\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3139 - pos_accuracy: 0.5494 - val_loss: 0.4512 - val_pos_accuracy: 0.5240\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2778 - pos_accuracy: 0.5788 - val_loss: 0.3587 - val_pos_accuracy: 0.6082\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2647 - pos_accuracy: 0.5675 - val_loss: 0.3971 - val_pos_accuracy: 0.5625\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2821 - pos_accuracy: 0.5462 - val_loss: 0.3397 - val_pos_accuracy: 0.6659\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1710 - pos_accuracy: 0.7206 - val_loss: 0.3552 - val_pos_accuracy: 0.6202\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1849 - pos_accuracy: 0.6775 - val_loss: 0.5150 - val_pos_accuracy: 0.4543\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2090 - pos_accuracy: 0.6388 - val_loss: 0.3770 - val_pos_accuracy: 0.5240\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1423 - pos_accuracy: 0.7688 - val_loss: 0.3248 - val_pos_accuracy: 0.6274\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1361 - pos_accuracy: 0.7656 - val_loss: 0.2902 - val_pos_accuracy: 0.6707\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1204 - pos_accuracy: 0.7981 - val_loss: 0.3023 - val_pos_accuracy: 0.6611\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1163 - pos_accuracy: 0.8044 - val_loss: 0.2546 - val_pos_accuracy: 0.7260\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1168 - pos_accuracy: 0.8081 - val_loss: 0.2701 - val_pos_accuracy: 0.6803\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0966 - pos_accuracy: 0.8494 - val_loss: 0.2728 - val_pos_accuracy: 0.6899\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0959 - pos_accuracy: 0.8531 - val_loss: 0.2447 - val_pos_accuracy: 0.7572\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0805 - pos_accuracy: 0.8806 - val_loss: 0.2778 - val_pos_accuracy: 0.6803\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0854 - pos_accuracy: 0.8844 - val_loss: 0.2540 - val_pos_accuracy: 0.7428\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0705 - pos_accuracy: 0.9031 - val_loss: 0.2414 - val_pos_accuracy: 0.7548\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0640 - pos_accuracy: 0.9144 - val_loss: 0.2225 - val_pos_accuracy: 0.7933\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0671 - pos_accuracy: 0.8981 - val_loss: 0.2290 - val_pos_accuracy: 0.7981\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0589 - pos_accuracy: 0.9187 - val_loss: 0.3071 - val_pos_accuracy: 0.6154\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0855 - pos_accuracy: 0.8594 - val_loss: 0.2098 - val_pos_accuracy: 0.7885\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0583 - pos_accuracy: 0.9181 - val_loss: 0.2515 - val_pos_accuracy: 0.7019\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0562 - pos_accuracy: 0.9206 - val_loss: 0.1978 - val_pos_accuracy: 0.8173\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0460 - pos_accuracy: 0.9431 - val_loss: 0.1979 - val_pos_accuracy: 0.8077\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0482 - pos_accuracy: 0.9356 - val_loss: 0.1960 - val_pos_accuracy: 0.8149\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0384 - pos_accuracy: 0.9544 - val_loss: 0.1894 - val_pos_accuracy: 0.8245\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0378 - pos_accuracy: 0.9600 - val_loss: 0.1911 - val_pos_accuracy: 0.8293\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0476 - pos_accuracy: 0.9431 - val_loss: 0.1907 - val_pos_accuracy: 0.8534\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0394 - pos_accuracy: 0.9588 - val_loss: 0.2346 - val_pos_accuracy: 0.7572\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0310 - pos_accuracy: 0.9688 - val_loss: 0.1847 - val_pos_accuracy: 0.8293\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0289 - pos_accuracy: 0.9756 - val_loss: 0.1866 - val_pos_accuracy: 0.8365\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0287 - pos_accuracy: 0.9737 - val_loss: 0.1901 - val_pos_accuracy: 0.8365\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0264 - pos_accuracy: 0.9762 - val_loss: 0.1854 - val_pos_accuracy: 0.8486\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0242 - pos_accuracy: 0.9787 - val_loss: 0.1841 - val_pos_accuracy: 0.8389\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0297 - pos_accuracy: 0.9750 - val_loss: 0.1894 - val_pos_accuracy: 0.8582\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0259 - pos_accuracy: 0.9781 - val_loss: 0.1782 - val_pos_accuracy: 0.8341\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0203 - pos_accuracy: 0.9850 - val_loss: 0.2033 - val_pos_accuracy: 0.8077\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0271 - pos_accuracy: 0.9794 - val_loss: 0.1773 - val_pos_accuracy: 0.8486\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0301 - pos_accuracy: 0.9700 - val_loss: 0.1743 - val_pos_accuracy: 0.8438\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9856 - val_loss: 0.1719 - val_pos_accuracy: 0.8462\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0168 - pos_accuracy: 0.9881 - val_loss: 0.1807 - val_pos_accuracy: 0.8534\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0174 - pos_accuracy: 0.9881 - val_loss: 0.1683 - val_pos_accuracy: 0.8558\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0213 - pos_accuracy: 0.9844 - val_loss: 0.1706 - val_pos_accuracy: 0.8486\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0171 - pos_accuracy: 0.9862 - val_loss: 0.1730 - val_pos_accuracy: 0.8462\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0143 - pos_accuracy: 0.9887 - val_loss: 0.1680 - val_pos_accuracy: 0.8534\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0131 - pos_accuracy: 0.9906 - val_loss: 0.1671 - val_pos_accuracy: 0.8510\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0130 - pos_accuracy: 0.9900 - val_loss: 0.1681 - val_pos_accuracy: 0.8558\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0128 - pos_accuracy: 0.9931 - val_loss: 0.1650 - val_pos_accuracy: 0.8582\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0145 - pos_accuracy: 0.9912 - val_loss: 0.1637 - val_pos_accuracy: 0.8606\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0120 - pos_accuracy: 0.9919 - val_loss: 0.1655 - val_pos_accuracy: 0.8558\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0163 - pos_accuracy: 0.9912 - val_loss: 0.1638 - val_pos_accuracy: 0.8606\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0119 - pos_accuracy: 0.9931 - val_loss: 0.1718 - val_pos_accuracy: 0.8462\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0100 - pos_accuracy: 0.9937 - val_loss: 0.1616 - val_pos_accuracy: 0.8606\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0116 - pos_accuracy: 0.9950 - val_loss: 0.1712 - val_pos_accuracy: 0.8486\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0100 - pos_accuracy: 0.9937 - val_loss: 0.1671 - val_pos_accuracy: 0.8606\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0084 - pos_accuracy: 0.9956 - val_loss: 0.1626 - val_pos_accuracy: 0.8606\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0100 - pos_accuracy: 0.9944 - val_loss: 0.1609 - val_pos_accuracy: 0.8606\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0084 - pos_accuracy: 0.9950 - val_loss: 0.1631 - val_pos_accuracy: 0.8606\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0090 - pos_accuracy: 0.9944 - val_loss: 0.1601 - val_pos_accuracy: 0.8606\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0077 - pos_accuracy: 0.9950 - val_loss: 0.1608 - val_pos_accuracy: 0.8606\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0077 - pos_accuracy: 0.9950 - val_loss: 0.1588 - val_pos_accuracy: 0.8606\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0066 - pos_accuracy: 0.9944 - val_loss: 0.1597 - val_pos_accuracy: 0.8606\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0069 - pos_accuracy: 0.9956 - val_loss: 0.1578 - val_pos_accuracy: 0.8606\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0076 - pos_accuracy: 0.9950 - val_loss: 0.1634 - val_pos_accuracy: 0.8630\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0123 - pos_accuracy: 0.9937 - val_loss: 0.1616 - val_pos_accuracy: 0.8606\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9956 - val_loss: 0.1643 - val_pos_accuracy: 0.8606\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9969 - val_loss: 0.1594 - val_pos_accuracy: 0.8654\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0078 - pos_accuracy: 0.9975 - val_loss: 0.1560 - val_pos_accuracy: 0.8606\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9981 - val_loss: 0.1562 - val_pos_accuracy: 0.8606\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9975 - val_loss: 0.1545 - val_pos_accuracy: 0.8606\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9981 - val_loss: 0.1552 - val_pos_accuracy: 0.8606\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9981 - val_loss: 0.1590 - val_pos_accuracy: 0.8654\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9981 - val_loss: 0.1551 - val_pos_accuracy: 0.8606\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9981 - val_loss: 0.1572 - val_pos_accuracy: 0.8606\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9981 - val_loss: 0.1576 - val_pos_accuracy: 0.8630\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9981 - val_loss: 0.1589 - val_pos_accuracy: 0.8606\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9981 - val_loss: 0.1621 - val_pos_accuracy: 0.8582\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9981 - val_loss: 0.1603 - val_pos_accuracy: 0.8606\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9981 - val_loss: 0.1566 - val_pos_accuracy: 0.8582\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9981 - val_loss: 0.1540 - val_pos_accuracy: 0.8606\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9981 - val_loss: 0.1544 - val_pos_accuracy: 0.8606\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1519 - val_pos_accuracy: 0.8606\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9981 - val_loss: 0.1519 - val_pos_accuracy: 0.8606\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9981 - val_loss: 0.1534 - val_pos_accuracy: 0.8606\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1527 - val_pos_accuracy: 0.8606\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9987 - val_loss: 0.1508 - val_pos_accuracy: 0.8606\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9987 - val_loss: 0.1511 - val_pos_accuracy: 0.8606\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9987 - val_loss: 0.1529 - val_pos_accuracy: 0.8606\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9994 - val_loss: 0.1507 - val_pos_accuracy: 0.8606\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9987 - val_loss: 0.1514 - val_pos_accuracy: 0.8606\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:35:57.982885: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "[[0.79607843 0.79215686 0.79215686]\n",
      " [0.8        0.79607843 0.78823529]\n",
      " [0.8        0.79607843 0.79607843]]\n",
      "0.7952069716775599\n",
      "0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1774: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할 : 선택한 축방향으로 배열을 연결해주는 역할과\\naxis=1의 의미 : 객체를 연결하는 방향이 열방향임을 의미 \\n\",\n",
      "        \"\\nsubplot에서 121의 의미 : 1행째의 2열의 첫번째 \\nsubplot에서 122의 의미 : 1행째의 2열의 두번째 \\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할 : Loss를 파악하여 모델의 정확성 판단하기 위해 사용\\n\",\n",
      "        false,\n",
      "        \"\\nrandom seed의 사용 이유 : 재현 가능한 난수 사용으로 인하여 모델 평가/검증 가능\\n\",\n",
      "        false,\n",
      "        \"\\n\\n\"\n",
      "    ],\n",
      "    \"score\": 69.69696969696969,\n",
      "    \"accuracy\": 0.863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\"report1/최원석_46047.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/최원석_46047.ipynb to python\n",
      "[NbConvertApp] Writing 37484 bytes to report1/최원석_46047.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(2,)\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:246: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(2,)\n",
      "0\n",
      "[0.17 0.39]\n",
      "1\n",
      "[0.095 0.207]\n",
      "2\n",
      "[0.0509 0.1113]\n",
      "3\n",
      "[0.02735 0.05979]\n",
      "4\n",
      "[0.014693 0.032121]\n",
      "5\n",
      "[0.0078935 0.0172563]\n",
      "6\n",
      "[0.00424061 0.00927057]\n",
      "7\n",
      "[0.00227818 0.00498041]\n",
      "8\n",
      "[0.0012239  0.00267562]\n",
      "9\n",
      "[0.00065751 0.00143742]\n",
      "10\n",
      "[0.00035323 0.00077222]\n",
      "11\n",
      "[0.00018977 0.00041486]\n",
      "12\n",
      "[0.00010195 0.00022287]\n",
      "13\n",
      "[5.47695991e-05 1.19734048e-04]\n",
      "14\n",
      "[2.94237694e-05 6.43244988e-05]\n",
      "15\n",
      "[1.58072767e-05 3.45569303e-05]\n",
      "16\n",
      "[8.49211374e-06 1.85649551e-05]\n",
      "17\n",
      "[4.56220240e-06 9.97361618e-06]\n",
      "18\n",
      "[2.45094348e-06 5.35810719e-06]\n",
      "19\n",
      "[1.31671579e-06 2.87852592e-06]\n",
      "20\n",
      "[7.07376763e-07 1.54642510e-06]\n",
      "21\n",
      "[3.80022697e-07 8.30783070e-07]\n",
      "22\n",
      "[2.04158884e-07 4.46320037e-07]\n",
      "23\n",
      "[1.09679896e-07 2.39775680e-07]\n",
      "24\n",
      "[5.89231256e-08 1.28814241e-07]\n",
      "25\n",
      "[3.16551607e-08 6.92026340e-08]\n",
      "26\n",
      "[1.70060429e-08 3.71776018e-08]\n",
      "27\n",
      "[9.13612465e-09 1.99728536e-08]\n",
      "28\n",
      "[4.90818318e-09 1.07299788e-08]\n",
      "29\n",
      "[2.63681408e-09 5.76444648e-09]\n",
      "30\n",
      "[1.41657071e-09 3.09682282e-09]\n",
      "31\n",
      "[7.61021634e-10 1.66370034e-09]\n",
      "32\n",
      "[4.08842231e-10 8.93786626e-10]\n",
      "33\n",
      "[2.19641548e-10 4.80167320e-10]\n",
      "34\n",
      "[1.17997619e-10 2.57959392e-10]\n",
      "35\n",
      "[6.33916404e-11 1.38583043e-10]\n",
      "36\n",
      "[3.40557726e-11 7.44507091e-11]\n",
      "37\n",
      "[1.82957191e-11 3.99970154e-11]\n",
      "38\n",
      "[9.82897499e-12 2.14875219e-11]\n",
      "39\n",
      "[5.28040188e-12 1.15437013e-11]\n",
      "40\n",
      "[2.83678044e-12 6.20160107e-12]\n",
      "41\n",
      "[1.52399826e-12 3.33167456e-12]\n",
      "42\n",
      "[8.18734737e-13 1.78986930e-12]\n",
      "43\n",
      "[4.39847334e-13 9.61568141e-13]\n",
      "44\n",
      "[2.36298362e-13 5.16581457e-13]\n",
      "45\n",
      "[1.26946127e-13 2.77522091e-13]\n",
      "46\n",
      "[6.81990310e-14 1.49092675e-13]\n",
      "47\n",
      "[3.66384380e-14 8.00967792e-14]\n",
      "48\n",
      "[1.96831996e-14 4.30302431e-14]\n",
      "49\n",
      "[1.05743686e-14 2.31170571e-14]\n",
      "50\n",
      "[5.68084828e-15 1.24191334e-14]\n",
      "51\n",
      "[3.05191151e-15 6.67190785e-15]\n",
      "52\n",
      "[1.63957272e-15 3.58433660e-15]\n",
      "53\n",
      "[8.80824591e-16 1.92560646e-15]\n",
      "54\n",
      "[4.73203750e-16 1.03448996e-15]\n",
      "55\n",
      "[2.54218367e-16 5.55757109e-16]\n",
      "56\n",
      "[1.36573258e-16 2.98568354e-16]\n",
      "57\n",
      "[7.33709966e-17 1.60399319e-16]\n",
      "58\n",
      "[3.94169635e-17 8.61710266e-17]\n",
      "59\n",
      "[2.11759017e-17 4.62934997e-17]\n",
      "60\n",
      "[1.13762901e-17 2.48701704e-17]\n",
      "61\n",
      "[6.11166308e-18 1.33609552e-17]\n",
      "62\n",
      "[3.28335734e-18 7.17788099e-18]\n",
      "63\n",
      "[1.76391193e-18 3.85615960e-18]\n",
      "64\n",
      "[9.47623113e-19 2.07163742e-18]\n",
      "65\n",
      "[5.09089795e-19 1.11294190e-18]\n",
      "66\n",
      "[2.73497360e-19 5.97903699e-19]\n",
      "67\n",
      "[1.46930476e-19 3.21210688e-19]\n",
      "68\n",
      "[7.89351851e-20 1.72563418e-19]\n",
      "69\n",
      "[4.24062021e-20 9.27059227e-20]\n",
      "70\n",
      "[2.27818047e-20 4.98042297e-20]\n",
      "71\n",
      "[1.22390264e-20 2.67562333e-20]\n",
      "72\n",
      "[6.57514930e-21 1.43742012e-20]\n",
      "73\n",
      "[3.53235518e-21 7.72222529e-21]\n",
      "74\n",
      "[1.89768058e-21 4.14859667e-21]\n",
      "75\n",
      "[1.01948739e-21 2.22874284e-21]\n",
      "76\n",
      "[5.47697307e-22 1.19734335e-21]\n",
      "77\n",
      "[2.94238401e-22 6.43246534e-22]\n",
      "78\n",
      "[1.58073147e-22 3.45570134e-22]\n",
      "79\n",
      "[8.49213415e-23 1.85649998e-22]\n",
      "80\n",
      "[4.56221337e-23 9.97364015e-23]\n",
      "81\n",
      "[2.45094937e-23 5.35812007e-23]\n",
      "82\n",
      "[1.31671895e-23 2.87853284e-23]\n",
      "83\n",
      "[7.07378463e-24 1.54642882e-23]\n",
      "84\n",
      "[3.80023610e-24 8.30785067e-24]\n",
      "85\n",
      "[2.04159374e-24 4.46321110e-24]\n",
      "86\n",
      "[1.09680159e-24 2.39776256e-24]\n",
      "87\n",
      "[5.89232672e-25 1.28814550e-24]\n",
      "88\n",
      "[3.16552368e-25 6.92028003e-25]\n",
      "89\n",
      "[1.70060837e-25 3.71776912e-25]\n",
      "90\n",
      "[9.13614660e-26 1.99729016e-25]\n",
      "91\n",
      "[4.90819498e-26 1.07300046e-25]\n",
      "92\n",
      "[2.63682042e-26 5.76446034e-26]\n",
      "93\n",
      "[1.41657411e-26 3.09683026e-26]\n",
      "94\n",
      "[7.61023463e-27 1.66370434e-26]\n",
      "95\n",
      "[4.08843214e-27 8.93788774e-27]\n",
      "96\n",
      "[2.19642076e-27 4.80168474e-27]\n",
      "97\n",
      "[1.17997902e-27 2.57960012e-27]\n",
      "98\n",
      "[6.33917927e-28 1.38583376e-27]\n",
      "99\n",
      "[3.40558544e-28 7.44508881e-28]\n",
      "100\n",
      "[1.82957631e-28 3.99971116e-28]\n",
      "101\n",
      "[9.82899862e-29 2.14875735e-28]\n",
      "102\n",
      "[5.28041457e-29 1.15437290e-28]\n",
      "103\n",
      "[2.83678726e-29 6.20161597e-29]\n",
      "104\n",
      "[1.52400192e-29 3.33168257e-29]\n",
      "105\n",
      "[8.18736705e-30 1.78987360e-29]\n",
      "106\n",
      "[4.39848391e-30 9.61570452e-30]\n",
      "107\n",
      "[2.36298930e-30 5.16582698e-30]\n",
      "108\n",
      "[1.26946433e-30 2.77522758e-30]\n",
      "109\n",
      "[6.81991949e-31 1.49093033e-30]\n",
      "110\n",
      "[3.66385261e-31 8.00969717e-31]\n",
      "111\n",
      "[1.96832469e-31 4.30303465e-31]\n",
      "112\n",
      "[1.05743940e-31 2.31171127e-31]\n",
      "113\n",
      "[5.68086194e-32 1.24191633e-31]\n",
      "114\n",
      "[3.05191885e-32 6.67192389e-32]\n",
      "115\n",
      "[1.63957666e-32 3.58434521e-32]\n",
      "116\n",
      "[8.80826708e-33 1.92561108e-32]\n",
      "117\n",
      "[4.73204887e-33 1.03449245e-32]\n",
      "118\n",
      "[2.54218978e-33 5.55758445e-33]\n",
      "119\n",
      "[1.36573587e-33 2.98569071e-33]\n",
      "120\n",
      "[7.33711729e-34 1.60399704e-33]\n",
      "121\n",
      "[3.94170582e-34 8.61712337e-34]\n",
      "122\n",
      "[2.11759526e-34 4.62936109e-34]\n",
      "123\n",
      "[1.13763174e-34 2.48702301e-34]\n",
      "124\n",
      "[6.11167777e-35 1.33609873e-34]\n",
      "125\n",
      "[3.28336523e-35 7.17789825e-35]\n",
      "126\n",
      "[1.76391617e-35 3.85616887e-35]\n",
      "127\n",
      "[9.47625391e-36 2.07164240e-35]\n",
      "128\n",
      "[5.09091019e-36 1.11294458e-35]\n",
      "129\n",
      "[2.73498017e-36 5.97905136e-36]\n",
      "130\n",
      "[1.46930829e-36 3.21211460e-36]\n",
      "131\n",
      "[7.89353748e-37 1.72563833e-36]\n",
      "132\n",
      "[4.24063040e-37 9.27061455e-37]\n",
      "133\n",
      "[2.27818595e-37 4.98043494e-37]\n",
      "134\n",
      "[1.22390558e-37 2.67562976e-37]\n",
      "135\n",
      "[6.57516510e-38 1.43742358e-37]\n",
      "136\n",
      "[3.53236367e-38 7.72224385e-38]\n",
      "137\n",
      "[1.89768514e-38 4.14860664e-38]\n",
      "138\n",
      "[1.01948984e-38 2.22874820e-38]\n",
      "139\n",
      "[5.47698624e-39 1.19734623e-38]\n",
      "140\n",
      "[2.94239109e-39 6.43248080e-39]\n",
      "141\n",
      "[1.58073527e-39 3.45570964e-39]\n",
      "142\n",
      "[8.49215456e-40 1.85650444e-39]\n",
      "143\n",
      "[4.56222433e-40 9.97366412e-40]\n",
      "144\n",
      "[2.45095526e-40 5.35813295e-40]\n",
      "145\n",
      "[1.31672212e-40 2.87853976e-40]\n",
      "146\n",
      "[7.07380163e-41 1.54643254e-40]\n",
      "147\n",
      "[3.80024524e-41 8.30787064e-41]\n",
      "148\n",
      "[2.04159865e-41 4.46322183e-41]\n",
      "149\n",
      "[1.09680423e-41 2.39776833e-41]\n",
      "150\n",
      "[5.89234088e-42 1.28814860e-41]\n",
      "151\n",
      "[3.16553129e-42 6.92029666e-42]\n",
      "152\n",
      "[1.70061246e-42 3.71777805e-42]\n",
      "153\n",
      "[9.13616856e-43 1.99729496e-42]\n",
      "154\n",
      "[4.90820677e-43 1.07300304e-42]\n",
      "155\n",
      "[2.63682676e-43 5.76447419e-43]\n",
      "156\n",
      "[1.41657751e-43 3.09683770e-43]\n",
      "157\n",
      "[7.61025292e-44 1.66370834e-43]\n",
      "158\n",
      "[4.08844196e-44 8.93790922e-44]\n",
      "159\n",
      "[2.19642604e-44 4.80169628e-44]\n",
      "160\n",
      "[1.17998186e-44 2.57960632e-44]\n",
      "161\n",
      "[6.33919451e-45 1.38583709e-44]\n",
      "162\n",
      "[3.40559363e-45 7.44510670e-45]\n",
      "163\n",
      "[1.82958070e-45 3.99972077e-45]\n",
      "164\n",
      "[9.82902224e-46 2.14876252e-45]\n",
      "165\n",
      "[5.28042726e-46 1.15437567e-45]\n",
      "166\n",
      "[2.83679407e-46 6.20163088e-46]\n",
      "167\n",
      "[1.52400558e-46 3.33169057e-46]\n",
      "168\n",
      "[8.18738673e-47 1.78987790e-46]\n",
      "169\n",
      "[4.39849448e-47 9.61572763e-47]\n",
      "170\n",
      "[2.36299497e-47 5.16583940e-47]\n",
      "171\n",
      "[1.26946738e-47 2.77523425e-47]\n",
      "172\n",
      "[6.81993588e-48 1.49093391e-47]\n",
      "173\n",
      "[3.66386142e-48 8.00971642e-48]\n",
      "174\n",
      "[1.96832943e-48 4.30304499e-48]\n",
      "175\n",
      "[1.05744194e-48 2.31171682e-48]\n",
      "176\n",
      "[5.68087559e-49 1.24191931e-48]\n",
      "177\n",
      "[3.05192618e-49 6.67193992e-49]\n",
      "178\n",
      "[1.63958060e-49 3.58435382e-49]\n",
      "179\n",
      "[8.80828825e-50 1.92561571e-49]\n",
      "180\n",
      "[4.73206025e-50 1.03449493e-49]\n",
      "181\n",
      "[2.54219589e-50 5.55759780e-50]\n",
      "182\n",
      "[1.36573915e-50 2.98569789e-50]\n",
      "183\n",
      "[7.33713492e-51 1.60400090e-50]\n",
      "184\n",
      "[3.94171529e-51 8.61714408e-51]\n",
      "185\n",
      "[2.11760034e-51 4.62937222e-51]\n",
      "186\n",
      "[1.13763448e-51 2.48702899e-51]\n",
      "187\n",
      "[6.11169246e-52 1.33610194e-51]\n",
      "188\n",
      "[3.28337313e-52 7.17791550e-52]\n",
      "189\n",
      "[1.76392041e-52 3.85617814e-52]\n",
      "190\n",
      "[9.47627668e-53 2.07164738e-52]\n",
      "191\n",
      "[5.09092242e-53 1.11294725e-52]\n",
      "192\n",
      "[2.73498675e-53 5.97906573e-53]\n",
      "193\n",
      "[1.46931182e-53 3.21212232e-53]\n",
      "194\n",
      "[7.89355646e-54 1.72564247e-53]\n",
      "195\n",
      "[4.24064059e-54 9.27063683e-54]\n",
      "196\n",
      "[2.27819143e-54 4.98044691e-54]\n",
      "197\n",
      "[1.22390852e-54 2.67563619e-54]\n",
      "198\n",
      "[6.57518091e-55 1.43742703e-54]\n",
      "199\n",
      "[3.53237216e-55 7.72226241e-55]\n",
      "200\n",
      "[1.89768970e-55 4.14861661e-55]\n",
      "201\n",
      "[1.01949229e-55 2.22875355e-55]\n",
      "202\n",
      "[5.47699940e-56 1.19734911e-55]\n",
      "203\n",
      "[2.94239816e-56 6.43249626e-56]\n",
      "204\n",
      "[1.58073907e-56 3.45571795e-56]\n",
      "205\n",
      "[8.49217497e-57 1.85650890e-56]\n",
      "206\n",
      "[4.56223530e-57 9.97368809e-57]\n",
      "207\n",
      "[2.45096115e-57 5.35814582e-57]\n",
      "208\n",
      "[1.31672528e-57 2.87854667e-57]\n",
      "209\n",
      "[7.07381863e-58 1.54643625e-57]\n",
      "210\n",
      "[3.80025437e-58 8.30789060e-58]\n",
      "211\n",
      "[2.04160356e-58 4.46323255e-58]\n",
      "212\n",
      "[1.09680687e-58 2.39777409e-58]\n",
      "213\n",
      "[5.89235504e-59 1.28815170e-58]\n",
      "214\n",
      "[3.16553889e-59 6.92031329e-59]\n",
      "215\n",
      "[1.70061655e-59 3.71778699e-59]\n",
      "216\n",
      "[9.13619052e-60 1.99729976e-59]\n",
      "217\n",
      "[4.90821857e-60 1.07300562e-59]\n",
      "218\n",
      "[2.63683309e-60 5.76448805e-60]\n",
      "219\n",
      "[1.41658092e-60 3.09684515e-60]\n",
      "220\n",
      "[7.61027121e-61 1.66371233e-60]\n",
      "221\n",
      "[4.08845179e-61 8.93793070e-61]\n",
      "222\n",
      "[2.19643132e-61 4.80170782e-61]\n",
      "223\n",
      "[1.17998470e-61 2.57961252e-61]\n",
      "224\n",
      "[6.33920974e-62 1.38584042e-61]\n",
      "225\n",
      "[3.40560181e-62 7.44512459e-62]\n",
      "226\n",
      "[1.82958510e-62 3.99973038e-62]\n",
      "227\n",
      "[9.82904586e-63 2.14876768e-62]\n",
      "228\n",
      "[5.28043995e-63 1.15437845e-62]\n",
      "229\n",
      "[2.83680089e-63 6.20164578e-63]\n",
      "230\n",
      "[1.52400925e-63 3.33169858e-63]\n",
      "231\n",
      "[8.18740640e-64 1.78988221e-63]\n",
      "232\n",
      "[4.39850505e-64 9.61575074e-64]\n",
      "233\n",
      "[2.36300065e-64 5.16585181e-64]\n",
      "234\n",
      "[1.26947043e-64 2.77524092e-64]\n",
      "235\n",
      "[6.81995227e-65 1.49093750e-64]\n",
      "236\n",
      "[3.66387022e-65 8.00973567e-65]\n",
      "237\n",
      "[1.96833416e-65 4.30305533e-65]\n",
      "238\n",
      "[1.05744448e-65 2.31172238e-65]\n",
      "239\n",
      "[5.68088924e-66 1.24192230e-65]\n",
      "240\n",
      "[3.05193352e-66 6.67195596e-66]\n",
      "241\n",
      "[1.63958454e-66 3.58436244e-66]\n",
      "242\n",
      "[8.80830942e-67 1.92562034e-66]\n",
      "243\n",
      "[4.73207162e-67 1.03449742e-66]\n",
      "244\n",
      "[2.54220200e-67 5.55761116e-67]\n",
      "245\n",
      "[1.36574243e-67 2.98570506e-67]\n",
      "246\n",
      "[7.33715256e-68 1.60400475e-67]\n",
      "247\n",
      "[3.94172477e-68 8.61716479e-68]\n",
      "248\n",
      "[2.11760543e-68 4.62938334e-68]\n",
      "249\n",
      "[1.13763721e-68 2.48703497e-68]\n",
      "250\n",
      "[6.11170715e-69 1.33610515e-68]\n",
      "251\n",
      "[3.28338102e-69 7.17793275e-69]\n",
      "252\n",
      "[1.76392465e-69 3.85618740e-69]\n",
      "253\n",
      "[9.47629946e-70 2.07165236e-69]\n",
      "254\n",
      "[5.09093466e-70 1.11294993e-69]\n",
      "255\n",
      "[2.73499332e-70 5.97908010e-70]\n",
      "256\n",
      "[1.46931535e-70 3.21213004e-70]\n",
      "257\n",
      "[7.89357543e-71 1.72564662e-70]\n",
      "258\n",
      "[4.24065078e-71 9.27065911e-71]\n",
      "259\n",
      "[2.27819690e-71 4.98045888e-71]\n",
      "260\n",
      "[1.22391147e-71 2.67564262e-71]\n",
      "261\n",
      "[6.57519671e-72 1.43743049e-71]\n",
      "262\n",
      "[3.53238065e-72 7.72228097e-72]\n",
      "263\n",
      "[1.89769426e-72 4.14862658e-72]\n",
      "264\n",
      "[1.01949474e-72 2.22875891e-72]\n",
      "265\n",
      "[5.47701256e-73 1.19735199e-72]\n",
      "266\n",
      "[2.94240523e-73 6.43251172e-73]\n",
      "267\n",
      "[1.58074287e-73 3.45572625e-73]\n",
      "268\n",
      "[8.49219538e-74 1.85651336e-73]\n",
      "269\n",
      "[4.56224626e-74 9.97371206e-74]\n",
      "270\n",
      "[2.45096704e-74 5.35815870e-74]\n",
      "271\n",
      "[1.31672844e-74 2.87855359e-74]\n",
      "272\n",
      "[7.07383563e-75 1.54643997e-74]\n",
      "273\n",
      "[3.80026350e-75 8.30791057e-75]\n",
      "274\n",
      "[2.04160846e-75 4.46324328e-75]\n",
      "275\n",
      "[1.09680950e-75 2.39777985e-75]\n",
      "276\n",
      "[5.89236920e-76 1.28815479e-75]\n",
      "277\n",
      "[3.16554650e-76 6.92032993e-76]\n",
      "278\n",
      "[1.70062064e-76 3.71779592e-76]\n",
      "279\n",
      "[9.13621248e-77 1.99730456e-76]\n",
      "280\n",
      "[4.90823037e-77 1.07300820e-76]\n",
      "281\n",
      "[2.63683943e-77 5.76450190e-77]\n",
      "282\n",
      "[1.41658432e-77 3.09685259e-77]\n",
      "283\n",
      "[7.61028950e-78 1.66371633e-77]\n",
      "284\n",
      "[4.08846162e-78 8.93795218e-78]\n",
      "285\n",
      "[2.19643660e-78 4.80171936e-78]\n",
      "286\n",
      "[1.17998753e-78 2.57961872e-78]\n",
      "287\n",
      "[6.33922498e-79 1.38584375e-78]\n",
      "288\n",
      "[3.40560999e-79 7.44514249e-79]\n",
      "289\n",
      "[1.82958950e-79 3.99973999e-79]\n",
      "290\n",
      "[9.82906948e-80 2.14877285e-79]\n",
      "291\n",
      "[5.28045264e-80 1.15438122e-79]\n",
      "292\n",
      "[2.83680771e-80 6.20166068e-80]\n",
      "293\n",
      "[1.52401291e-80 3.33170659e-80]\n",
      "294\n",
      "[8.18742608e-81 1.78988651e-80]\n",
      "295\n",
      "[4.39851562e-81 9.61577385e-81]\n",
      "296\n",
      "[2.36300633e-81 5.16586423e-81]\n",
      "297\n",
      "[1.26947348e-81 2.77524759e-81]\n",
      "298\n",
      "[6.81996866e-82 1.49094108e-81]\n",
      "299\n",
      "[3.66387903e-82 8.00975492e-82]\n",
      "300\n",
      "[1.96833889e-82 4.30306568e-82]\n",
      "301\n",
      "[1.05744702e-82 2.31172794e-82]\n",
      "302\n",
      "[5.68090290e-83 1.24192528e-82]\n",
      "303\n",
      "[3.05194085e-83 6.67197200e-83]\n",
      "304\n",
      "[1.63958848e-83 3.58437105e-83]\n",
      "305\n",
      "[8.80833059e-84 1.92562497e-83]\n",
      "306\n",
      "[4.73208299e-84 1.03449990e-83]\n",
      "307\n",
      "[2.54220811e-84 5.55762452e-84]\n",
      "308\n",
      "[1.36574571e-84 2.98571224e-84]\n",
      "309\n",
      "[7.33717019e-85 1.60400861e-84]\n",
      "310\n",
      "[3.94173424e-85 8.61718550e-85]\n",
      "311\n",
      "[2.11761052e-85 4.62939447e-85]\n",
      "312\n",
      "[1.13763995e-85 2.48704094e-85]\n",
      "313\n",
      "[6.11172184e-86 1.33610836e-85]\n",
      "314\n",
      "[3.28338891e-86 7.17795000e-86]\n",
      "315\n",
      "[1.76392889e-86 3.85619667e-86]\n",
      "316\n",
      "[9.47632223e-87 2.07165734e-86]\n",
      "317\n",
      "[5.09094689e-87 1.11295260e-86]\n",
      "318\n",
      "[2.73499989e-87 5.97909447e-87]\n",
      "319\n",
      "[1.46931888e-87 3.21213776e-87]\n",
      "320\n",
      "[7.89359440e-88 1.72565077e-87]\n",
      "321\n",
      "[4.24066098e-88 9.27068139e-88]\n",
      "322\n",
      "[2.27820238e-88 4.98047085e-88]\n",
      "323\n",
      "[1.22391441e-88 2.67564905e-88]\n",
      "324\n",
      "[6.57521251e-89 1.43743394e-88]\n",
      "325\n",
      "[3.53238914e-89 7.72229953e-89]\n",
      "326\n",
      "[1.89769882e-89 4.14863655e-89]\n",
      "327\n",
      "[1.01949719e-89 2.22876427e-89]\n",
      "328\n",
      "[5.47702573e-90 1.19735486e-89]\n",
      "329\n",
      "[2.94241230e-90 6.43252717e-90]\n",
      "330\n",
      "[1.58074667e-90 3.45573456e-90]\n",
      "331\n",
      "[8.49221579e-91 1.85651782e-90]\n",
      "332\n",
      "[4.56225723e-91 9.97373603e-91]\n",
      "333\n",
      "[2.45097293e-91 5.35817158e-91]\n",
      "334\n",
      "[1.31673161e-91 2.87856051e-91]\n",
      "335\n",
      "[7.07385263e-92 1.54644369e-91]\n",
      "336\n",
      "[3.80027264e-92 8.30793054e-92]\n",
      "337\n",
      "[2.04161337e-92 4.46325401e-92]\n",
      "338\n",
      "[1.09681214e-92 2.39778561e-92]\n",
      "339\n",
      "[5.89238337e-93 1.28815789e-92]\n",
      "340\n",
      "[3.16555411e-93 6.92034656e-93]\n",
      "341\n",
      "[1.70062472e-93 3.71780486e-93]\n",
      "342\n",
      "[9.13623443e-94 1.99730936e-93]\n",
      "343\n",
      "[4.90824216e-94 1.07301078e-93]\n",
      "344\n",
      "[2.63684577e-94 5.76451576e-94]\n",
      "345\n",
      "[1.41658773e-94 3.09686003e-94]\n",
      "346\n",
      "[7.61030779e-95 1.66372033e-94]\n",
      "347\n",
      "[4.08847144e-95 8.93797366e-95]\n",
      "348\n",
      "[2.19644188e-95 4.80173090e-95]\n",
      "349\n",
      "[1.17999037e-95 2.57962492e-95]\n",
      "350\n",
      "[6.33924021e-96 1.38584708e-95]\n",
      "351\n",
      "[3.40561818e-96 7.44516038e-96]\n",
      "352\n",
      "[1.82959389e-96 3.99974961e-96]\n",
      "353\n",
      "[9.82909311e-97 2.14877801e-96]\n",
      "354\n",
      "[5.28046533e-97 1.15438400e-96]\n",
      "355\n",
      "[2.83681453e-97 6.20167559e-97]\n",
      "356\n",
      "[1.52401657e-97 3.33171459e-97]\n",
      "357\n",
      "[8.18744576e-98 1.78989081e-97]\n",
      "358\n",
      "[4.39852619e-98 9.61579696e-98]\n",
      "359\n",
      "[2.36301201e-98 5.16587664e-98]\n",
      "360\n",
      "[1.26947653e-98 2.77525426e-98]\n",
      "361\n",
      "[6.81998505e-99 1.49094466e-98]\n",
      "362\n",
      "[3.66388783e-99 8.00977417e-99]\n",
      "363\n",
      "[1.96834362e-99 4.30307602e-99]\n",
      "364\n",
      "[1.05744957e-99 2.31173349e-99]\n",
      "365\n",
      "[5.68091655e-100 1.24192827e-099]\n",
      "366\n",
      "[3.05194819e-100 6.67198803e-100]\n",
      "367\n",
      "[1.63959242e-100 3.58437967e-100]\n",
      "368\n",
      "[8.80835176e-101 1.92562959e-100]\n",
      "369\n",
      "[4.73209437e-101 1.03450239e-100]\n",
      "370\n",
      "[2.54221422e-101 5.55763787e-101]\n",
      "371\n",
      "[1.36574900e-101 2.98571941e-101]\n",
      "372\n",
      "[7.33718783e-102 1.60401246e-101]\n",
      "373\n",
      "[3.94174371e-102 8.61720621e-102]\n",
      "374\n",
      "[2.11761561e-102 4.62940560e-102]\n",
      "375\n",
      "[1.13764268e-102 2.48704692e-102]\n",
      "376\n",
      "[6.11173652e-103 1.33611157e-102]\n",
      "377\n",
      "[3.28339680e-103 7.17796725e-103]\n",
      "378\n",
      "[1.76393313e-103 3.85620594e-103]\n",
      "379\n",
      "[9.47634501e-104 2.07166231e-103]\n",
      "380\n",
      "[5.09095913e-104 1.11295528e-103]\n",
      "381\n",
      "[2.73500647e-104 5.97910884e-104]\n",
      "382\n",
      "[1.46932242e-104 3.21214548e-104]\n",
      "383\n",
      "[7.89361337e-105 1.72565492e-104]\n",
      "384\n",
      "[4.24067117e-105 9.27070367e-105]\n",
      "385\n",
      "[2.27820785e-105 4.98048282e-105]\n",
      "386\n",
      "[1.22391735e-105 2.67565548e-105]\n",
      "387\n",
      "[6.57522832e-106 1.43743740e-105]\n",
      "388\n",
      "[3.53239763e-106 7.72231809e-106]\n",
      "389\n",
      "[1.89770338e-106 4.14864652e-106]\n",
      "390\n",
      "[1.01949964e-106 2.22876962e-106]\n",
      "391\n",
      "[5.47703889e-107 1.19735774e-106]\n",
      "392\n",
      "[2.94241937e-107 6.43254263e-107]\n",
      "393\n",
      "[1.58075046e-107 3.45574287e-107]\n",
      "394\n",
      "[8.49223620e-108 1.85652229e-107]\n",
      "395\n",
      "[4.56226819e-108 9.97376000e-108]\n",
      "396\n",
      "[2.45097882e-108 5.35818446e-108]\n",
      "397\n",
      "[1.31673477e-108 2.87856743e-108]\n",
      "398\n",
      "[7.07386963e-109 1.54644740e-108]\n",
      "399\n",
      "[3.80028177e-109 8.30795050e-109]\n",
      "400\n",
      "[2.04161828e-109 4.46326473e-109]\n",
      "401\n",
      "[1.09681477e-109 2.39779138e-109]\n",
      "402\n",
      "[5.89239753e-110 1.28816098e-109]\n",
      "403\n",
      "[3.16556172e-110 6.92036319e-110]\n",
      "404\n",
      "[1.70062881e-110 3.71781379e-110]\n",
      "405\n",
      "[9.13625639e-111 1.99731416e-110]\n",
      "406\n",
      "[4.90825396e-111 1.07301336e-110]\n",
      "407\n",
      "[2.63685211e-111 5.76452961e-111]\n",
      "408\n",
      "[1.41659113e-111 3.09686748e-111]\n",
      "409\n",
      "[7.61032608e-112 1.66372433e-111]\n",
      "410\n",
      "[4.08848127e-112 8.93799515e-112]\n",
      "411\n",
      "[2.19644716e-112 4.80174244e-112]\n",
      "412\n",
      "[1.17999320e-112 2.57963112e-112]\n",
      "413\n",
      "[6.33925545e-113 1.38585041e-112]\n",
      "414\n",
      "[3.40562636e-113 7.44517827e-113]\n",
      "415\n",
      "[1.82959829e-113 3.99975922e-113]\n",
      "416\n",
      "[9.82911673e-114 2.14878318e-113]\n",
      "417\n",
      "[5.28047802e-114 1.15438677e-113]\n",
      "418\n",
      "[2.83682135e-114 6.20169049e-114]\n",
      "419\n",
      "[1.52402023e-114 3.33172260e-114]\n",
      "420\n",
      "[8.18746544e-115 1.78989511e-114]\n",
      "421\n",
      "[4.39853677e-115 9.61582007e-115]\n",
      "422\n",
      "[2.36301769e-115 5.16588906e-115]\n",
      "423\n",
      "[1.26947958e-115 2.77526093e-115]\n",
      "424\n",
      "[6.82000144e-116 1.49094825e-115]\n",
      "425\n",
      "[3.66389664e-116 8.00979342e-116]\n",
      "426\n",
      "[1.96834835e-116 4.30308636e-116]\n",
      "427\n",
      "[1.05745211e-116 2.31173905e-116]\n",
      "428\n",
      "[5.68093020e-117 1.24193125e-116]\n",
      "429\n",
      "[3.05195552e-117 6.67200407e-117]\n",
      "430\n",
      "[1.63959637e-117 3.58438828e-117]\n",
      "431\n",
      "[8.80837293e-118 1.92563422e-117]\n",
      "432\n",
      "[4.73210574e-118 1.03450488e-117]\n",
      "433\n",
      "[2.54222033e-118 5.55765123e-118]\n",
      "434\n",
      "[1.36575228e-118 2.98572659e-118]\n",
      "435\n",
      "[7.33720546e-119 1.60401632e-118]\n",
      "436\n",
      "[3.94175319e-119 8.61722692e-119]\n",
      "437\n",
      "[2.11762070e-119 4.62941672e-119]\n",
      "438\n",
      "[1.13764541e-119 2.48705290e-119]\n",
      "439\n",
      "[6.11175121e-120 1.33611478e-119]\n",
      "440\n",
      "[3.28340469e-120 7.17798450e-120]\n",
      "441\n",
      "[1.76393737e-120 3.85621521e-120]\n",
      "442\n",
      "[9.47636778e-121 2.07166729e-120]\n",
      "443\n",
      "[5.09097137e-121 1.11295795e-120]\n",
      "444\n",
      "[2.73501304e-121 5.97912321e-121]\n",
      "445\n",
      "[1.46932595e-121 3.21215320e-121]\n",
      "446\n",
      "[7.89363234e-122 1.72565906e-121]\n",
      "447\n",
      "[4.24068136e-122 9.27072595e-122]\n",
      "448\n",
      "[2.27821333e-122 4.98049479e-122]\n",
      "449\n",
      "[1.22392029e-122 2.67566191e-122]\n",
      "450\n",
      "[6.57524412e-123 1.43744085e-122]\n",
      "451\n",
      "[3.53240612e-123 7.72233665e-123]\n",
      "452\n",
      "[1.89770794e-123 4.14865649e-123]\n",
      "453\n",
      "[1.01950209e-123 2.22877498e-123]\n",
      "454\n",
      "[5.47705205e-124 1.19736062e-123]\n",
      "455\n",
      "[2.94242644e-124 6.43255809e-124]\n",
      "456\n",
      "[1.58075426e-124 3.45575117e-124]\n",
      "457\n",
      "[8.49225661e-125 1.85652675e-124]\n",
      "458\n",
      "[4.56227916e-125 9.97378397e-125]\n",
      "459\n",
      "[2.45098471e-125 5.35819734e-125]\n",
      "460\n",
      "[1.31673794e-125 2.87857435e-125]\n",
      "461\n",
      "[7.07388663e-126 1.54645112e-125]\n",
      "462\n",
      "[3.80029090e-126 8.30797047e-126]\n",
      "463\n",
      "[2.04162318e-126 4.46327546e-126]\n",
      "464\n",
      "[1.09681741e-126 2.39779714e-126]\n",
      "465\n",
      "[5.89241169e-127 1.28816408e-126]\n",
      "466\n",
      "[3.16556933e-127 6.92037982e-127]\n",
      "467\n",
      "[1.70063290e-127 3.71782273e-127]\n",
      "468\n",
      "[9.13627835e-128 1.99731896e-127]\n",
      "469\n",
      "[4.90826575e-128 1.07301593e-127]\n",
      "470\n",
      "[2.63685844e-128 5.76454346e-128]\n",
      "471\n",
      "[1.41659454e-128 3.09687492e-128]\n",
      "472\n",
      "[7.61034437e-129 1.66372833e-128]\n",
      "473\n",
      "[4.08849109e-129 8.93801663e-129]\n",
      "474\n",
      "[2.19645243e-129 4.80175398e-129]\n",
      "475\n",
      "[1.17999604e-129 2.57963732e-129]\n",
      "476\n",
      "[6.33927068e-130 1.38585374e-129]\n",
      "477\n",
      "[3.40563455e-130 7.44519617e-130]\n",
      "478\n",
      "[1.82960269e-130 3.99976883e-130]\n",
      "479\n",
      "[9.82914035e-131 2.14878834e-130]\n",
      "480\n",
      "[5.28049071e-131 1.15438955e-130]\n",
      "481\n",
      "[2.83682816e-131 6.20170540e-131]\n",
      "482\n",
      "[1.52402390e-131 3.33173061e-131]\n",
      "483\n",
      "[8.18748511e-132 1.78989941e-131]\n",
      "484\n",
      "[4.39854734e-132 9.61584318e-132]\n",
      "485\n",
      "[2.36302337e-132 5.16590147e-132]\n",
      "486\n",
      "[1.26948263e-132 2.77526760e-132]\n",
      "487\n",
      "[6.82001783e-133 1.49095183e-132]\n",
      "488\n",
      "[3.66390544e-133 8.00981267e-133]\n",
      "489\n",
      "[1.96835308e-133 4.30309670e-133]\n",
      "490\n",
      "[1.05745465e-133 2.31174460e-133]\n",
      "491\n",
      "[5.68094386e-134 1.24193424e-133]\n",
      "492\n",
      "[3.05196286e-134 6.67202010e-134]\n",
      "493\n",
      "[1.63960031e-134 3.58439690e-134]\n",
      "494\n",
      "[8.80839410e-135 1.92563885e-134]\n",
      "495\n",
      "[4.73211711e-135 1.03450736e-134]\n",
      "496\n",
      "[2.54222644e-135 5.55766459e-135]\n",
      "497\n",
      "[1.36575556e-135 2.98573377e-135]\n",
      "498\n",
      "[7.33722309e-136 1.60402017e-135]\n",
      "499\n",
      "[3.94176266e-136 8.61724763e-136]\n",
      "500\n",
      "[2.11762579e-136 4.62942785e-136]\n",
      "501\n",
      "[1.13764815e-136 2.48705888e-136]\n",
      "502\n",
      "[6.1117659e-137 1.3361180e-136]\n",
      "503\n",
      "[3.28341258e-137 7.17800175e-137]\n",
      "504\n",
      "[1.76394161e-137 3.85622448e-137]\n",
      "505\n",
      "[9.47639056e-138 2.07167227e-137]\n",
      "506\n",
      "[5.09098360e-138 1.11296063e-137]\n",
      "507\n",
      "[2.73501961e-138 5.97913758e-138]\n",
      "508\n",
      "[1.46932948e-138 3.21216092e-138]\n",
      "509\n",
      "[7.89365131e-139 1.72566321e-138]\n",
      "510\n",
      "[4.24069155e-139 9.27074823e-139]\n",
      "511\n",
      "[2.27821880e-139 4.98050676e-139]\n",
      "512\n",
      "[1.22392323e-139 2.67566834e-139]\n",
      "513\n",
      "[6.57525992e-140 1.43744431e-139]\n",
      "514\n",
      "[3.53241461e-140 7.72235521e-140]\n",
      "515\n",
      "[1.89771250e-140 4.14866646e-140]\n",
      "516\n",
      "[1.01950454e-140 2.22878034e-140]\n",
      "517\n",
      "[5.47706522e-141 1.19736350e-140]\n",
      "518\n",
      "[2.94243352e-141 6.43257355e-141]\n",
      "519\n",
      "[1.58075806e-141 3.45575948e-141]\n",
      "520\n",
      "[8.49227702e-142 1.85653121e-141]\n",
      "521\n",
      "[4.56229012e-142 9.97380794e-142]\n",
      "522\n",
      "[2.45099060e-142 5.35821021e-142]\n",
      "523\n",
      "[1.31674110e-142 2.87858127e-142]\n",
      "524\n",
      "[7.07390363e-143 1.54645484e-142]\n",
      "525\n",
      "[3.80030004e-143 8.30799044e-143]\n",
      "526\n",
      "[2.04162809e-143 4.46328619e-143]\n",
      "527\n",
      "[1.09682005e-143 2.39780290e-143]\n",
      "528\n",
      "[5.89242585e-144 1.28816717e-143]\n",
      "529\n",
      "[3.16557693e-144 6.92039645e-144]\n",
      "530\n",
      "[1.70063698e-144 3.71783166e-144]\n",
      "531\n",
      "[9.13630031e-145 1.99732376e-144]\n",
      "532\n",
      "[4.90827755e-145 1.07301851e-144]\n",
      "533\n",
      "[2.63686478e-145 5.76455732e-145]\n",
      "534\n",
      "[1.41659794e-145 3.09688236e-145]\n",
      "535\n",
      "[7.61036267e-146 1.66373233e-145]\n",
      "536\n",
      "[4.08850092e-146 8.93803811e-146]\n",
      "537\n",
      "[2.19645771e-146 4.80176552e-146]\n",
      "538\n",
      "[1.17999888e-146 2.57964352e-146]\n",
      "539\n",
      "[6.33928592e-147 1.38585707e-146]\n",
      "540\n",
      "[3.40564273e-147 7.44521406e-147]\n",
      "541\n",
      "[1.82960709e-147 3.99977844e-147]\n",
      "542\n",
      "[9.82916398e-148 2.14879350e-147]\n",
      "543\n",
      "[5.28050340e-148 1.15439232e-147]\n",
      "544\n",
      "[2.83683498e-148 6.20172030e-148]\n",
      "545\n",
      "[1.52402756e-148 3.33173862e-148]\n",
      "546\n",
      "[8.18750479e-149 1.78990371e-148]\n",
      "547\n",
      "[4.39855791e-149 9.61586629e-149]\n",
      "548\n",
      "[2.36302905e-149 5.16591389e-149]\n",
      "549\n",
      "[1.26948568e-149 2.77527427e-149]\n",
      "550\n",
      "[6.82003422e-150 1.49095541e-149]\n",
      "551\n",
      "[3.66391425e-150 8.00983192e-150]\n",
      "552\n",
      "[1.96835781e-150 4.30310704e-150]\n",
      "553\n",
      "[1.05745719e-150 2.31175016e-150]\n",
      "554\n",
      "[5.68095751e-151 1.24193722e-150]\n",
      "555\n",
      "[3.05197019e-151 6.67203614e-151]\n",
      "556\n",
      "[1.63960425e-151 3.58440551e-151]\n",
      "557\n",
      "[8.80841527e-152 1.92564348e-151]\n",
      "558\n",
      "[4.73212848e-152 1.03450985e-151]\n",
      "559\n",
      "[2.54223255e-152 5.55767794e-152]\n",
      "560\n",
      "[1.36575884e-152 2.98574094e-152]\n",
      "561\n",
      "[7.33724073e-153 1.60402403e-152]\n",
      "562\n",
      "[3.94177213e-153 8.61726834e-153]\n",
      "563\n",
      "[2.11763088e-153 4.62943897e-153]\n",
      "564\n",
      "[1.13765088e-153 2.48706485e-153]\n",
      "565\n",
      "[6.11178059e-154 1.33612121e-153]\n",
      "566\n",
      "[3.28342047e-154 7.17801900e-154]\n",
      "567\n",
      "[1.76394585e-154 3.85623374e-154]\n",
      "568\n",
      "[9.47641333e-155 2.07167725e-154]\n",
      "569\n",
      "[5.09099584e-155 1.11296330e-154]\n",
      "570\n",
      "[2.73502619e-155 5.97915195e-155]\n",
      "571\n",
      "[1.46933301e-155 3.21216864e-155]\n",
      "572\n",
      "[7.89367028e-156 1.72566736e-155]\n",
      "573\n",
      "[4.24070174e-156 9.27077052e-156]\n",
      "574\n",
      "[2.27822428e-156 4.98051873e-156]\n",
      "575\n",
      "[1.22392617e-156 2.67567477e-156]\n",
      "576\n",
      "[6.57527572e-157 1.43744776e-156]\n",
      "577\n",
      "[3.53242310e-157 7.72237377e-157]\n",
      "578\n",
      "[1.89771706e-157 4.14867643e-157]\n",
      "579\n",
      "[1.01950699e-157 2.22878569e-157]\n",
      "580\n",
      "[5.47707838e-158 1.19736638e-157]\n",
      "581\n",
      "[2.94244059e-158 6.43258901e-158]\n",
      "582\n",
      "[1.58076186e-158 3.45576778e-158]\n",
      "583\n",
      "[8.49229743e-159 1.85653567e-158]\n",
      "584\n",
      "[4.56230109e-159 9.97383191e-159]\n",
      "585\n",
      "[2.45099649e-159 5.35822309e-159]\n",
      "586\n",
      "[1.31674427e-159 2.87858818e-159]\n",
      "587\n",
      "[7.07392063e-160 1.54645855e-159]\n",
      "588\n",
      "[3.80030917e-160 8.30801040e-160]\n",
      "589\n",
      "[2.04163300e-160 4.46329691e-160]\n",
      "590\n",
      "[1.09682268e-160 2.39780866e-160]\n",
      "591\n",
      "[5.89244001e-161 1.28817027e-160]\n",
      "592\n",
      "[3.16558454e-161 6.92041309e-161]\n",
      "593\n",
      "[1.70064107e-161 3.71784060e-161]\n",
      "594\n",
      "[9.13632227e-162 1.99732856e-161]\n",
      "595\n",
      "[4.90828935e-162 1.07302109e-161]\n",
      "596\n",
      "[2.63687112e-162 5.76457117e-162]\n",
      "597\n",
      "[1.41660135e-162 3.09688980e-162]\n",
      "598\n",
      "[7.61038096e-163 1.66373633e-162]\n",
      "599\n",
      "[4.08851075e-163 8.93805959e-163]\n",
      "600\n",
      "[2.19646299e-163 4.80177706e-163]\n",
      "601\n",
      "[1.18000171e-163 2.57964972e-163]\n",
      "602\n",
      "[6.33930115e-164 1.38586040e-163]\n",
      "603\n",
      "[3.40565092e-164 7.44523195e-164]\n",
      "604\n",
      "[1.82961148e-164 3.99978806e-164]\n",
      "605\n",
      "[9.82918760e-165 2.14879867e-164]\n",
      "606\n",
      "[5.2805161e-165 1.1543951e-164]\n",
      "607\n",
      "[2.83684180e-165 6.20173521e-165]\n",
      "608\n",
      "[1.52403122e-165 3.33174662e-165]\n",
      "609\n",
      "[8.18752447e-166 1.78990802e-165]\n",
      "610\n",
      "[4.39856848e-166 9.61588941e-166]\n",
      "611\n",
      "[2.36303473e-166 5.16592631e-166]\n",
      "612\n",
      "[1.26948873e-166 2.77528094e-166]\n",
      "613\n",
      "[6.82005062e-167 1.49095900e-166]\n",
      "614\n",
      "[3.66392305e-167 8.00985117e-167]\n",
      "615\n",
      "[1.96836254e-167 4.30311738e-167]\n",
      "616\n",
      "[1.05745973e-167 2.31175572e-167]\n",
      "617\n",
      "[5.68097116e-168 1.24194021e-167]\n",
      "618\n",
      "[3.05197753e-168 6.67205217e-168]\n",
      "619\n",
      "[1.63960819e-168 3.58441413e-168]\n",
      "620\n",
      "[8.80843644e-169 1.92564811e-168]\n",
      "621\n",
      "[4.73213986e-169 1.03451234e-168]\n",
      "622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.54223866e-169 5.55769130e-169]\r\n",
      "623\r\n",
      "[1.36576213e-169 2.98574812e-169]\r\n",
      "624\r\n",
      "[7.33725836e-170 1.60402788e-169]\r\n",
      "625\r\n",
      "[3.94178161e-170 8.61728905e-170]\r\n",
      "626\r\n",
      "[2.11763597e-170 4.62945010e-170]\r\n",
      "627\r\n",
      "[1.13765362e-170 2.48707083e-170]\r\n",
      "628\r\n",
      "[6.11179528e-171 1.33612442e-170]\r\n",
      "629\r\n",
      "[3.28342836e-171 7.17803626e-171]\r\n",
      "630\r\n",
      "[1.76395009e-171 3.85624301e-171]\r\n",
      "631\r\n",
      "[9.47643611e-172 2.07168223e-171]\r\n",
      "632\r\n",
      "[5.09100807e-172 1.11296598e-171]\r\n",
      "633\r\n",
      "[2.73503276e-172 5.97916632e-172]\r\n",
      "634\r\n",
      "[1.46933654e-172 3.21217636e-172]\r\n",
      "635\r\n",
      "[7.89368925e-173 1.72567150e-172]\r\n",
      "636\r\n",
      "[4.24071194e-173 9.27079280e-173]\r\n",
      "637\r\n",
      "[2.27822975e-173 4.98053070e-173]\r\n",
      "638\r\n",
      "[1.22392912e-173 2.67568121e-173]\r\n",
      "639\r\n",
      "[6.57529153e-174 1.43745122e-173]\r\n",
      "640\r\n",
      "[3.53243159e-174 7.72239232e-174]\r\n",
      "641\r\n",
      "[1.89772162e-174 4.14868641e-174]\r\n",
      "642\r\n",
      "[1.01950944e-174 2.22879105e-174]\r\n",
      "643\r\n",
      "[5.47709154e-175 1.19736925e-174]\r\n",
      "644\r\n",
      "[2.94244766e-175 6.43260447e-175]\r\n",
      "645\r\n",
      "[1.58076566e-175 3.45577609e-175]\r\n",
      "646\r\n",
      "[8.49231784e-176 1.85654013e-175]\r\n",
      "647\r\n",
      "[4.56231205e-176 9.97385588e-176]\r\n",
      "648\r\n",
      "[2.45100238e-176 5.35823597e-176]\r\n",
      "649\r\n",
      "[1.31674743e-176 2.87859510e-176]\r\n",
      "650\r\n",
      "[7.07393764e-177 1.54646227e-176]\r\n",
      "651\r\n",
      "[3.80031830e-177 8.30803037e-177]\r\n",
      "652\r\n",
      "[2.04163790e-177 4.46330764e-177]\r\n",
      "653\r\n",
      "[1.09682532e-177 2.39781443e-177]\r\n",
      "654\r\n",
      "[5.89245417e-178 1.28817337e-177]\r\n",
      "655\r\n",
      "[3.16559215e-178 6.92042972e-178]\r\n",
      "656\r\n",
      "[1.70064516e-178 3.71784953e-178]\r\n",
      "657\r\n",
      "[9.13634422e-179 1.99733336e-178]\r\n",
      "658\r\n",
      "[4.90830114e-179 1.07302367e-178]\r\n",
      "659\r\n",
      "[2.63687746e-179 5.76458503e-179]\r\n",
      "660\r\n",
      "[1.41660475e-179 3.09689725e-179]\r\n",
      "661\r\n",
      "[7.61039925e-180 1.66374032e-179]\r\n",
      "662\r\n",
      "[4.08852057e-180 8.93808107e-180]\r\n",
      "663\r\n",
      "[2.19646827e-180 4.80178860e-180]\r\n",
      "664\r\n",
      "[1.18000455e-180 2.57965592e-180]\r\n",
      "665\r\n",
      "[6.33931639e-181 1.38586373e-180]\r\n",
      "666\r\n",
      "[3.40565910e-181 7.44524985e-181]\r\n",
      "667\r\n",
      "[1.82961588e-181 3.99979767e-181]\r\n",
      "668\r\n",
      "[9.82921122e-182 2.14880383e-181]\r\n",
      "669\r\n",
      "[5.28052879e-182 1.15439787e-181]\r\n",
      "670\r\n",
      "[2.83684862e-182 6.20175011e-182]\r\n",
      "671\r\n",
      "[1.52403488e-182 3.33175463e-182]\r\n",
      "672\r\n",
      "[8.18754415e-183 1.78991232e-182]\r\n",
      "673\r\n",
      "[4.39857905e-183 9.61591252e-183]\r\n",
      "674\r\n",
      "[2.36304041e-183 5.16593872e-183]\r\n",
      "675\r\n",
      "[1.26949179e-183 2.77528761e-183]\r\n",
      "676\r\n",
      "[6.82006701e-184 1.49096258e-183]\r\n",
      "677\r\n",
      "[3.66393186e-184 8.00987042e-184]\r\n",
      "678\r\n",
      "[1.96836727e-184 4.30312773e-184]\r\n",
      "679\r\n",
      "[1.05746227e-184 2.31176127e-184]\r\n",
      "680\r\n",
      "[5.68098482e-185 1.24194319e-184]\r\n",
      "681\r\n",
      "[3.05198486e-185 6.67206821e-185]\r\n",
      "682\r\n",
      "[1.63961213e-185 3.58442274e-185]\r\n",
      "683\r\n",
      "[8.80845761e-186 1.92565273e-185]\r\n",
      "684\r\n",
      "[4.73215123e-186 1.03451482e-185]\r\n",
      "685\r\n",
      "[2.54224477e-186 5.55770466e-186]\r\n",
      "686\r\n",
      "[1.36576541e-186 2.98575529e-186]\r\n",
      "687\r\n",
      "[7.33727600e-187 1.60403174e-186]\r\n",
      "688\r\n",
      "[3.94179108e-187 8.61730976e-187]\r\n",
      "689\r\n",
      "[2.11764106e-187 4.62946123e-187]\r\n",
      "690\r\n",
      "[1.13765635e-187 2.48707681e-187]\r\n",
      "691\r\n",
      "[6.11180997e-188 1.33612763e-187]\r\n",
      "692\r\n",
      "[3.28343625e-188 7.17805351e-188]\r\n",
      "693\r\n",
      "[1.76395433e-188 3.85625228e-188]\r\n",
      "694\r\n",
      "[9.47645888e-189 2.07168721e-188]\r\n",
      "695\r\n",
      "[5.09102031e-189 1.11296865e-188]\r\n",
      "696\r\n",
      "[2.73503933e-189 5.97918069e-189]\r\n",
      "697\r\n",
      "[1.46934007e-189 3.21218408e-189]\r\n",
      "698\r\n",
      "[7.89370823e-190 1.72567565e-189]\r\n",
      "699\r\n",
      "[4.24072213e-190 9.27081508e-190]\r\n",
      "700\r\n",
      "[2.27823523e-190 4.98054267e-190]\r\n",
      "701\r\n",
      "[1.22393206e-190 2.67568764e-190]\r\n",
      "702\r\n",
      "[6.57530733e-191 1.43745467e-190]\r\n",
      "703\r\n",
      "[3.53244008e-191 7.72241088e-191]\r\n",
      "704\r\n",
      "[1.89772618e-191 4.14869638e-191]\r\n",
      "705\r\n",
      "[1.01951189e-191 2.22879641e-191]\r\n",
      "706\r\n",
      "[5.47710471e-192 1.19737213e-191]\r\n",
      "707\r\n",
      "[2.94245473e-192 6.43261993e-192]\r\n",
      "708\r\n",
      "[1.58076946e-192 3.45578439e-192]\r\n",
      "709\r\n",
      "[8.49233825e-193 1.85654460e-192]\r\n",
      "710\r\n",
      "[4.56232301e-193 9.97387985e-193]\r\n",
      "711\r\n",
      "[2.45100827e-193 5.35824885e-193]\r\n",
      "712\r\n",
      "[1.31675060e-193 2.87860202e-193]\r\n",
      "713\r\n",
      "[7.07395464e-194 1.54646599e-193]\r\n",
      "714\r\n",
      "[3.80032744e-194 8.30805034e-194]\r\n",
      "715\r\n",
      "[2.04164281e-194 4.46331837e-194]\r\n",
      "716\r\n",
      "[1.09682795e-194 2.39782019e-194]\r\n",
      "717\r\n",
      "[5.89246834e-195 1.28817646e-194]\r\n",
      "718\r\n",
      "[3.16559976e-195 6.92044635e-195]\r\n",
      "719\r\n",
      "[1.70064925e-195 3.71785847e-195]\r\n",
      "720\r\n",
      "[9.13636618e-196 1.99733816e-195]\r\n",
      "721\r\n",
      "[4.90831294e-196 1.07302625e-195]\r\n",
      "722\r\n",
      "[2.63688379e-196 5.76459888e-196]\r\n",
      "723\r\n",
      "[1.41660816e-196 3.09690469e-196]\r\n",
      "724\r\n",
      "[7.61041754e-197 1.66374432e-196]\r\n",
      "725\r\n",
      "[4.08853040e-197 8.93810255e-197]\r\n",
      "726\r\n",
      "[2.19647355e-197 4.80180014e-197]\r\n",
      "727\r\n",
      "[1.18000738e-197 2.57966212e-197]\r\n",
      "728\r\n",
      "[6.33933163e-198 1.38586706e-197]\r\n",
      "729\r\n",
      "[3.40566729e-198 7.44526774e-198]\r\n",
      "730\r\n",
      "[1.82962028e-198 3.99980728e-198]\r\n",
      "731\r\n",
      "[9.82923484e-199 2.14880900e-198]\r\n",
      "732\r\n",
      "[5.28054148e-199 1.15440064e-198]\r\n",
      "733\r\n",
      "[2.83685544e-199 6.20176502e-199]\r\n",
      "734\r\n",
      "[1.52403855e-199 3.33176264e-199]\r\n",
      "735\r\n",
      "[8.18756382e-200 1.78991662e-199]\r\n",
      "736\r\n",
      "[4.39858962e-200 9.61593563e-200]\r\n",
      "737\r\n",
      "[2.36304609e-200 5.16595114e-200]\r\n",
      "738\r\n",
      "[1.26949484e-200 2.77529428e-200]\r\n",
      "739\r\n",
      "[6.82008340e-201 1.49096616e-200]\r\n",
      "740\r\n",
      "[3.66394067e-201 8.00988967e-201]\r\n",
      "741\r\n",
      "[1.96837200e-201 4.30313807e-201]\r\n",
      "742\r\n",
      "[1.05746481e-201 2.31176683e-201]\r\n",
      "743\r\n",
      "[5.68099847e-202 1.24194618e-201]\r\n",
      "744\r\n",
      "[3.05199220e-202 6.67208424e-202]\r\n",
      "745\r\n",
      "[1.63961607e-202 3.58443136e-202]\r\n",
      "746\r\n",
      "[8.80847878e-203 1.92565736e-202]\r\n",
      "747\r\n",
      "[4.73216260e-203 1.03451731e-202]\r\n",
      "748\r\n",
      "[2.54225088e-203 5.55771802e-203]\r\n",
      "749\r\n",
      "[1.36576869e-203 2.98576247e-203]\r\n",
      "750\r\n",
      "[7.33729363e-204 1.60403560e-203]\r\n",
      "751\r\n",
      "[3.94180055e-204 8.61733047e-204]\r\n",
      "752\r\n",
      "[2.11764615e-204 4.62947235e-204]\r\n",
      "753\r\n",
      "[1.13765909e-204 2.48708279e-204]\r\n",
      "754\r\n",
      "[6.11182466e-205 1.33613084e-204]\r\n",
      "755\r\n",
      "[3.28344415e-205 7.17807076e-205]\r\n",
      "756\r\n",
      "[1.76395857e-205 3.85626155e-205]\r\n",
      "757\r\n",
      "[9.47648166e-206 2.07169219e-205]\r\n",
      "758\r\n",
      "[5.09103254e-206 1.11297133e-205]\r\n",
      "759\r\n",
      "[2.73504590e-206 5.97919506e-206]\r\n",
      "760\r\n",
      "[1.4693436e-206 3.2121918e-206]\r\n",
      "761\r\n",
      "[7.8937272e-207 1.7256798e-206]\r\n",
      "762\r\n",
      "[4.24073232e-207 9.27083736e-207]\r\n",
      "763\r\n",
      "[2.27824070e-207 4.98055464e-207]\r\n",
      "764\r\n",
      "[1.22393500e-207 2.67569407e-207]\r\n",
      "765\r\n",
      "[6.57532313e-208 1.43745813e-207]\r\n",
      "766\r\n",
      "[3.53244857e-208 7.72242944e-208]\r\n",
      "767\r\n",
      "[1.89773075e-208 4.14870635e-208]\r\n",
      "768\r\n",
      "[1.01951434e-208 2.22880176e-208]\r\n",
      "769\r\n",
      "[5.47711787e-209 1.19737501e-208]\r\n",
      "770\r\n",
      "[2.94246180e-209 6.43263539e-209]\r\n",
      "771\r\n",
      "[1.58077326e-209 3.45579270e-209]\r\n",
      "772\r\n",
      "[8.49235866e-210 1.85654906e-209]\r\n",
      "773\r\n",
      "[4.56233398e-210 9.97390383e-210]\r\n",
      "774\r\n",
      "[2.45101416e-210 5.35826172e-210]\r\n",
      "775\r\n",
      "[1.31675376e-210 2.87860894e-210]\r\n",
      "776\r\n",
      "[7.07397164e-211 1.54646970e-210]\r\n",
      "777\r\n",
      "[3.80033657e-211 8.30807031e-211]\r\n",
      "778\r\n",
      "[2.04164772e-211 4.46332909e-211]\r\n",
      "779\r\n",
      "[1.09683059e-211 2.39782595e-211]\r\n",
      "780\r\n",
      "[5.89248250e-212 1.28817956e-211]\r\n",
      "781\r\n",
      "[3.16560737e-212 6.92046298e-212]\r\n",
      "782\r\n",
      "[1.70065333e-212 3.71786740e-212]\r\n",
      "783\r\n",
      "[9.13638814e-213 1.99734296e-212]\r\n",
      "784\r\n",
      "[4.90832474e-213 1.07302883e-212]\r\n",
      "785\r\n",
      "[2.63689013e-213 5.76461274e-213]\r\n",
      "786\r\n",
      "[1.41661156e-213 3.09691213e-213]\r\n",
      "787\r\n",
      "[7.61043583e-214 1.66374832e-213]\r\n",
      "788\r\n",
      "[4.08854023e-214 8.93812403e-214]\r\n",
      "789\r\n",
      "[2.19647883e-214 4.80181168e-214]\r\n",
      "790\r\n",
      "[1.18001022e-214 2.57966832e-214]\r\n",
      "791\r\n",
      "[6.33934686e-215 1.38587039e-214]\r\n",
      "792\r\n",
      "[3.40567547e-215 7.44528564e-215]\r\n",
      "793\r\n",
      "[1.82962467e-215 3.99981690e-215]\r\n",
      "794\r\n",
      "[9.82925847e-216 2.14881416e-215]\r\n",
      "795\r\n",
      "[5.28055417e-216 1.15440342e-215]\r\n",
      "796\r\n",
      "[2.83686225e-216 6.20177992e-216]\r\n",
      "797\r\n",
      "[1.52404221e-216 3.33177065e-216]\r\n",
      "798\r\n",
      "[8.18758350e-217 1.78992092e-216]\r\n",
      "799\r\n",
      "[4.39860019e-217 9.61595874e-217]\r\n",
      "800\r\n",
      "[2.36305177e-217 5.16596355e-217]\r\n",
      "801\r\n",
      "[1.26949789e-217 2.77530095e-217]\r\n",
      "802\r\n",
      "[6.82009979e-218 1.49096975e-217]\r\n",
      "803\r\n",
      "[3.66394947e-218 8.00990892e-218]\r\n",
      "804\r\n",
      "[1.96837673e-218 4.30314841e-218]\r\n",
      "805\r\n",
      "[1.05746736e-218 2.31177238e-218]\r\n",
      "806\r\n",
      "[5.68101212e-219 1.24194916e-218]\r\n",
      "807\r\n",
      "[3.05199953e-219 6.67210028e-219]\r\n",
      "808\r\n",
      "[1.63962001e-219 3.58443997e-219]\r\n",
      "809\r\n",
      "[8.80849995e-220 1.92566199e-219]\r\n",
      "810\r\n",
      "[4.73217398e-220 1.03451979e-219]\r\n",
      "811\r\n",
      "[2.54225699e-220 5.55773137e-220]\r\n",
      "812\r\n",
      "[1.36577197e-220 2.98576965e-220]\r\n",
      "813\r\n",
      "[7.33731126e-221 1.60403945e-220]\r\n",
      "814\r\n",
      "[3.94181003e-221 8.61735118e-221]\r\n",
      "815\r\n",
      "[2.11765124e-221 4.62948348e-221]\r\n",
      "816\r\n",
      "[1.13766182e-221 2.48708876e-221]\r\n",
      "817\r\n",
      "[6.11183935e-222 1.33613405e-221]\r\n",
      "818\r\n",
      "[3.28345204e-222 7.17808801e-222]\r\n",
      "819\r\n",
      "[1.76396281e-222 3.85627082e-222]\r\n",
      "820\r\n",
      "[9.47650444e-223 2.07169717e-222]\r\n",
      "821\r\n",
      "[5.09104478e-223 1.11297400e-222]\r\n",
      "822\r\n",
      "[2.73505248e-223 5.97920943e-223]\r\n",
      "823\r\n",
      "[1.46934713e-223 3.21219952e-223]\r\n",
      "824\r\n",
      "[7.89374617e-224 1.72568395e-223]\r\n",
      "825\r\n",
      "[4.24074251e-224 9.27085964e-224]\r\n",
      "826\r\n",
      "[2.27824618e-224 4.98056661e-224]\r\n",
      "827\r\n",
      "[1.22393794e-224 2.67570050e-224]\r\n",
      "828\r\n",
      "[6.57533893e-225 1.43746158e-224]\r\n",
      "829\r\n",
      "[3.53245706e-225 7.72244800e-225]\r\n",
      "830\r\n",
      "[1.89773531e-225 4.14871632e-225]\r\n",
      "831\r\n",
      "[1.01951679e-225 2.22880712e-225]\r\n",
      "832\r\n",
      "[5.47713103e-226 1.19737789e-225]\r\n",
      "833\r\n",
      "[2.94246888e-226 6.43265085e-226]\r\n",
      "834\r\n",
      "[1.58077706e-226 3.45580100e-226]\r\n",
      "835\r\n",
      "[8.49237907e-227 1.85655352e-226]\r\n",
      "836\r\n",
      "[4.56234494e-227 9.97392780e-227]\r\n",
      "837\r\n",
      "[2.45102005e-227 5.35827460e-227]\r\n",
      "838\r\n",
      "[1.31675693e-227 2.87861586e-227]\r\n",
      "839\r\n",
      "[7.07398864e-228 1.54647342e-227]\r\n",
      "840\r\n",
      "[3.80034570e-228 8.30809027e-228]\r\n",
      "841\r\n",
      "[2.04165263e-228 4.46333982e-228]\r\n",
      "842\r\n",
      "[1.09683323e-228 2.39783172e-228]\r\n",
      "843\r\n",
      "[5.89249666e-229 1.28818265e-228]\r\n",
      "844\r\n",
      "[3.16561497e-229 6.92047962e-229]\r\n",
      "845\r\n",
      "[1.70065742e-229 3.71787634e-229]\r\n",
      "846\r\n",
      "[9.13641010e-230 1.99734776e-229]\r\n",
      "847\r\n",
      "[4.90833653e-230 1.07303141e-229]\r\n",
      "848\r\n",
      "[2.63689647e-230 5.76462659e-230]\r\n",
      "849\r\n",
      "[1.41661496e-230 3.09691958e-230]\r\n",
      "850\r\n",
      "[7.61045412e-231 1.66375232e-230]\r\n",
      "851\r\n",
      "[4.08855005e-231 8.93814552e-231]\r\n",
      "852\r\n",
      "[2.19648411e-231 4.80182322e-231]\r\n",
      "853\r\n",
      "[1.18001306e-231 2.57967452e-231]\r\n",
      "854\r\n",
      "[6.33936210e-232 1.38587373e-231]\r\n",
      "855\r\n",
      "[3.40568366e-232 7.44530353e-232]\r\n",
      "856\r\n",
      "[1.82962907e-232 3.99982651e-232]\r\n",
      "857\r\n",
      "[9.82928209e-233 2.14881933e-232]\r\n",
      "858\r\n",
      "[5.28056686e-233 1.15440619e-232]\r\n",
      "859\r\n",
      "[2.83686907e-233 6.20179483e-233]\r\n",
      "860\r\n",
      "[1.52404587e-233 3.33177865e-233]\r\n",
      "861\r\n",
      "[8.18760318e-234 1.78992522e-233]\r\n",
      "862\r\n",
      "[4.39861076e-234 9.61598185e-234]\r\n",
      "863\r\n",
      "[2.36305745e-234 5.16597597e-234]\r\n",
      "864\r\n",
      "[1.26950094e-234 2.77530762e-234]\r\n",
      "865\r\n",
      "[6.82011618e-235 1.49097333e-234]\r\n",
      "866\r\n",
      "[3.66395828e-235 8.00992817e-235]\r\n",
      "867\r\n",
      "[1.96838146e-235 4.30315875e-235]\r\n",
      "868\r\n",
      "[1.05746990e-235 2.31177794e-235]\r\n",
      "869\r\n",
      "[5.68102578e-236 1.24195215e-235]\r\n",
      "870\r\n",
      "[3.05200687e-236 6.67211631e-236]\r\n",
      "871\r\n",
      "[1.63962395e-236 3.58444859e-236]\r\n",
      "872\r\n",
      "[8.80852112e-237 1.92566662e-236]\r\n",
      "873\r\n",
      "[4.73218535e-237 1.03452228e-236]\r\n",
      "874\r\n",
      "[2.54226310e-237 5.55774473e-237]\r\n",
      "875\r\n",
      "[1.36577526e-237 2.98577682e-237]\r\n",
      "876\r\n",
      "[7.33732890e-238 1.60404331e-237]\r\n",
      "877\r\n",
      "[3.94181950e-238 8.61737189e-238]\r\n",
      "878\r\n",
      "[2.11765633e-238 4.62949461e-238]\r\n",
      "879\r\n",
      "[1.13766455e-238 2.48709474e-238]\r\n",
      "880\r\n",
      "[6.11185404e-239 1.33613726e-238]\r\n",
      "881\r\n",
      "[3.28345993e-239 7.17810526e-239]\r\n",
      "882\r\n",
      "[1.76396705e-239 3.85628008e-239]\r\n",
      "883\r\n",
      "[9.47652721e-240 2.07170215e-239]\r\n",
      "884\r\n",
      "[5.09105701e-240 1.11297668e-239]\r\n",
      "885\r\n",
      "[2.73505905e-240 5.97922380e-240]\r\n",
      "886\r\n",
      "[1.46935067e-240 3.21220724e-240]\r\n",
      "887\r\n",
      "[7.89376514e-241 1.72568809e-240]\r\n",
      "888\r\n",
      "[4.24075270e-241 9.27088192e-241]\r\n",
      "889\r\n",
      "[2.27825165e-241 4.98057858e-241]\r\n",
      "890\r\n",
      "[1.22394088e-241 2.67570693e-241]\r\n",
      "891\r\n",
      "[6.57535474e-242 1.43746504e-241]\r\n",
      "892\r\n",
      "[3.53246555e-242 7.72246656e-242]\r\n",
      "893\r\n",
      "[1.89773987e-242 4.14872629e-242]\r\n",
      "894\r\n",
      "[1.01951924e-242 2.22881248e-242]\r\n",
      "895\r\n",
      "[5.47714420e-243 1.19738076e-242]\r\n",
      "896\r\n",
      "[2.94247595e-243 6.43266631e-243]\r\n",
      "897\r\n",
      "[1.58078086e-243 3.45580931e-243]\r\n",
      "898\r\n",
      "[8.49239948e-244 1.85655798e-243]\r\n",
      "899\r\n",
      "[4.56235591e-244 9.97395177e-244]\r\n",
      "900\r\n",
      "[2.45102594e-244 5.35828748e-244]\r\n",
      "901\r\n",
      "[1.31676009e-244 2.87862278e-244]\r\n",
      "902\r\n",
      "[7.07400564e-245 1.54647714e-244]\r\n",
      "903\r\n",
      "[3.80035484e-245 8.30811024e-245]\r\n",
      "904\r\n",
      "[2.04165753e-245 4.46335055e-245]\r\n",
      "905\r\n",
      "[1.09683586e-245 2.39783748e-245]\r\n",
      "906\r\n",
      "[5.89251082e-246 1.28818575e-245]\r\n",
      "907\r\n",
      "[3.16562258e-246 6.92049625e-246]\r\n",
      "908\r\n",
      "[1.70066151e-246 3.71788527e-246]\r\n",
      "909\r\n",
      "[9.13643206e-247 1.99735256e-246]\r\n",
      "910\r\n",
      "[4.90834833e-247 1.07303399e-246]\r\n",
      "911\r\n",
      "[2.63690281e-247 5.76464044e-247]\r\n",
      "912\r\n",
      "[1.41661837e-247 3.09692702e-247]\r\n",
      "913\r\n",
      "[7.61047241e-248 1.66375632e-247]\r\n",
      "914\r\n",
      "[4.08855988e-248 8.93816700e-248]\r\n",
      "915\r\n",
      "[2.19648939e-248 4.80183476e-248]\r\n",
      "916\r\n",
      "[1.18001589e-248 2.57968072e-248]\r\n",
      "917\r\n",
      "[6.33937733e-249 1.38587706e-248]\r\n",
      "918\r\n",
      "[3.40569184e-249 7.44532142e-249]\r\n",
      "919\r\n",
      "[1.82963347e-249 3.99983612e-249]\r\n",
      "920\r\n",
      "[9.82930571e-250 2.14882449e-249]\r\n",
      "921\r\n",
      "[5.28057955e-250 1.15440897e-249]\r\n",
      "922\r\n",
      "[2.83687589e-250 6.20180973e-250]\r\n",
      "923\r\n",
      "[1.52404954e-250 3.33178666e-250]\r\n",
      "924\r\n",
      "[8.18762286e-251 1.78992953e-250]\r\n",
      "925\r\n",
      "[4.39862134e-251 9.61600496e-251]\r\n",
      "926\r\n",
      "[2.36306313e-251 5.16598838e-251]\r\n",
      "927\r\n",
      "[1.26950399e-251 2.77531429e-251]\r\n",
      "928\r\n",
      "[6.82013257e-252 1.49097691e-251]\r\n",
      "929\r\n",
      "[3.66396708e-252 8.00994742e-252]\r\n",
      "930\r\n",
      "[1.96838619e-252 4.30316909e-252]\r\n",
      "931\r\n",
      "[1.05747244e-252 2.31178350e-252]\r\n",
      "932\r\n",
      "[5.68103943e-253 1.24195513e-252]\r\n",
      "933\r\n",
      "[3.05201420e-253 6.67213235e-253]\r\n",
      "934\r\n",
      "[1.63962789e-253 3.58445720e-253]\r\n",
      "935\r\n",
      "[8.80854229e-254 1.92567125e-253]\r\n",
      "936\r\n",
      "[4.73219672e-254 1.03452477e-253]\r\n",
      "937\r\n",
      "[2.54226921e-254 5.55775809e-254]\r\n",
      "938\r\n",
      "[1.36577854e-254 2.98578400e-254]\r\n",
      "939\r\n",
      "[7.33734653e-255 1.60404716e-254]\r\n",
      "940\r\n",
      "[3.94182897e-255 8.61739260e-255]\r\n",
      "941\r\n",
      "[2.11766142e-255 4.62950573e-255]\r\n",
      "942\r\n",
      "[1.13766729e-255 2.48710072e-255]\r\n",
      "943\r\n",
      "[6.11186872e-256 1.33614047e-255]\r\n",
      "944\r\n",
      "[3.28346782e-256 7.17812251e-256]\r\n",
      "945\r\n",
      "[1.76397128e-256 3.85628935e-256]\r\n",
      "946\r\n",
      "[9.47654999e-257 2.07170713e-256]\r\n",
      "947\r\n",
      "[5.09106925e-257 1.11297935e-256]\r\n",
      "948\r\n",
      "[2.73506562e-257 5.97923817e-257]\r\n",
      "949\r\n",
      "[1.46935420e-257 3.21221496e-257]\r\n",
      "950\r\n",
      "[7.89378411e-258 1.72569224e-257]\r\n",
      "951\r\n",
      "[4.2407629e-258 9.2709042e-258]\r\n",
      "952\r\n",
      "[2.27825713e-258 4.98059055e-258]\r\n",
      "953\r\n",
      "[1.22394382e-258 2.67571336e-258]\r\n",
      "954\r\n",
      "[6.57537054e-259 1.43746849e-258]\r\n",
      "955\r\n",
      "[3.53247403e-259 7.72248512e-259]\r\n",
      "956\r\n",
      "[1.89774443e-259 4.14873626e-259]\r\n",
      "957\r\n",
      "[1.01952169e-259 2.22881783e-259]\r\n",
      "958\r\n",
      "[5.47715736e-260 1.19738364e-259]\r\n",
      "959\r\n",
      "[2.94248302e-260 6.43268177e-260]\r\n",
      "960\r\n",
      "[1.58078466e-260 3.45581762e-260]\r\n",
      "961\r\n",
      "[8.49241989e-261 1.85656244e-260]\r\n",
      "962\r\n",
      "[4.56236687e-261 9.97397574e-261]\r\n",
      "963\r\n",
      "[2.45103184e-261 5.35830036e-261]\r\n",
      "964\r\n",
      "[1.31676326e-261 2.87862969e-261]\r\n",
      "965\r\n",
      "[7.07402264e-262 1.54648085e-261]\r\n",
      "966\r\n",
      "[3.80036397e-262 8.30813021e-262]\r\n",
      "967\r\n",
      "[2.04166244e-262 4.46336128e-262]\r\n",
      "968\r\n",
      "[1.09683850e-262 2.39784324e-262]\r\n",
      "969\r\n",
      "[5.89252498e-263 1.28818885e-262]\r\n",
      "970\r\n",
      "[3.16563019e-263 6.92051288e-263]\r\n",
      "971\r\n",
      "[1.70066560e-263 3.71789421e-263]\r\n",
      "972\r\n",
      "[9.13645401e-264 1.99735736e-263]\r\n",
      "973\r\n",
      "[4.90836013e-264 1.07303657e-263]\r\n",
      "974\r\n",
      "[2.63690914e-264 5.76465430e-264]\r\n",
      "975\r\n",
      "[1.41662177e-264 3.09693446e-264]\r\n",
      "976\r\n",
      "[7.61049070e-265 1.66376032e-264]\r\n",
      "977\r\n",
      "[4.08856970e-265 8.93818848e-265]\r\n",
      "978\r\n",
      "[2.19649467e-265 4.80184630e-265]\r\n",
      "979\r\n",
      "[1.18001873e-265 2.57968692e-265]\r\n",
      "980\r\n",
      "[6.33939257e-266 1.38588039e-265]\r\n",
      "981\r\n",
      "[3.40570003e-266 7.44533932e-266]\r\n",
      "982\r\n",
      "[1.82963787e-266 3.99984574e-266]\r\n",
      "983\r\n",
      "[9.82932934e-267 2.14882965e-266]\r\n",
      "984\r\n",
      "[5.28059224e-267 1.15441174e-266]\r\n",
      "985\r\n",
      "[2.83688271e-267 6.20182464e-267]\r\n",
      "986\r\n",
      "[1.52405320e-267 3.33179467e-267]\r\n",
      "987\r\n",
      "[8.18764254e-268 1.78993383e-267]\r\n",
      "988\r\n",
      "[4.39863191e-268 9.61602807e-268]\r\n",
      "989\r\n",
      "[2.3630688e-268 5.1660008e-268]\r\n",
      "990\r\n",
      "[1.26950704e-268 2.77532096e-268]\r\n",
      "991\r\n",
      "[6.82014896e-269 1.49098050e-268]\r\n",
      "992\r\n",
      "[3.66397589e-269 8.00996668e-269]\r\n",
      "993\r\n",
      "[1.96839092e-269 4.30317944e-269]\r\n",
      "994\r\n",
      "[1.05747498e-269 2.31178905e-269]\r\n",
      "995\r\n",
      "[5.68105308e-270 1.24195811e-269]\r\n",
      "996\r\n",
      "[3.05202154e-270 6.67214838e-270]\r\n",
      "997\r\n",
      "[1.63963183e-270 3.58446582e-270]\r\n",
      "998\r\n",
      "[8.80856346e-271 1.92567588e-270]\r\n",
      "999\r\n",
      "[4.73220810e-271 1.03452725e-270]\r\n",
      "[4.73220810e-271 1.03452725e-270]\r\n",
      "[0. 0.]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -12. ]\n",
      " [  0.    1.2 -12. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 303122.35it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:36:04.866360: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:36:05.243635: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:36:05.243673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:36:05.394581: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:36:06.114240: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 106685.93it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 91.6766 - pos_accuracy: 0.0094 - val_loss: 20.9054 - val_pos_accuracy: 0.0481\n",
      "Epoch 2/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.4457 - pos_accuracy: 0.0944 - val_loss: 3.4129 - val_pos_accuracy: 0.1611\n",
      "Epoch 3/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.2828 - pos_accuracy: 0.1869 - val_loss: 2.0082 - val_pos_accuracy: 0.2260\n",
      "Epoch 4/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.4990 - pos_accuracy: 0.2375 - val_loss: 1.5178 - val_pos_accuracy: 0.2620\n",
      "Epoch 5/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.1586 - pos_accuracy: 0.2725 - val_loss: 1.2663 - val_pos_accuracy: 0.2740\n",
      "Epoch 6/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.9607 - pos_accuracy: 0.3056 - val_loss: 1.1234 - val_pos_accuracy: 0.3149\n",
      "Epoch 7/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.8241 - pos_accuracy: 0.3606 - val_loss: 0.9780 - val_pos_accuracy: 0.3774\n",
      "Epoch 8/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.7219 - pos_accuracy: 0.3963 - val_loss: 0.9465 - val_pos_accuracy: 0.3389\n",
      "Epoch 9/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6424 - pos_accuracy: 0.4056 - val_loss: 0.8268 - val_pos_accuracy: 0.3966\n",
      "Epoch 10/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5808 - pos_accuracy: 0.4462 - val_loss: 0.7621 - val_pos_accuracy: 0.4231\n",
      "Epoch 11/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5247 - pos_accuracy: 0.4837 - val_loss: 0.7057 - val_pos_accuracy: 0.4231\n",
      "Epoch 12/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4913 - pos_accuracy: 0.5100 - val_loss: 0.6633 - val_pos_accuracy: 0.4327\n",
      "Epoch 13/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4511 - pos_accuracy: 0.5238 - val_loss: 0.6295 - val_pos_accuracy: 0.4423\n",
      "Epoch 14/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4185 - pos_accuracy: 0.5587 - val_loss: 0.5956 - val_pos_accuracy: 0.4760\n",
      "Epoch 15/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3849 - pos_accuracy: 0.5631 - val_loss: 0.6271 - val_pos_accuracy: 0.4087\n",
      "Epoch 16/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3625 - pos_accuracy: 0.5725 - val_loss: 0.5570 - val_pos_accuracy: 0.4712\n",
      "Epoch 17/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3400 - pos_accuracy: 0.5969 - val_loss: 0.5158 - val_pos_accuracy: 0.5120\n",
      "Epoch 18/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3195 - pos_accuracy: 0.6169 - val_loss: 0.5308 - val_pos_accuracy: 0.4663\n",
      "Epoch 19/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3019 - pos_accuracy: 0.6256 - val_loss: 0.4908 - val_pos_accuracy: 0.5048\n",
      "Epoch 20/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2881 - pos_accuracy: 0.6494 - val_loss: 0.4844 - val_pos_accuracy: 0.5144\n",
      "Epoch 21/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2721 - pos_accuracy: 0.6781 - val_loss: 0.4594 - val_pos_accuracy: 0.5361\n",
      "Epoch 22/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2583 - pos_accuracy: 0.6712 - val_loss: 0.4388 - val_pos_accuracy: 0.5697\n",
      "Epoch 23/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2457 - pos_accuracy: 0.6925 - val_loss: 0.4239 - val_pos_accuracy: 0.5625\n",
      "Epoch 24/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2323 - pos_accuracy: 0.7038 - val_loss: 0.4078 - val_pos_accuracy: 0.5697\n",
      "Epoch 25/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2228 - pos_accuracy: 0.7169 - val_loss: 0.4022 - val_pos_accuracy: 0.5938\n",
      "Epoch 26/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2115 - pos_accuracy: 0.7275 - val_loss: 0.3850 - val_pos_accuracy: 0.6058\n",
      "Epoch 27/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2014 - pos_accuracy: 0.7431 - val_loss: 0.3738 - val_pos_accuracy: 0.6106\n",
      "Epoch 28/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1925 - pos_accuracy: 0.7481 - val_loss: 0.3676 - val_pos_accuracy: 0.6346\n",
      "Epoch 29/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1840 - pos_accuracy: 0.7544 - val_loss: 0.3595 - val_pos_accuracy: 0.6442\n",
      "Epoch 30/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1763 - pos_accuracy: 0.7713 - val_loss: 0.3587 - val_pos_accuracy: 0.6106\n",
      "Epoch 31/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1692 - pos_accuracy: 0.7700 - val_loss: 0.3403 - val_pos_accuracy: 0.6538\n",
      "Epoch 32/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1630 - pos_accuracy: 0.7831 - val_loss: 0.3378 - val_pos_accuracy: 0.6538\n",
      "Epoch 33/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1556 - pos_accuracy: 0.7937 - val_loss: 0.3305 - val_pos_accuracy: 0.6635\n",
      "Epoch 34/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1487 - pos_accuracy: 0.8112 - val_loss: 0.3261 - val_pos_accuracy: 0.6635\n",
      "Epoch 35/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1438 - pos_accuracy: 0.8062 - val_loss: 0.3153 - val_pos_accuracy: 0.6731\n",
      "Epoch 36/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1388 - pos_accuracy: 0.8112 - val_loss: 0.3101 - val_pos_accuracy: 0.6851\n",
      "Epoch 37/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1338 - pos_accuracy: 0.8163 - val_loss: 0.3031 - val_pos_accuracy: 0.6851\n",
      "Epoch 38/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1277 - pos_accuracy: 0.8288 - val_loss: 0.2985 - val_pos_accuracy: 0.6923\n",
      "Epoch 39/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1229 - pos_accuracy: 0.8431 - val_loss: 0.2949 - val_pos_accuracy: 0.7043\n",
      "Epoch 40/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1192 - pos_accuracy: 0.8356 - val_loss: 0.2914 - val_pos_accuracy: 0.7188\n",
      "Epoch 41/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1160 - pos_accuracy: 0.8456 - val_loss: 0.2867 - val_pos_accuracy: 0.7212\n",
      "Epoch 42/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1109 - pos_accuracy: 0.8519 - val_loss: 0.2870 - val_pos_accuracy: 0.7212\n",
      "Epoch 43/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1074 - pos_accuracy: 0.8581 - val_loss: 0.2777 - val_pos_accuracy: 0.7284\n",
      "Epoch 44/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1024 - pos_accuracy: 0.8612 - val_loss: 0.2751 - val_pos_accuracy: 0.7212\n",
      "Epoch 45/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1012 - pos_accuracy: 0.8681 - val_loss: 0.2708 - val_pos_accuracy: 0.7380\n",
      "Epoch 46/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0970 - pos_accuracy: 0.8669 - val_loss: 0.2689 - val_pos_accuracy: 0.7452\n",
      "Epoch 47/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0925 - pos_accuracy: 0.8788 - val_loss: 0.2708 - val_pos_accuracy: 0.7308\n",
      "Epoch 48/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0906 - pos_accuracy: 0.8781 - val_loss: 0.2654 - val_pos_accuracy: 0.7572\n",
      "Epoch 49/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0877 - pos_accuracy: 0.8856 - val_loss: 0.2611 - val_pos_accuracy: 0.7812\n",
      "Epoch 50/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0845 - pos_accuracy: 0.8944 - val_loss: 0.2552 - val_pos_accuracy: 0.7716\n",
      "Epoch 51/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0817 - pos_accuracy: 0.8925 - val_loss: 0.2617 - val_pos_accuracy: 0.7596\n",
      "Epoch 52/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0795 - pos_accuracy: 0.9006 - val_loss: 0.2491 - val_pos_accuracy: 0.7692\n",
      "Epoch 53/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0770 - pos_accuracy: 0.9038 - val_loss: 0.2481 - val_pos_accuracy: 0.7837\n",
      "Epoch 54/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0744 - pos_accuracy: 0.9094 - val_loss: 0.2495 - val_pos_accuracy: 0.7716\n",
      "Epoch 55/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0719 - pos_accuracy: 0.9106 - val_loss: 0.2426 - val_pos_accuracy: 0.7933\n",
      "Epoch 56/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0701 - pos_accuracy: 0.9187 - val_loss: 0.2409 - val_pos_accuracy: 0.8005\n",
      "Epoch 57/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0677 - pos_accuracy: 0.9187 - val_loss: 0.2382 - val_pos_accuracy: 0.7981\n",
      "Epoch 58/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0655 - pos_accuracy: 0.9187 - val_loss: 0.2375 - val_pos_accuracy: 0.8029\n",
      "Epoch 59/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0645 - pos_accuracy: 0.9219 - val_loss: 0.2337 - val_pos_accuracy: 0.8053\n",
      "Epoch 60/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0623 - pos_accuracy: 0.9275 - val_loss: 0.2366 - val_pos_accuracy: 0.7957\n",
      "Epoch 61/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0606 - pos_accuracy: 0.9256 - val_loss: 0.2310 - val_pos_accuracy: 0.8125\n",
      "Epoch 62/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0589 - pos_accuracy: 0.9312 - val_loss: 0.2280 - val_pos_accuracy: 0.8077\n",
      "Epoch 63/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0575 - pos_accuracy: 0.9344 - val_loss: 0.2271 - val_pos_accuracy: 0.8149\n",
      "Epoch 64/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0550 - pos_accuracy: 0.9356 - val_loss: 0.2282 - val_pos_accuracy: 0.8221\n",
      "Epoch 65/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0541 - pos_accuracy: 0.9381 - val_loss: 0.2252 - val_pos_accuracy: 0.8197\n",
      "Epoch 66/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0523 - pos_accuracy: 0.9375 - val_loss: 0.2238 - val_pos_accuracy: 0.8269\n",
      "Epoch 67/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0514 - pos_accuracy: 0.9406 - val_loss: 0.2210 - val_pos_accuracy: 0.8269\n",
      "Epoch 68/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0499 - pos_accuracy: 0.9425 - val_loss: 0.2198 - val_pos_accuracy: 0.8293\n",
      "Epoch 69/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0485 - pos_accuracy: 0.9419 - val_loss: 0.2191 - val_pos_accuracy: 0.8293\n",
      "Epoch 70/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0473 - pos_accuracy: 0.9438 - val_loss: 0.2176 - val_pos_accuracy: 0.8341\n",
      "Epoch 71/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0464 - pos_accuracy: 0.9481 - val_loss: 0.2151 - val_pos_accuracy: 0.8341\n",
      "Epoch 72/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0449 - pos_accuracy: 0.9488 - val_loss: 0.2153 - val_pos_accuracy: 0.8341\n",
      "Epoch 73/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0445 - pos_accuracy: 0.9488 - val_loss: 0.2125 - val_pos_accuracy: 0.8269\n",
      "Epoch 74/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0427 - pos_accuracy: 0.9525 - val_loss: 0.2100 - val_pos_accuracy: 0.8341\n",
      "Epoch 75/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0417 - pos_accuracy: 0.9513 - val_loss: 0.2119 - val_pos_accuracy: 0.8317\n",
      "Epoch 76/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0405 - pos_accuracy: 0.9556 - val_loss: 0.2110 - val_pos_accuracy: 0.8341\n",
      "Epoch 77/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0397 - pos_accuracy: 0.9556 - val_loss: 0.2085 - val_pos_accuracy: 0.8341\n",
      "Epoch 78/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0384 - pos_accuracy: 0.9563 - val_loss: 0.2078 - val_pos_accuracy: 0.8341\n",
      "Epoch 79/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0378 - pos_accuracy: 0.9556 - val_loss: 0.2079 - val_pos_accuracy: 0.8389\n",
      "Epoch 80/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0367 - pos_accuracy: 0.9550 - val_loss: 0.2067 - val_pos_accuracy: 0.8341\n",
      "Epoch 81/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0357 - pos_accuracy: 0.9600 - val_loss: 0.2045 - val_pos_accuracy: 0.8389\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0350 - pos_accuracy: 0.9600 - val_loss: 0.2030 - val_pos_accuracy: 0.8365\n",
      "Epoch 83/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0339 - pos_accuracy: 0.9631 - val_loss: 0.2031 - val_pos_accuracy: 0.8389\n",
      "Epoch 84/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0334 - pos_accuracy: 0.9625 - val_loss: 0.2051 - val_pos_accuracy: 0.8365\n",
      "Epoch 85/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0325 - pos_accuracy: 0.9613 - val_loss: 0.2003 - val_pos_accuracy: 0.8438\n",
      "Epoch 86/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0320 - pos_accuracy: 0.9663 - val_loss: 0.2010 - val_pos_accuracy: 0.8438\n",
      "Epoch 87/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0310 - pos_accuracy: 0.9638 - val_loss: 0.1991 - val_pos_accuracy: 0.8462\n",
      "Epoch 88/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0305 - pos_accuracy: 0.9656 - val_loss: 0.1992 - val_pos_accuracy: 0.8438\n",
      "Epoch 89/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0299 - pos_accuracy: 0.9650 - val_loss: 0.1984 - val_pos_accuracy: 0.8462\n",
      "Epoch 90/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0288 - pos_accuracy: 0.9669 - val_loss: 0.1979 - val_pos_accuracy: 0.8486\n",
      "Epoch 91/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0282 - pos_accuracy: 0.9681 - val_loss: 0.1967 - val_pos_accuracy: 0.8462\n",
      "Epoch 92/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0279 - pos_accuracy: 0.9694 - val_loss: 0.1971 - val_pos_accuracy: 0.8534\n",
      "Epoch 93/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0269 - pos_accuracy: 0.9688 - val_loss: 0.1960 - val_pos_accuracy: 0.8510\n",
      "Epoch 94/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0264 - pos_accuracy: 0.9700 - val_loss: 0.1947 - val_pos_accuracy: 0.8510\n",
      "Epoch 95/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0260 - pos_accuracy: 0.9688 - val_loss: 0.1942 - val_pos_accuracy: 0.8486\n",
      "Epoch 96/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0255 - pos_accuracy: 0.9706 - val_loss: 0.1926 - val_pos_accuracy: 0.8510\n",
      "Epoch 97/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0249 - pos_accuracy: 0.9700 - val_loss: 0.1922 - val_pos_accuracy: 0.8462\n",
      "Epoch 98/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0243 - pos_accuracy: 0.9700 - val_loss: 0.1915 - val_pos_accuracy: 0.8510\n",
      "Epoch 99/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0237 - pos_accuracy: 0.9731 - val_loss: 0.1922 - val_pos_accuracy: 0.8510\n",
      "Epoch 100/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0233 - pos_accuracy: 0.9719 - val_loss: 0.1912 - val_pos_accuracy: 0.8558\n",
      "Epoch 101/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0229 - pos_accuracy: 0.9737 - val_loss: 0.1895 - val_pos_accuracy: 0.8534\n",
      "Epoch 102/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0225 - pos_accuracy: 0.9719 - val_loss: 0.1891 - val_pos_accuracy: 0.8558\n",
      "Epoch 103/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0220 - pos_accuracy: 0.9737 - val_loss: 0.1891 - val_pos_accuracy: 0.8558\n",
      "Epoch 104/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0213 - pos_accuracy: 0.9737 - val_loss: 0.1888 - val_pos_accuracy: 0.8558\n",
      "Epoch 105/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0209 - pos_accuracy: 0.9756 - val_loss: 0.1876 - val_pos_accuracy: 0.8534\n",
      "Epoch 106/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0205 - pos_accuracy: 0.9762 - val_loss: 0.1869 - val_pos_accuracy: 0.8558\n",
      "Epoch 107/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0202 - pos_accuracy: 0.9737 - val_loss: 0.1882 - val_pos_accuracy: 0.8534\n",
      "Epoch 108/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0197 - pos_accuracy: 0.9769 - val_loss: 0.1873 - val_pos_accuracy: 0.8558\n",
      "Epoch 109/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0193 - pos_accuracy: 0.9787 - val_loss: 0.1865 - val_pos_accuracy: 0.8534\n",
      "Epoch 110/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0191 - pos_accuracy: 0.9800 - val_loss: 0.1856 - val_pos_accuracy: 0.8558\n",
      "Epoch 111/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0186 - pos_accuracy: 0.9787 - val_loss: 0.1845 - val_pos_accuracy: 0.8558\n",
      "Epoch 112/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0181 - pos_accuracy: 0.9800 - val_loss: 0.1854 - val_pos_accuracy: 0.8606\n",
      "Epoch 113/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0180 - pos_accuracy: 0.9787 - val_loss: 0.1846 - val_pos_accuracy: 0.8606\n",
      "Epoch 114/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9812 - val_loss: 0.1847 - val_pos_accuracy: 0.8558\n",
      "Epoch 115/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0173 - pos_accuracy: 0.9787 - val_loss: 0.1835 - val_pos_accuracy: 0.8582\n",
      "Epoch 116/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0170 - pos_accuracy: 0.9812 - val_loss: 0.1838 - val_pos_accuracy: 0.8630\n",
      "Epoch 117/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0166 - pos_accuracy: 0.9812 - val_loss: 0.1834 - val_pos_accuracy: 0.8606\n",
      "Epoch 118/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0163 - pos_accuracy: 0.9806 - val_loss: 0.1828 - val_pos_accuracy: 0.8630\n",
      "Epoch 119/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0160 - pos_accuracy: 0.9825 - val_loss: 0.1827 - val_pos_accuracy: 0.8630\n",
      "Epoch 120/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0156 - pos_accuracy: 0.9831 - val_loss: 0.1822 - val_pos_accuracy: 0.8630\n",
      "Epoch 121/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0155 - pos_accuracy: 0.9831 - val_loss: 0.1816 - val_pos_accuracy: 0.8582\n",
      "Epoch 122/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0152 - pos_accuracy: 0.9837 - val_loss: 0.1808 - val_pos_accuracy: 0.8654\n",
      "Epoch 123/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0149 - pos_accuracy: 0.9831 - val_loss: 0.1803 - val_pos_accuracy: 0.8678\n",
      "Epoch 124/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0146 - pos_accuracy: 0.9844 - val_loss: 0.1797 - val_pos_accuracy: 0.8654\n",
      "Epoch 125/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0145 - pos_accuracy: 0.9844 - val_loss: 0.1796 - val_pos_accuracy: 0.8678\n",
      "Epoch 126/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0140 - pos_accuracy: 0.9862 - val_loss: 0.1795 - val_pos_accuracy: 0.8654\n",
      "Epoch 127/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0138 - pos_accuracy: 0.9850 - val_loss: 0.1798 - val_pos_accuracy: 0.8678\n",
      "Epoch 128/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0135 - pos_accuracy: 0.9862 - val_loss: 0.1787 - val_pos_accuracy: 0.8678\n",
      "Epoch 129/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0134 - pos_accuracy: 0.9856 - val_loss: 0.1781 - val_pos_accuracy: 0.8678\n",
      "Epoch 130/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0131 - pos_accuracy: 0.9856 - val_loss: 0.1781 - val_pos_accuracy: 0.8702\n",
      "Epoch 131/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0128 - pos_accuracy: 0.9869 - val_loss: 0.1781 - val_pos_accuracy: 0.8702\n",
      "Epoch 132/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0126 - pos_accuracy: 0.9869 - val_loss: 0.1766 - val_pos_accuracy: 0.8726\n",
      "Epoch 133/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0124 - pos_accuracy: 0.9862 - val_loss: 0.1777 - val_pos_accuracy: 0.8726\n",
      "Epoch 134/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0122 - pos_accuracy: 0.9869 - val_loss: 0.1778 - val_pos_accuracy: 0.8678\n",
      "Epoch 135/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0120 - pos_accuracy: 0.9869 - val_loss: 0.1769 - val_pos_accuracy: 0.8702\n",
      "Epoch 136/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0117 - pos_accuracy: 0.9875 - val_loss: 0.1775 - val_pos_accuracy: 0.8702\n",
      "Epoch 137/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0115 - pos_accuracy: 0.9881 - val_loss: 0.1766 - val_pos_accuracy: 0.8702\n",
      "Epoch 138/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0114 - pos_accuracy: 0.9869 - val_loss: 0.1760 - val_pos_accuracy: 0.8726\n",
      "Epoch 139/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0112 - pos_accuracy: 0.9881 - val_loss: 0.1753 - val_pos_accuracy: 0.8726\n",
      "Epoch 140/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0111 - pos_accuracy: 0.9875 - val_loss: 0.1758 - val_pos_accuracy: 0.8726\n",
      "Epoch 141/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0108 - pos_accuracy: 0.9881 - val_loss: 0.1753 - val_pos_accuracy: 0.8726\n",
      "Epoch 142/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0107 - pos_accuracy: 0.9894 - val_loss: 0.1748 - val_pos_accuracy: 0.8678\n",
      "Epoch 143/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0104 - pos_accuracy: 0.9887 - val_loss: 0.1751 - val_pos_accuracy: 0.8750\n",
      "Epoch 144/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0103 - pos_accuracy: 0.9894 - val_loss: 0.1755 - val_pos_accuracy: 0.8678\n",
      "Epoch 145/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0102 - pos_accuracy: 0.9894 - val_loss: 0.1739 - val_pos_accuracy: 0.8750\n",
      "Epoch 146/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0100 - pos_accuracy: 0.9894 - val_loss: 0.1749 - val_pos_accuracy: 0.8774\n",
      "Epoch 147/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0098 - pos_accuracy: 0.9900 - val_loss: 0.1737 - val_pos_accuracy: 0.8750\n",
      "Epoch 148/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0096 - pos_accuracy: 0.9900 - val_loss: 0.1732 - val_pos_accuracy: 0.8774\n",
      "Epoch 149/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9919 - val_loss: 0.1734 - val_pos_accuracy: 0.8750\n",
      "Epoch 150/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0093 - pos_accuracy: 0.9912 - val_loss: 0.1730 - val_pos_accuracy: 0.8750\n",
      "Epoch 151/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0092 - pos_accuracy: 0.9906 - val_loss: 0.1731 - val_pos_accuracy: 0.8750\n",
      "Epoch 152/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0091 - pos_accuracy: 0.9912 - val_loss: 0.1729 - val_pos_accuracy: 0.8774\n",
      "Epoch 153/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0089 - pos_accuracy: 0.9919 - val_loss: 0.1726 - val_pos_accuracy: 0.8750\n",
      "Epoch 154/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0088 - pos_accuracy: 0.9919 - val_loss: 0.1721 - val_pos_accuracy: 0.8774\n",
      "Epoch 155/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0086 - pos_accuracy: 0.9925 - val_loss: 0.1730 - val_pos_accuracy: 0.8774\n",
      "Epoch 156/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0085 - pos_accuracy: 0.9919 - val_loss: 0.1725 - val_pos_accuracy: 0.8774\n",
      "Epoch 157/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9925 - val_loss: 0.1716 - val_pos_accuracy: 0.8774\n",
      "Epoch 158/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0082 - pos_accuracy: 0.9925 - val_loss: 0.1718 - val_pos_accuracy: 0.8798\n",
      "Epoch 159/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0080 - pos_accuracy: 0.9931 - val_loss: 0.1720 - val_pos_accuracy: 0.8798\n",
      "Epoch 160/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0080 - pos_accuracy: 0.9925 - val_loss: 0.1711 - val_pos_accuracy: 0.8774\n",
      "Epoch 161/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9925 - val_loss: 0.1710 - val_pos_accuracy: 0.8798\n",
      "Epoch 162/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0077 - pos_accuracy: 0.9931 - val_loss: 0.1709 - val_pos_accuracy: 0.8798\n",
      "Epoch 163/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0076 - pos_accuracy: 0.9919 - val_loss: 0.1716 - val_pos_accuracy: 0.8774\n",
      "Epoch 164/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0075 - pos_accuracy: 0.9931 - val_loss: 0.1710 - val_pos_accuracy: 0.8774\n",
      "Epoch 165/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0074 - pos_accuracy: 0.9931 - val_loss: 0.1703 - val_pos_accuracy: 0.8798\n",
      "Epoch 166/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0072 - pos_accuracy: 0.9937 - val_loss: 0.1702 - val_pos_accuracy: 0.8798\n",
      "Epoch 167/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0071 - pos_accuracy: 0.9937 - val_loss: 0.1698 - val_pos_accuracy: 0.8774\n",
      "Epoch 168/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9944 - val_loss: 0.1696 - val_pos_accuracy: 0.8822\n",
      "Epoch 169/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9937 - val_loss: 0.1696 - val_pos_accuracy: 0.8822\n",
      "Epoch 170/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0068 - pos_accuracy: 0.9937 - val_loss: 0.1695 - val_pos_accuracy: 0.8822\n",
      "Epoch 171/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9944 - val_loss: 0.1693 - val_pos_accuracy: 0.8822\n",
      "Epoch 172/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0066 - pos_accuracy: 0.9937 - val_loss: 0.1687 - val_pos_accuracy: 0.8822\n",
      "Epoch 173/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9937 - val_loss: 0.1694 - val_pos_accuracy: 0.8774\n",
      "Epoch 174/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9944 - val_loss: 0.1695 - val_pos_accuracy: 0.8822\n",
      "Epoch 175/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0063 - pos_accuracy: 0.9944 - val_loss: 0.1696 - val_pos_accuracy: 0.8774\n",
      "Epoch 176/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0062 - pos_accuracy: 0.9950 - val_loss: 0.1682 - val_pos_accuracy: 0.8822\n",
      "Epoch 177/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0062 - pos_accuracy: 0.9944 - val_loss: 0.1683 - val_pos_accuracy: 0.8822\n",
      "Epoch 178/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9944 - val_loss: 0.1685 - val_pos_accuracy: 0.8822\n",
      "Epoch 179/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9944 - val_loss: 0.1682 - val_pos_accuracy: 0.8822\n",
      "Epoch 180/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0059 - pos_accuracy: 0.9950 - val_loss: 0.1683 - val_pos_accuracy: 0.8822\n",
      "Epoch 181/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9956 - val_loss: 0.1681 - val_pos_accuracy: 0.8822\n",
      "Epoch 182/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9956 - val_loss: 0.1682 - val_pos_accuracy: 0.8846\n",
      "Epoch 183/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9950 - val_loss: 0.1679 - val_pos_accuracy: 0.8798\n",
      "Epoch 184/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9950 - val_loss: 0.1675 - val_pos_accuracy: 0.8846\n",
      "Epoch 185/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9956 - val_loss: 0.1673 - val_pos_accuracy: 0.8846\n",
      "Epoch 186/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9956 - val_loss: 0.1673 - val_pos_accuracy: 0.8798\n",
      "Epoch 187/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9969 - val_loss: 0.1673 - val_pos_accuracy: 0.8798\n",
      "Epoch 188/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9956 - val_loss: 0.1670 - val_pos_accuracy: 0.8798\n",
      "Epoch 189/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9962 - val_loss: 0.1669 - val_pos_accuracy: 0.8846\n",
      "Epoch 190/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9969 - val_loss: 0.1671 - val_pos_accuracy: 0.8798\n",
      "Epoch 191/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9969 - val_loss: 0.1670 - val_pos_accuracy: 0.8798\n",
      "Epoch 192/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9969 - val_loss: 0.1670 - val_pos_accuracy: 0.8822\n",
      "Epoch 193/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9969 - val_loss: 0.1667 - val_pos_accuracy: 0.8798\n",
      "Epoch 194/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9969 - val_loss: 0.1669 - val_pos_accuracy: 0.8798\n",
      "Epoch 195/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9969 - val_loss: 0.1664 - val_pos_accuracy: 0.8846\n",
      "Epoch 196/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9969 - val_loss: 0.1666 - val_pos_accuracy: 0.8846\n",
      "Epoch 197/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9969 - val_loss: 0.1661 - val_pos_accuracy: 0.8798\n",
      "Epoch 198/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9975 - val_loss: 0.1658 - val_pos_accuracy: 0.8798\n",
      "Epoch 199/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9975 - val_loss: 0.1663 - val_pos_accuracy: 0.8774\n",
      "Epoch 200/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9975 - val_loss: 0.1658 - val_pos_accuracy: 0.8798\n",
      "Epoch 201/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9975 - val_loss: 0.1653 - val_pos_accuracy: 0.8846\n",
      "Epoch 202/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9975 - val_loss: 0.1650 - val_pos_accuracy: 0.8846\n",
      "Epoch 203/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0042 - pos_accuracy: 0.9975 - val_loss: 0.1656 - val_pos_accuracy: 0.8774\n",
      "Epoch 204/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0042 - pos_accuracy: 0.9975 - val_loss: 0.1654 - val_pos_accuracy: 0.8798\n",
      "Epoch 205/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9975 - val_loss: 0.1652 - val_pos_accuracy: 0.8798\n",
      "Epoch 206/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9975 - val_loss: 0.1653 - val_pos_accuracy: 0.8846\n",
      "Epoch 207/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9975 - val_loss: 0.1652 - val_pos_accuracy: 0.8798\n",
      "Epoch 208/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9975 - val_loss: 0.1646 - val_pos_accuracy: 0.8846\n",
      "Epoch 209/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9975 - val_loss: 0.1651 - val_pos_accuracy: 0.8846\n",
      "Epoch 210/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9975 - val_loss: 0.1652 - val_pos_accuracy: 0.8798\n",
      "Epoch 211/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9975 - val_loss: 0.1651 - val_pos_accuracy: 0.8774\n",
      "Epoch 212/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9975 - val_loss: 0.1650 - val_pos_accuracy: 0.8774\n",
      "Epoch 213/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9975 - val_loss: 0.1649 - val_pos_accuracy: 0.8774\n",
      "Epoch 214/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9975 - val_loss: 0.1645 - val_pos_accuracy: 0.8774\n",
      "Epoch 215/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9975 - val_loss: 0.1643 - val_pos_accuracy: 0.8774\n",
      "Epoch 216/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9975 - val_loss: 0.1643 - val_pos_accuracy: 0.8798\n",
      "Epoch 217/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9975 - val_loss: 0.1641 - val_pos_accuracy: 0.8774\n",
      "Epoch 218/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9975 - val_loss: 0.1647 - val_pos_accuracy: 0.8774\n",
      "Epoch 219/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9975 - val_loss: 0.1642 - val_pos_accuracy: 0.8774\n",
      "Epoch 220/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9975 - val_loss: 0.1639 - val_pos_accuracy: 0.8774\n",
      "Epoch 221/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9975 - val_loss: 0.1638 - val_pos_accuracy: 0.8774\n",
      "Epoch 222/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9975 - val_loss: 0.1638 - val_pos_accuracy: 0.8774\n",
      "Epoch 223/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9975 - val_loss: 0.1639 - val_pos_accuracy: 0.8774\n",
      "Epoch 224/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9975 - val_loss: 0.1636 - val_pos_accuracy: 0.8750\n",
      "Epoch 225/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9975 - val_loss: 0.1634 - val_pos_accuracy: 0.8774\n",
      "Epoch 226/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9975 - val_loss: 0.1638 - val_pos_accuracy: 0.8774\n",
      "Epoch 227/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9975 - val_loss: 0.1634 - val_pos_accuracy: 0.8774\n",
      "Epoch 228/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9975 - val_loss: 0.1638 - val_pos_accuracy: 0.8774\n",
      "Epoch 229/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9975 - val_loss: 0.1633 - val_pos_accuracy: 0.8798\n",
      "Epoch 230/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9975 - val_loss: 0.1630 - val_pos_accuracy: 0.8774\n",
      "Epoch 231/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9975 - val_loss: 0.1632 - val_pos_accuracy: 0.8774\n",
      "Epoch 232/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0029 - pos_accuracy: 0.9975 - val_loss: 0.1631 - val_pos_accuracy: 0.8798\n",
      "Epoch 233/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9975 - val_loss: 0.1629 - val_pos_accuracy: 0.8774\n",
      "Epoch 234/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9975 - val_loss: 0.1631 - val_pos_accuracy: 0.8774\n",
      "Epoch 235/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9975 - val_loss: 0.1627 - val_pos_accuracy: 0.8774\n",
      "Epoch 236/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9975 - val_loss: 0.1632 - val_pos_accuracy: 0.8750\n",
      "Epoch 237/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9975 - val_loss: 0.1626 - val_pos_accuracy: 0.8774\n",
      "Epoch 238/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9975 - val_loss: 0.1630 - val_pos_accuracy: 0.8774\n",
      "Epoch 239/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9975 - val_loss: 0.1628 - val_pos_accuracy: 0.8774\n",
      "Epoch 240/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9975 - val_loss: 0.1628 - val_pos_accuracy: 0.8774\n",
      "Epoch 241/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9975 - val_loss: 0.1628 - val_pos_accuracy: 0.8750\n",
      "Epoch 242/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9975 - val_loss: 0.1622 - val_pos_accuracy: 0.8750\n",
      "Epoch 243/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9975 - val_loss: 0.1622 - val_pos_accuracy: 0.8750\n",
      "Epoch 244/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9975 - val_loss: 0.1623 - val_pos_accuracy: 0.8750\n",
      "Epoch 245/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9975 - val_loss: 0.1622 - val_pos_accuracy: 0.8750\n",
      "Epoch 246/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9981 - val_loss: 0.1621 - val_pos_accuracy: 0.8774\n",
      "Epoch 247/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9975 - val_loss: 0.1618 - val_pos_accuracy: 0.8798\n",
      "Epoch 248/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9981 - val_loss: 0.1621 - val_pos_accuracy: 0.8774\n",
      "Epoch 249/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1620 - val_pos_accuracy: 0.8750\n",
      "Epoch 250/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9981 - val_loss: 0.1620 - val_pos_accuracy: 0.8750\n",
      "Epoch 251/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9981 - val_loss: 0.1620 - val_pos_accuracy: 0.8750\n",
      "Epoch 252/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.1622 - val_pos_accuracy: 0.8774\n",
      "Epoch 253/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.1617 - val_pos_accuracy: 0.8798\n",
      "Epoch 254/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1619 - val_pos_accuracy: 0.8774\n",
      "Epoch 255/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.1616 - val_pos_accuracy: 0.8750\n",
      "Epoch 256/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1615 - val_pos_accuracy: 0.8750\n",
      "Epoch 257/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1617 - val_pos_accuracy: 0.8750\n",
      "Epoch 258/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1616 - val_pos_accuracy: 0.8774\n",
      "Epoch 259/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1615 - val_pos_accuracy: 0.8750\n",
      "Epoch 260/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1614 - val_pos_accuracy: 0.8750\n",
      "Epoch 261/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1612 - val_pos_accuracy: 0.8750\n",
      "Epoch 262/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1614 - val_pos_accuracy: 0.8750\n",
      "Epoch 263/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1613 - val_pos_accuracy: 0.8750\n",
      "Epoch 264/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1616 - val_pos_accuracy: 0.8750\n",
      "Epoch 265/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1612 - val_pos_accuracy: 0.8750\n",
      "Epoch 266/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1615 - val_pos_accuracy: 0.8750\n",
      "Epoch 267/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1615 - val_pos_accuracy: 0.8750\n",
      "Epoch 268/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1612 - val_pos_accuracy: 0.8750\n",
      "Epoch 269/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1610 - val_pos_accuracy: 0.8750\n",
      "Epoch 270/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1611 - val_pos_accuracy: 0.8750\n",
      "Epoch 271/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1609 - val_pos_accuracy: 0.8750\n",
      "Epoch 272/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1613 - val_pos_accuracy: 0.8774\n",
      "Epoch 273/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1606 - val_pos_accuracy: 0.8750\n",
      "Epoch 274/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1608 - val_pos_accuracy: 0.8750\n",
      "Epoch 275/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1608 - val_pos_accuracy: 0.8750\n",
      "Epoch 276/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1607 - val_pos_accuracy: 0.8750\n",
      "Epoch 277/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1607 - val_pos_accuracy: 0.8750\n",
      "Epoch 278/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1609 - val_pos_accuracy: 0.8774\n",
      "Epoch 279/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1606 - val_pos_accuracy: 0.8750\n",
      "Epoch 280/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1607 - val_pos_accuracy: 0.8750\n",
      "Epoch 281/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1603 - val_pos_accuracy: 0.8750\n",
      "Epoch 282/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1606 - val_pos_accuracy: 0.8750\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:37:01.599037: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.795207\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1761: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\na = np.array([[0, 1, 2], [3, 4, 5,]])\\nb = np.array([[6, 7, 8], [9, 10, 11]])\\nnp.concatenate((a, b), axis = 1) \\n결과 \\n[[ 0  1  2  6  7  8]\\n [ 3  4  5  9 10 11]]\\n\\n2d 배열을 왼쪽에 오른쪽을 붙여줍니다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\nsubplot(nrows,ncols,index)은 plot을 배열 처럼 사용 가능합니다.\\nnrows는 행의 수, ncols는 열의 수, index는 위치를 나타낸다. 좌측 상단에서 1로 시작하며 오른쪽으로 세어나갑니다. 해당 행을 다 센 경우 바로 아래의 행부터 이어서 셉니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\nAND 연산으로 모든 값이 1일때 1을 반납하고 나머지는 모두 0으로 리턴합니다. axis=1의 뜻은 인풋값을 1 차원으로 줄여서 결과를 보여줍니다. input shape (2,2) -> output shape(2,)\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n 저장되어 있는 라벨값과 모델에서 계산된 값이 일치하는지 여부(True, False)를 tf.cast 함수를 사용하여 실수로 변환한 후, tf.reduce_mean를 사용하여 평균을 계산합니다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n\\\"linear\\\" activation을 의미합니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n모델 학습시 '무작위'의 결과를 특정한 값으로 고정할 수 있습니다.\\n\\n\",\n",
      "        true,\n",
      "        \"\\n고수준 API를 사용하다 궁금한 점이 많이 생겼는데 수업을 들으면서 단계별로 결과를 확인해보니 도움이 많이 되었습니다. 교수님을 일찍 만났으면 좋았을거 같다는 생각이 드네요. 감사합니다. ^^\\n\"\n",
      "    ],\n",
      "    \"score\": 90.0,\n",
      "    \"accuracy\": 0.8798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\"report1/황주훈_46062.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/황주훈_46062.ipynb to python\n",
      "[NbConvertApp] Writing 35591 bytes to report1/황주훈_46062.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 5.  7.]\n",
      " [11. 13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:218: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[8.18746544e+307 1.78989511e+308]\n",
      "[0. 0.]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 308904.40it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:37:09.044533: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:37:09.422152: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:37:09.422222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:37:09.646187: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:37:10.252590: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 106023.86it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(2, 2)\n",
      "(2,)\n",
      "[ True False]\n",
      "[1. 0.]\n",
      "0.5\n",
      "Epoch 1/19\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 38.0050 - pos_accuracy: 0.0437 - val_loss: 3.8470 - val_pos_accuracy: 0.0875\n",
      "Epoch 2/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 2.5996 - pos_accuracy: 0.2250 - val_loss: 1.1704 - val_pos_accuracy: 0.2350\n",
      "Epoch 3/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 1.6635 - pos_accuracy: 0.3175 - val_loss: 1.3313 - val_pos_accuracy: 0.3075\n",
      "Epoch 4/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.5816 - pos_accuracy: 0.4738 - val_loss: 0.7312 - val_pos_accuracy: 0.4675\n",
      "Epoch 5/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.8557 - pos_accuracy: 0.4944 - val_loss: 0.4063 - val_pos_accuracy: 0.5800\n",
      "Epoch 6/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.2646 - pos_accuracy: 0.6525 - val_loss: 0.2895 - val_pos_accuracy: 0.6550\n",
      "Epoch 7/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.3288 - pos_accuracy: 0.6294 - val_loss: 0.2295 - val_pos_accuracy: 0.7150\n",
      "Epoch 8/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.1181 - pos_accuracy: 0.8094 - val_loss: 0.2721 - val_pos_accuracy: 0.7125\n",
      "Epoch 9/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.1046 - pos_accuracy: 0.8475 - val_loss: 0.1981 - val_pos_accuracy: 0.7975\n",
      "Epoch 10/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0914 - pos_accuracy: 0.8763 - val_loss: 0.2010 - val_pos_accuracy: 0.8050\n",
      "Epoch 11/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0647 - pos_accuracy: 0.9163 - val_loss: 0.1767 - val_pos_accuracy: 0.8250\n",
      "Epoch 12/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0485 - pos_accuracy: 0.9331 - val_loss: 0.1374 - val_pos_accuracy: 0.8850\n",
      "Epoch 13/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0453 - pos_accuracy: 0.9538 - val_loss: 0.1308 - val_pos_accuracy: 0.8875\n",
      "Epoch 14/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0386 - pos_accuracy: 0.9513 - val_loss: 0.1417 - val_pos_accuracy: 0.8825\n",
      "Epoch 15/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0239 - pos_accuracy: 0.9787 - val_loss: 0.1952 - val_pos_accuracy: 0.7775\n",
      "Epoch 16/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0184 - pos_accuracy: 0.9888 - val_loss: 0.1245 - val_pos_accuracy: 0.8800\n",
      "Epoch 17/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0194 - pos_accuracy: 0.9844 - val_loss: 0.1135 - val_pos_accuracy: 0.8900\n",
      "Epoch 18/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0177 - pos_accuracy: 0.9850 - val_loss: 0.1151 - val_pos_accuracy: 0.9025\n",
      "Epoch 19/19\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0154 - pos_accuracy: 0.9881 - val_loss: 0.1268 - val_pos_accuracy: 0.8800\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:37:17.515663: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.7952071\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1679: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할 : 선택한 축(axis) 방향으로 배열을 합쳐주는 역할\\naxis=1 : 2차원 배열에서 1은 열 방향을 의미\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n121 : 1행, 2열 배치의 첫번째 Plot을 지정\\n122 : 1행, 2열 배치의 두번째 Plot을 지정\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 92.72727272727272,\n",
      "    \"accuracy\": 0.9025\n",
      "}\"report1/윤두환_46057.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/윤두환_46057.ipynb to python\n",
      "[NbConvertApp] Writing 35722 bytes to report1/윤두환_46057.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]]\n",
      "[[ 5  7  9]\n",
      " [ 6  8 10]]\n",
      "[[17 23 29]\n",
      " [39 53 67]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 287882.49it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:37:25.735149: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:37:26.111113: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:37:26.111155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:37:26.334064: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:37:26.957249: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 91409.04it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 434,882\n",
      "Trainable params: 434,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 30.1974 - pos_accuracy: 0.0850 - val_loss: 2.5104 - val_pos_accuracy: 0.1635\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.7829 - pos_accuracy: 0.1975 - val_loss: 1.2191 - val_pos_accuracy: 0.2981\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.2115 - pos_accuracy: 0.2525 - val_loss: 1.6914 - val_pos_accuracy: 0.1683\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6275 - pos_accuracy: 0.4450 - val_loss: 0.6347 - val_pos_accuracy: 0.4279\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4888 - pos_accuracy: 0.4781 - val_loss: 0.6101 - val_pos_accuracy: 0.4183\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3803 - pos_accuracy: 0.5775 - val_loss: 0.4527 - val_pos_accuracy: 0.5745\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3260 - pos_accuracy: 0.6194 - val_loss: 0.3366 - val_pos_accuracy: 0.6851\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2283 - pos_accuracy: 0.6856 - val_loss: 0.3129 - val_pos_accuracy: 0.6562\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1897 - pos_accuracy: 0.7350 - val_loss: 0.3204 - val_pos_accuracy: 0.6442\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1503 - pos_accuracy: 0.7881 - val_loss: 0.2622 - val_pos_accuracy: 0.7356\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0999 - pos_accuracy: 0.8706 - val_loss: 0.2314 - val_pos_accuracy: 0.7500\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0982 - pos_accuracy: 0.8737 - val_loss: 0.1972 - val_pos_accuracy: 0.8101\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0830 - pos_accuracy: 0.8925 - val_loss: 0.2040 - val_pos_accuracy: 0.7909\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0783 - pos_accuracy: 0.9025 - val_loss: 0.1763 - val_pos_accuracy: 0.8413\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0496 - pos_accuracy: 0.9469 - val_loss: 0.1856 - val_pos_accuracy: 0.8245\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0470 - pos_accuracy: 0.9506 - val_loss: 0.2396 - val_pos_accuracy: 0.7356\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0450 - pos_accuracy: 0.9544 - val_loss: 0.1709 - val_pos_accuracy: 0.8197\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0353 - pos_accuracy: 0.9663 - val_loss: 0.1605 - val_pos_accuracy: 0.8606\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0319 - pos_accuracy: 0.9706 - val_loss: 0.1519 - val_pos_accuracy: 0.8534\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0312 - pos_accuracy: 0.9731 - val_loss: 0.1514 - val_pos_accuracy: 0.8678\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:37:33.223020: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1678: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\nconcatenate는 Numpy의 행렬을 자유롭게 합치는 메소드입니다.\\naxis=1의 의미는 열방향으로(좌우)로 합쳐주는 기준을 설정하는 것입니다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\nsubplot(121)은 subplot(1,2,1)과 같은 의미며, 각각은 nrows,ncols,index를 의미합니다.\\nnrows는 행의 수, ncols는 열의 수, index는 위치를 나타냅니다.\\nsubplot(121)은 행의 수1개, 열의 수 2개, 위치는 1번, subplot(122)는 121과 행과 열의 수는 같지만 위치가 2번입니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n행렬의 요소의 논리적 and 값을 나타냅니다. axis=1은 열기준으로 계산을 합니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n행렬의 요소의 평균을 계산합니다. 코드에서는 true와 pred의 정확도의 평균을 계산해줍니다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n패턴의 위치를 찾는 신경망으로, sigmoid or relu 함수의 값으로 분류하는 모델이 아니기 때문입니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\nrandom성을 제어하여, 난수의 생성패턴을 동일하게 관리하기 위해서입니다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n이론과 실습이 병행되어 강의를 이해 하는데 많은 도움이 되고 있습니다.\\n항상 좋은 강의 감사 드립니다. \\n\"\n",
      "    ],\n",
      "    \"score\": 93.33333333333333,\n",
      "    \"accuracy\": 0.8678\n",
      "}\"report1/이수정_46098.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이수정_46098.ipynb to python\n",
      "[NbConvertApp] Writing 35678 bytes to report1/이수정_46098.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 5.  7.]\n",
      " [11. 13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 307140.01it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:37:41.520533: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:37:41.900088: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:37:41.900126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:37:42.123920: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:37:42.731749: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 171416.47it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 6280      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 6,298\n",
      "Trainable params: 6,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 156.5827 - accuracy: 0.7319 - val_loss: 35.0722 - val_accuracy: 0.9225\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 23.7996 - accuracy: 0.9488 - val_loss: 14.9287 - val_accuracy: 0.9600\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:37:45.090803: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.7952071\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1680: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1701: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concatenate : 선택한 축을 따라 배열 시퀀스를 결합함.\\naxis=1 : 열방향(좌->우)를 의미함\\n\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\nsubplot : 단일 호출로 둘러싸는 Figure 객체를 포함하여 서브플롯의 공통 레이아웃을 편리하게 생성가능.\\n\\n plt.subplot(121) : 1개의 행, 2개의 열, 첫 번째 서브 플롯\\n plt.subplot(122) : 1개의 행, 2개의 열, 두 번째 서브 플롯.\\n\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\n-> tf.reduce_all : tensor가 지정한 축 방향에 있는 각 요소의 논리(and 연산)를 계산\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n\\n-> tf.reduce_mean : 배열에서 평균값 계산\\n   tf.reduce_mean(x) : 두번째 인자를 적지 않은 경우  변수 x가 가리키는 배열 전체  원소의 합을 원소 개수로 나누어 계산\\n   tf.reduce_mean(x, 0) : 열 단위 평균 계산\\n   tf.reduce_mean(x, 1) : 행 단위 평균 계산\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n-> 풀링층의 정보를 조합해서 최종 결과를 예측하기 때문\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n-> 컴퓨터의 난수는 정해진 알고리즘에 따라 값을 출력함\\n   seed명령어를 사용하지 않으면 무작위로 숫자가 출력됨.\\n   seed를 사용하면 seed값으로 같은 값을 넣을때마다 같은 숫자가 출력 가능하게 됨.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 74.54545454545455,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/이우근_46090.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이우근_46090.ipynb to python\n",
      "[NbConvertApp] Writing 36634 bytes to report1/이우근_46090.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "51\n",
      "421\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 301802.77it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:37:52.459644: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:37:52.840726: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:37:52.840780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:37:53.061575: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:37:53.664949: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 176175.74it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(2, 2)\n",
      "(2,)\n",
      "[ True False]\n",
      "[1. 0.]\n",
      "0.5\n",
      "\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 240.9341 - pos_accuracy: 0.0000e+00 - val_loss: 212.3778 - val_pos_accuracy: 0.0045\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - pos_accuracy: 6.2500e-04 - val_loss: 195.9663 - val_pos_accuracy: 0.0000e+00\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:37:55.953299: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 0 Axes>\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1715: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nconcatenate 함수는 Numpy 배열들을 하나로 합치는데 이용되며, axis 값을 조절하여 어떤 축을 기준으로 배열을 합칠지 정할 수 있습니다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n121 = 1x2(행x열)의 subplot을 생성한다는 의미이고 세 번째 인자 1은 생성된 두 개의 subplot 중 첫 번째 subplot을 의미합니다.\\n122 = 1x2(행x열)의 subplot을 생성한다는 의미이고 세 번째 인자 2는 생성된 두 개의 subplot 중 두 번째 subplot을 의미합니다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\nreduce_all의 역할은 And 연산을 수행하고 axis는 줄일 치수로 없을 경우 모든 치수를 줄입니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n특정 차원을 제거하고 평균을 구하는 함수입니다.\\n\",\n",
      "        false,\n",
      "        \"\\nrandom seed의 사용 이유 \\n난수를 생성할때마다 변경되면 일관된 결과를 얻기 힘들어 random seed를 사용해 난수를 생성하는 방식을 고정시킵니다.\\n\",\n",
      "        true,\n",
      "        \"\\n인터넷에서 검색하며 하나씩 풀어가는 과정에서 어려움은 있었지만, 개념에 대해 알 수 있었고\\n많이 배울 수 있었습니다. \\n\"\n",
      "    ],\n",
      "    \"score\": 90.9090909090909,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/서정민_46091.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/서정민_46091.ipynb to python\n",
      "[NbConvertApp] Writing 34629 bytes to report1/서정민_46091.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 285861.58it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:38:04.148683: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:38:04.534155: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:38:04.534194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:38:04.757018: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:38:05.357507: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 162841.32it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 108,866\n",
      "Trainable params: 108,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 152.3934 - pos_accuracy: 0.0012 - val_loss: 85.1816 - val_pos_accuracy: 0.0024\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 75.1301 - pos_accuracy: 0.0012 - val_loss: 61.9036 - val_pos_accuracy: 0.0048\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 60.7688 - pos_accuracy: 0.0025 - val_loss: 57.2728 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 56.0382 - pos_accuracy: 0.0031 - val_loss: 53.8520 - val_pos_accuracy: 0.0048\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 52.2731 - pos_accuracy: 0.0075 - val_loss: 50.1477 - val_pos_accuracy: 0.0048\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 48.5845 - pos_accuracy: 0.0081 - val_loss: 46.3299 - val_pos_accuracy: 0.0096\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 45.0474 - pos_accuracy: 0.0113 - val_loss: 42.6292 - val_pos_accuracy: 0.0096\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 41.6621 - pos_accuracy: 0.0100 - val_loss: 39.1059 - val_pos_accuracy: 0.0120\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 38.4267 - pos_accuracy: 0.0100 - val_loss: 35.6821 - val_pos_accuracy: 0.0096\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 35.2773 - pos_accuracy: 0.0094 - val_loss: 32.4756 - val_pos_accuracy: 0.0144\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:38:09.451634: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1656: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할: 배열들을 합치는데데 사용\\naxis: 합쳐지는 배열들의의 기준축을 설정\\n\",\n",
      "        \"\\nsubplot\\n121: 1행 2열 중 첫번째 위치에 사진 표시\\n122: 1행 2열 중 두번째째 위치에 사진 표시\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n텐서들간의 AND 연산을 한다.\\naxis = 해당 차원을을 기준으로 연산한다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다.\\n연산된 score의의 평균값을 출력한다. \\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n신경망에서 나온 값을 그대로 출력하기 위해\\n\",\n",
      "        \"\\nrandom seed의 사용 이유\\n난수를 고정시켜 일정한 환경에서 학습하기 위해서\\n\",\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요.\\n\"\n",
      "    ],\n",
      "    \"score\": 72.36363636363636,\n",
      "    \"accuracy\": 0.0144\n",
      "}\"report1/김지은_46012.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김지은_46012.ipynb to python\n",
      "[NbConvertApp] Writing 36161 bytes to report1/김지은_46012.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.40558544e+73 7.44508881e+73]\n",
      "[0. 0.]\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "(3, 2)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 289571.89it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:38:15.919358: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:38:16.299150: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:38:16.299188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:38:16.522325: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:38:17.125992: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 214.1780 - val_loss: 160.1478\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 145.9341 - val_loss: 109.9643\n",
      "(3, 2)\n",
      "(3,)\n",
      "[ True False  True]\n",
      "[1. 0. 1.]\n",
      "0.6666667\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 100894.95it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 104,674\n",
      "Trainable params: 104,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 227.1019 - pos_accuracy: 6.2500e-04 - val_loss: 179.6730 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 131.7850 - pos_accuracy: 0.0012 - val_loss: 52.4972 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 40.2428 - pos_accuracy: 0.0044 - val_loss: 31.2926 - val_pos_accuracy: 0.0045\n",
      "Epoch 4/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 30.2151 - pos_accuracy: 0.0050 - val_loss: 25.5636 - val_pos_accuracy: 0.0112\n",
      "Epoch 5/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 24.6807 - pos_accuracy: 0.0119 - val_loss: 20.6268 - val_pos_accuracy: 0.0089\n",
      "Epoch 6/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 19.2442 - pos_accuracy: 0.0181 - val_loss: 15.6469 - val_pos_accuracy: 0.0246\n",
      "Epoch 7/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 13.9216 - pos_accuracy: 0.0388 - val_loss: 11.0848 - val_pos_accuracy: 0.0469\n",
      "Epoch 8/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 9.4076 - pos_accuracy: 0.0675 - val_loss: 7.6320 - val_pos_accuracy: 0.0603\n",
      "Epoch 9/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.3338 - pos_accuracy: 0.0856 - val_loss: 5.5015 - val_pos_accuracy: 0.1429\n",
      "Epoch 10/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4.5551 - pos_accuracy: 0.1363 - val_loss: 4.2914 - val_pos_accuracy: 0.1830\n",
      "Epoch 11/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 3.5673 - pos_accuracy: 0.1694 - val_loss: 3.5899 - val_pos_accuracy: 0.1942\n",
      "Epoch 12/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.9845 - pos_accuracy: 0.1863 - val_loss: 3.1351 - val_pos_accuracy: 0.2121\n",
      "Epoch 13/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2.5983 - pos_accuracy: 0.1994 - val_loss: 2.8098 - val_pos_accuracy: 0.2188\n",
      "Epoch 14/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2.3165 - pos_accuracy: 0.2106 - val_loss: 2.5613 - val_pos_accuracy: 0.2545\n",
      "Epoch 15/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2.0992 - pos_accuracy: 0.2244 - val_loss: 2.3581 - val_pos_accuracy: 0.2701\n",
      "Epoch 16/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.9267 - pos_accuracy: 0.2275 - val_loss: 2.1883 - val_pos_accuracy: 0.3036\n",
      "Epoch 17/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7807 - pos_accuracy: 0.2519 - val_loss: 2.0384 - val_pos_accuracy: 0.2969\n",
      "Epoch 18/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6620 - pos_accuracy: 0.2700 - val_loss: 1.9367 - val_pos_accuracy: 0.3147\n",
      "Epoch 19/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5616 - pos_accuracy: 0.2775 - val_loss: 1.8253 - val_pos_accuracy: 0.3036\n",
      "Epoch 20/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.4733 - pos_accuracy: 0.2831 - val_loss: 1.7364 - val_pos_accuracy: 0.3080\n",
      "Epoch 21/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.3949 - pos_accuracy: 0.2937 - val_loss: 1.6616 - val_pos_accuracy: 0.3192\n",
      "Epoch 22/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.3228 - pos_accuracy: 0.2875 - val_loss: 1.5870 - val_pos_accuracy: 0.3036\n",
      "Epoch 23/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2630 - pos_accuracy: 0.2956 - val_loss: 1.5215 - val_pos_accuracy: 0.2991\n",
      "Epoch 24/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.2061 - pos_accuracy: 0.2994 - val_loss: 1.4619 - val_pos_accuracy: 0.2946\n",
      "Epoch 25/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1581 - pos_accuracy: 0.3019 - val_loss: 1.4168 - val_pos_accuracy: 0.3125\n",
      "Epoch 26/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1111 - pos_accuracy: 0.3119 - val_loss: 1.3625 - val_pos_accuracy: 0.2924\n",
      "Epoch 27/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.0725 - pos_accuracy: 0.3181 - val_loss: 1.3200 - val_pos_accuracy: 0.2946\n",
      "Epoch 28/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0355 - pos_accuracy: 0.3225 - val_loss: 1.2793 - val_pos_accuracy: 0.3013\n",
      "Epoch 29/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.9981 - pos_accuracy: 0.3262 - val_loss: 1.2430 - val_pos_accuracy: 0.3036\n",
      "Epoch 30/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9644 - pos_accuracy: 0.3256 - val_loss: 1.2125 - val_pos_accuracy: 0.2969\n",
      "Epoch 31/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.9349 - pos_accuracy: 0.3262 - val_loss: 1.1836 - val_pos_accuracy: 0.3326\n",
      "Epoch 32/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9066 - pos_accuracy: 0.3319 - val_loss: 1.1540 - val_pos_accuracy: 0.3371\n",
      "Epoch 33/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.8797 - pos_accuracy: 0.3438 - val_loss: 1.1259 - val_pos_accuracy: 0.3571\n",
      "Epoch 34/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8540 - pos_accuracy: 0.3512 - val_loss: 1.0984 - val_pos_accuracy: 0.3393\n",
      "Epoch 35/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8321 - pos_accuracy: 0.3581 - val_loss: 1.0739 - val_pos_accuracy: 0.3638\n",
      "Epoch 36/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8109 - pos_accuracy: 0.3650 - val_loss: 1.0497 - val_pos_accuracy: 0.3438\n",
      "Epoch 37/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7899 - pos_accuracy: 0.3688 - val_loss: 1.0257 - val_pos_accuracy: 0.3438\n",
      "Epoch 38/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.7691 - pos_accuracy: 0.3725 - val_loss: 1.0063 - val_pos_accuracy: 0.3571\n",
      "Epoch 39/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.7521 - pos_accuracy: 0.3769 - val_loss: 0.9913 - val_pos_accuracy: 0.3594\n",
      "Epoch 40/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.7338 - pos_accuracy: 0.3906 - val_loss: 0.9693 - val_pos_accuracy: 0.3460\n",
      "Epoch 41/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.7174 - pos_accuracy: 0.3913 - val_loss: 0.9552 - val_pos_accuracy: 0.3728\n",
      "Epoch 42/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.7022 - pos_accuracy: 0.4038 - val_loss: 0.9386 - val_pos_accuracy: 0.3616\n",
      "Epoch 43/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6876 - pos_accuracy: 0.4119 - val_loss: 0.9202 - val_pos_accuracy: 0.3683\n",
      "Epoch 44/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6719 - pos_accuracy: 0.4187 - val_loss: 0.9033 - val_pos_accuracy: 0.3973\n",
      "Epoch 45/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6590 - pos_accuracy: 0.4244 - val_loss: 0.8890 - val_pos_accuracy: 0.3772\n",
      "Epoch 46/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6442 - pos_accuracy: 0.4306 - val_loss: 0.8755 - val_pos_accuracy: 0.3862\n",
      "Epoch 47/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6304 - pos_accuracy: 0.4419 - val_loss: 0.8597 - val_pos_accuracy: 0.4018\n",
      "Epoch 48/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6194 - pos_accuracy: 0.4481 - val_loss: 0.8488 - val_pos_accuracy: 0.4040\n",
      "Epoch 49/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6065 - pos_accuracy: 0.4575 - val_loss: 0.8395 - val_pos_accuracy: 0.4353\n",
      "Epoch 50/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5960 - pos_accuracy: 0.4594 - val_loss: 0.8222 - val_pos_accuracy: 0.4263\n",
      "Epoch 51/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.5836 - pos_accuracy: 0.4706 - val_loss: 0.8082 - val_pos_accuracy: 0.4129\n",
      "Epoch 52/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.5730 - pos_accuracy: 0.4769 - val_loss: 0.7985 - val_pos_accuracy: 0.4219\n",
      "Epoch 53/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5620 - pos_accuracy: 0.4762 - val_loss: 0.7859 - val_pos_accuracy: 0.4308\n",
      "Epoch 54/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.5535 - pos_accuracy: 0.4850 - val_loss: 0.7806 - val_pos_accuracy: 0.4375\n",
      "Epoch 55/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.5428 - pos_accuracy: 0.4850 - val_loss: 0.7684 - val_pos_accuracy: 0.4308\n",
      "Epoch 56/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5326 - pos_accuracy: 0.4900 - val_loss: 0.7551 - val_pos_accuracy: 0.4308\n",
      "Epoch 57/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5240 - pos_accuracy: 0.4950 - val_loss: 0.7455 - val_pos_accuracy: 0.4330\n",
      "Epoch 58/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5148 - pos_accuracy: 0.5050 - val_loss: 0.7388 - val_pos_accuracy: 0.4420\n",
      "Epoch 59/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5072 - pos_accuracy: 0.5075 - val_loss: 0.7292 - val_pos_accuracy: 0.4442\n",
      "Epoch 60/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4976 - pos_accuracy: 0.5113 - val_loss: 0.7191 - val_pos_accuracy: 0.4420\n",
      "Epoch 61/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4901 - pos_accuracy: 0.5156 - val_loss: 0.7115 - val_pos_accuracy: 0.4464\n",
      "Epoch 62/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4826 - pos_accuracy: 0.5250 - val_loss: 0.7028 - val_pos_accuracy: 0.4554\n",
      "Epoch 63/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4748 - pos_accuracy: 0.5288 - val_loss: 0.6950 - val_pos_accuracy: 0.4531\n",
      "Epoch 64/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4662 - pos_accuracy: 0.5306 - val_loss: 0.6883 - val_pos_accuracy: 0.4576\n",
      "Epoch 65/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4593 - pos_accuracy: 0.5362 - val_loss: 0.6816 - val_pos_accuracy: 0.4688\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4528 - pos_accuracy: 0.5425 - val_loss: 0.6732 - val_pos_accuracy: 0.4643\n",
      "Epoch 67/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4450 - pos_accuracy: 0.5444 - val_loss: 0.6690 - val_pos_accuracy: 0.4554\n",
      "Epoch 68/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4392 - pos_accuracy: 0.5450 - val_loss: 0.6576 - val_pos_accuracy: 0.4643\n",
      "Epoch 69/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4327 - pos_accuracy: 0.5469 - val_loss: 0.6499 - val_pos_accuracy: 0.4554\n",
      "Epoch 70/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4277 - pos_accuracy: 0.5537 - val_loss: 0.6442 - val_pos_accuracy: 0.4598\n",
      "Epoch 71/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4208 - pos_accuracy: 0.5525 - val_loss: 0.6369 - val_pos_accuracy: 0.4643\n",
      "Epoch 72/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4139 - pos_accuracy: 0.5656 - val_loss: 0.6324 - val_pos_accuracy: 0.4665\n",
      "Epoch 73/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4085 - pos_accuracy: 0.5625 - val_loss: 0.6252 - val_pos_accuracy: 0.4688\n",
      "Epoch 74/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4024 - pos_accuracy: 0.5775 - val_loss: 0.6210 - val_pos_accuracy: 0.4911\n",
      "Epoch 75/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3983 - pos_accuracy: 0.5750 - val_loss: 0.6161 - val_pos_accuracy: 0.4888\n",
      "Epoch 76/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3913 - pos_accuracy: 0.5744 - val_loss: 0.6078 - val_pos_accuracy: 0.4754\n",
      "Epoch 77/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3867 - pos_accuracy: 0.5838 - val_loss: 0.6039 - val_pos_accuracy: 0.4933\n",
      "Epoch 78/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3814 - pos_accuracy: 0.5856 - val_loss: 0.5964 - val_pos_accuracy: 0.4732\n",
      "Epoch 79/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3764 - pos_accuracy: 0.5850 - val_loss: 0.5917 - val_pos_accuracy: 0.4978\n",
      "Epoch 80/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3714 - pos_accuracy: 0.5925 - val_loss: 0.5872 - val_pos_accuracy: 0.4866\n",
      "Epoch 81/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3663 - pos_accuracy: 0.5894 - val_loss: 0.5805 - val_pos_accuracy: 0.4799\n",
      "Epoch 82/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3615 - pos_accuracy: 0.5987 - val_loss: 0.5753 - val_pos_accuracy: 0.4978\n",
      "Epoch 83/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3564 - pos_accuracy: 0.5931 - val_loss: 0.5713 - val_pos_accuracy: 0.4978\n",
      "Epoch 84/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3521 - pos_accuracy: 0.6006 - val_loss: 0.5663 - val_pos_accuracy: 0.4955\n",
      "Epoch 85/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3483 - pos_accuracy: 0.6012 - val_loss: 0.5621 - val_pos_accuracy: 0.4866\n",
      "Epoch 86/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3431 - pos_accuracy: 0.6012 - val_loss: 0.5566 - val_pos_accuracy: 0.4978\n",
      "Epoch 87/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3396 - pos_accuracy: 0.6031 - val_loss: 0.5532 - val_pos_accuracy: 0.4955\n",
      "Epoch 88/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3354 - pos_accuracy: 0.6050 - val_loss: 0.5481 - val_pos_accuracy: 0.4955\n",
      "Epoch 89/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3315 - pos_accuracy: 0.6069 - val_loss: 0.5438 - val_pos_accuracy: 0.4978\n",
      "Epoch 90/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3272 - pos_accuracy: 0.6031 - val_loss: 0.5400 - val_pos_accuracy: 0.4955\n",
      "Epoch 91/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3220 - pos_accuracy: 0.6100 - val_loss: 0.5355 - val_pos_accuracy: 0.5045\n",
      "Epoch 92/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3187 - pos_accuracy: 0.6081 - val_loss: 0.5318 - val_pos_accuracy: 0.5000\n",
      "Epoch 93/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3148 - pos_accuracy: 0.6056 - val_loss: 0.5292 - val_pos_accuracy: 0.5000\n",
      "Epoch 94/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3116 - pos_accuracy: 0.6150 - val_loss: 0.5246 - val_pos_accuracy: 0.4978\n",
      "Epoch 95/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3077 - pos_accuracy: 0.6187 - val_loss: 0.5212 - val_pos_accuracy: 0.4955\n",
      "Epoch 96/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3043 - pos_accuracy: 0.6206 - val_loss: 0.5181 - val_pos_accuracy: 0.4911\n",
      "Epoch 97/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3007 - pos_accuracy: 0.6212 - val_loss: 0.5135 - val_pos_accuracy: 0.5089\n",
      "Epoch 98/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2982 - pos_accuracy: 0.6263 - val_loss: 0.5101 - val_pos_accuracy: 0.5022\n",
      "Epoch 99/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2938 - pos_accuracy: 0.6225 - val_loss: 0.5073 - val_pos_accuracy: 0.4978\n",
      "Epoch 100/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2917 - pos_accuracy: 0.6313 - val_loss: 0.5033 - val_pos_accuracy: 0.4933\n",
      "Epoch 101/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2873 - pos_accuracy: 0.6319 - val_loss: 0.4996 - val_pos_accuracy: 0.5022\n",
      "Epoch 102/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2844 - pos_accuracy: 0.6369 - val_loss: 0.4961 - val_pos_accuracy: 0.5156\n",
      "Epoch 103/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2813 - pos_accuracy: 0.6456 - val_loss: 0.4923 - val_pos_accuracy: 0.5067\n",
      "Epoch 104/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2784 - pos_accuracy: 0.6356 - val_loss: 0.4894 - val_pos_accuracy: 0.5156\n",
      "Epoch 105/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2749 - pos_accuracy: 0.6444 - val_loss: 0.4861 - val_pos_accuracy: 0.5134\n",
      "Epoch 106/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2728 - pos_accuracy: 0.6550 - val_loss: 0.4827 - val_pos_accuracy: 0.5179\n",
      "Epoch 107/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2695 - pos_accuracy: 0.6550 - val_loss: 0.4794 - val_pos_accuracy: 0.5179\n",
      "Epoch 108/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2664 - pos_accuracy: 0.6513 - val_loss: 0.4764 - val_pos_accuracy: 0.5223\n",
      "Epoch 109/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2636 - pos_accuracy: 0.6556 - val_loss: 0.4736 - val_pos_accuracy: 0.5201\n",
      "Epoch 110/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2608 - pos_accuracy: 0.6637 - val_loss: 0.4708 - val_pos_accuracy: 0.5246\n",
      "Epoch 111/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2581 - pos_accuracy: 0.6600 - val_loss: 0.4675 - val_pos_accuracy: 0.5223\n",
      "Epoch 112/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2550 - pos_accuracy: 0.6625 - val_loss: 0.4646 - val_pos_accuracy: 0.5312\n",
      "Epoch 113/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2524 - pos_accuracy: 0.6681 - val_loss: 0.4619 - val_pos_accuracy: 0.5290\n",
      "Epoch 114/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2499 - pos_accuracy: 0.6675 - val_loss: 0.4602 - val_pos_accuracy: 0.5223\n",
      "Epoch 115/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2470 - pos_accuracy: 0.6725 - val_loss: 0.4562 - val_pos_accuracy: 0.5335\n",
      "Epoch 116/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2452 - pos_accuracy: 0.6794 - val_loss: 0.4536 - val_pos_accuracy: 0.5446\n",
      "Epoch 117/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2423 - pos_accuracy: 0.6806 - val_loss: 0.4513 - val_pos_accuracy: 0.5469\n",
      "Epoch 118/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2400 - pos_accuracy: 0.6812 - val_loss: 0.4491 - val_pos_accuracy: 0.5402\n",
      "Epoch 119/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2367 - pos_accuracy: 0.6819 - val_loss: 0.4457 - val_pos_accuracy: 0.5469\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2352 - pos_accuracy: 0.6850 - val_loss: 0.4435 - val_pos_accuracy: 0.5536\n",
      "Epoch 121/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2327 - pos_accuracy: 0.6919 - val_loss: 0.4417 - val_pos_accuracy: 0.5536\n",
      "Epoch 122/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2303 - pos_accuracy: 0.6906 - val_loss: 0.4387 - val_pos_accuracy: 0.5603\n",
      "Epoch 123/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2281 - pos_accuracy: 0.6931 - val_loss: 0.4357 - val_pos_accuracy: 0.5625\n",
      "Epoch 124/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2258 - pos_accuracy: 0.6969 - val_loss: 0.4324 - val_pos_accuracy: 0.5670\n",
      "Epoch 125/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2239 - pos_accuracy: 0.6969 - val_loss: 0.4304 - val_pos_accuracy: 0.5647\n",
      "Epoch 126/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2214 - pos_accuracy: 0.6944 - val_loss: 0.4285 - val_pos_accuracy: 0.5625\n",
      "Epoch 127/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2190 - pos_accuracy: 0.7075 - val_loss: 0.4263 - val_pos_accuracy: 0.5670\n",
      "Epoch 128/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2167 - pos_accuracy: 0.7119 - val_loss: 0.4235 - val_pos_accuracy: 0.5647\n",
      "Epoch 129/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2148 - pos_accuracy: 0.7044 - val_loss: 0.4211 - val_pos_accuracy: 0.5714\n",
      "Epoch 130/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2128 - pos_accuracy: 0.7113 - val_loss: 0.4192 - val_pos_accuracy: 0.5737\n",
      "Epoch 131/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2110 - pos_accuracy: 0.7050 - val_loss: 0.4172 - val_pos_accuracy: 0.5714\n",
      "Epoch 132/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2086 - pos_accuracy: 0.7150 - val_loss: 0.4149 - val_pos_accuracy: 0.5781\n",
      "Epoch 133/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2066 - pos_accuracy: 0.7175 - val_loss: 0.4136 - val_pos_accuracy: 0.5737\n",
      "Epoch 134/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2052 - pos_accuracy: 0.7225 - val_loss: 0.4113 - val_pos_accuracy: 0.5938\n",
      "Epoch 135/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2028 - pos_accuracy: 0.7250 - val_loss: 0.4095 - val_pos_accuracy: 0.5871\n",
      "Epoch 136/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2011 - pos_accuracy: 0.7237 - val_loss: 0.4076 - val_pos_accuracy: 0.5848\n",
      "Epoch 137/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1990 - pos_accuracy: 0.7312 - val_loss: 0.4049 - val_pos_accuracy: 0.5848\n",
      "Epoch 138/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1975 - pos_accuracy: 0.7325 - val_loss: 0.4025 - val_pos_accuracy: 0.5871\n",
      "Epoch 139/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1949 - pos_accuracy: 0.7369 - val_loss: 0.4020 - val_pos_accuracy: 0.6116\n",
      "Epoch 140/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1936 - pos_accuracy: 0.7356 - val_loss: 0.3984 - val_pos_accuracy: 0.6027\n",
      "Epoch 141/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1920 - pos_accuracy: 0.7419 - val_loss: 0.3969 - val_pos_accuracy: 0.6094\n",
      "Epoch 142/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1899 - pos_accuracy: 0.7456 - val_loss: 0.3954 - val_pos_accuracy: 0.5938\n",
      "Epoch 143/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1885 - pos_accuracy: 0.7456 - val_loss: 0.3934 - val_pos_accuracy: 0.6116\n",
      "Epoch 144/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1870 - pos_accuracy: 0.7425 - val_loss: 0.3913 - val_pos_accuracy: 0.6138\n",
      "Epoch 145/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1848 - pos_accuracy: 0.7469 - val_loss: 0.3897 - val_pos_accuracy: 0.6161\n",
      "Epoch 146/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1829 - pos_accuracy: 0.7494 - val_loss: 0.3875 - val_pos_accuracy: 0.6183\n",
      "Epoch 147/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1815 - pos_accuracy: 0.7500 - val_loss: 0.3853 - val_pos_accuracy: 0.6161\n",
      "Epoch 148/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1801 - pos_accuracy: 0.7544 - val_loss: 0.3832 - val_pos_accuracy: 0.6205\n",
      "Epoch 149/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1785 - pos_accuracy: 0.7513 - val_loss: 0.3821 - val_pos_accuracy: 0.6183\n",
      "Epoch 150/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1767 - pos_accuracy: 0.7544 - val_loss: 0.3803 - val_pos_accuracy: 0.6205\n",
      "Epoch 151/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1751 - pos_accuracy: 0.7519 - val_loss: 0.3787 - val_pos_accuracy: 0.6228\n",
      "Epoch 152/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1737 - pos_accuracy: 0.7556 - val_loss: 0.3767 - val_pos_accuracy: 0.6228\n",
      "Epoch 153/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1725 - pos_accuracy: 0.7588 - val_loss: 0.3755 - val_pos_accuracy: 0.6250\n",
      "Epoch 154/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1710 - pos_accuracy: 0.7619 - val_loss: 0.3736 - val_pos_accuracy: 0.6272\n",
      "Epoch 155/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1692 - pos_accuracy: 0.7631 - val_loss: 0.3730 - val_pos_accuracy: 0.6295\n",
      "Epoch 156/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1676 - pos_accuracy: 0.7613 - val_loss: 0.3707 - val_pos_accuracy: 0.6295\n",
      "Epoch 157/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1664 - pos_accuracy: 0.7606 - val_loss: 0.3691 - val_pos_accuracy: 0.6272\n",
      "Epoch 158/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1647 - pos_accuracy: 0.7669 - val_loss: 0.3672 - val_pos_accuracy: 0.6339\n",
      "Epoch 159/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1632 - pos_accuracy: 0.7656 - val_loss: 0.3655 - val_pos_accuracy: 0.6339\n",
      "Epoch 160/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1616 - pos_accuracy: 0.7713 - val_loss: 0.3641 - val_pos_accuracy: 0.6362\n",
      "Epoch 161/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1605 - pos_accuracy: 0.7713 - val_loss: 0.3630 - val_pos_accuracy: 0.6362\n",
      "Epoch 162/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1593 - pos_accuracy: 0.7756 - val_loss: 0.3611 - val_pos_accuracy: 0.6384\n",
      "Epoch 163/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1576 - pos_accuracy: 0.7756 - val_loss: 0.3598 - val_pos_accuracy: 0.6362\n",
      "Epoch 164/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1562 - pos_accuracy: 0.7763 - val_loss: 0.3582 - val_pos_accuracy: 0.6339\n",
      "Epoch 165/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1554 - pos_accuracy: 0.7781 - val_loss: 0.3561 - val_pos_accuracy: 0.6406\n",
      "Epoch 166/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1539 - pos_accuracy: 0.7788 - val_loss: 0.3548 - val_pos_accuracy: 0.6384\n",
      "Epoch 167/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1522 - pos_accuracy: 0.7831 - val_loss: 0.3536 - val_pos_accuracy: 0.6406\n",
      "Epoch 168/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1513 - pos_accuracy: 0.7875 - val_loss: 0.3517 - val_pos_accuracy: 0.6451\n",
      "Epoch 169/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1500 - pos_accuracy: 0.7881 - val_loss: 0.3500 - val_pos_accuracy: 0.6451\n",
      "Epoch 170/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1486 - pos_accuracy: 0.7906 - val_loss: 0.3487 - val_pos_accuracy: 0.6429\n",
      "Epoch 171/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1470 - pos_accuracy: 0.7887 - val_loss: 0.3473 - val_pos_accuracy: 0.6496\n",
      "Epoch 172/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1460 - pos_accuracy: 0.7962 - val_loss: 0.3455 - val_pos_accuracy: 0.6540\n",
      "Epoch 173/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1451 - pos_accuracy: 0.7944 - val_loss: 0.3450 - val_pos_accuracy: 0.6473\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1437 - pos_accuracy: 0.7962 - val_loss: 0.3434 - val_pos_accuracy: 0.6451\n",
      "Epoch 175/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1426 - pos_accuracy: 0.7894 - val_loss: 0.3420 - val_pos_accuracy: 0.6652\n",
      "Epoch 176/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1413 - pos_accuracy: 0.8019 - val_loss: 0.3406 - val_pos_accuracy: 0.6518\n",
      "Epoch 177/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1403 - pos_accuracy: 0.8000 - val_loss: 0.3395 - val_pos_accuracy: 0.6540\n",
      "Epoch 178/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1394 - pos_accuracy: 0.8019 - val_loss: 0.3389 - val_pos_accuracy: 0.6696\n",
      "Epoch 179/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1379 - pos_accuracy: 0.8050 - val_loss: 0.3375 - val_pos_accuracy: 0.6562\n",
      "Epoch 180/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1368 - pos_accuracy: 0.8037 - val_loss: 0.3356 - val_pos_accuracy: 0.6808\n",
      "Epoch 181/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1358 - pos_accuracy: 0.8069 - val_loss: 0.3345 - val_pos_accuracy: 0.6830\n",
      "Epoch 182/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1350 - pos_accuracy: 0.8100 - val_loss: 0.3340 - val_pos_accuracy: 0.6652\n",
      "Epoch 183/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1338 - pos_accuracy: 0.8125 - val_loss: 0.3320 - val_pos_accuracy: 0.6830\n",
      "Epoch 184/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1326 - pos_accuracy: 0.8131 - val_loss: 0.3307 - val_pos_accuracy: 0.6808\n",
      "Epoch 185/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1315 - pos_accuracy: 0.8156 - val_loss: 0.3300 - val_pos_accuracy: 0.6808\n",
      "Epoch 186/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1306 - pos_accuracy: 0.8175 - val_loss: 0.3283 - val_pos_accuracy: 0.6786\n",
      "Epoch 187/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1291 - pos_accuracy: 0.8169 - val_loss: 0.3284 - val_pos_accuracy: 0.6741\n",
      "Epoch 188/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1285 - pos_accuracy: 0.8244 - val_loss: 0.3268 - val_pos_accuracy: 0.6719\n",
      "Epoch 189/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1273 - pos_accuracy: 0.8231 - val_loss: 0.3253 - val_pos_accuracy: 0.6920\n",
      "Epoch 190/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1264 - pos_accuracy: 0.8194 - val_loss: 0.3244 - val_pos_accuracy: 0.6763\n",
      "Epoch 191/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1256 - pos_accuracy: 0.8269 - val_loss: 0.3227 - val_pos_accuracy: 0.6920\n",
      "Epoch 192/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1242 - pos_accuracy: 0.8231 - val_loss: 0.3228 - val_pos_accuracy: 0.6964\n",
      "Epoch 193/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1235 - pos_accuracy: 0.8256 - val_loss: 0.3207 - val_pos_accuracy: 0.6964\n",
      "Epoch 194/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1227 - pos_accuracy: 0.8306 - val_loss: 0.3201 - val_pos_accuracy: 0.6942\n",
      "Epoch 195/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1216 - pos_accuracy: 0.8300 - val_loss: 0.3193 - val_pos_accuracy: 0.6987\n",
      "Epoch 196/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1206 - pos_accuracy: 0.8294 - val_loss: 0.3172 - val_pos_accuracy: 0.6964\n",
      "Epoch 197/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1197 - pos_accuracy: 0.8300 - val_loss: 0.3166 - val_pos_accuracy: 0.6964\n",
      "Epoch 198/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1188 - pos_accuracy: 0.8313 - val_loss: 0.3155 - val_pos_accuracy: 0.6987\n",
      "Epoch 199/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1181 - pos_accuracy: 0.8344 - val_loss: 0.3145 - val_pos_accuracy: 0.6987\n",
      "Epoch 200/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1169 - pos_accuracy: 0.8331 - val_loss: 0.3134 - val_pos_accuracy: 0.6987\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:38:41.980433: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.8466230936819171\n",
      "convolution result = 0.84662306\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1711: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n: input_tensor의 논리 함수가 옳으면 true, 1을 반환하는 함수임. \\n  axis=1은 tensor에서 행 방향으로 element를 비교하기 위해 지정함.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n: tf.reduce_all을 통해 얻어진 논리 연산 값의 평균 계산. true 갑싱 많을 수록 1에 가까움\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n: true, false의 두 class의 이진분류를 구현하기 위함\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n: 매번 난수가 새로 발생하면 model의 효율성을 확인할 수 없기 때문임\\n\",\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 80.0,\n",
      "    \"accuracy\": 0.6987\n",
      "}\"report1/문예지_46065.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/문예지_46065.ipynb to python\n",
      "[NbConvertApp] Writing 35262 bytes to report1/문예지_46065.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:243: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -12. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 300032.48it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:38:50.136501: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:38:50.517618: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:38:50.517656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:38:50.741076: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:38:51.342455: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 102683.28it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 120.5008 - pos_accuracy: 0.0012 - val_loss: 80.0006 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 22.3188 - pos_accuracy: 0.0200 - val_loss: 16.0323 - val_pos_accuracy: 0.0089\n",
      "Epoch 3/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.6869 - pos_accuracy: 0.0556 - val_loss: 3.9610 - val_pos_accuracy: 0.0513\n",
      "Epoch 4/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.4256 - pos_accuracy: 0.1863 - val_loss: 0.8789 - val_pos_accuracy: 0.2969\n",
      "Epoch 5/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2.0244 - pos_accuracy: 0.1444 - val_loss: 1.2251 - val_pos_accuracy: 0.1830\n",
      "Epoch 6/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7595 - pos_accuracy: 0.2350 - val_loss: 0.5951 - val_pos_accuracy: 0.4442\n",
      "Epoch 7/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4354 - pos_accuracy: 0.4381 - val_loss: 0.4101 - val_pos_accuracy: 0.5156\n",
      "Epoch 8/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2615 - pos_accuracy: 0.5981 - val_loss: 0.3822 - val_pos_accuracy: 0.5558\n",
      "Epoch 9/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6898 - pos_accuracy: 0.2881 - val_loss: 0.3080 - val_pos_accuracy: 0.5938\n",
      "Epoch 10/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2750 - pos_accuracy: 0.5556 - val_loss: 0.3018 - val_pos_accuracy: 0.5759\n",
      "Epoch 11/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6654 - pos_accuracy: 0.3625 - val_loss: 1.0429 - val_pos_accuracy: 0.2098\n",
      "Epoch 12/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3755 - pos_accuracy: 0.4200 - val_loss: 0.2990 - val_pos_accuracy: 0.5826\n",
      "Epoch 13/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1920 - pos_accuracy: 0.6488 - val_loss: 0.2160 - val_pos_accuracy: 0.7835\n",
      "Epoch 14/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1242 - pos_accuracy: 0.8006 - val_loss: 0.2236 - val_pos_accuracy: 0.7165\n",
      "Epoch 15/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2262 - pos_accuracy: 0.5869 - val_loss: 0.2479 - val_pos_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2291 - pos_accuracy: 0.5825 - val_loss: 0.3207 - val_pos_accuracy: 0.4621\n",
      "Epoch 17/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1451 - pos_accuracy: 0.7606 - val_loss: 0.2590 - val_pos_accuracy: 0.5848\n",
      "Epoch 18/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1353 - pos_accuracy: 0.7694 - val_loss: 0.4004 - val_pos_accuracy: 0.4263\n",
      "Epoch 19/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1531 - pos_accuracy: 0.7013 - val_loss: 0.2241 - val_pos_accuracy: 0.6183\n",
      "Epoch 20/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2336 - pos_accuracy: 0.5644 - val_loss: 0.2933 - val_pos_accuracy: 0.5871\n",
      "Epoch 21/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2807 - pos_accuracy: 0.5169 - val_loss: 0.1849 - val_pos_accuracy: 0.7500\n",
      "Epoch 22/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0911 - pos_accuracy: 0.8600 - val_loss: 0.1575 - val_pos_accuracy: 0.7924\n",
      "Epoch 23/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0565 - pos_accuracy: 0.9244 - val_loss: 0.1406 - val_pos_accuracy: 0.8393\n",
      "Epoch 24/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0558 - pos_accuracy: 0.9300 - val_loss: 0.1483 - val_pos_accuracy: 0.8393\n",
      "Epoch 25/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0951 - pos_accuracy: 0.8375 - val_loss: 0.1459 - val_pos_accuracy: 0.8281\n",
      "Epoch 26/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0414 - pos_accuracy: 0.9544 - val_loss: 0.1402 - val_pos_accuracy: 0.8549\n",
      "Epoch 27/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0392 - pos_accuracy: 0.9506 - val_loss: 0.1142 - val_pos_accuracy: 0.8951\n",
      "Epoch 28/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0748 - pos_accuracy: 0.8813 - val_loss: 0.1655 - val_pos_accuracy: 0.7768\n",
      "Epoch 29/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0380 - pos_accuracy: 0.9606 - val_loss: 0.1056 - val_pos_accuracy: 0.9241\n",
      "Epoch 30/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0351 - pos_accuracy: 0.9663 - val_loss: 0.1322 - val_pos_accuracy: 0.8504\n",
      "Epoch 31/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0488 - pos_accuracy: 0.9488 - val_loss: 0.1159 - val_pos_accuracy: 0.8929\n",
      "Epoch 32/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0359 - pos_accuracy: 0.9644 - val_loss: 0.1037 - val_pos_accuracy: 0.9174\n",
      "Epoch 33/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0540 - pos_accuracy: 0.9200 - val_loss: 0.1070 - val_pos_accuracy: 0.9018\n",
      "Epoch 34/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0361 - pos_accuracy: 0.9663 - val_loss: 0.0989 - val_pos_accuracy: 0.9018\n",
      "Epoch 35/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0227 - pos_accuracy: 0.9794 - val_loss: 0.1087 - val_pos_accuracy: 0.9062\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0212 - pos_accuracy: 0.9850 - val_loss: 0.0969 - val_pos_accuracy: 0.9219\n",
      "Epoch 37/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0506 - pos_accuracy: 0.9200 - val_loss: 0.1423 - val_pos_accuracy: 0.8170\n",
      "Epoch 38/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0395 - pos_accuracy: 0.9581 - val_loss: 0.0953 - val_pos_accuracy: 0.8996\n",
      "Epoch 39/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0202 - pos_accuracy: 0.9862 - val_loss: 0.0909 - val_pos_accuracy: 0.9062\n",
      "Epoch 40/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0164 - pos_accuracy: 0.9894 - val_loss: 0.0865 - val_pos_accuracy: 0.9107\n",
      "Epoch 41/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0162 - pos_accuracy: 0.9869 - val_loss: 0.0855 - val_pos_accuracy: 0.9107\n",
      "Epoch 42/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0382 - pos_accuracy: 0.9456 - val_loss: 0.1048 - val_pos_accuracy: 0.9085\n",
      "Epoch 43/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0165 - pos_accuracy: 0.9894 - val_loss: 0.0858 - val_pos_accuracy: 0.9129\n",
      "Epoch 44/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0113 - pos_accuracy: 0.9937 - val_loss: 0.0903 - val_pos_accuracy: 0.9018\n",
      "Epoch 45/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0156 - pos_accuracy: 0.9919 - val_loss: 0.1099 - val_pos_accuracy: 0.9152\n",
      "Epoch 46/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0484 - pos_accuracy: 0.9388 - val_loss: 0.0878 - val_pos_accuracy: 0.9241\n",
      "Epoch 47/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0167 - pos_accuracy: 0.9887 - val_loss: 0.0803 - val_pos_accuracy: 0.9129\n",
      "Epoch 48/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0165 - pos_accuracy: 0.9881 - val_loss: 0.0896 - val_pos_accuracy: 0.9085\n",
      "Epoch 49/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0109 - pos_accuracy: 0.9937 - val_loss: 0.0968 - val_pos_accuracy: 0.9018\n",
      "Epoch 50/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9887 - val_loss: 0.0775 - val_pos_accuracy: 0.9129\n",
      "Epoch 51/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0090 - pos_accuracy: 0.9956 - val_loss: 0.0767 - val_pos_accuracy: 0.9152\n",
      "Epoch 52/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0114 - pos_accuracy: 0.9956 - val_loss: 0.0777 - val_pos_accuracy: 0.9152\n",
      "Epoch 53/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0089 - pos_accuracy: 0.9969 - val_loss: 0.0774 - val_pos_accuracy: 0.9152\n",
      "Epoch 54/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0078 - pos_accuracy: 0.9962 - val_loss: 0.0772 - val_pos_accuracy: 0.9152\n",
      "Epoch 55/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0073 - pos_accuracy: 0.9969 - val_loss: 0.0843 - val_pos_accuracy: 0.9129\n",
      "Epoch 56/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0199 - pos_accuracy: 0.9925 - val_loss: 0.0815 - val_pos_accuracy: 0.9263\n",
      "Epoch 57/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0097 - pos_accuracy: 0.9962 - val_loss: 0.0767 - val_pos_accuracy: 0.9152\n",
      "Epoch 58/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0067 - pos_accuracy: 0.9969 - val_loss: 0.0732 - val_pos_accuracy: 0.9152\n",
      "Epoch 59/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0071 - pos_accuracy: 0.9981 - val_loss: 0.0741 - val_pos_accuracy: 0.9152\n",
      "Epoch 60/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0086 - pos_accuracy: 0.9975 - val_loss: 0.0767 - val_pos_accuracy: 0.9152\n",
      "Epoch 61/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0076 - pos_accuracy: 0.9987 - val_loss: 0.0771 - val_pos_accuracy: 0.9129\n",
      "Epoch 62/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0080 - pos_accuracy: 0.9981 - val_loss: 0.0764 - val_pos_accuracy: 0.9286\n",
      "Epoch 63/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0078 - pos_accuracy: 0.9969 - val_loss: 0.0762 - val_pos_accuracy: 0.9152\n",
      "Epoch 64/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9987 - val_loss: 0.0724 - val_pos_accuracy: 0.9152\n",
      "Epoch 65/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0100 - pos_accuracy: 0.9975 - val_loss: 0.0728 - val_pos_accuracy: 0.9152\n",
      "Epoch 66/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0047 - pos_accuracy: 0.9987 - val_loss: 0.0724 - val_pos_accuracy: 0.9152\n",
      "Epoch 67/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9987 - val_loss: 0.0713 - val_pos_accuracy: 0.9152\n",
      "Epoch 68/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0061 - pos_accuracy: 0.9987 - val_loss: 0.0702 - val_pos_accuracy: 0.9152\n",
      "Epoch 69/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0054 - pos_accuracy: 0.9987 - val_loss: 0.0737 - val_pos_accuracy: 0.9152\n",
      "Epoch 70/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0069 - pos_accuracy: 0.9987 - val_loss: 0.0722 - val_pos_accuracy: 0.9152\n",
      "Epoch 71/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0055 - pos_accuracy: 0.9987 - val_loss: 0.0764 - val_pos_accuracy: 0.9129\n",
      "Epoch 72/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0049 - pos_accuracy: 0.9987 - val_loss: 0.0735 - val_pos_accuracy: 0.9286\n",
      "Epoch 73/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0086 - pos_accuracy: 0.9975 - val_loss: 0.0692 - val_pos_accuracy: 0.9152\n",
      "Epoch 74/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9987 - val_loss: 0.0709 - val_pos_accuracy: 0.9308\n",
      "Epoch 75/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0072 - pos_accuracy: 0.9987 - val_loss: 0.0700 - val_pos_accuracy: 0.9129\n",
      "Epoch 76/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0049 - pos_accuracy: 0.9987 - val_loss: 0.0701 - val_pos_accuracy: 0.9152\n",
      "Epoch 77/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0044 - pos_accuracy: 0.9987 - val_loss: 0.0717 - val_pos_accuracy: 0.9152\n",
      "Epoch 78/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0034 - pos_accuracy: 0.9987 - val_loss: 0.0673 - val_pos_accuracy: 0.9152\n",
      "Epoch 79/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0030 - pos_accuracy: 0.9987 - val_loss: 0.0681 - val_pos_accuracy: 0.9152\n",
      "Epoch 80/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0030 - pos_accuracy: 0.9987 - val_loss: 0.0689 - val_pos_accuracy: 0.9152\n",
      "Epoch 81/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0070 - pos_accuracy: 0.9975 - val_loss: 0.0675 - val_pos_accuracy: 0.9129\n",
      "Epoch 82/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.0684 - val_pos_accuracy: 0.9129\n",
      "Epoch 83/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0057 - pos_accuracy: 0.9987 - val_loss: 0.0668 - val_pos_accuracy: 0.9152\n",
      "Epoch 84/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0054 - pos_accuracy: 0.9987 - val_loss: 0.0785 - val_pos_accuracy: 0.9107\n",
      "Epoch 85/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0177 - pos_accuracy: 0.9919 - val_loss: 0.0731 - val_pos_accuracy: 0.9286\n",
      "Epoch 86/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9987 - val_loss: 0.0674 - val_pos_accuracy: 0.9129\n",
      "Epoch 87/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9987 - val_loss: 0.0673 - val_pos_accuracy: 0.9152\n",
      "Epoch 88/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.0671 - val_pos_accuracy: 0.9152\n",
      "Epoch 89/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.0669 - val_pos_accuracy: 0.9107\n",
      "Epoch 90/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0027 - pos_accuracy: 0.9987 - val_loss: 0.0657 - val_pos_accuracy: 0.9152\n",
      "Epoch 91/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.0650 - val_pos_accuracy: 0.9152\n",
      "Epoch 92/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.0666 - val_pos_accuracy: 0.9129\n",
      "Epoch 93/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.0655 - val_pos_accuracy: 0.9152\n",
      "Epoch 94/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.0653 - val_pos_accuracy: 0.9152\n",
      "Epoch 95/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0019 - pos_accuracy: 0.9987 - val_loss: 0.0652 - val_pos_accuracy: 0.9107\n",
      "Epoch 96/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.0651 - val_pos_accuracy: 0.9152\n",
      "Epoch 97/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.0652 - val_pos_accuracy: 0.9152\n",
      "Epoch 98/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0039 - pos_accuracy: 0.9994 - val_loss: 0.0719 - val_pos_accuracy: 0.9062\n",
      "Epoch 99/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 1.0000 - val_loss: 0.0648 - val_pos_accuracy: 0.9107\n",
      "Epoch 100/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9994 - val_loss: 0.0666 - val_pos_accuracy: 0.9107\n",
      "Epoch 101/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 1.0000 - val_loss: 0.0669 - val_pos_accuracy: 0.9152\n",
      "Epoch 102/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0023 - pos_accuracy: 1.0000 - val_loss: 0.0642 - val_pos_accuracy: 0.9152\n",
      "Epoch 103/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 1.0000 - val_loss: 0.0647 - val_pos_accuracy: 0.9152\n",
      "Epoch 104/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0016 - pos_accuracy: 1.0000 - val_loss: 0.0650 - val_pos_accuracy: 0.9152\n",
      "Epoch 105/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0015 - pos_accuracy: 1.0000 - val_loss: 0.0646 - val_pos_accuracy: 0.9085\n",
      "Epoch 106/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 1.0000 - val_loss: 0.0642 - val_pos_accuracy: 0.9152\n",
      "Epoch 107/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0016 - pos_accuracy: 1.0000 - val_loss: 0.0638 - val_pos_accuracy: 0.9152\n",
      "Epoch 108/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0664 - val_pos_accuracy: 0.9152\n",
      "Epoch 109/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 1.0000 - val_loss: 0.0669 - val_pos_accuracy: 0.9152\n",
      "Epoch 110/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0066 - pos_accuracy: 0.9994 - val_loss: 0.0638 - val_pos_accuracy: 0.9152\n",
      "Epoch 111/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 1.0000 - val_loss: 0.0633 - val_pos_accuracy: 0.9152\n",
      "Epoch 112/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0016 - pos_accuracy: 1.0000 - val_loss: 0.0636 - val_pos_accuracy: 0.9152\n",
      "Epoch 113/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 1.0000 - val_loss: 0.0645 - val_pos_accuracy: 0.9152\n",
      "Epoch 114/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 1.0000 - val_loss: 0.0635 - val_pos_accuracy: 0.9107\n",
      "Epoch 115/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 1.0000 - val_loss: 0.0637 - val_pos_accuracy: 0.9152\n",
      "Epoch 116/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 1.0000 - val_loss: 0.0645 - val_pos_accuracy: 0.9152\n",
      "Epoch 117/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0019 - pos_accuracy: 1.0000 - val_loss: 0.0639 - val_pos_accuracy: 0.9107\n",
      "Epoch 118/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 1.0000 - val_loss: 0.0636 - val_pos_accuracy: 0.9263\n",
      "Epoch 119/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 1.0000 - val_loss: 0.0641 - val_pos_accuracy: 0.9152\n",
      "Epoch 120/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 1.0000 - val_loss: 0.0635 - val_pos_accuracy: 0.9085\n",
      "Epoch 121/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 1.0000 - val_loss: 0.0631 - val_pos_accuracy: 0.9107\n",
      "Epoch 122/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 1.0000 - val_loss: 0.0629 - val_pos_accuracy: 0.9308\n",
      "Epoch 123/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 9.7639e-04 - pos_accuracy: 1.0000 - val_loss: 0.0629 - val_pos_accuracy: 0.9107\n",
      "Epoch 124/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0010 - pos_accuracy: 1.0000 - val_loss: 0.0632 - val_pos_accuracy: 0.9107\n",
      "Epoch 125/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0013 - pos_accuracy: 1.0000 - val_loss: 0.0625 - val_pos_accuracy: 0.9152\n",
      "Epoch 126/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0017 - pos_accuracy: 1.0000 - val_loss: 0.0632 - val_pos_accuracy: 0.9152\n",
      "Epoch 127/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 1.0000 - val_loss: 0.0624 - val_pos_accuracy: 0.9107\n",
      "Epoch 128/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.7687e-04 - pos_accuracy: 1.0000 - val_loss: 0.0624 - val_pos_accuracy: 0.9152\n",
      "Epoch 129/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 9.0503e-04 - pos_accuracy: 1.0000 - val_loss: 0.0630 - val_pos_accuracy: 0.9152\n",
      "Epoch 130/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0010 - pos_accuracy: 1.0000 - val_loss: 0.0623 - val_pos_accuracy: 0.9107\n",
      "Epoch 131/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 1.0000 - val_loss: 0.0625 - val_pos_accuracy: 0.9152\n",
      "Epoch 132/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 1.0000 - val_loss: 0.0621 - val_pos_accuracy: 0.9107\n",
      "Epoch 133/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 1.0000 - val_loss: 0.0643 - val_pos_accuracy: 0.9263\n",
      "Epoch 134/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 1.0000 - val_loss: 0.0638 - val_pos_accuracy: 0.9107\n",
      "Epoch 135/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.8893e-04 - pos_accuracy: 1.0000 - val_loss: 0.0622 - val_pos_accuracy: 0.9107\n",
      "Epoch 136/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0012 - pos_accuracy: 1.0000 - val_loss: 0.0628 - val_pos_accuracy: 0.9107\n",
      "Epoch 137/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 8.4990e-04 - pos_accuracy: 1.0000 - val_loss: 0.0622 - val_pos_accuracy: 0.9107\n",
      "Epoch 138/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 8.7571e-04 - pos_accuracy: 1.0000 - val_loss: 0.0632 - val_pos_accuracy: 0.9152\n",
      "Epoch 139/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 8.0006e-04 - pos_accuracy: 1.0000 - val_loss: 0.0622 - val_pos_accuracy: 0.9107\n",
      "Epoch 140/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 9.5851e-04 - pos_accuracy: 1.0000 - val_loss: 0.0621 - val_pos_accuracy: 0.9263\n",
      "Epoch 141/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0014 - pos_accuracy: 1.0000 - val_loss: 0.0638 - val_pos_accuracy: 0.9152\n",
      "Epoch 142/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.4805e-04 - pos_accuracy: 1.0000 - val_loss: 0.0619 - val_pos_accuracy: 0.9107\n",
      "Epoch 143/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.1705e-04 - pos_accuracy: 1.0000 - val_loss: 0.0619 - val_pos_accuracy: 0.9152\n",
      "Epoch 144/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.6740e-04 - pos_accuracy: 1.0000 - val_loss: 0.0626 - val_pos_accuracy: 0.9107\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 1.0000 - val_loss: 0.0617 - val_pos_accuracy: 0.9286\n",
      "Epoch 146/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.7844e-04 - pos_accuracy: 1.0000 - val_loss: 0.0614 - val_pos_accuracy: 0.9107\n",
      "Epoch 147/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0011 - pos_accuracy: 1.0000 - val_loss: 0.0619 - val_pos_accuracy: 0.9107\n",
      "Epoch 148/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.8083e-04 - pos_accuracy: 1.0000 - val_loss: 0.0632 - val_pos_accuracy: 0.9107\n",
      "Epoch 149/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 8.9863e-04 - pos_accuracy: 1.0000 - val_loss: 0.0617 - val_pos_accuracy: 0.9107\n",
      "Epoch 150/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.4999e-04 - pos_accuracy: 1.0000 - val_loss: 0.0616 - val_pos_accuracy: 0.9107\n",
      "Epoch 151/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 6.2652e-04 - pos_accuracy: 1.0000 - val_loss: 0.0613 - val_pos_accuracy: 0.9107\n",
      "Epoch 152/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.3492e-04 - pos_accuracy: 1.0000 - val_loss: 0.0617 - val_pos_accuracy: 0.9107\n",
      "Epoch 153/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.7793e-04 - pos_accuracy: 1.0000 - val_loss: 0.0619 - val_pos_accuracy: 0.9107\n",
      "Epoch 154/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.8673e-04 - pos_accuracy: 1.0000 - val_loss: 0.0614 - val_pos_accuracy: 0.9107\n",
      "Epoch 155/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.8780e-04 - pos_accuracy: 1.0000 - val_loss: 0.0617 - val_pos_accuracy: 0.9152\n",
      "Epoch 156/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 8.9000e-04 - pos_accuracy: 1.0000 - val_loss: 0.0622 - val_pos_accuracy: 0.9263\n",
      "Epoch 157/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.8234e-04 - pos_accuracy: 1.0000 - val_loss: 0.0613 - val_pos_accuracy: 0.9263\n",
      "Epoch 158/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 8.5709e-04 - pos_accuracy: 1.0000 - val_loss: 0.0622 - val_pos_accuracy: 0.9107\n",
      "Epoch 159/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.7617e-04 - pos_accuracy: 1.0000 - val_loss: 0.0619 - val_pos_accuracy: 0.9107\n",
      "Epoch 160/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 8.9272e-04 - pos_accuracy: 1.0000 - val_loss: 0.0612 - val_pos_accuracy: 0.9107\n",
      "Epoch 161/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 9.4759e-04 - pos_accuracy: 1.0000 - val_loss: 0.0619 - val_pos_accuracy: 0.9286\n",
      "Epoch 162/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 6.8753e-04 - pos_accuracy: 1.0000 - val_loss: 0.0612 - val_pos_accuracy: 0.9263\n",
      "Epoch 163/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 1.0000 - val_loss: 0.0618 - val_pos_accuracy: 0.9263\n",
      "Epoch 164/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 8.1579e-04 - pos_accuracy: 1.0000 - val_loss: 0.0611 - val_pos_accuracy: 0.9107\n",
      "Epoch 165/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.6200e-04 - pos_accuracy: 1.0000 - val_loss: 0.0612 - val_pos_accuracy: 0.9107\n",
      "Epoch 166/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.4346e-04 - pos_accuracy: 1.0000 - val_loss: 0.0613 - val_pos_accuracy: 0.9107\n",
      "Epoch 167/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.7000e-04 - pos_accuracy: 1.0000 - val_loss: 0.0628 - val_pos_accuracy: 0.9107\n",
      "Epoch 168/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0012 - pos_accuracy: 1.0000 - val_loss: 0.0614 - val_pos_accuracy: 0.9263\n",
      "Epoch 169/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.5389e-04 - pos_accuracy: 1.0000 - val_loss: 0.0611 - val_pos_accuracy: 0.9107\n",
      "Epoch 170/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.4814e-04 - pos_accuracy: 1.0000 - val_loss: 0.0610 - val_pos_accuracy: 0.9107\n",
      "Epoch 171/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.1425e-04 - pos_accuracy: 1.0000 - val_loss: 0.0614 - val_pos_accuracy: 0.9107\n",
      "Epoch 172/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 8.7162e-04 - pos_accuracy: 1.0000 - val_loss: 0.0612 - val_pos_accuracy: 0.9107\n",
      "Epoch 173/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 5.8237e-04 - pos_accuracy: 1.0000 - val_loss: 0.0616 - val_pos_accuracy: 0.9152\n",
      "Epoch 174/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 6.3046e-04 - pos_accuracy: 1.0000 - val_loss: 0.0611 - val_pos_accuracy: 0.9107\n",
      "Epoch 175/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.0901e-04 - pos_accuracy: 1.0000 - val_loss: 0.0614 - val_pos_accuracy: 0.9263\n",
      "Epoch 176/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.3472e-04 - pos_accuracy: 1.0000 - val_loss: 0.0611 - val_pos_accuracy: 0.9152\n",
      "Epoch 177/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 5.2999e-04 - pos_accuracy: 1.0000 - val_loss: 0.0609 - val_pos_accuracy: 0.9263\n",
      "Epoch 178/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4.9985e-04 - pos_accuracy: 1.0000 - val_loss: 0.0608 - val_pos_accuracy: 0.9263\n",
      "Epoch 179/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4.4408e-04 - pos_accuracy: 1.0000 - val_loss: 0.0612 - val_pos_accuracy: 0.9107\n",
      "Epoch 180/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.5353e-04 - pos_accuracy: 1.0000 - val_loss: 0.0611 - val_pos_accuracy: 0.9263\n",
      "Epoch 181/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4.7466e-04 - pos_accuracy: 1.0000 - val_loss: 0.0608 - val_pos_accuracy: 0.9263\n",
      "Epoch 182/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 5.2213e-04 - pos_accuracy: 1.0000 - val_loss: 0.0606 - val_pos_accuracy: 0.9263\n",
      "Epoch 183/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.9305e-04 - pos_accuracy: 1.0000 - val_loss: 0.0608 - val_pos_accuracy: 0.9107\n",
      "Epoch 184/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.6565e-04 - pos_accuracy: 1.0000 - val_loss: 0.0611 - val_pos_accuracy: 0.9107\n",
      "Epoch 185/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.4288e-04 - pos_accuracy: 1.0000 - val_loss: 0.0609 - val_pos_accuracy: 0.9263\n",
      "Epoch 186/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.9081e-04 - pos_accuracy: 1.0000 - val_loss: 0.0607 - val_pos_accuracy: 0.9263\n",
      "Epoch 187/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4.7594e-04 - pos_accuracy: 1.0000 - val_loss: 0.0609 - val_pos_accuracy: 0.9107\n",
      "Epoch 188/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.9660e-04 - pos_accuracy: 1.0000 - val_loss: 0.0606 - val_pos_accuracy: 0.9263\n",
      "Epoch 189/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.3102e-04 - pos_accuracy: 1.0000 - val_loss: 0.0606 - val_pos_accuracy: 0.9107\n",
      "Epoch 190/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 5.4018e-04 - pos_accuracy: 1.0000 - val_loss: 0.0607 - val_pos_accuracy: 0.9263\n",
      "Epoch 191/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 5.1783e-04 - pos_accuracy: 1.0000 - val_loss: 0.0605 - val_pos_accuracy: 0.9263\n",
      "Epoch 192/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.9797e-04 - pos_accuracy: 1.0000 - val_loss: 0.0607 - val_pos_accuracy: 0.9263\n",
      "Epoch 193/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 8.4441e-04 - pos_accuracy: 1.0000 - val_loss: 0.0606 - val_pos_accuracy: 0.9263\n",
      "Epoch 194/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 7.7802e-04 - pos_accuracy: 1.0000 - val_loss: 0.0610 - val_pos_accuracy: 0.9263\n",
      "Epoch 195/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 8.4826e-04 - pos_accuracy: 1.0000 - val_loss: 0.0608 - val_pos_accuracy: 0.9263\n",
      "Epoch 196/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.9040e-04 - pos_accuracy: 1.0000 - val_loss: 0.0605 - val_pos_accuracy: 0.9286\n",
      "Epoch 197/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4.8753e-04 - pos_accuracy: 1.0000 - val_loss: 0.0608 - val_pos_accuracy: 0.9263\n",
      "Epoch 198/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 4.9287e-04 - pos_accuracy: 1.0000 - val_loss: 0.0603 - val_pos_accuracy: 0.9263\n",
      "Epoch 199/200\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.4164e-04 - pos_accuracy: 1.0000 - val_loss: 0.0605 - val_pos_accuracy: 0.9263\n",
      "Epoch 200/200\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 5.2449e-04 - pos_accuracy: 1.0000 - val_loss: 0.0615 - val_pos_accuracy: 0.9263\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:39:16.368246: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1681: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.comcatenate는 numpy 배열들을 선택한 축(axios) 방향으로 합쳐주는 메소드입니다.\\naxios=1은 2차원에선 열방향(좌->우)를 의미합니다.\\n\",\n",
      "        \"\\n각각의 숫자들은 subplot의 행의 갯수,subplot의 열의 갯수, index를 붙여서 작성한것이다.\\nsubplot(121)은 subplot(1,2,1)과 같다. \\n- subplot의 행의 갯수: 1\\n- subplot의 열의 갯수: 2\\n- index: 1 이다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\naxis 에 주어진 차원을 따라 input_tensor를 줄입니다. axis=1은 1차원을 의미합니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n텐서 차원의 요소 평균을 계산합니다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n회귀 문제이기 때문입니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n난수 생성 패턴을 동일하게 관리할 수 있습니다.\\n\",\n",
      "        false,\n",
      "        \"\\n막상 혼자 과제하면서 하려니까 어렵네요ㅠ\\n\"\n",
      "    ],\n",
      "    \"score\": 80.0,\n",
      "    \"accuracy\": 0.9152\n",
      "}\"report1/양해영_46086.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/양해영_46086.ipynb to python\n",
      "[NbConvertApp] Writing 13919 bytes to report1/양해영_46086.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007ans01: [[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "ans02: [[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "ans03: 52\n",
      "ans04: 422\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "ans05: [[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "ans09: \n",
      "np.concatenate의 역할과 axis=1의 의미에 대해 간단히 적어봅시다.\n",
      "np.concatenate함수는 선택한 축(axis)방향으로 배열을 연결(병합)해 주는 메소드이다.\n",
      "2차원 배열이므로 axis=1은 열을 의미한다.\n",
      "\n",
      "ans10: \n",
      "subplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\n",
      "입력되는 정수121, 122는 plot의 좌표를 나타낸다.\n",
      "subplot(121)는 nrow:1, ncols:2, index:1.\n",
      "subplot(122)는 nrow:1, ncols:2, index:2.\n",
      "\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 284881.07it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:39:22.714664: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:39:23.096841: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:39:23.096880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:39:23.323814: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/138\n",
      "2022-10-14 10:39:23.942393: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "50/50 [==============================] - 1s 4ms/step - loss: 35.3191 - accuracy: 0.9469 - val_loss: 4.8732 - val_accuracy: 0.9625\n",
      "Epoch 2/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 3.1180 - accuracy: 0.9787 - val_loss: 2.8261 - val_accuracy: 0.9800\n",
      "Epoch 3/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 2.0292 - accuracy: 0.9844 - val_loss: 2.1152 - val_accuracy: 0.9700\n",
      "Epoch 4/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.5732 - accuracy: 0.9844 - val_loss: 1.7396 - val_accuracy: 0.9700\n",
      "Epoch 5/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.3120 - accuracy: 0.9825 - val_loss: 1.4829 - val_accuracy: 0.9700\n",
      "Epoch 6/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.1390 - accuracy: 0.9819 - val_loss: 1.3263 - val_accuracy: 0.9750\n",
      "Epoch 7/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.0169 - accuracy: 0.9819 - val_loss: 1.1956 - val_accuracy: 0.9700\n",
      "Epoch 8/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.9210 - accuracy: 0.9819 - val_loss: 1.1114 - val_accuracy: 0.9650\n",
      "Epoch 9/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.8477 - accuracy: 0.9806 - val_loss: 1.0254 - val_accuracy: 0.9650\n",
      "Epoch 10/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.7889 - accuracy: 0.9806 - val_loss: 0.9569 - val_accuracy: 0.9675\n",
      "Epoch 11/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.7390 - accuracy: 0.9812 - val_loss: 0.9080 - val_accuracy: 0.9625\n",
      "Epoch 12/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.7001 - accuracy: 0.9819 - val_loss: 0.8555 - val_accuracy: 0.9675\n",
      "Epoch 13/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.9806 - val_loss: 0.8262 - val_accuracy: 0.9675\n",
      "Epoch 14/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6315 - accuracy: 0.9819 - val_loss: 0.7944 - val_accuracy: 0.9625\n",
      "Epoch 15/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6025 - accuracy: 0.9794 - val_loss: 0.7745 - val_accuracy: 0.9675\n",
      "Epoch 16/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5791 - accuracy: 0.9812 - val_loss: 0.7478 - val_accuracy: 0.9675\n",
      "Epoch 17/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5571 - accuracy: 0.9819 - val_loss: 0.7189 - val_accuracy: 0.9675\n",
      "Epoch 18/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5392 - accuracy: 0.9806 - val_loss: 0.7066 - val_accuracy: 0.9700\n",
      "Epoch 19/138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5217 - accuracy: 0.9819 - val_loss: 0.6840 - val_accuracy: 0.9675\n",
      "Epoch 20/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5076 - accuracy: 0.9812 - val_loss: 0.6705 - val_accuracy: 0.9625\n",
      "Epoch 21/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4923 - accuracy: 0.9781 - val_loss: 0.6590 - val_accuracy: 0.9700\n",
      "Epoch 22/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4772 - accuracy: 0.9781 - val_loss: 0.6477 - val_accuracy: 0.9675\n",
      "Epoch 23/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4677 - accuracy: 0.9781 - val_loss: 0.6345 - val_accuracy: 0.9700\n",
      "Epoch 24/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4536 - accuracy: 0.9756 - val_loss: 0.6189 - val_accuracy: 0.9700\n",
      "Epoch 25/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.9787 - val_loss: 0.6126 - val_accuracy: 0.9725\n",
      "Epoch 26/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.9800 - val_loss: 0.6043 - val_accuracy: 0.9700\n",
      "Epoch 27/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4241 - accuracy: 0.9787 - val_loss: 0.5946 - val_accuracy: 0.9725\n",
      "Epoch 28/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4155 - accuracy: 0.9762 - val_loss: 0.5856 - val_accuracy: 0.9725\n",
      "Epoch 29/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4058 - accuracy: 0.9794 - val_loss: 0.5800 - val_accuracy: 0.9700\n",
      "Epoch 30/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.9800 - val_loss: 0.5768 - val_accuracy: 0.9825\n",
      "Epoch 31/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.9806 - val_loss: 0.5680 - val_accuracy: 0.9700\n",
      "Epoch 32/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3849 - accuracy: 0.9812 - val_loss: 0.5651 - val_accuracy: 0.9750\n",
      "Epoch 33/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.9794 - val_loss: 0.5587 - val_accuracy: 0.9650\n",
      "Epoch 34/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.9781 - val_loss: 0.5552 - val_accuracy: 0.9750\n",
      "Epoch 35/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3644 - accuracy: 0.9825 - val_loss: 0.5459 - val_accuracy: 0.9725\n",
      "Epoch 36/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.9781 - val_loss: 0.5388 - val_accuracy: 0.9900\n",
      "Epoch 37/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3528 - accuracy: 0.9844 - val_loss: 0.5349 - val_accuracy: 0.9650\n",
      "Epoch 38/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3468 - accuracy: 0.9806 - val_loss: 0.5301 - val_accuracy: 0.9700\n",
      "Epoch 39/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3418 - accuracy: 0.9806 - val_loss: 0.5324 - val_accuracy: 0.9775\n",
      "Epoch 40/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.9806 - val_loss: 0.5280 - val_accuracy: 0.9700\n",
      "Epoch 41/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.9812 - val_loss: 0.5263 - val_accuracy: 0.9750\n",
      "Epoch 42/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.9794 - val_loss: 0.5241 - val_accuracy: 0.9725\n",
      "Epoch 43/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.9831 - val_loss: 0.5183 - val_accuracy: 0.9650\n",
      "Epoch 44/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3174 - accuracy: 0.9787 - val_loss: 0.5172 - val_accuracy: 0.9675\n",
      "Epoch 45/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.9806 - val_loss: 0.5133 - val_accuracy: 0.9850\n",
      "Epoch 46/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3111 - accuracy: 0.9831 - val_loss: 0.5120 - val_accuracy: 0.9850\n",
      "Epoch 47/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3072 - accuracy: 0.9819 - val_loss: 0.5079 - val_accuracy: 0.9800\n",
      "Epoch 48/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3031 - accuracy: 0.9812 - val_loss: 0.5064 - val_accuracy: 0.9675\n",
      "Epoch 49/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2988 - accuracy: 0.9825 - val_loss: 0.5056 - val_accuracy: 0.9650\n",
      "Epoch 50/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2959 - accuracy: 0.9819 - val_loss: 0.5007 - val_accuracy: 0.9775\n",
      "Epoch 51/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2922 - accuracy: 0.9819 - val_loss: 0.4975 - val_accuracy: 0.9750\n",
      "Epoch 52/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.9825 - val_loss: 0.4957 - val_accuracy: 0.9825\n",
      "Epoch 53/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.9837 - val_loss: 0.4960 - val_accuracy: 0.9750\n",
      "Epoch 54/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2829 - accuracy: 0.9794 - val_loss: 0.4938 - val_accuracy: 0.9800\n",
      "Epoch 55/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.9806 - val_loss: 0.4930 - val_accuracy: 0.9800\n",
      "Epoch 56/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2766 - accuracy: 0.9837 - val_loss: 0.4901 - val_accuracy: 0.9675\n",
      "Epoch 57/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2727 - accuracy: 0.9850 - val_loss: 0.4885 - val_accuracy: 0.9725\n",
      "Epoch 58/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.9819 - val_loss: 0.4880 - val_accuracy: 0.9675\n",
      "Epoch 59/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2684 - accuracy: 0.9831 - val_loss: 0.4854 - val_accuracy: 0.9725\n",
      "Epoch 60/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2654 - accuracy: 0.9812 - val_loss: 0.4857 - val_accuracy: 0.9725\n",
      "Epoch 61/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2633 - accuracy: 0.9831 - val_loss: 0.4852 - val_accuracy: 0.9825\n",
      "Epoch 62/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.9825 - val_loss: 0.4810 - val_accuracy: 0.9775\n",
      "Epoch 63/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.9837 - val_loss: 0.4802 - val_accuracy: 0.9750\n",
      "Epoch 64/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2552 - accuracy: 0.9794 - val_loss: 0.4813 - val_accuracy: 0.9750\n",
      "Epoch 65/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2531 - accuracy: 0.9850 - val_loss: 0.4790 - val_accuracy: 0.9700\n",
      "Epoch 66/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2506 - accuracy: 0.9781 - val_loss: 0.4817 - val_accuracy: 0.9825\n",
      "Epoch 67/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2491 - accuracy: 0.9844 - val_loss: 0.4778 - val_accuracy: 0.9775\n",
      "Epoch 68/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.9831 - val_loss: 0.4755 - val_accuracy: 0.9675\n",
      "Epoch 69/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.9812 - val_loss: 0.4712 - val_accuracy: 0.9800\n",
      "Epoch 70/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2422 - accuracy: 0.9812 - val_loss: 0.4746 - val_accuracy: 0.9750\n",
      "Epoch 71/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2404 - accuracy: 0.9825 - val_loss: 0.4713 - val_accuracy: 0.9675\n",
      "Epoch 72/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.9794 - val_loss: 0.4708 - val_accuracy: 0.9825\n",
      "Epoch 73/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.9844 - val_loss: 0.4701 - val_accuracy: 0.9775\n",
      "Epoch 74/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2340 - accuracy: 0.9831 - val_loss: 0.4677 - val_accuracy: 0.9675\n",
      "Epoch 75/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2327 - accuracy: 0.9787 - val_loss: 0.4708 - val_accuracy: 0.9825\n",
      "Epoch 76/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.9812 - val_loss: 0.4691 - val_accuracy: 0.9800\n",
      "Epoch 77/138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.9837 - val_loss: 0.4666 - val_accuracy: 0.9650\n",
      "Epoch 78/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2265 - accuracy: 0.9825 - val_loss: 0.4653 - val_accuracy: 0.9775\n",
      "Epoch 79/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2257 - accuracy: 0.9831 - val_loss: 0.4673 - val_accuracy: 0.9650\n",
      "Epoch 80/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.9806 - val_loss: 0.4666 - val_accuracy: 0.9775\n",
      "Epoch 81/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.9831 - val_loss: 0.4631 - val_accuracy: 0.9750\n",
      "Epoch 82/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.9819 - val_loss: 0.4640 - val_accuracy: 0.9800\n",
      "Epoch 83/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.9844 - val_loss: 0.4653 - val_accuracy: 0.9825\n",
      "Epoch 84/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2167 - accuracy: 0.9812 - val_loss: 0.4656 - val_accuracy: 0.9700\n",
      "Epoch 85/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.9831 - val_loss: 0.4620 - val_accuracy: 0.9650\n",
      "Epoch 86/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2136 - accuracy: 0.9806 - val_loss: 0.4639 - val_accuracy: 0.9725\n",
      "Epoch 87/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.9819 - val_loss: 0.4611 - val_accuracy: 0.9675\n",
      "Epoch 88/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9837 - val_loss: 0.4624 - val_accuracy: 0.9750\n",
      "Epoch 89/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2093 - accuracy: 0.9794 - val_loss: 0.4628 - val_accuracy: 0.9800\n",
      "Epoch 90/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2074 - accuracy: 0.9812 - val_loss: 0.4594 - val_accuracy: 0.9800\n",
      "Epoch 91/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2060 - accuracy: 0.9819 - val_loss: 0.4603 - val_accuracy: 0.9750\n",
      "Epoch 92/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9812 - val_loss: 0.4612 - val_accuracy: 0.9800\n",
      "Epoch 93/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2028 - accuracy: 0.9831 - val_loss: 0.4598 - val_accuracy: 0.9700\n",
      "Epoch 94/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2019 - accuracy: 0.9812 - val_loss: 0.4577 - val_accuracy: 0.9675\n",
      "Epoch 95/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9825 - val_loss: 0.4576 - val_accuracy: 0.9650\n",
      "Epoch 96/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9819 - val_loss: 0.4572 - val_accuracy: 0.9750\n",
      "Epoch 97/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.9800 - val_loss: 0.4588 - val_accuracy: 0.9725\n",
      "Epoch 98/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1970 - accuracy: 0.9806 - val_loss: 0.4586 - val_accuracy: 0.9800\n",
      "Epoch 99/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9837 - val_loss: 0.4560 - val_accuracy: 0.9675\n",
      "Epoch 100/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1948 - accuracy: 0.9806 - val_loss: 0.4574 - val_accuracy: 0.9700\n",
      "Epoch 101/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9794 - val_loss: 0.4553 - val_accuracy: 0.9750\n",
      "Epoch 102/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1922 - accuracy: 0.9800 - val_loss: 0.4549 - val_accuracy: 0.9800\n",
      "Epoch 103/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9787 - val_loss: 0.4550 - val_accuracy: 0.9725\n",
      "Epoch 104/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1892 - accuracy: 0.9825 - val_loss: 0.4551 - val_accuracy: 0.9800\n",
      "Epoch 105/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1884 - accuracy: 0.9812 - val_loss: 0.4550 - val_accuracy: 0.9800\n",
      "Epoch 106/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1870 - accuracy: 0.9794 - val_loss: 0.4536 - val_accuracy: 0.9625\n",
      "Epoch 107/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.9819 - val_loss: 0.4538 - val_accuracy: 0.9725\n",
      "Epoch 108/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9831 - val_loss: 0.4542 - val_accuracy: 0.9800\n",
      "Epoch 109/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9825 - val_loss: 0.4537 - val_accuracy: 0.9725\n",
      "Epoch 110/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.9794 - val_loss: 0.4540 - val_accuracy: 0.9800\n",
      "Epoch 111/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9806 - val_loss: 0.4544 - val_accuracy: 0.9650\n",
      "Epoch 112/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1806 - accuracy: 0.9800 - val_loss: 0.4535 - val_accuracy: 0.9800\n",
      "Epoch 113/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9825 - val_loss: 0.4533 - val_accuracy: 0.9750\n",
      "Epoch 114/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1783 - accuracy: 0.9831 - val_loss: 0.4554 - val_accuracy: 0.9800\n",
      "Epoch 115/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1773 - accuracy: 0.9812 - val_loss: 0.4532 - val_accuracy: 0.9800\n",
      "Epoch 116/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1768 - accuracy: 0.9819 - val_loss: 0.4545 - val_accuracy: 0.9800\n",
      "Epoch 117/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1753 - accuracy: 0.9812 - val_loss: 0.4561 - val_accuracy: 0.9800\n",
      "Epoch 118/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1749 - accuracy: 0.9812 - val_loss: 0.4562 - val_accuracy: 0.9800\n",
      "Epoch 119/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 0.9837 - val_loss: 0.4541 - val_accuracy: 0.9700\n",
      "Epoch 120/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9800 - val_loss: 0.4569 - val_accuracy: 0.9800\n",
      "Epoch 121/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.9825 - val_loss: 0.4541 - val_accuracy: 0.9700\n",
      "Epoch 122/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9819 - val_loss: 0.4535 - val_accuracy: 0.9725\n",
      "Epoch 123/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9812 - val_loss: 0.4501 - val_accuracy: 0.9650\n",
      "Epoch 124/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9787 - val_loss: 0.4488 - val_accuracy: 0.9725\n",
      "Epoch 125/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1685 - accuracy: 0.9825 - val_loss: 0.4511 - val_accuracy: 0.9750\n",
      "Epoch 126/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9831 - val_loss: 0.4551 - val_accuracy: 0.9650\n",
      "Epoch 127/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.9812 - val_loss: 0.4522 - val_accuracy: 0.9750\n",
      "Epoch 128/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.9819 - val_loss: 0.4510 - val_accuracy: 0.9650\n",
      "Epoch 129/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9800 - val_loss: 0.4525 - val_accuracy: 0.9700\n",
      "Epoch 130/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1639 - accuracy: 0.9781 - val_loss: 0.4521 - val_accuracy: 0.9800\n",
      "Epoch 131/138\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.9812 - val_loss: 0.4551 - val_accuracy: 0.9800\n",
      "Epoch 132/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9806 - val_loss: 0.4502 - val_accuracy: 0.9750\n",
      "Epoch 133/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 0.9800 - val_loss: 0.4521 - val_accuracy: 0.9800\n",
      "Epoch 134/138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1606 - accuracy: 0.9831 - val_loss: 0.4543 - val_accuracy: 0.9750\n",
      "Epoch 135/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1594 - accuracy: 0.9819 - val_loss: 0.4537 - val_accuracy: 0.9800\n",
      "Epoch 136/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1587 - accuracy: 0.9800 - val_loss: 0.4555 - val_accuracy: 0.9750\n",
      "Epoch 137/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9812 - val_loss: 0.4518 - val_accuracy: 0.9800\n",
      "Epoch 138/138\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.9787 - val_loss: 0.4510 - val_accuracy: 0.9800\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.9800\n",
      "ans18: \n",
      "tf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\n",
      "tensor가 지정한 축 방향에 있는 각 요소의 논리와(and)을 계산한다.\n",
      "axis=1은 두번째로 높은 차원을 의미한다\n",
      "\n",
      "ans19: \n",
      "tf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \n",
      "tensor가 지정한 축(tensor의 특정한 차원)의 평균값을 계산하는데 사용되며, 주로 내림차원이나 tensor(이미지)의 평균값을 계산하는데 사용된다\n",
      "\n",
      "ans20: \n",
      "최종 출력층의 activation=None인 이유 \n",
      "출력값에 특별히 제한이 없기 때문이다\n",
      "\n",
      "ans21: \n",
      "random seed의 사용 이유 \n",
      "특정 시작숫자를 정해주면 컴퓨터가 정해진 알고리즘에 따라 난수를 생성한다. 이런 시작 숫자를 seed라고 한다\n",
      "\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:40:05.938062: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans22: 0.795207\n",
      "ans23: \n",
      " 많이 어렵지만 열심히 해보겠습니다. 감사합니다.\n",
      "\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:654: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과 axis=1의 의미에 대해 간단히 적어봅시다.\\nnp.concatenate함수는 선택한 축(axis)방향으로 배열을 연결(병합)해 주는 메소드이다.\\n2차원 배열이므로 axis=1은 열을 의미한다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n입력되는 정수121, 122는 plot의 좌표를 나타낸다.\\nsubplot(121)는 nrow:1, ncols:2, index:1.\\nsubplot(122)는 nrow:1, ncols:2, index:2.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntensor가 지정한 축 방향에 있는 각 요소의 논리와(and)을 계산한다.\\naxis=1은 두번째로 높은 차원을 의미한다\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\ntensor가 지정한 축(tensor의 특정한 차원)의 평균값을 계산하는데 사용되며, 주로 내림차원이나 tensor(이미지)의 평균값을 계산하는데 사용된다\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n출력값에 특별히 제한이 없기 때문이다\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n특정 시작숫자를 정해주면 컴퓨터가 정해진 알고리즘에 따라 난수를 생성한다. 이런 시작 숫자를 seed라고 한다\\n\",\n",
      "        false,\n",
      "        \"\\n 많이 어렵지만 열심히 해보겠습니다. 감사합니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 74.54545454545455,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/윤울암_46060.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/윤울암_46060.ipynb to python\n",
      "[NbConvertApp] Writing 32614 bytes to report1/윤울암_46060.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:243: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.1232343e-17  1.0000000e+00 -1.4210855e-14]\n",
      " [-1.0000000e+00  6.1232343e-17  2.0000000e+02]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 296962.90it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:40:12.306509: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:40:12.688137: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:40:12.688178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:40:12.925905: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:40:13.522363: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.5000 - val_loss: 211.8815\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 221.8156 - val_loss: 195.5106\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:40:15.177286: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1569: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할: 선택한 축 방향으로 배열을 연결\\naxis=1의 의미: 1차원 배열에서는 직선만 존재하므로 불가, 2차원 배열에서는 열방향을 의미, 3차원 배열에서는 행방향을 의미\\n\",\n",
      "        \"subplot에서 121, 122의 의미: 두개의 원소가 전체 그리드 행렬의 모양을 지시하고, 세번째 원자가 그 중 어느 것인지를 의미하는 위치에 대한 숫자\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"tensor 가 지정 한 축 방향 에 있 는 각 요소 의 논리 와(and 연산)를 계산\",\n",
      "        \"tf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다.\",\n",
      "        \"tensor 가 지정 한 축(tensor 의 특정한 차원)의 평균 값 을 계산 하 는 데 사용 되 며,주로 내 림 차원 이나 tensor(이미지)의 평균 값 을 계산 하 는 데 사용 된다.\",\n",
      "        \"난수를 예측 가능하도록 만들어줌\",\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 54.72727272727273,\n",
      "    \"accuracy\": 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\"report1/윤현기_46095.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/윤현기_46095.ipynb to python\n",
      "[NbConvertApp] Writing 34463 bytes to report1/윤현기_46095.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -13. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 295435.94it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:40:21.563164: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:40:21.946225: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:40:21.946262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:40:22.167816: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:40:22.790052: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 113345.78it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 42.3551 - val_loss: 5.8140\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.6661 - val_loss: 3.2219\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:40:25.047263: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1648: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할 : numpy 배열들을 하나로 합치는 것에 사용\\naxis=1의 의미 : axis는 축을 의미하고(0~2까지), axis =1 은 열을 기준으로 동작하는 것\\n\",\n",
      "        \"\\n3개의 정수(10보다 작은 수)를 입력 받는데, 첫번째 자리부터 각각 행의 수, 열의 수, 인덱스를 나타낸다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 65.45454545454545,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/백근화_46025.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/백근화_46025.ipynb to python\n",
      "[NbConvertApp] Writing 35608 bytes to report1/백근화_46025.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[5. 6.]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   1.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 288397.15it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:40:31.455524: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:40:31.833955: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:40:31.833994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:40:32.058362: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:40:32.689589: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 131956.52it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                15700     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 15,742\n",
      "Trainable params: 15,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 90.7658 - accuracy: 0.8669 - val_loss: 16.5250 - val_accuracy: 0.9500\n",
      "Epoch 2/7\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 8.6226 - accuracy: 0.9644 - val_loss: 4.6239 - val_accuracy: 0.9650\n",
      "Epoch 3/7\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 3.2642 - accuracy: 0.9762 - val_loss: 2.9786 - val_accuracy: 0.9575\n",
      "Epoch 4/7\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 2.3116 - accuracy: 0.9750 - val_loss: 2.3509 - val_accuracy: 0.9650\n",
      "Epoch 5/7\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.8445 - accuracy: 0.9762 - val_loss: 1.9422 - val_accuracy: 0.9700\n",
      "Epoch 6/7\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.5573 - accuracy: 0.9794 - val_loss: 1.7044 - val_accuracy: 0.9750\n",
      "Epoch 7/7\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.3650 - accuracy: 0.9769 - val_loss: 1.5140 - val_accuracy: 0.9650\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:40:35.680511: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 0 Axes>\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1695: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할은 선택한 축(axis)의 방향으로 배열을 연결해주는 메소드이다.\\n2차원에서 axis=1의 의미는 열방향, 좌->우를 의미한다.\\n\",\n",
      "        \"\\nsubplot에서 의미는 아래와 같고, 겹치지 않게 한다.\\n121 = 세로1개, 가로2개에 1번째\\n122 = 세로1개, 가로2개에 2번째\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all : 지정한 축 방향의 각 요소의 논리와 (and 연산) 계산하기\\naxis=1: 차원 축소\\n\",\n",
      "        \"\\ntf.reduce_mean: 정확도 계산한다\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n: 딥러닝 모델이 알아서 weight를 조절해서 비슷한 출력을 낸다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유\\n매번 다르게 나오는 난수를 프로그램 시행마다 같은 결과를 나와야 할 경우에 초기값을 할당하는 방법으로 사용한다.\\n\",\n",
      "        false,\n",
      "        \"\\n배우고있는 딥러닝이 흥미롭고 더 자세히 알아가고싶습니다.\\n아직은 코드 딥러닝 코드 분석이 더뎌서.. 강의에 조금만 더 부가 설명이 있으면 좋을 것 같습니다.\\n감사합니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 78.18181818181819,\n",
      "    \"accuracy\": 0.965\n",
      "}\"report1/이강희_46077.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이강희_46077.ipynb to python\n",
      "[NbConvertApp] Writing 35804 bytes to report1/이강희_46077.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.1 0.3 0.4]\n",
      " [0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[0.2 0.6 0.8]\n",
      " [1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:209: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:240: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[5 6]\n",
      " [7 8]\n",
      " [9 0]]\n",
      "[[5 7 9]\n",
      " [6 8 0]]\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -22. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 299635.95it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:40:42.179279: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:40:42.560088: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:40:42.560129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:40:42.781147: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:40:43.400172: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 94489.72it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 6280      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 6,326\n",
      "Trainable params: 6,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 248.2666 - pos_accuracy: 0.0000e+00 - val_loss: 224.1228 - val_pos_accuracy: 0.0050\n",
      "Epoch 2/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 242.6980 - pos_accuracy: 0.0000e+00 - val_loss: 218.5715 - val_pos_accuracy: 0.0050\n",
      "Epoch 3/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 236.5045 - pos_accuracy: 0.0000e+00 - val_loss: 211.4279 - val_pos_accuracy: 0.0050\n",
      "Epoch 4/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 228.2650 - pos_accuracy: 0.0018 - val_loss: 201.0220 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 5/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 216.0132 - pos_accuracy: 0.0022 - val_loss: 184.5305 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 6/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 196.2615 - pos_accuracy: 8.6806e-04 - val_loss: 156.9131 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 7/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 163.3652 - pos_accuracy: 4.8828e-04 - val_loss: 113.9811 - val_pos_accuracy: 0.0050\n",
      "Epoch 8/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 114.1682 - pos_accuracy: 0.0026 - val_loss: 66.5243 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 9/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 66.2345 - pos_accuracy: 0.0028 - val_loss: 45.3953 - val_pos_accuracy: 0.0025\n",
      "Epoch 10/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 46.6416 - pos_accuracy: 9.7656e-04 - val_loss: 39.8909 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 11/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 41.6008 - pos_accuracy: 0.0031 - val_loss: 36.8683 - val_pos_accuracy: 0.0075\n",
      "Epoch 12/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 38.8789 - pos_accuracy: 0.0046 - val_loss: 34.8049 - val_pos_accuracy: 0.0075\n",
      "Epoch 13/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 37.0337 - pos_accuracy: 0.0064 - val_loss: 33.3586 - val_pos_accuracy: 0.0125\n",
      "Epoch 14/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 35.7019 - pos_accuracy: 0.0134 - val_loss: 32.2507 - val_pos_accuracy: 0.0125\n",
      "Epoch 15/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 34.6035 - pos_accuracy: 0.0084 - val_loss: 31.2858 - val_pos_accuracy: 0.0275\n",
      "Epoch 16/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 33.6782 - pos_accuracy: 0.0137 - val_loss: 30.4625 - val_pos_accuracy: 0.0275\n",
      "Epoch 17/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 32.8352 - pos_accuracy: 0.0184 - val_loss: 29.6537 - val_pos_accuracy: 0.0275\n",
      "Epoch 18/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 32.0185 - pos_accuracy: 0.0146 - val_loss: 28.8684 - val_pos_accuracy: 0.0275\n",
      "Epoch 19/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 31.1880 - pos_accuracy: 0.0166 - val_loss: 28.0512 - val_pos_accuracy: 0.0250\n",
      "Epoch 20/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 30.2969 - pos_accuracy: 0.0207 - val_loss: 27.1929 - val_pos_accuracy: 0.0300\n",
      "Epoch 21/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 29.3560 - pos_accuracy: 0.0213 - val_loss: 26.2825 - val_pos_accuracy: 0.0250\n",
      "Epoch 22/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 28.3239 - pos_accuracy: 0.0190 - val_loss: 25.2900 - val_pos_accuracy: 0.0275\n",
      "Epoch 23/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 27.2081 - pos_accuracy: 0.0214 - val_loss: 24.2463 - val_pos_accuracy: 0.0275\n",
      "Epoch 24/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 25.9743 - pos_accuracy: 0.0244 - val_loss: 23.0682 - val_pos_accuracy: 0.0275\n",
      "Epoch 25/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 24.6400 - pos_accuracy: 0.0244 - val_loss: 21.8122 - val_pos_accuracy: 0.0350\n",
      "Epoch 26/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 23.1835 - pos_accuracy: 0.0285 - val_loss: 20.4318 - val_pos_accuracy: 0.0400\n",
      "Epoch 27/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 21.6044 - pos_accuracy: 0.0334 - val_loss: 18.9572 - val_pos_accuracy: 0.0400\n",
      "Epoch 28/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 19.9488 - pos_accuracy: 0.0372 - val_loss: 17.4499 - val_pos_accuracy: 0.0400\n",
      "Epoch 29/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 18.1962 - pos_accuracy: 0.0430 - val_loss: 15.8509 - val_pos_accuracy: 0.0450\n",
      "Epoch 30/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 16.3890 - pos_accuracy: 0.0590 - val_loss: 14.2451 - val_pos_accuracy: 0.0550\n",
      "Epoch 31/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 14.5811 - pos_accuracy: 0.0654 - val_loss: 12.6629 - val_pos_accuracy: 0.0575\n",
      "Epoch 32/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 12.8081 - pos_accuracy: 0.0627 - val_loss: 11.1794 - val_pos_accuracy: 0.0625\n",
      "Epoch 33/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 11.1521 - pos_accuracy: 0.0840 - val_loss: 9.8133 - val_pos_accuracy: 0.0625\n",
      "Epoch 34/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 9.6361 - pos_accuracy: 0.0919 - val_loss: 8.5775 - val_pos_accuracy: 0.0725\n",
      "Epoch 35/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8.2855 - pos_accuracy: 0.1284 - val_loss: 7.5237 - val_pos_accuracy: 0.1175\n",
      "Epoch 36/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 7.1408 - pos_accuracy: 0.1374 - val_loss: 6.6181 - val_pos_accuracy: 0.1400\n",
      "Epoch 37/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.1797 - pos_accuracy: 0.1305 - val_loss: 5.8787 - val_pos_accuracy: 0.1175\n",
      "Epoch 38/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.3955 - pos_accuracy: 0.1277 - val_loss: 5.3069 - val_pos_accuracy: 0.1300\n",
      "Epoch 39/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.7866 - pos_accuracy: 0.1299 - val_loss: 4.8452 - val_pos_accuracy: 0.1400\n",
      "Epoch 40/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.3108 - pos_accuracy: 0.1326 - val_loss: 4.4690 - val_pos_accuracy: 0.1300\n",
      "Epoch 41/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 3.9242 - pos_accuracy: 0.1404 - val_loss: 4.1536 - val_pos_accuracy: 0.1400\n",
      "Epoch 42/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3.6084 - pos_accuracy: 0.1600 - val_loss: 3.8952 - val_pos_accuracy: 0.1475\n",
      "Epoch 43/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 3.3534 - pos_accuracy: 0.1649 - val_loss: 3.6819 - val_pos_accuracy: 0.1675\n",
      "Epoch 44/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 3.1443 - pos_accuracy: 0.1621 - val_loss: 3.4987 - val_pos_accuracy: 0.1600\n",
      "Epoch 45/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 25ms/step - loss: 2.9628 - pos_accuracy: 0.1728 - val_loss: 3.3276 - val_pos_accuracy: 0.1650\n",
      "Epoch 46/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 2.8086 - pos_accuracy: 0.1714 - val_loss: 3.1938 - val_pos_accuracy: 0.1725\n",
      "Epoch 47/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.6763 - pos_accuracy: 0.1888 - val_loss: 3.0537 - val_pos_accuracy: 0.1625\n",
      "Epoch 48/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.5600 - pos_accuracy: 0.1815 - val_loss: 2.9410 - val_pos_accuracy: 0.1625\n",
      "Epoch 49/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.4611 - pos_accuracy: 0.1774 - val_loss: 2.8668 - val_pos_accuracy: 0.1875\n",
      "Epoch 50/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 2.3721 - pos_accuracy: 0.1938 - val_loss: 2.7640 - val_pos_accuracy: 0.1850\n",
      "Epoch 51/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.2880 - pos_accuracy: 0.2001 - val_loss: 2.6741 - val_pos_accuracy: 0.1900\n",
      "Epoch 52/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2.2176 - pos_accuracy: 0.1865 - val_loss: 2.5996 - val_pos_accuracy: 0.1875\n",
      "Epoch 53/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 2.1574 - pos_accuracy: 0.1950 - val_loss: 2.5374 - val_pos_accuracy: 0.1950\n",
      "Epoch 54/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2.0961 - pos_accuracy: 0.1828 - val_loss: 2.4751 - val_pos_accuracy: 0.2050\n",
      "Epoch 55/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.0394 - pos_accuracy: 0.1962 - val_loss: 2.4212 - val_pos_accuracy: 0.2075\n",
      "Epoch 56/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.9886 - pos_accuracy: 0.2127 - val_loss: 2.3554 - val_pos_accuracy: 0.2200\n",
      "Epoch 57/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.9414 - pos_accuracy: 0.1967 - val_loss: 2.3100 - val_pos_accuracy: 0.2225\n",
      "Epoch 58/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.8956 - pos_accuracy: 0.2134 - val_loss: 2.2635 - val_pos_accuracy: 0.2325\n",
      "Epoch 59/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.8553 - pos_accuracy: 0.2093 - val_loss: 2.2250 - val_pos_accuracy: 0.2250\n",
      "Epoch 60/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.8183 - pos_accuracy: 0.2088 - val_loss: 2.1862 - val_pos_accuracy: 0.2250\n",
      "Epoch 61/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.7816 - pos_accuracy: 0.2183 - val_loss: 2.1399 - val_pos_accuracy: 0.2300\n",
      "Epoch 62/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.7466 - pos_accuracy: 0.2249 - val_loss: 2.1060 - val_pos_accuracy: 0.2300\n",
      "Epoch 63/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.7135 - pos_accuracy: 0.2205 - val_loss: 2.0647 - val_pos_accuracy: 0.2225\n",
      "Epoch 64/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.6830 - pos_accuracy: 0.2176 - val_loss: 2.0369 - val_pos_accuracy: 0.2100\n",
      "Epoch 65/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.6533 - pos_accuracy: 0.2135 - val_loss: 1.9980 - val_pos_accuracy: 0.2300\n",
      "Epoch 66/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 1.6246 - pos_accuracy: 0.2326 - val_loss: 1.9709 - val_pos_accuracy: 0.2250\n",
      "Epoch 67/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.6008 - pos_accuracy: 0.2227 - val_loss: 1.9458 - val_pos_accuracy: 0.2150\n",
      "Epoch 68/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.5738 - pos_accuracy: 0.2311 - val_loss: 1.9141 - val_pos_accuracy: 0.2250\n",
      "Epoch 69/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.5491 - pos_accuracy: 0.2351 - val_loss: 1.8786 - val_pos_accuracy: 0.2125\n",
      "Epoch 70/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.5269 - pos_accuracy: 0.2182 - val_loss: 1.8577 - val_pos_accuracy: 0.2250\n",
      "Epoch 71/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.5033 - pos_accuracy: 0.2400 - val_loss: 1.8280 - val_pos_accuracy: 0.2275\n",
      "Epoch 72/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.4821 - pos_accuracy: 0.2274 - val_loss: 1.8122 - val_pos_accuracy: 0.2325\n",
      "Epoch 73/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.4592 - pos_accuracy: 0.2377 - val_loss: 1.7789 - val_pos_accuracy: 0.2325\n",
      "Epoch 74/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.4373 - pos_accuracy: 0.2387 - val_loss: 1.7571 - val_pos_accuracy: 0.2275\n",
      "Epoch 75/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.4165 - pos_accuracy: 0.2501 - val_loss: 1.7360 - val_pos_accuracy: 0.2300\n",
      "Epoch 76/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.3984 - pos_accuracy: 0.2453 - val_loss: 1.7153 - val_pos_accuracy: 0.2325\n",
      "Epoch 77/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.3817 - pos_accuracy: 0.2536 - val_loss: 1.7021 - val_pos_accuracy: 0.2375\n",
      "Epoch 78/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.3643 - pos_accuracy: 0.2480 - val_loss: 1.6778 - val_pos_accuracy: 0.2400\n",
      "Epoch 79/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.3459 - pos_accuracy: 0.2533 - val_loss: 1.6581 - val_pos_accuracy: 0.2425\n",
      "Epoch 80/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.3310 - pos_accuracy: 0.2527 - val_loss: 1.6470 - val_pos_accuracy: 0.2325\n",
      "Epoch 81/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.3140 - pos_accuracy: 0.2537 - val_loss: 1.6193 - val_pos_accuracy: 0.2425\n",
      "Epoch 82/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.2986 - pos_accuracy: 0.2426 - val_loss: 1.6018 - val_pos_accuracy: 0.2425\n",
      "Epoch 83/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.2829 - pos_accuracy: 0.2457 - val_loss: 1.5849 - val_pos_accuracy: 0.2375\n",
      "Epoch 84/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.2676 - pos_accuracy: 0.2483 - val_loss: 1.5692 - val_pos_accuracy: 0.2375\n",
      "Epoch 85/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.2536 - pos_accuracy: 0.2523 - val_loss: 1.5541 - val_pos_accuracy: 0.2375\n",
      "Epoch 86/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.2410 - pos_accuracy: 0.2505 - val_loss: 1.5386 - val_pos_accuracy: 0.2325\n",
      "Epoch 87/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.2277 - pos_accuracy: 0.2417 - val_loss: 1.5231 - val_pos_accuracy: 0.2450\n",
      "Epoch 88/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.2141 - pos_accuracy: 0.2534 - val_loss: 1.5079 - val_pos_accuracy: 0.2425\n",
      "Epoch 89/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 1.2035 - pos_accuracy: 0.2528 - val_loss: 1.4914 - val_pos_accuracy: 0.2425\n",
      "Epoch 90/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.1912 - pos_accuracy: 0.2600 - val_loss: 1.4912 - val_pos_accuracy: 0.2350\n",
      "Epoch 91/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1789 - pos_accuracy: 0.2491 - val_loss: 1.4668 - val_pos_accuracy: 0.2450\n",
      "Epoch 92/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.1661 - pos_accuracy: 0.2525 - val_loss: 1.4572 - val_pos_accuracy: 0.2425\n",
      "Epoch 93/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.1570 - pos_accuracy: 0.2563 - val_loss: 1.4519 - val_pos_accuracy: 0.2450\n",
      "Epoch 94/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.1474 - pos_accuracy: 0.2616 - val_loss: 1.4377 - val_pos_accuracy: 0.2475\n",
      "Epoch 95/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.1355 - pos_accuracy: 0.2619 - val_loss: 1.4266 - val_pos_accuracy: 0.2475\n",
      "Epoch 96/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1255 - pos_accuracy: 0.2662 - val_loss: 1.4201 - val_pos_accuracy: 0.2475\n",
      "Epoch 97/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.1144 - pos_accuracy: 0.2709 - val_loss: 1.4009 - val_pos_accuracy: 0.2500\n",
      "Epoch 98/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1048 - pos_accuracy: 0.2799 - val_loss: 1.3924 - val_pos_accuracy: 0.2450\n",
      "Epoch 99/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.0994 - pos_accuracy: 0.2724 - val_loss: 1.3937 - val_pos_accuracy: 0.2350\n",
      "Epoch 100/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0885 - pos_accuracy: 0.2825 - val_loss: 1.3740 - val_pos_accuracy: 0.2500\n",
      "Epoch 101/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0759 - pos_accuracy: 0.2729 - val_loss: 1.3631 - val_pos_accuracy: 0.2475\n",
      "Epoch 102/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0672 - pos_accuracy: 0.2802 - val_loss: 1.3510 - val_pos_accuracy: 0.2525\n",
      "Epoch 103/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.0593 - pos_accuracy: 0.2783 - val_loss: 1.3451 - val_pos_accuracy: 0.2450\n",
      "Epoch 104/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.0503 - pos_accuracy: 0.2775 - val_loss: 1.3300 - val_pos_accuracy: 0.2500\n",
      "Epoch 105/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.0421 - pos_accuracy: 0.2785 - val_loss: 1.3231 - val_pos_accuracy: 0.2525\n",
      "Epoch 106/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.0326 - pos_accuracy: 0.2836 - val_loss: 1.3152 - val_pos_accuracy: 0.2475\n",
      "Epoch 107/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0242 - pos_accuracy: 0.2855 - val_loss: 1.3070 - val_pos_accuracy: 0.2450\n",
      "Epoch 108/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0157 - pos_accuracy: 0.2859 - val_loss: 1.2961 - val_pos_accuracy: 0.2450\n",
      "Epoch 109/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.0071 - pos_accuracy: 0.2827 - val_loss: 1.2886 - val_pos_accuracy: 0.2425\n",
      "Epoch 110/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.0005 - pos_accuracy: 0.2845 - val_loss: 1.2827 - val_pos_accuracy: 0.2400\n",
      "Epoch 111/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9926 - pos_accuracy: 0.2874 - val_loss: 1.2672 - val_pos_accuracy: 0.2500\n",
      "Epoch 112/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9848 - pos_accuracy: 0.2913 - val_loss: 1.2608 - val_pos_accuracy: 0.2450\n",
      "Epoch 113/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.9766 - pos_accuracy: 0.2877 - val_loss: 1.2499 - val_pos_accuracy: 0.2575\n",
      "Epoch 114/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.9703 - pos_accuracy: 0.2917 - val_loss: 1.2523 - val_pos_accuracy: 0.2500\n",
      "Epoch 115/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9647 - pos_accuracy: 0.2892 - val_loss: 1.2378 - val_pos_accuracy: 0.2550\n",
      "Epoch 116/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.9571 - pos_accuracy: 0.2883 - val_loss: 1.2293 - val_pos_accuracy: 0.2550\n",
      "Epoch 117/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.9485 - pos_accuracy: 0.2960 - val_loss: 1.2169 - val_pos_accuracy: 0.2700\n",
      "Epoch 118/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9417 - pos_accuracy: 0.3009 - val_loss: 1.2128 - val_pos_accuracy: 0.2525\n",
      "Epoch 119/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.9363 - pos_accuracy: 0.2945 - val_loss: 1.1988 - val_pos_accuracy: 0.2675\n",
      "Epoch 120/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9297 - pos_accuracy: 0.3174 - val_loss: 1.1993 - val_pos_accuracy: 0.2550\n",
      "Epoch 121/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.9244 - pos_accuracy: 0.3042 - val_loss: 1.1980 - val_pos_accuracy: 0.2550\n",
      "Epoch 122/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.9168 - pos_accuracy: 0.3027 - val_loss: 1.1801 - val_pos_accuracy: 0.2800\n",
      "Epoch 123/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9107 - pos_accuracy: 0.3145 - val_loss: 1.1770 - val_pos_accuracy: 0.2675\n",
      "Epoch 124/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.9038 - pos_accuracy: 0.3142 - val_loss: 1.1714 - val_pos_accuracy: 0.2650\n",
      "Epoch 125/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.8983 - pos_accuracy: 0.3145 - val_loss: 1.1574 - val_pos_accuracy: 0.2750\n",
      "Epoch 126/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.8923 - pos_accuracy: 0.3223 - val_loss: 1.1563 - val_pos_accuracy: 0.2650\n",
      "Epoch 127/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.8853 - pos_accuracy: 0.3159 - val_loss: 1.1498 - val_pos_accuracy: 0.2625\n",
      "Epoch 128/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.8799 - pos_accuracy: 0.3205 - val_loss: 1.1439 - val_pos_accuracy: 0.2675\n",
      "Epoch 129/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.8744 - pos_accuracy: 0.3232 - val_loss: 1.1352 - val_pos_accuracy: 0.2700\n",
      "Epoch 130/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8689 - pos_accuracy: 0.3264 - val_loss: 1.1337 - val_pos_accuracy: 0.2825\n",
      "Epoch 131/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.8651 - pos_accuracy: 0.3236 - val_loss: 1.1329 - val_pos_accuracy: 0.2775\n",
      "Epoch 132/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.8594 - pos_accuracy: 0.3349 - val_loss: 1.1247 - val_pos_accuracy: 0.2650\n",
      "Epoch 133/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.8539 - pos_accuracy: 0.3183 - val_loss: 1.1211 - val_pos_accuracy: 0.2725\n",
      "Epoch 134/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.8475 - pos_accuracy: 0.3279 - val_loss: 1.1116 - val_pos_accuracy: 0.2750\n",
      "Epoch 135/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8422 - pos_accuracy: 0.3287 - val_loss: 1.1066 - val_pos_accuracy: 0.2725\n",
      "Epoch 136/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8377 - pos_accuracy: 0.3315 - val_loss: 1.1041 - val_pos_accuracy: 0.2775\n",
      "Epoch 137/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8331 - pos_accuracy: 0.3363 - val_loss: 1.0954 - val_pos_accuracy: 0.2825\n",
      "Epoch 138/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.8278 - pos_accuracy: 0.3359 - val_loss: 1.0937 - val_pos_accuracy: 0.2775\n",
      "Epoch 139/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.8234 - pos_accuracy: 0.3263 - val_loss: 1.0813 - val_pos_accuracy: 0.2800\n",
      "Epoch 140/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.8184 - pos_accuracy: 0.3345 - val_loss: 1.0771 - val_pos_accuracy: 0.2825\n",
      "Epoch 141/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.8136 - pos_accuracy: 0.3349 - val_loss: 1.0730 - val_pos_accuracy: 0.2775\n",
      "Epoch 142/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.8091 - pos_accuracy: 0.3359 - val_loss: 1.0722 - val_pos_accuracy: 0.2800\n",
      "Epoch 143/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.8046 - pos_accuracy: 0.3416 - val_loss: 1.0648 - val_pos_accuracy: 0.2900\n",
      "Epoch 144/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7995 - pos_accuracy: 0.3351 - val_loss: 1.0547 - val_pos_accuracy: 0.2850\n",
      "Epoch 145/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.7952 - pos_accuracy: 0.3302 - val_loss: 1.0567 - val_pos_accuracy: 0.2875\n",
      "Epoch 146/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7912 - pos_accuracy: 0.3398 - val_loss: 1.0526 - val_pos_accuracy: 0.2800\n",
      "Epoch 147/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7873 - pos_accuracy: 0.3423 - val_loss: 1.0490 - val_pos_accuracy: 0.2800\n",
      "Epoch 148/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7821 - pos_accuracy: 0.3474 - val_loss: 1.0367 - val_pos_accuracy: 0.2900\n",
      "Epoch 149/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7774 - pos_accuracy: 0.3385 - val_loss: 1.0333 - val_pos_accuracy: 0.2875\n",
      "Epoch 150/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7735 - pos_accuracy: 0.3482 - val_loss: 1.0287 - val_pos_accuracy: 0.2875\n",
      "Epoch 151/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7694 - pos_accuracy: 0.3496 - val_loss: 1.0240 - val_pos_accuracy: 0.2950\n",
      "Epoch 152/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7659 - pos_accuracy: 0.3496 - val_loss: 1.0244 - val_pos_accuracy: 0.2975\n",
      "Epoch 153/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7631 - pos_accuracy: 0.3560 - val_loss: 1.0221 - val_pos_accuracy: 0.2950\n",
      "Epoch 154/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7567 - pos_accuracy: 0.3458 - val_loss: 1.0106 - val_pos_accuracy: 0.2850\n",
      "Epoch 155/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7526 - pos_accuracy: 0.3391 - val_loss: 1.0067 - val_pos_accuracy: 0.2775\n",
      "Epoch 156/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7484 - pos_accuracy: 0.3419 - val_loss: 1.0033 - val_pos_accuracy: 0.2825\n",
      "Epoch 157/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.7437 - pos_accuracy: 0.3411 - val_loss: 1.0009 - val_pos_accuracy: 0.2825\n",
      "Epoch 158/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7401 - pos_accuracy: 0.3506 - val_loss: 0.9965 - val_pos_accuracy: 0.2825\n",
      "Epoch 159/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7358 - pos_accuracy: 0.3494 - val_loss: 0.9904 - val_pos_accuracy: 0.2900\n",
      "Epoch 160/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7323 - pos_accuracy: 0.3585 - val_loss: 0.9870 - val_pos_accuracy: 0.2925\n",
      "Epoch 161/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7282 - pos_accuracy: 0.3594 - val_loss: 0.9802 - val_pos_accuracy: 0.2950\n",
      "Epoch 162/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7254 - pos_accuracy: 0.3556 - val_loss: 0.9744 - val_pos_accuracy: 0.3025\n",
      "Epoch 163/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7212 - pos_accuracy: 0.3619 - val_loss: 0.9724 - val_pos_accuracy: 0.3025\n",
      "Epoch 164/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7174 - pos_accuracy: 0.3769 - val_loss: 0.9654 - val_pos_accuracy: 0.3025\n",
      "Epoch 165/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7139 - pos_accuracy: 0.3627 - val_loss: 0.9636 - val_pos_accuracy: 0.2975\n",
      "Epoch 166/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.7104 - pos_accuracy: 0.3702 - val_loss: 0.9571 - val_pos_accuracy: 0.3100\n",
      "Epoch 167/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7064 - pos_accuracy: 0.3821 - val_loss: 0.9529 - val_pos_accuracy: 0.3100\n",
      "Epoch 168/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7041 - pos_accuracy: 0.3812 - val_loss: 0.9488 - val_pos_accuracy: 0.3150\n",
      "Epoch 169/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7025 - pos_accuracy: 0.3720 - val_loss: 0.9428 - val_pos_accuracy: 0.3150\n",
      "Epoch 170/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6973 - pos_accuracy: 0.3775 - val_loss: 0.9421 - val_pos_accuracy: 0.3075\n",
      "Epoch 171/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6930 - pos_accuracy: 0.3863 - val_loss: 0.9430 - val_pos_accuracy: 0.2950\n",
      "Epoch 172/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6905 - pos_accuracy: 0.3843 - val_loss: 0.9384 - val_pos_accuracy: 0.2950\n",
      "Epoch 173/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6864 - pos_accuracy: 0.3752 - val_loss: 0.9305 - val_pos_accuracy: 0.3125\n",
      "Epoch 174/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6834 - pos_accuracy: 0.3916 - val_loss: 0.9262 - val_pos_accuracy: 0.3100\n",
      "Epoch 175/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6817 - pos_accuracy: 0.3874 - val_loss: 0.9328 - val_pos_accuracy: 0.2875\n",
      "Epoch 176/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6780 - pos_accuracy: 0.3895 - val_loss: 0.9221 - val_pos_accuracy: 0.3225\n",
      "Epoch 177/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6739 - pos_accuracy: 0.3885 - val_loss: 0.9192 - val_pos_accuracy: 0.3150\n",
      "Epoch 178/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6705 - pos_accuracy: 0.3940 - val_loss: 0.9208 - val_pos_accuracy: 0.3075\n",
      "Epoch 179/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6678 - pos_accuracy: 0.4001 - val_loss: 0.9156 - val_pos_accuracy: 0.3125\n",
      "Epoch 180/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6637 - pos_accuracy: 0.4007 - val_loss: 0.9123 - val_pos_accuracy: 0.3150\n",
      "Epoch 181/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6609 - pos_accuracy: 0.3997 - val_loss: 0.9097 - val_pos_accuracy: 0.3150\n",
      "Epoch 182/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6583 - pos_accuracy: 0.3971 - val_loss: 0.9030 - val_pos_accuracy: 0.3125\n",
      "Epoch 183/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6548 - pos_accuracy: 0.4017 - val_loss: 0.9012 - val_pos_accuracy: 0.3175\n",
      "Epoch 184/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6522 - pos_accuracy: 0.4031 - val_loss: 0.9007 - val_pos_accuracy: 0.3150\n",
      "Epoch 185/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6501 - pos_accuracy: 0.4019 - val_loss: 0.8986 - val_pos_accuracy: 0.3200\n",
      "Epoch 186/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.6473 - pos_accuracy: 0.4054 - val_loss: 0.8937 - val_pos_accuracy: 0.3250\n",
      "Epoch 187/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6450 - pos_accuracy: 0.4004 - val_loss: 0.8872 - val_pos_accuracy: 0.3325\n",
      "Epoch 188/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6419 - pos_accuracy: 0.4044 - val_loss: 0.8868 - val_pos_accuracy: 0.3200\n",
      "Epoch 189/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6379 - pos_accuracy: 0.4049 - val_loss: 0.8853 - val_pos_accuracy: 0.3200\n",
      "Epoch 190/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6349 - pos_accuracy: 0.4053 - val_loss: 0.8810 - val_pos_accuracy: 0.3300\n",
      "Epoch 191/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6334 - pos_accuracy: 0.4031 - val_loss: 0.8813 - val_pos_accuracy: 0.3150\n",
      "Epoch 192/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6298 - pos_accuracy: 0.4185 - val_loss: 0.8778 - val_pos_accuracy: 0.3250\n",
      "Epoch 193/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6270 - pos_accuracy: 0.4218 - val_loss: 0.8730 - val_pos_accuracy: 0.3300\n",
      "Epoch 194/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6242 - pos_accuracy: 0.4131 - val_loss: 0.8681 - val_pos_accuracy: 0.3300\n",
      "Epoch 195/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6219 - pos_accuracy: 0.4060 - val_loss: 0.8651 - val_pos_accuracy: 0.3350\n",
      "Epoch 196/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6195 - pos_accuracy: 0.4147 - val_loss: 0.8655 - val_pos_accuracy: 0.3200\n",
      "Epoch 197/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6160 - pos_accuracy: 0.4190 - val_loss: 0.8592 - val_pos_accuracy: 0.3325\n",
      "Epoch 198/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.6137 - pos_accuracy: 0.4132 - val_loss: 0.8574 - val_pos_accuracy: 0.3300\n",
      "Epoch 199/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6118 - pos_accuracy: 0.4150 - val_loss: 0.8583 - val_pos_accuracy: 0.3250\n",
      "Epoch 200/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6089 - pos_accuracy: 0.4257 - val_loss: 0.8541 - val_pos_accuracy: 0.3300\n",
      "Epoch 201/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6061 - pos_accuracy: 0.4236 - val_loss: 0.8488 - val_pos_accuracy: 0.3350\n",
      "Epoch 202/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6037 - pos_accuracy: 0.4138 - val_loss: 0.8467 - val_pos_accuracy: 0.3325\n",
      "Epoch 203/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6013 - pos_accuracy: 0.4218 - val_loss: 0.8400 - val_pos_accuracy: 0.3375\n",
      "Epoch 204/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5992 - pos_accuracy: 0.4122 - val_loss: 0.8384 - val_pos_accuracy: 0.3400\n",
      "Epoch 205/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5970 - pos_accuracy: 0.4196 - val_loss: 0.8351 - val_pos_accuracy: 0.3425\n",
      "Epoch 206/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5936 - pos_accuracy: 0.4131 - val_loss: 0.8352 - val_pos_accuracy: 0.3400\n",
      "Epoch 207/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5913 - pos_accuracy: 0.4376 - val_loss: 0.8357 - val_pos_accuracy: 0.3375\n",
      "Epoch 208/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5905 - pos_accuracy: 0.4257 - val_loss: 0.8314 - val_pos_accuracy: 0.3450\n",
      "Epoch 209/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5863 - pos_accuracy: 0.4193 - val_loss: 0.8263 - val_pos_accuracy: 0.3400\n",
      "Epoch 210/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5844 - pos_accuracy: 0.4234 - val_loss: 0.8231 - val_pos_accuracy: 0.3450\n",
      "Epoch 211/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5814 - pos_accuracy: 0.4333 - val_loss: 0.8226 - val_pos_accuracy: 0.3500\n",
      "Epoch 212/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5803 - pos_accuracy: 0.4323 - val_loss: 0.8266 - val_pos_accuracy: 0.3475\n",
      "Epoch 213/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5783 - pos_accuracy: 0.4296 - val_loss: 0.8185 - val_pos_accuracy: 0.3475\n",
      "Epoch 214/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5754 - pos_accuracy: 0.4376 - val_loss: 0.8166 - val_pos_accuracy: 0.3550\n",
      "Epoch 215/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5728 - pos_accuracy: 0.4318 - val_loss: 0.8146 - val_pos_accuracy: 0.3625\n",
      "Epoch 216/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5708 - pos_accuracy: 0.4416 - val_loss: 0.8110 - val_pos_accuracy: 0.3600\n",
      "Epoch 217/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5688 - pos_accuracy: 0.4404 - val_loss: 0.8073 - val_pos_accuracy: 0.3575\n",
      "Epoch 218/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5663 - pos_accuracy: 0.4357 - val_loss: 0.8083 - val_pos_accuracy: 0.3550\n",
      "Epoch 219/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5641 - pos_accuracy: 0.4382 - val_loss: 0.8085 - val_pos_accuracy: 0.3575\n",
      "Epoch 220/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5624 - pos_accuracy: 0.4454 - val_loss: 0.8020 - val_pos_accuracy: 0.3625\n",
      "Epoch 221/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5597 - pos_accuracy: 0.4453 - val_loss: 0.7984 - val_pos_accuracy: 0.3625\n",
      "Epoch 222/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5578 - pos_accuracy: 0.4463 - val_loss: 0.7974 - val_pos_accuracy: 0.3675\n",
      "Epoch 223/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5558 - pos_accuracy: 0.4463 - val_loss: 0.7976 - val_pos_accuracy: 0.3600\n",
      "Epoch 224/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5533 - pos_accuracy: 0.4481 - val_loss: 0.7930 - val_pos_accuracy: 0.3675\n",
      "Epoch 225/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5514 - pos_accuracy: 0.4516 - val_loss: 0.7922 - val_pos_accuracy: 0.3675\n",
      "Epoch 226/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5496 - pos_accuracy: 0.4540 - val_loss: 0.7905 - val_pos_accuracy: 0.3725\n",
      "Epoch 227/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5478 - pos_accuracy: 0.4450 - val_loss: 0.7858 - val_pos_accuracy: 0.3625\n",
      "Epoch 228/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5461 - pos_accuracy: 0.4499 - val_loss: 0.7888 - val_pos_accuracy: 0.3600\n",
      "Epoch 229/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5439 - pos_accuracy: 0.4574 - val_loss: 0.7827 - val_pos_accuracy: 0.3700\n",
      "Epoch 230/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5415 - pos_accuracy: 0.4522 - val_loss: 0.7800 - val_pos_accuracy: 0.3675\n",
      "Epoch 231/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5399 - pos_accuracy: 0.4552 - val_loss: 0.7781 - val_pos_accuracy: 0.3675\n",
      "Epoch 232/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5381 - pos_accuracy: 0.4487 - val_loss: 0.7794 - val_pos_accuracy: 0.3650\n",
      "Epoch 233/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5367 - pos_accuracy: 0.4598 - val_loss: 0.7772 - val_pos_accuracy: 0.3675\n",
      "Epoch 234/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5342 - pos_accuracy: 0.4537 - val_loss: 0.7753 - val_pos_accuracy: 0.3675\n",
      "Epoch 235/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5335 - pos_accuracy: 0.4599 - val_loss: 0.7755 - val_pos_accuracy: 0.3600\n",
      "Epoch 236/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5308 - pos_accuracy: 0.4622 - val_loss: 0.7693 - val_pos_accuracy: 0.3700\n",
      "Epoch 237/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5290 - pos_accuracy: 0.4498 - val_loss: 0.7718 - val_pos_accuracy: 0.3650\n",
      "Epoch 238/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5282 - pos_accuracy: 0.4494 - val_loss: 0.7702 - val_pos_accuracy: 0.3675\n",
      "Epoch 239/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5251 - pos_accuracy: 0.4607 - val_loss: 0.7638 - val_pos_accuracy: 0.3725\n",
      "Epoch 240/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5227 - pos_accuracy: 0.4549 - val_loss: 0.7622 - val_pos_accuracy: 0.3700\n",
      "Epoch 241/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5206 - pos_accuracy: 0.4680 - val_loss: 0.7593 - val_pos_accuracy: 0.3725\n",
      "Epoch 242/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5190 - pos_accuracy: 0.4635 - val_loss: 0.7566 - val_pos_accuracy: 0.3750\n",
      "Epoch 243/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5177 - pos_accuracy: 0.4592 - val_loss: 0.7558 - val_pos_accuracy: 0.3725\n",
      "Epoch 244/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5158 - pos_accuracy: 0.4655 - val_loss: 0.7547 - val_pos_accuracy: 0.3675\n",
      "Epoch 245/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5135 - pos_accuracy: 0.4651 - val_loss: 0.7517 - val_pos_accuracy: 0.3800\n",
      "Epoch 246/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5121 - pos_accuracy: 0.4718 - val_loss: 0.7532 - val_pos_accuracy: 0.3675\n",
      "Epoch 247/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5105 - pos_accuracy: 0.4719 - val_loss: 0.7494 - val_pos_accuracy: 0.3750\n",
      "Epoch 248/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5087 - pos_accuracy: 0.4626 - val_loss: 0.7478 - val_pos_accuracy: 0.3800\n",
      "Epoch 249/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5076 - pos_accuracy: 0.4651 - val_loss: 0.7441 - val_pos_accuracy: 0.3825\n",
      "Epoch 250/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5069 - pos_accuracy: 0.4647 - val_loss: 0.7416 - val_pos_accuracy: 0.3775\n",
      "Epoch 251/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5040 - pos_accuracy: 0.4762 - val_loss: 0.7415 - val_pos_accuracy: 0.3725\n",
      "Epoch 252/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5023 - pos_accuracy: 0.4739 - val_loss: 0.7406 - val_pos_accuracy: 0.3800\n",
      "Epoch 253/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5006 - pos_accuracy: 0.4734 - val_loss: 0.7387 - val_pos_accuracy: 0.3750\n",
      "Epoch 254/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4986 - pos_accuracy: 0.4764 - val_loss: 0.7360 - val_pos_accuracy: 0.3825\n",
      "Epoch 255/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4967 - pos_accuracy: 0.4865 - val_loss: 0.7335 - val_pos_accuracy: 0.4000\n",
      "Epoch 256/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4955 - pos_accuracy: 0.4843 - val_loss: 0.7333 - val_pos_accuracy: 0.3825\n",
      "Epoch 257/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4936 - pos_accuracy: 0.4765 - val_loss: 0.7292 - val_pos_accuracy: 0.3750\n",
      "Epoch 258/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4925 - pos_accuracy: 0.4805 - val_loss: 0.7277 - val_pos_accuracy: 0.3975\n",
      "Epoch 259/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4909 - pos_accuracy: 0.4841 - val_loss: 0.7257 - val_pos_accuracy: 0.3800\n",
      "Epoch 260/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4896 - pos_accuracy: 0.4844 - val_loss: 0.7252 - val_pos_accuracy: 0.3800\n",
      "Epoch 261/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4873 - pos_accuracy: 0.4804 - val_loss: 0.7224 - val_pos_accuracy: 0.3750\n",
      "Epoch 262/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4867 - pos_accuracy: 0.4788 - val_loss: 0.7218 - val_pos_accuracy: 0.3925\n",
      "Epoch 263/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4845 - pos_accuracy: 0.4854 - val_loss: 0.7230 - val_pos_accuracy: 0.3825\n",
      "Epoch 264/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4836 - pos_accuracy: 0.4897 - val_loss: 0.7199 - val_pos_accuracy: 0.3850\n",
      "Epoch 265/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4814 - pos_accuracy: 0.4864 - val_loss: 0.7176 - val_pos_accuracy: 0.3800\n",
      "Epoch 266/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4794 - pos_accuracy: 0.4883 - val_loss: 0.7157 - val_pos_accuracy: 0.3800\n",
      "Epoch 267/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4780 - pos_accuracy: 0.4865 - val_loss: 0.7158 - val_pos_accuracy: 0.3825\n",
      "Epoch 268/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4773 - pos_accuracy: 0.4918 - val_loss: 0.7110 - val_pos_accuracy: 0.3925\n",
      "Epoch 269/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4757 - pos_accuracy: 0.4961 - val_loss: 0.7107 - val_pos_accuracy: 0.3775\n",
      "Epoch 270/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4745 - pos_accuracy: 0.4960 - val_loss: 0.7124 - val_pos_accuracy: 0.3850\n",
      "Epoch 271/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.4728 - pos_accuracy: 0.5007 - val_loss: 0.7087 - val_pos_accuracy: 0.3825\n",
      "Epoch 272/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4711 - pos_accuracy: 0.4979 - val_loss: 0.7072 - val_pos_accuracy: 0.3800\n",
      "Epoch 273/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4699 - pos_accuracy: 0.4979 - val_loss: 0.7069 - val_pos_accuracy: 0.3975\n",
      "Epoch 274/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4684 - pos_accuracy: 0.4978 - val_loss: 0.7047 - val_pos_accuracy: 0.3750\n",
      "Epoch 275/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4670 - pos_accuracy: 0.4902 - val_loss: 0.7015 - val_pos_accuracy: 0.3900\n",
      "Epoch 276/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4664 - pos_accuracy: 0.4901 - val_loss: 0.7018 - val_pos_accuracy: 0.3925\n",
      "Epoch 277/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4653 - pos_accuracy: 0.5068 - val_loss: 0.6997 - val_pos_accuracy: 0.4000\n",
      "Epoch 278/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4631 - pos_accuracy: 0.4975 - val_loss: 0.6984 - val_pos_accuracy: 0.3875\n",
      "Epoch 279/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4608 - pos_accuracy: 0.4980 - val_loss: 0.6943 - val_pos_accuracy: 0.4000\n",
      "Epoch 280/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4595 - pos_accuracy: 0.5012 - val_loss: 0.6939 - val_pos_accuracy: 0.3925\n",
      "Epoch 281/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4578 - pos_accuracy: 0.5092 - val_loss: 0.6929 - val_pos_accuracy: 0.3950\n",
      "Epoch 282/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4568 - pos_accuracy: 0.5035 - val_loss: 0.6918 - val_pos_accuracy: 0.3875\n",
      "Epoch 283/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4556 - pos_accuracy: 0.5033 - val_loss: 0.6886 - val_pos_accuracy: 0.3975\n",
      "Epoch 284/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4540 - pos_accuracy: 0.5091 - val_loss: 0.6893 - val_pos_accuracy: 0.3875\n",
      "Epoch 285/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4527 - pos_accuracy: 0.5104 - val_loss: 0.6880 - val_pos_accuracy: 0.3950\n",
      "Epoch 286/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4517 - pos_accuracy: 0.5068 - val_loss: 0.6877 - val_pos_accuracy: 0.3875\n",
      "Epoch 287/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4509 - pos_accuracy: 0.4983 - val_loss: 0.6844 - val_pos_accuracy: 0.4000\n",
      "Epoch 288/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4493 - pos_accuracy: 0.5084 - val_loss: 0.6829 - val_pos_accuracy: 0.3975\n",
      "Epoch 289/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4474 - pos_accuracy: 0.5043 - val_loss: 0.6798 - val_pos_accuracy: 0.4100\n",
      "Epoch 290/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4471 - pos_accuracy: 0.5075 - val_loss: 0.6782 - val_pos_accuracy: 0.4075\n",
      "Epoch 291/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4454 - pos_accuracy: 0.5120 - val_loss: 0.6775 - val_pos_accuracy: 0.4100\n",
      "Epoch 292/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4434 - pos_accuracy: 0.5149 - val_loss: 0.6774 - val_pos_accuracy: 0.3900\n",
      "Epoch 293/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4424 - pos_accuracy: 0.5129 - val_loss: 0.6761 - val_pos_accuracy: 0.4075\n",
      "Epoch 294/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4410 - pos_accuracy: 0.5084 - val_loss: 0.6740 - val_pos_accuracy: 0.4025\n",
      "Epoch 295/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4396 - pos_accuracy: 0.5141 - val_loss: 0.6742 - val_pos_accuracy: 0.4000\n",
      "Epoch 296/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4392 - pos_accuracy: 0.5203 - val_loss: 0.6772 - val_pos_accuracy: 0.3925\n",
      "Epoch 297/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4377 - pos_accuracy: 0.5043 - val_loss: 0.6708 - val_pos_accuracy: 0.4125\n",
      "Epoch 298/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4363 - pos_accuracy: 0.5198 - val_loss: 0.6734 - val_pos_accuracy: 0.3925\n",
      "Epoch 299/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4349 - pos_accuracy: 0.5141 - val_loss: 0.6690 - val_pos_accuracy: 0.4025\n",
      "Epoch 300/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4338 - pos_accuracy: 0.5114 - val_loss: 0.6686 - val_pos_accuracy: 0.3950\n",
      "Epoch 301/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4324 - pos_accuracy: 0.5024 - val_loss: 0.6660 - val_pos_accuracy: 0.4150\n",
      "Epoch 302/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4312 - pos_accuracy: 0.5170 - val_loss: 0.6651 - val_pos_accuracy: 0.3950\n",
      "Epoch 303/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4299 - pos_accuracy: 0.5222 - val_loss: 0.6626 - val_pos_accuracy: 0.4050\n",
      "Epoch 304/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4288 - pos_accuracy: 0.5182 - val_loss: 0.6611 - val_pos_accuracy: 0.4050\n",
      "Epoch 305/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4276 - pos_accuracy: 0.5172 - val_loss: 0.6605 - val_pos_accuracy: 0.4050\n",
      "Epoch 306/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4263 - pos_accuracy: 0.5208 - val_loss: 0.6611 - val_pos_accuracy: 0.4000\n",
      "Epoch 307/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4250 - pos_accuracy: 0.5124 - val_loss: 0.6586 - val_pos_accuracy: 0.4100\n",
      "Epoch 308/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4239 - pos_accuracy: 0.5186 - val_loss: 0.6566 - val_pos_accuracy: 0.4100\n",
      "Epoch 309/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.4227 - pos_accuracy: 0.5203 - val_loss: 0.6582 - val_pos_accuracy: 0.3975\n",
      "Epoch 310/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4234 - pos_accuracy: 0.5182 - val_loss: 0.6596 - val_pos_accuracy: 0.3800\n",
      "Epoch 311/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4211 - pos_accuracy: 0.5077 - val_loss: 0.6541 - val_pos_accuracy: 0.4100\n",
      "Epoch 312/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4195 - pos_accuracy: 0.5235 - val_loss: 0.6542 - val_pos_accuracy: 0.4025\n",
      "Epoch 313/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4184 - pos_accuracy: 0.5191 - val_loss: 0.6533 - val_pos_accuracy: 0.3975\n",
      "Epoch 314/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4173 - pos_accuracy: 0.5209 - val_loss: 0.6512 - val_pos_accuracy: 0.4000\n",
      "Epoch 315/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.4174 - pos_accuracy: 0.5153 - val_loss: 0.6486 - val_pos_accuracy: 0.4025\n",
      "Epoch 316/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4155 - pos_accuracy: 0.5167 - val_loss: 0.6468 - val_pos_accuracy: 0.4075\n",
      "Epoch 317/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4140 - pos_accuracy: 0.5278 - val_loss: 0.6455 - val_pos_accuracy: 0.4075\n",
      "Epoch 318/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4127 - pos_accuracy: 0.5183 - val_loss: 0.6442 - val_pos_accuracy: 0.4050\n",
      "Epoch 319/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4117 - pos_accuracy: 0.5189 - val_loss: 0.6433 - val_pos_accuracy: 0.4025\n",
      "Epoch 320/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4105 - pos_accuracy: 0.5327 - val_loss: 0.6438 - val_pos_accuracy: 0.4025\n",
      "Epoch 321/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4093 - pos_accuracy: 0.5189 - val_loss: 0.6421 - val_pos_accuracy: 0.4125\n",
      "Epoch 322/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4085 - pos_accuracy: 0.5207 - val_loss: 0.6410 - val_pos_accuracy: 0.4025\n",
      "Epoch 323/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4072 - pos_accuracy: 0.5300 - val_loss: 0.6396 - val_pos_accuracy: 0.4100\n",
      "Epoch 324/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4061 - pos_accuracy: 0.5266 - val_loss: 0.6395 - val_pos_accuracy: 0.4025\n",
      "Epoch 325/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4051 - pos_accuracy: 0.5170 - val_loss: 0.6381 - val_pos_accuracy: 0.4125\n",
      "Epoch 326/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4041 - pos_accuracy: 0.5240 - val_loss: 0.6361 - val_pos_accuracy: 0.4125\n",
      "Epoch 327/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4037 - pos_accuracy: 0.5220 - val_loss: 0.6339 - val_pos_accuracy: 0.4175\n",
      "Epoch 328/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4019 - pos_accuracy: 0.5372 - val_loss: 0.6338 - val_pos_accuracy: 0.4175\n",
      "Epoch 329/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4007 - pos_accuracy: 0.5347 - val_loss: 0.6352 - val_pos_accuracy: 0.3875\n",
      "Epoch 330/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4001 - pos_accuracy: 0.5164 - val_loss: 0.6315 - val_pos_accuracy: 0.4175\n",
      "Epoch 331/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3991 - pos_accuracy: 0.5305 - val_loss: 0.6291 - val_pos_accuracy: 0.4225\n",
      "Epoch 332/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3983 - pos_accuracy: 0.5365 - val_loss: 0.6278 - val_pos_accuracy: 0.4225\n",
      "Epoch 333/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3966 - pos_accuracy: 0.5343 - val_loss: 0.6280 - val_pos_accuracy: 0.4175\n",
      "Epoch 334/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3956 - pos_accuracy: 0.5292 - val_loss: 0.6266 - val_pos_accuracy: 0.4225\n",
      "Epoch 335/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3944 - pos_accuracy: 0.5263 - val_loss: 0.6253 - val_pos_accuracy: 0.4175\n",
      "Epoch 336/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3934 - pos_accuracy: 0.5273 - val_loss: 0.6243 - val_pos_accuracy: 0.4175\n",
      "Epoch 337/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3928 - pos_accuracy: 0.5277 - val_loss: 0.6235 - val_pos_accuracy: 0.4225\n",
      "Epoch 338/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3914 - pos_accuracy: 0.5290 - val_loss: 0.6229 - val_pos_accuracy: 0.4225\n",
      "Epoch 339/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3907 - pos_accuracy: 0.5357 - val_loss: 0.6238 - val_pos_accuracy: 0.4075\n",
      "Epoch 340/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3903 - pos_accuracy: 0.5280 - val_loss: 0.6241 - val_pos_accuracy: 0.3975\n",
      "Epoch 341/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3888 - pos_accuracy: 0.5179 - val_loss: 0.6205 - val_pos_accuracy: 0.4200\n",
      "Epoch 342/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3875 - pos_accuracy: 0.5317 - val_loss: 0.6208 - val_pos_accuracy: 0.4200\n",
      "Epoch 343/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3875 - pos_accuracy: 0.5301 - val_loss: 0.6209 - val_pos_accuracy: 0.4050\n",
      "Epoch 344/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3863 - pos_accuracy: 0.5248 - val_loss: 0.6188 - val_pos_accuracy: 0.4225\n",
      "Epoch 345/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3851 - pos_accuracy: 0.5307 - val_loss: 0.6162 - val_pos_accuracy: 0.4225\n",
      "Epoch 346/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3844 - pos_accuracy: 0.5324 - val_loss: 0.6166 - val_pos_accuracy: 0.4100\n",
      "Epoch 347/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3830 - pos_accuracy: 0.5272 - val_loss: 0.6152 - val_pos_accuracy: 0.4100\n",
      "Epoch 348/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3820 - pos_accuracy: 0.5311 - val_loss: 0.6139 - val_pos_accuracy: 0.4200\n",
      "Epoch 349/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3809 - pos_accuracy: 0.5331 - val_loss: 0.6134 - val_pos_accuracy: 0.4125\n",
      "Epoch 350/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3804 - pos_accuracy: 0.5307 - val_loss: 0.6111 - val_pos_accuracy: 0.4250\n",
      "Epoch 351/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3792 - pos_accuracy: 0.5322 - val_loss: 0.6114 - val_pos_accuracy: 0.4200\n",
      "Epoch 352/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3778 - pos_accuracy: 0.5356 - val_loss: 0.6101 - val_pos_accuracy: 0.4250\n",
      "Epoch 353/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3773 - pos_accuracy: 0.5306 - val_loss: 0.6076 - val_pos_accuracy: 0.4125\n",
      "Epoch 354/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3764 - pos_accuracy: 0.5372 - val_loss: 0.6065 - val_pos_accuracy: 0.4175\n",
      "Epoch 355/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3755 - pos_accuracy: 0.5367 - val_loss: 0.6060 - val_pos_accuracy: 0.4250\n",
      "Epoch 356/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3745 - pos_accuracy: 0.5353 - val_loss: 0.6052 - val_pos_accuracy: 0.4275\n",
      "Epoch 357/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3734 - pos_accuracy: 0.5379 - val_loss: 0.6050 - val_pos_accuracy: 0.4150\n",
      "Epoch 358/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3733 - pos_accuracy: 0.5389 - val_loss: 0.6039 - val_pos_accuracy: 0.4225\n",
      "Epoch 359/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3721 - pos_accuracy: 0.5455 - val_loss: 0.6035 - val_pos_accuracy: 0.4250\n",
      "Epoch 360/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3706 - pos_accuracy: 0.5320 - val_loss: 0.6027 - val_pos_accuracy: 0.4250\n",
      "Epoch 361/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3695 - pos_accuracy: 0.5332 - val_loss: 0.6015 - val_pos_accuracy: 0.4325\n",
      "Epoch 362/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3688 - pos_accuracy: 0.5303 - val_loss: 0.6006 - val_pos_accuracy: 0.4250\n",
      "Epoch 363/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3682 - pos_accuracy: 0.5365 - val_loss: 0.6011 - val_pos_accuracy: 0.4200\n",
      "Epoch 364/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3674 - pos_accuracy: 0.5314 - val_loss: 0.5990 - val_pos_accuracy: 0.4250\n",
      "Epoch 365/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3665 - pos_accuracy: 0.5362 - val_loss: 0.5978 - val_pos_accuracy: 0.4350\n",
      "Epoch 366/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3651 - pos_accuracy: 0.5484 - val_loss: 0.5979 - val_pos_accuracy: 0.4275\n",
      "Epoch 367/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3652 - pos_accuracy: 0.5246 - val_loss: 0.5971 - val_pos_accuracy: 0.4275\n",
      "Epoch 368/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3641 - pos_accuracy: 0.5411 - val_loss: 0.5958 - val_pos_accuracy: 0.4325\n",
      "Epoch 369/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3633 - pos_accuracy: 0.5455 - val_loss: 0.5966 - val_pos_accuracy: 0.4200\n",
      "Epoch 370/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3624 - pos_accuracy: 0.5348 - val_loss: 0.5952 - val_pos_accuracy: 0.4350\n",
      "Epoch 371/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3611 - pos_accuracy: 0.5419 - val_loss: 0.5922 - val_pos_accuracy: 0.4250\n",
      "Epoch 372/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3610 - pos_accuracy: 0.5495 - val_loss: 0.5912 - val_pos_accuracy: 0.4375\n",
      "Epoch 373/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3593 - pos_accuracy: 0.5579 - val_loss: 0.5910 - val_pos_accuracy: 0.4400\n",
      "Epoch 374/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3592 - pos_accuracy: 0.5378 - val_loss: 0.5910 - val_pos_accuracy: 0.4325\n",
      "Epoch 375/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3579 - pos_accuracy: 0.5393 - val_loss: 0.5885 - val_pos_accuracy: 0.4375\n",
      "Epoch 376/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3576 - pos_accuracy: 0.5376 - val_loss: 0.5867 - val_pos_accuracy: 0.4350\n",
      "Epoch 377/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3563 - pos_accuracy: 0.5526 - val_loss: 0.5881 - val_pos_accuracy: 0.4300\n",
      "Epoch 378/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3554 - pos_accuracy: 0.5345 - val_loss: 0.5858 - val_pos_accuracy: 0.4425\n",
      "Epoch 379/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3542 - pos_accuracy: 0.5413 - val_loss: 0.5858 - val_pos_accuracy: 0.4400\n",
      "Epoch 380/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3543 - pos_accuracy: 0.5429 - val_loss: 0.5839 - val_pos_accuracy: 0.4275\n",
      "Epoch 381/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3529 - pos_accuracy: 0.5495 - val_loss: 0.5842 - val_pos_accuracy: 0.4400\n",
      "Epoch 382/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3525 - pos_accuracy: 0.5509 - val_loss: 0.5855 - val_pos_accuracy: 0.4275\n",
      "Epoch 383/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3520 - pos_accuracy: 0.5419 - val_loss: 0.5839 - val_pos_accuracy: 0.4325\n",
      "Epoch 384/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3507 - pos_accuracy: 0.5473 - val_loss: 0.5811 - val_pos_accuracy: 0.4350\n",
      "Epoch 385/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3497 - pos_accuracy: 0.5632 - val_loss: 0.5812 - val_pos_accuracy: 0.4375\n",
      "Epoch 386/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3494 - pos_accuracy: 0.5501 - val_loss: 0.5807 - val_pos_accuracy: 0.4350\n",
      "Epoch 387/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3480 - pos_accuracy: 0.5446 - val_loss: 0.5802 - val_pos_accuracy: 0.4400\n",
      "Epoch 388/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3473 - pos_accuracy: 0.5494 - val_loss: 0.5787 - val_pos_accuracy: 0.4425\n",
      "Epoch 389/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3464 - pos_accuracy: 0.5557 - val_loss: 0.5767 - val_pos_accuracy: 0.4350\n",
      "Epoch 390/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3458 - pos_accuracy: 0.5581 - val_loss: 0.5765 - val_pos_accuracy: 0.4475\n",
      "Epoch 391/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3452 - pos_accuracy: 0.5549 - val_loss: 0.5747 - val_pos_accuracy: 0.4400\n",
      "Epoch 392/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3442 - pos_accuracy: 0.5496 - val_loss: 0.5740 - val_pos_accuracy: 0.4375\n",
      "Epoch 393/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3434 - pos_accuracy: 0.5567 - val_loss: 0.5742 - val_pos_accuracy: 0.4400\n",
      "Epoch 394/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3427 - pos_accuracy: 0.5472 - val_loss: 0.5734 - val_pos_accuracy: 0.4425\n",
      "Epoch 395/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3419 - pos_accuracy: 0.5541 - val_loss: 0.5728 - val_pos_accuracy: 0.4400\n",
      "Epoch 396/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3410 - pos_accuracy: 0.5610 - val_loss: 0.5702 - val_pos_accuracy: 0.4425\n",
      "Epoch 397/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3404 - pos_accuracy: 0.5552 - val_loss: 0.5705 - val_pos_accuracy: 0.4375\n",
      "Epoch 398/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3408 - pos_accuracy: 0.5511 - val_loss: 0.5689 - val_pos_accuracy: 0.4425\n",
      "Epoch 399/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3391 - pos_accuracy: 0.5524 - val_loss: 0.5694 - val_pos_accuracy: 0.4475\n",
      "Epoch 400/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3380 - pos_accuracy: 0.5565 - val_loss: 0.5676 - val_pos_accuracy: 0.4425\n",
      "Epoch 401/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3373 - pos_accuracy: 0.5671 - val_loss: 0.5668 - val_pos_accuracy: 0.4450\n",
      "Epoch 402/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3371 - pos_accuracy: 0.5593 - val_loss: 0.5651 - val_pos_accuracy: 0.4425\n",
      "Epoch 403/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3358 - pos_accuracy: 0.5654 - val_loss: 0.5648 - val_pos_accuracy: 0.4450\n",
      "Epoch 404/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3348 - pos_accuracy: 0.5603 - val_loss: 0.5651 - val_pos_accuracy: 0.4425\n",
      "Epoch 405/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3344 - pos_accuracy: 0.5611 - val_loss: 0.5632 - val_pos_accuracy: 0.4450\n",
      "Epoch 406/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3335 - pos_accuracy: 0.5578 - val_loss: 0.5631 - val_pos_accuracy: 0.4525\n",
      "Epoch 407/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3331 - pos_accuracy: 0.5650 - val_loss: 0.5628 - val_pos_accuracy: 0.4500\n",
      "Epoch 408/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3325 - pos_accuracy: 0.5608 - val_loss: 0.5616 - val_pos_accuracy: 0.4400\n",
      "Epoch 409/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3321 - pos_accuracy: 0.5675 - val_loss: 0.5607 - val_pos_accuracy: 0.4450\n",
      "Epoch 410/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3310 - pos_accuracy: 0.5691 - val_loss: 0.5606 - val_pos_accuracy: 0.4475\n",
      "Epoch 411/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3307 - pos_accuracy: 0.5665 - val_loss: 0.5591 - val_pos_accuracy: 0.4450\n",
      "Epoch 412/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3296 - pos_accuracy: 0.5649 - val_loss: 0.5586 - val_pos_accuracy: 0.4425\n",
      "Epoch 413/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3286 - pos_accuracy: 0.5629 - val_loss: 0.5589 - val_pos_accuracy: 0.4500\n",
      "Epoch 414/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3286 - pos_accuracy: 0.5661 - val_loss: 0.5589 - val_pos_accuracy: 0.4350\n",
      "Epoch 415/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3274 - pos_accuracy: 0.5674 - val_loss: 0.5579 - val_pos_accuracy: 0.4550\n",
      "Epoch 416/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3265 - pos_accuracy: 0.5625 - val_loss: 0.5550 - val_pos_accuracy: 0.4400\n",
      "Epoch 417/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3272 - pos_accuracy: 0.5693 - val_loss: 0.5544 - val_pos_accuracy: 0.4475\n",
      "Epoch 418/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3263 - pos_accuracy: 0.5678 - val_loss: 0.5531 - val_pos_accuracy: 0.4400\n",
      "Epoch 419/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3252 - pos_accuracy: 0.5745 - val_loss: 0.5548 - val_pos_accuracy: 0.4525\n",
      "Epoch 420/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3239 - pos_accuracy: 0.5760 - val_loss: 0.5528 - val_pos_accuracy: 0.4600\n",
      "Epoch 421/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3231 - pos_accuracy: 0.5780 - val_loss: 0.5530 - val_pos_accuracy: 0.4525\n",
      "Epoch 422/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3224 - pos_accuracy: 0.5713 - val_loss: 0.5513 - val_pos_accuracy: 0.4575\n",
      "Epoch 423/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3217 - pos_accuracy: 0.5749 - val_loss: 0.5521 - val_pos_accuracy: 0.4525\n",
      "Epoch 424/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3210 - pos_accuracy: 0.5851 - val_loss: 0.5496 - val_pos_accuracy: 0.4550\n",
      "Epoch 425/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3203 - pos_accuracy: 0.5824 - val_loss: 0.5497 - val_pos_accuracy: 0.4600\n",
      "Epoch 426/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3196 - pos_accuracy: 0.5777 - val_loss: 0.5492 - val_pos_accuracy: 0.4625\n",
      "Epoch 427/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3190 - pos_accuracy: 0.5798 - val_loss: 0.5485 - val_pos_accuracy: 0.4600\n",
      "Epoch 428/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3186 - pos_accuracy: 0.5827 - val_loss: 0.5481 - val_pos_accuracy: 0.4575\n",
      "Epoch 429/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3183 - pos_accuracy: 0.5862 - val_loss: 0.5481 - val_pos_accuracy: 0.4575\n",
      "Epoch 430/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3176 - pos_accuracy: 0.5820 - val_loss: 0.5477 - val_pos_accuracy: 0.4550\n",
      "Epoch 431/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3170 - pos_accuracy: 0.5826 - val_loss: 0.5482 - val_pos_accuracy: 0.4575\n",
      "Epoch 432/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3160 - pos_accuracy: 0.5793 - val_loss: 0.5465 - val_pos_accuracy: 0.4600\n",
      "Epoch 433/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3149 - pos_accuracy: 0.5854 - val_loss: 0.5456 - val_pos_accuracy: 0.4625\n",
      "Epoch 434/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3144 - pos_accuracy: 0.5822 - val_loss: 0.5439 - val_pos_accuracy: 0.4650\n",
      "Epoch 435/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3139 - pos_accuracy: 0.5896 - val_loss: 0.5441 - val_pos_accuracy: 0.4650\n",
      "Epoch 436/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3131 - pos_accuracy: 0.5882 - val_loss: 0.5429 - val_pos_accuracy: 0.4625\n",
      "Epoch 437/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3126 - pos_accuracy: 0.5939 - val_loss: 0.5418 - val_pos_accuracy: 0.4600\n",
      "Epoch 438/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3122 - pos_accuracy: 0.5825 - val_loss: 0.5423 - val_pos_accuracy: 0.4625\n",
      "Epoch 439/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3115 - pos_accuracy: 0.5849 - val_loss: 0.5422 - val_pos_accuracy: 0.4500\n",
      "Epoch 440/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3111 - pos_accuracy: 0.5877 - val_loss: 0.5405 - val_pos_accuracy: 0.4650\n",
      "Epoch 441/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3100 - pos_accuracy: 0.5888 - val_loss: 0.5393 - val_pos_accuracy: 0.4650\n",
      "Epoch 442/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.3093 - pos_accuracy: 0.5938 - val_loss: 0.5404 - val_pos_accuracy: 0.4600\n",
      "Epoch 443/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3092 - pos_accuracy: 0.5830 - val_loss: 0.5390 - val_pos_accuracy: 0.4525\n",
      "Epoch 444/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3083 - pos_accuracy: 0.5873 - val_loss: 0.5376 - val_pos_accuracy: 0.4675\n",
      "Epoch 445/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3076 - pos_accuracy: 0.5891 - val_loss: 0.5372 - val_pos_accuracy: 0.4650\n",
      "Epoch 446/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3068 - pos_accuracy: 0.5872 - val_loss: 0.5368 - val_pos_accuracy: 0.4650\n",
      "Epoch 447/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3072 - pos_accuracy: 0.5826 - val_loss: 0.5354 - val_pos_accuracy: 0.4650\n",
      "Epoch 448/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3056 - pos_accuracy: 0.5908 - val_loss: 0.5356 - val_pos_accuracy: 0.4700\n",
      "Epoch 449/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3048 - pos_accuracy: 0.5844 - val_loss: 0.5345 - val_pos_accuracy: 0.4700\n",
      "Epoch 450/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3046 - pos_accuracy: 0.5989 - val_loss: 0.5336 - val_pos_accuracy: 0.4625\n",
      "Epoch 451/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3039 - pos_accuracy: 0.5834 - val_loss: 0.5335 - val_pos_accuracy: 0.4550\n",
      "Epoch 452/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3033 - pos_accuracy: 0.5893 - val_loss: 0.5343 - val_pos_accuracy: 0.4600\n",
      "Epoch 453/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3030 - pos_accuracy: 0.5966 - val_loss: 0.5325 - val_pos_accuracy: 0.4600\n",
      "Epoch 454/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3020 - pos_accuracy: 0.5908 - val_loss: 0.5327 - val_pos_accuracy: 0.4700\n",
      "Epoch 455/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3015 - pos_accuracy: 0.5959 - val_loss: 0.5323 - val_pos_accuracy: 0.4625\n",
      "Epoch 456/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3011 - pos_accuracy: 0.5968 - val_loss: 0.5306 - val_pos_accuracy: 0.4650\n",
      "Epoch 457/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3006 - pos_accuracy: 0.6043 - val_loss: 0.5301 - val_pos_accuracy: 0.4725\n",
      "Epoch 458/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2995 - pos_accuracy: 0.6016 - val_loss: 0.5304 - val_pos_accuracy: 0.4675\n",
      "Epoch 459/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2991 - pos_accuracy: 0.6004 - val_loss: 0.5292 - val_pos_accuracy: 0.4525\n",
      "Epoch 460/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2987 - pos_accuracy: 0.5806 - val_loss: 0.5291 - val_pos_accuracy: 0.4725\n",
      "Epoch 461/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2977 - pos_accuracy: 0.6054 - val_loss: 0.5287 - val_pos_accuracy: 0.4700\n",
      "Epoch 462/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2975 - pos_accuracy: 0.6087 - val_loss: 0.5276 - val_pos_accuracy: 0.4675\n",
      "Epoch 463/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2975 - pos_accuracy: 0.6002 - val_loss: 0.5262 - val_pos_accuracy: 0.4750\n",
      "Epoch 464/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2961 - pos_accuracy: 0.6015 - val_loss: 0.5267 - val_pos_accuracy: 0.4675\n",
      "Epoch 465/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2955 - pos_accuracy: 0.6056 - val_loss: 0.5251 - val_pos_accuracy: 0.4725\n",
      "Epoch 466/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2951 - pos_accuracy: 0.6023 - val_loss: 0.5246 - val_pos_accuracy: 0.4625\n",
      "Epoch 467/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2953 - pos_accuracy: 0.5960 - val_loss: 0.5244 - val_pos_accuracy: 0.4625\n",
      "Epoch 468/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2941 - pos_accuracy: 0.6086 - val_loss: 0.5243 - val_pos_accuracy: 0.4675\n",
      "Epoch 469/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2939 - pos_accuracy: 0.6043 - val_loss: 0.5242 - val_pos_accuracy: 0.4750\n",
      "Epoch 470/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2930 - pos_accuracy: 0.6111 - val_loss: 0.5237 - val_pos_accuracy: 0.4725\n",
      "Epoch 471/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2923 - pos_accuracy: 0.6119 - val_loss: 0.5240 - val_pos_accuracy: 0.4675\n",
      "Epoch 472/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2919 - pos_accuracy: 0.6137 - val_loss: 0.5232 - val_pos_accuracy: 0.4700\n",
      "Epoch 473/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2910 - pos_accuracy: 0.6142 - val_loss: 0.5215 - val_pos_accuracy: 0.4775\n",
      "Epoch 474/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2903 - pos_accuracy: 0.6121 - val_loss: 0.5214 - val_pos_accuracy: 0.4750\n",
      "Epoch 475/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2898 - pos_accuracy: 0.6106 - val_loss: 0.5212 - val_pos_accuracy: 0.4750\n",
      "Epoch 476/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2893 - pos_accuracy: 0.6143 - val_loss: 0.5201 - val_pos_accuracy: 0.4675\n",
      "Epoch 477/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2888 - pos_accuracy: 0.6088 - val_loss: 0.5193 - val_pos_accuracy: 0.4750\n",
      "Epoch 478/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2883 - pos_accuracy: 0.6150 - val_loss: 0.5184 - val_pos_accuracy: 0.4750\n",
      "Epoch 479/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2877 - pos_accuracy: 0.6107 - val_loss: 0.5185 - val_pos_accuracy: 0.4725\n",
      "Epoch 480/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2869 - pos_accuracy: 0.6119 - val_loss: 0.5179 - val_pos_accuracy: 0.4750\n",
      "Epoch 481/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2865 - pos_accuracy: 0.6120 - val_loss: 0.5169 - val_pos_accuracy: 0.4825\n",
      "Epoch 482/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2859 - pos_accuracy: 0.6170 - val_loss: 0.5158 - val_pos_accuracy: 0.4800\n",
      "Epoch 483/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2853 - pos_accuracy: 0.6181 - val_loss: 0.5159 - val_pos_accuracy: 0.4800\n",
      "Epoch 484/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2849 - pos_accuracy: 0.6136 - val_loss: 0.5148 - val_pos_accuracy: 0.4725\n",
      "Epoch 485/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2843 - pos_accuracy: 0.6152 - val_loss: 0.5146 - val_pos_accuracy: 0.4750\n",
      "Epoch 486/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2839 - pos_accuracy: 0.6121 - val_loss: 0.5139 - val_pos_accuracy: 0.4700\n",
      "Epoch 487/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2836 - pos_accuracy: 0.6139 - val_loss: 0.5131 - val_pos_accuracy: 0.4800\n",
      "Epoch 488/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2829 - pos_accuracy: 0.6191 - val_loss: 0.5129 - val_pos_accuracy: 0.4800\n",
      "Epoch 489/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2824 - pos_accuracy: 0.6055 - val_loss: 0.5128 - val_pos_accuracy: 0.4750\n",
      "Epoch 490/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2813 - pos_accuracy: 0.6138 - val_loss: 0.5123 - val_pos_accuracy: 0.4750\n",
      "Epoch 491/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2821 - pos_accuracy: 0.6177 - val_loss: 0.5132 - val_pos_accuracy: 0.4775\n",
      "Epoch 492/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2815 - pos_accuracy: 0.6134 - val_loss: 0.5127 - val_pos_accuracy: 0.4825\n",
      "Epoch 493/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2802 - pos_accuracy: 0.6134 - val_loss: 0.5113 - val_pos_accuracy: 0.4825\n",
      "Epoch 494/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2794 - pos_accuracy: 0.6162 - val_loss: 0.5111 - val_pos_accuracy: 0.4775\n",
      "Epoch 495/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2791 - pos_accuracy: 0.6141 - val_loss: 0.5105 - val_pos_accuracy: 0.4800\n",
      "Epoch 496/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2789 - pos_accuracy: 0.6138 - val_loss: 0.5116 - val_pos_accuracy: 0.4800\n",
      "Epoch 497/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2781 - pos_accuracy: 0.6161 - val_loss: 0.5098 - val_pos_accuracy: 0.4850\n",
      "Epoch 498/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2772 - pos_accuracy: 0.6141 - val_loss: 0.5102 - val_pos_accuracy: 0.4825\n",
      "Epoch 499/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2774 - pos_accuracy: 0.6171 - val_loss: 0.5099 - val_pos_accuracy: 0.4850\n",
      "Epoch 500/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2766 - pos_accuracy: 0.6210 - val_loss: 0.5098 - val_pos_accuracy: 0.4825\n",
      "Epoch 501/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2759 - pos_accuracy: 0.6144 - val_loss: 0.5083 - val_pos_accuracy: 0.4800\n",
      "Epoch 502/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2759 - pos_accuracy: 0.6237 - val_loss: 0.5080 - val_pos_accuracy: 0.4825\n",
      "Epoch 503/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2749 - pos_accuracy: 0.6153 - val_loss: 0.5066 - val_pos_accuracy: 0.4825\n",
      "Epoch 504/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2743 - pos_accuracy: 0.6169 - val_loss: 0.5065 - val_pos_accuracy: 0.4825\n",
      "Epoch 505/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2738 - pos_accuracy: 0.6128 - val_loss: 0.5064 - val_pos_accuracy: 0.4875\n",
      "Epoch 506/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2734 - pos_accuracy: 0.6201 - val_loss: 0.5060 - val_pos_accuracy: 0.4900\n",
      "Epoch 507/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2733 - pos_accuracy: 0.6233 - val_loss: 0.5055 - val_pos_accuracy: 0.4875\n",
      "Epoch 508/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2724 - pos_accuracy: 0.6134 - val_loss: 0.5041 - val_pos_accuracy: 0.4800\n",
      "Epoch 509/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2724 - pos_accuracy: 0.6229 - val_loss: 0.5032 - val_pos_accuracy: 0.4825\n",
      "Epoch 510/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2714 - pos_accuracy: 0.6188 - val_loss: 0.5028 - val_pos_accuracy: 0.4875\n",
      "Epoch 511/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2707 - pos_accuracy: 0.6179 - val_loss: 0.5024 - val_pos_accuracy: 0.4850\n",
      "Epoch 512/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2704 - pos_accuracy: 0.6156 - val_loss: 0.5022 - val_pos_accuracy: 0.4900\n",
      "Epoch 513/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2698 - pos_accuracy: 0.6281 - val_loss: 0.5013 - val_pos_accuracy: 0.4800\n",
      "Epoch 514/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2694 - pos_accuracy: 0.6254 - val_loss: 0.5016 - val_pos_accuracy: 0.4900\n",
      "Epoch 515/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2691 - pos_accuracy: 0.6273 - val_loss: 0.5006 - val_pos_accuracy: 0.4900\n",
      "Epoch 516/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2686 - pos_accuracy: 0.6255 - val_loss: 0.5000 - val_pos_accuracy: 0.4900\n",
      "Epoch 517/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2680 - pos_accuracy: 0.6298 - val_loss: 0.4996 - val_pos_accuracy: 0.4875\n",
      "Epoch 518/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2672 - pos_accuracy: 0.6317 - val_loss: 0.4987 - val_pos_accuracy: 0.4900\n",
      "Epoch 519/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2676 - pos_accuracy: 0.6265 - val_loss: 0.4985 - val_pos_accuracy: 0.4900\n",
      "Epoch 520/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2666 - pos_accuracy: 0.6311 - val_loss: 0.4981 - val_pos_accuracy: 0.4925\n",
      "Epoch 521/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2662 - pos_accuracy: 0.6291 - val_loss: 0.4972 - val_pos_accuracy: 0.4875\n",
      "Epoch 522/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2659 - pos_accuracy: 0.6264 - val_loss: 0.4969 - val_pos_accuracy: 0.4900\n",
      "Epoch 523/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2658 - pos_accuracy: 0.6318 - val_loss: 0.4986 - val_pos_accuracy: 0.4925\n",
      "Epoch 524/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2648 - pos_accuracy: 0.6299 - val_loss: 0.4963 - val_pos_accuracy: 0.4850\n",
      "Epoch 525/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2643 - pos_accuracy: 0.6264 - val_loss: 0.4966 - val_pos_accuracy: 0.4950\n",
      "Epoch 526/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2642 - pos_accuracy: 0.6268 - val_loss: 0.4957 - val_pos_accuracy: 0.4925\n",
      "Epoch 527/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2636 - pos_accuracy: 0.6247 - val_loss: 0.4947 - val_pos_accuracy: 0.4925\n",
      "Epoch 528/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2629 - pos_accuracy: 0.6247 - val_loss: 0.4937 - val_pos_accuracy: 0.4925\n",
      "Epoch 529/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2622 - pos_accuracy: 0.6324 - val_loss: 0.4933 - val_pos_accuracy: 0.4925\n",
      "Epoch 530/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2619 - pos_accuracy: 0.6355 - val_loss: 0.4933 - val_pos_accuracy: 0.4925\n",
      "Epoch 531/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2613 - pos_accuracy: 0.6337 - val_loss: 0.4927 - val_pos_accuracy: 0.4925\n",
      "Epoch 532/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2608 - pos_accuracy: 0.6331 - val_loss: 0.4924 - val_pos_accuracy: 0.4900\n",
      "Epoch 533/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2604 - pos_accuracy: 0.6309 - val_loss: 0.4914 - val_pos_accuracy: 0.4925\n",
      "Epoch 534/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2602 - pos_accuracy: 0.6251 - val_loss: 0.4913 - val_pos_accuracy: 0.4950\n",
      "Epoch 535/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2598 - pos_accuracy: 0.6367 - val_loss: 0.4909 - val_pos_accuracy: 0.4925\n",
      "Epoch 536/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2601 - pos_accuracy: 0.6270 - val_loss: 0.4909 - val_pos_accuracy: 0.4925\n",
      "Epoch 537/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2585 - pos_accuracy: 0.6391 - val_loss: 0.4890 - val_pos_accuracy: 0.4950\n",
      "Epoch 538/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2579 - pos_accuracy: 0.6324 - val_loss: 0.4887 - val_pos_accuracy: 0.4975\n",
      "Epoch 539/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2580 - pos_accuracy: 0.6312 - val_loss: 0.4885 - val_pos_accuracy: 0.5000\n",
      "Epoch 540/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2574 - pos_accuracy: 0.6370 - val_loss: 0.4879 - val_pos_accuracy: 0.4975\n",
      "Epoch 541/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2568 - pos_accuracy: 0.6386 - val_loss: 0.4871 - val_pos_accuracy: 0.4950\n",
      "Epoch 542/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2563 - pos_accuracy: 0.6352 - val_loss: 0.4865 - val_pos_accuracy: 0.5050\n",
      "Epoch 543/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2557 - pos_accuracy: 0.6423 - val_loss: 0.4861 - val_pos_accuracy: 0.4975\n",
      "Epoch 544/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2555 - pos_accuracy: 0.6401 - val_loss: 0.4850 - val_pos_accuracy: 0.5025\n",
      "Epoch 545/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2549 - pos_accuracy: 0.6388 - val_loss: 0.4849 - val_pos_accuracy: 0.4975\n",
      "Epoch 546/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2544 - pos_accuracy: 0.6421 - val_loss: 0.4846 - val_pos_accuracy: 0.5000\n",
      "Epoch 547/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2543 - pos_accuracy: 0.6450 - val_loss: 0.4844 - val_pos_accuracy: 0.4950\n",
      "Epoch 548/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2539 - pos_accuracy: 0.6421 - val_loss: 0.4839 - val_pos_accuracy: 0.4975\n",
      "Epoch 549/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2533 - pos_accuracy: 0.6424 - val_loss: 0.4826 - val_pos_accuracy: 0.5000\n",
      "Epoch 550/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2526 - pos_accuracy: 0.6483 - val_loss: 0.4826 - val_pos_accuracy: 0.5025\n",
      "Epoch 551/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2526 - pos_accuracy: 0.6389 - val_loss: 0.4832 - val_pos_accuracy: 0.4950\n",
      "Epoch 552/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2525 - pos_accuracy: 0.6281 - val_loss: 0.4825 - val_pos_accuracy: 0.4975\n",
      "Epoch 553/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2515 - pos_accuracy: 0.6398 - val_loss: 0.4814 - val_pos_accuracy: 0.5000\n",
      "Epoch 554/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2510 - pos_accuracy: 0.6412 - val_loss: 0.4814 - val_pos_accuracy: 0.5000\n",
      "Epoch 555/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2508 - pos_accuracy: 0.6493 - val_loss: 0.4807 - val_pos_accuracy: 0.5000\n",
      "Epoch 556/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2502 - pos_accuracy: 0.6509 - val_loss: 0.4799 - val_pos_accuracy: 0.5025\n",
      "Epoch 557/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2497 - pos_accuracy: 0.6421 - val_loss: 0.4797 - val_pos_accuracy: 0.5025\n",
      "Epoch 558/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2493 - pos_accuracy: 0.6424 - val_loss: 0.4800 - val_pos_accuracy: 0.5000\n",
      "Epoch 559/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2487 - pos_accuracy: 0.6388 - val_loss: 0.4788 - val_pos_accuracy: 0.5025\n",
      "Epoch 560/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2483 - pos_accuracy: 0.6433 - val_loss: 0.4787 - val_pos_accuracy: 0.5025\n",
      "Epoch 561/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2483 - pos_accuracy: 0.6370 - val_loss: 0.4781 - val_pos_accuracy: 0.5025\n",
      "Epoch 562/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2474 - pos_accuracy: 0.6387 - val_loss: 0.4784 - val_pos_accuracy: 0.5025\n",
      "Epoch 563/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2474 - pos_accuracy: 0.6383 - val_loss: 0.4771 - val_pos_accuracy: 0.4975\n",
      "Epoch 564/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2469 - pos_accuracy: 0.6445 - val_loss: 0.4773 - val_pos_accuracy: 0.4975\n",
      "Epoch 565/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2467 - pos_accuracy: 0.6477 - val_loss: 0.4766 - val_pos_accuracy: 0.5000\n",
      "Epoch 566/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2459 - pos_accuracy: 0.6412 - val_loss: 0.4771 - val_pos_accuracy: 0.5025\n",
      "Epoch 567/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2458 - pos_accuracy: 0.6488 - val_loss: 0.4755 - val_pos_accuracy: 0.5050\n",
      "Epoch 568/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2453 - pos_accuracy: 0.6504 - val_loss: 0.4764 - val_pos_accuracy: 0.5025\n",
      "Epoch 569/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2453 - pos_accuracy: 0.6376 - val_loss: 0.4764 - val_pos_accuracy: 0.5050\n",
      "Epoch 570/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2446 - pos_accuracy: 0.6411 - val_loss: 0.4762 - val_pos_accuracy: 0.5050\n",
      "Epoch 571/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2438 - pos_accuracy: 0.6415 - val_loss: 0.4749 - val_pos_accuracy: 0.5025\n",
      "Epoch 572/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2437 - pos_accuracy: 0.6400 - val_loss: 0.4740 - val_pos_accuracy: 0.4975\n",
      "Epoch 573/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2433 - pos_accuracy: 0.6504 - val_loss: 0.4738 - val_pos_accuracy: 0.5025\n",
      "Epoch 574/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2425 - pos_accuracy: 0.6528 - val_loss: 0.4728 - val_pos_accuracy: 0.5025\n",
      "Epoch 575/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2422 - pos_accuracy: 0.6416 - val_loss: 0.4721 - val_pos_accuracy: 0.4950\n",
      "Epoch 576/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2418 - pos_accuracy: 0.6457 - val_loss: 0.4727 - val_pos_accuracy: 0.5025\n",
      "Epoch 577/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2417 - pos_accuracy: 0.6481 - val_loss: 0.4719 - val_pos_accuracy: 0.5075\n",
      "Epoch 578/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2410 - pos_accuracy: 0.6479 - val_loss: 0.4708 - val_pos_accuracy: 0.5025\n",
      "Epoch 579/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2409 - pos_accuracy: 0.6457 - val_loss: 0.4711 - val_pos_accuracy: 0.5075\n",
      "Epoch 580/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2402 - pos_accuracy: 0.6416 - val_loss: 0.4708 - val_pos_accuracy: 0.5050\n",
      "Epoch 581/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2398 - pos_accuracy: 0.6566 - val_loss: 0.4702 - val_pos_accuracy: 0.5050\n",
      "Epoch 582/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2397 - pos_accuracy: 0.6617 - val_loss: 0.4704 - val_pos_accuracy: 0.5050\n",
      "Epoch 583/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2391 - pos_accuracy: 0.6438 - val_loss: 0.4698 - val_pos_accuracy: 0.5050\n",
      "Epoch 584/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2386 - pos_accuracy: 0.6522 - val_loss: 0.4688 - val_pos_accuracy: 0.5050\n",
      "Epoch 585/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2385 - pos_accuracy: 0.6480 - val_loss: 0.4685 - val_pos_accuracy: 0.5050\n",
      "Epoch 586/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2378 - pos_accuracy: 0.6465 - val_loss: 0.4684 - val_pos_accuracy: 0.5025\n",
      "Epoch 587/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2375 - pos_accuracy: 0.6502 - val_loss: 0.4685 - val_pos_accuracy: 0.4975\n",
      "Epoch 588/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2374 - pos_accuracy: 0.6541 - val_loss: 0.4681 - val_pos_accuracy: 0.4975\n",
      "Epoch 589/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2370 - pos_accuracy: 0.6480 - val_loss: 0.4673 - val_pos_accuracy: 0.5075\n",
      "Epoch 590/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2366 - pos_accuracy: 0.6592 - val_loss: 0.4672 - val_pos_accuracy: 0.5100\n",
      "Epoch 591/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2358 - pos_accuracy: 0.6481 - val_loss: 0.4664 - val_pos_accuracy: 0.5075\n",
      "Epoch 592/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2356 - pos_accuracy: 0.6567 - val_loss: 0.4661 - val_pos_accuracy: 0.5050\n",
      "Epoch 593/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2350 - pos_accuracy: 0.6544 - val_loss: 0.4663 - val_pos_accuracy: 0.5025\n",
      "Epoch 594/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2347 - pos_accuracy: 0.6524 - val_loss: 0.4657 - val_pos_accuracy: 0.5075\n",
      "Epoch 595/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2347 - pos_accuracy: 0.6481 - val_loss: 0.4653 - val_pos_accuracy: 0.5075\n",
      "Epoch 596/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2347 - pos_accuracy: 0.6527 - val_loss: 0.4647 - val_pos_accuracy: 0.5050\n",
      "Epoch 597/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2338 - pos_accuracy: 0.6457 - val_loss: 0.4646 - val_pos_accuracy: 0.5025\n",
      "Epoch 598/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2335 - pos_accuracy: 0.6622 - val_loss: 0.4645 - val_pos_accuracy: 0.5025\n",
      "Epoch 599/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2331 - pos_accuracy: 0.6547 - val_loss: 0.4640 - val_pos_accuracy: 0.5075\n",
      "Epoch 600/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2327 - pos_accuracy: 0.6513 - val_loss: 0.4633 - val_pos_accuracy: 0.5050\n",
      "Epoch 601/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2321 - pos_accuracy: 0.6510 - val_loss: 0.4639 - val_pos_accuracy: 0.5025\n",
      "Epoch 602/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2317 - pos_accuracy: 0.6501 - val_loss: 0.4636 - val_pos_accuracy: 0.5025\n",
      "Epoch 603/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2314 - pos_accuracy: 0.6490 - val_loss: 0.4622 - val_pos_accuracy: 0.5025\n",
      "Epoch 604/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2312 - pos_accuracy: 0.6560 - val_loss: 0.4622 - val_pos_accuracy: 0.5025\n",
      "Epoch 605/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.2311 - pos_accuracy: 0.6579 - val_loss: 0.4617 - val_pos_accuracy: 0.5125\n",
      "Epoch 606/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2302 - pos_accuracy: 0.6565 - val_loss: 0.4609 - val_pos_accuracy: 0.5075\n",
      "Epoch 607/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2298 - pos_accuracy: 0.6591 - val_loss: 0.4607 - val_pos_accuracy: 0.4975\n",
      "Epoch 608/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2298 - pos_accuracy: 0.6571 - val_loss: 0.4601 - val_pos_accuracy: 0.5050\n",
      "Epoch 609/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2292 - pos_accuracy: 0.6562 - val_loss: 0.4596 - val_pos_accuracy: 0.4950\n",
      "Epoch 610/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2288 - pos_accuracy: 0.6609 - val_loss: 0.4600 - val_pos_accuracy: 0.5100\n",
      "Epoch 611/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2284 - pos_accuracy: 0.6593 - val_loss: 0.4591 - val_pos_accuracy: 0.4975\n",
      "Epoch 612/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2283 - pos_accuracy: 0.6630 - val_loss: 0.4594 - val_pos_accuracy: 0.4975\n",
      "Epoch 613/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2281 - pos_accuracy: 0.6566 - val_loss: 0.4591 - val_pos_accuracy: 0.5000\n",
      "Epoch 614/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2277 - pos_accuracy: 0.6613 - val_loss: 0.4583 - val_pos_accuracy: 0.5075\n",
      "Epoch 615/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2270 - pos_accuracy: 0.6581 - val_loss: 0.4576 - val_pos_accuracy: 0.5025\n",
      "Epoch 616/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2267 - pos_accuracy: 0.6606 - val_loss: 0.4578 - val_pos_accuracy: 0.5100\n",
      "Epoch 617/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2263 - pos_accuracy: 0.6578 - val_loss: 0.4578 - val_pos_accuracy: 0.5050\n",
      "Epoch 618/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2258 - pos_accuracy: 0.6562 - val_loss: 0.4579 - val_pos_accuracy: 0.5050\n",
      "Epoch 619/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2261 - pos_accuracy: 0.6603 - val_loss: 0.4575 - val_pos_accuracy: 0.5050\n",
      "Epoch 620/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2256 - pos_accuracy: 0.6644 - val_loss: 0.4566 - val_pos_accuracy: 0.5050\n",
      "Epoch 621/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2252 - pos_accuracy: 0.6657 - val_loss: 0.4566 - val_pos_accuracy: 0.5100\n",
      "Epoch 622/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2256 - pos_accuracy: 0.6565 - val_loss: 0.4561 - val_pos_accuracy: 0.5075\n",
      "Epoch 623/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2246 - pos_accuracy: 0.6630 - val_loss: 0.4556 - val_pos_accuracy: 0.5075\n",
      "Epoch 624/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2240 - pos_accuracy: 0.6516 - val_loss: 0.4549 - val_pos_accuracy: 0.5050\n",
      "Epoch 625/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2238 - pos_accuracy: 0.6609 - val_loss: 0.4546 - val_pos_accuracy: 0.5050\n",
      "Epoch 626/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2237 - pos_accuracy: 0.6637 - val_loss: 0.4544 - val_pos_accuracy: 0.5025\n",
      "Epoch 627/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2231 - pos_accuracy: 0.6564 - val_loss: 0.4545 - val_pos_accuracy: 0.5050\n",
      "Epoch 628/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2224 - pos_accuracy: 0.6605 - val_loss: 0.4532 - val_pos_accuracy: 0.5125\n",
      "Epoch 629/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2222 - pos_accuracy: 0.6636 - val_loss: 0.4528 - val_pos_accuracy: 0.5100\n",
      "Epoch 630/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2216 - pos_accuracy: 0.6592 - val_loss: 0.4526 - val_pos_accuracy: 0.5125\n",
      "Epoch 631/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2214 - pos_accuracy: 0.6566 - val_loss: 0.4523 - val_pos_accuracy: 0.5100\n",
      "Epoch 632/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2212 - pos_accuracy: 0.6659 - val_loss: 0.4520 - val_pos_accuracy: 0.5125\n",
      "Epoch 633/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2208 - pos_accuracy: 0.6659 - val_loss: 0.4514 - val_pos_accuracy: 0.5125\n",
      "Epoch 634/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2203 - pos_accuracy: 0.6719 - val_loss: 0.4515 - val_pos_accuracy: 0.5125\n",
      "Epoch 635/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2200 - pos_accuracy: 0.6681 - val_loss: 0.4510 - val_pos_accuracy: 0.5125\n",
      "Epoch 636/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2198 - pos_accuracy: 0.6677 - val_loss: 0.4503 - val_pos_accuracy: 0.5075\n",
      "Epoch 637/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2192 - pos_accuracy: 0.6649 - val_loss: 0.4501 - val_pos_accuracy: 0.5150\n",
      "Epoch 638/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2191 - pos_accuracy: 0.6719 - val_loss: 0.4499 - val_pos_accuracy: 0.5175\n",
      "Epoch 639/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2187 - pos_accuracy: 0.6637 - val_loss: 0.4493 - val_pos_accuracy: 0.5150\n",
      "Epoch 640/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2190 - pos_accuracy: 0.6725 - val_loss: 0.4501 - val_pos_accuracy: 0.5000\n",
      "Epoch 641/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2184 - pos_accuracy: 0.6621 - val_loss: 0.4495 - val_pos_accuracy: 0.5150\n",
      "Epoch 642/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2178 - pos_accuracy: 0.6771 - val_loss: 0.4485 - val_pos_accuracy: 0.5175\n",
      "Epoch 643/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2175 - pos_accuracy: 0.6799 - val_loss: 0.4492 - val_pos_accuracy: 0.5050\n",
      "Epoch 644/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2182 - pos_accuracy: 0.6680 - val_loss: 0.4483 - val_pos_accuracy: 0.5025\n",
      "Epoch 645/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2167 - pos_accuracy: 0.6759 - val_loss: 0.4480 - val_pos_accuracy: 0.5125\n",
      "Epoch 646/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2164 - pos_accuracy: 0.6734 - val_loss: 0.4474 - val_pos_accuracy: 0.5125\n",
      "Epoch 647/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2161 - pos_accuracy: 0.6740 - val_loss: 0.4472 - val_pos_accuracy: 0.5175\n",
      "Epoch 648/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2161 - pos_accuracy: 0.6689 - val_loss: 0.4476 - val_pos_accuracy: 0.5100\n",
      "Epoch 649/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2154 - pos_accuracy: 0.6757 - val_loss: 0.4464 - val_pos_accuracy: 0.5125\n",
      "Epoch 650/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2153 - pos_accuracy: 0.6751 - val_loss: 0.4461 - val_pos_accuracy: 0.5150\n",
      "Epoch 651/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2147 - pos_accuracy: 0.6704 - val_loss: 0.4462 - val_pos_accuracy: 0.5075\n",
      "Epoch 652/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2145 - pos_accuracy: 0.6675 - val_loss: 0.4457 - val_pos_accuracy: 0.5150\n",
      "Epoch 653/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2143 - pos_accuracy: 0.6766 - val_loss: 0.4452 - val_pos_accuracy: 0.5050\n",
      "Epoch 654/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2141 - pos_accuracy: 0.6784 - val_loss: 0.4445 - val_pos_accuracy: 0.5150\n",
      "Epoch 655/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2147 - pos_accuracy: 0.6737 - val_loss: 0.4442 - val_pos_accuracy: 0.5050\n",
      "Epoch 656/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2138 - pos_accuracy: 0.6634 - val_loss: 0.4437 - val_pos_accuracy: 0.5150\n",
      "Epoch 657/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2131 - pos_accuracy: 0.6719 - val_loss: 0.4438 - val_pos_accuracy: 0.5150\n",
      "Epoch 658/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2124 - pos_accuracy: 0.6709 - val_loss: 0.4435 - val_pos_accuracy: 0.5175\n",
      "Epoch 659/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2125 - pos_accuracy: 0.6684 - val_loss: 0.4433 - val_pos_accuracy: 0.5100\n",
      "Epoch 660/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2121 - pos_accuracy: 0.6708 - val_loss: 0.4423 - val_pos_accuracy: 0.5150\n",
      "Epoch 661/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2116 - pos_accuracy: 0.6738 - val_loss: 0.4427 - val_pos_accuracy: 0.5125\n",
      "Epoch 662/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2114 - pos_accuracy: 0.6716 - val_loss: 0.4420 - val_pos_accuracy: 0.5175\n",
      "Epoch 663/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2109 - pos_accuracy: 0.6733 - val_loss: 0.4419 - val_pos_accuracy: 0.5150\n",
      "Epoch 664/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2108 - pos_accuracy: 0.6781 - val_loss: 0.4413 - val_pos_accuracy: 0.5150\n",
      "Epoch 665/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2108 - pos_accuracy: 0.6722 - val_loss: 0.4419 - val_pos_accuracy: 0.5100\n",
      "Epoch 666/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2104 - pos_accuracy: 0.6692 - val_loss: 0.4410 - val_pos_accuracy: 0.5175\n",
      "Epoch 667/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2098 - pos_accuracy: 0.6758 - val_loss: 0.4408 - val_pos_accuracy: 0.5150\n",
      "Epoch 668/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2096 - pos_accuracy: 0.6780 - val_loss: 0.4415 - val_pos_accuracy: 0.5175\n",
      "Epoch 669/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2096 - pos_accuracy: 0.6787 - val_loss: 0.4404 - val_pos_accuracy: 0.5150\n",
      "Epoch 670/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2091 - pos_accuracy: 0.6776 - val_loss: 0.4399 - val_pos_accuracy: 0.5175\n",
      "Epoch 671/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2084 - pos_accuracy: 0.6771 - val_loss: 0.4400 - val_pos_accuracy: 0.5175\n",
      "Epoch 672/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2081 - pos_accuracy: 0.6829 - val_loss: 0.4390 - val_pos_accuracy: 0.5175\n",
      "Epoch 673/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2080 - pos_accuracy: 0.6751 - val_loss: 0.4398 - val_pos_accuracy: 0.5175\n",
      "Epoch 674/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2077 - pos_accuracy: 0.6699 - val_loss: 0.4393 - val_pos_accuracy: 0.5100\n",
      "Epoch 675/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2076 - pos_accuracy: 0.6734 - val_loss: 0.4391 - val_pos_accuracy: 0.5175\n",
      "Epoch 676/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2072 - pos_accuracy: 0.6837 - val_loss: 0.4388 - val_pos_accuracy: 0.5125\n",
      "Epoch 677/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2069 - pos_accuracy: 0.6790 - val_loss: 0.4387 - val_pos_accuracy: 0.5150\n",
      "Epoch 678/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2064 - pos_accuracy: 0.6828 - val_loss: 0.4387 - val_pos_accuracy: 0.5175\n",
      "Epoch 679/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2061 - pos_accuracy: 0.6810 - val_loss: 0.4376 - val_pos_accuracy: 0.5150\n",
      "Epoch 680/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2060 - pos_accuracy: 0.6885 - val_loss: 0.4379 - val_pos_accuracy: 0.5200\n",
      "Epoch 681/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2055 - pos_accuracy: 0.6744 - val_loss: 0.4386 - val_pos_accuracy: 0.5175\n",
      "Epoch 682/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2055 - pos_accuracy: 0.6794 - val_loss: 0.4381 - val_pos_accuracy: 0.5175\n",
      "Epoch 683/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2050 - pos_accuracy: 0.6812 - val_loss: 0.4384 - val_pos_accuracy: 0.5125\n",
      "Epoch 684/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2048 - pos_accuracy: 0.6912 - val_loss: 0.4366 - val_pos_accuracy: 0.5150\n",
      "Epoch 685/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2043 - pos_accuracy: 0.6870 - val_loss: 0.4364 - val_pos_accuracy: 0.5175\n",
      "Epoch 686/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2039 - pos_accuracy: 0.6823 - val_loss: 0.4367 - val_pos_accuracy: 0.5150\n",
      "Epoch 687/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2041 - pos_accuracy: 0.6842 - val_loss: 0.4360 - val_pos_accuracy: 0.5200\n",
      "Epoch 688/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2034 - pos_accuracy: 0.6859 - val_loss: 0.4360 - val_pos_accuracy: 0.5225\n",
      "Epoch 689/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2035 - pos_accuracy: 0.6852 - val_loss: 0.4354 - val_pos_accuracy: 0.5225\n",
      "Epoch 690/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2030 - pos_accuracy: 0.6802 - val_loss: 0.4349 - val_pos_accuracy: 0.5150\n",
      "Epoch 691/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2024 - pos_accuracy: 0.6832 - val_loss: 0.4352 - val_pos_accuracy: 0.5125\n",
      "Epoch 692/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2023 - pos_accuracy: 0.6927 - val_loss: 0.4345 - val_pos_accuracy: 0.5100\n",
      "Epoch 693/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2018 - pos_accuracy: 0.6867 - val_loss: 0.4337 - val_pos_accuracy: 0.5175\n",
      "Epoch 694/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.2016 - pos_accuracy: 0.6790 - val_loss: 0.4338 - val_pos_accuracy: 0.5200\n",
      "Epoch 695/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2011 - pos_accuracy: 0.6846 - val_loss: 0.4333 - val_pos_accuracy: 0.5200\n",
      "Epoch 696/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2010 - pos_accuracy: 0.6865 - val_loss: 0.4329 - val_pos_accuracy: 0.5200\n",
      "Epoch 697/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2006 - pos_accuracy: 0.6967 - val_loss: 0.4330 - val_pos_accuracy: 0.5225\n",
      "Epoch 698/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2003 - pos_accuracy: 0.6893 - val_loss: 0.4324 - val_pos_accuracy: 0.5225\n",
      "Epoch 699/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.2002 - pos_accuracy: 0.6960 - val_loss: 0.4326 - val_pos_accuracy: 0.5175\n",
      "Epoch 700/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1999 - pos_accuracy: 0.6892 - val_loss: 0.4325 - val_pos_accuracy: 0.5225\n",
      "Epoch 701/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1997 - pos_accuracy: 0.6897 - val_loss: 0.4327 - val_pos_accuracy: 0.5225\n",
      "Epoch 702/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1999 - pos_accuracy: 0.6903 - val_loss: 0.4324 - val_pos_accuracy: 0.5175\n",
      "Epoch 703/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1991 - pos_accuracy: 0.6909 - val_loss: 0.4320 - val_pos_accuracy: 0.5250\n",
      "Epoch 704/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1988 - pos_accuracy: 0.6931 - val_loss: 0.4309 - val_pos_accuracy: 0.5225\n",
      "Epoch 705/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1986 - pos_accuracy: 0.6937 - val_loss: 0.4305 - val_pos_accuracy: 0.5225\n",
      "Epoch 706/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1982 - pos_accuracy: 0.6910 - val_loss: 0.4314 - val_pos_accuracy: 0.5175\n",
      "Epoch 707/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1982 - pos_accuracy: 0.6919 - val_loss: 0.4308 - val_pos_accuracy: 0.5225\n",
      "Epoch 708/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1977 - pos_accuracy: 0.6914 - val_loss: 0.4302 - val_pos_accuracy: 0.5200\n",
      "Epoch 709/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1975 - pos_accuracy: 0.6973 - val_loss: 0.4296 - val_pos_accuracy: 0.5175\n",
      "Epoch 710/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1969 - pos_accuracy: 0.6969 - val_loss: 0.4303 - val_pos_accuracy: 0.5175\n",
      "Epoch 711/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1966 - pos_accuracy: 0.6896 - val_loss: 0.4300 - val_pos_accuracy: 0.5175\n",
      "Epoch 712/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1963 - pos_accuracy: 0.6924 - val_loss: 0.4286 - val_pos_accuracy: 0.5200\n",
      "Epoch 713/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1962 - pos_accuracy: 0.6966 - val_loss: 0.4294 - val_pos_accuracy: 0.5200\n",
      "Epoch 714/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1963 - pos_accuracy: 0.6936 - val_loss: 0.4291 - val_pos_accuracy: 0.5250\n",
      "Epoch 715/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1956 - pos_accuracy: 0.6976 - val_loss: 0.4289 - val_pos_accuracy: 0.5200\n",
      "Epoch 716/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1957 - pos_accuracy: 0.6989 - val_loss: 0.4289 - val_pos_accuracy: 0.5225\n",
      "Epoch 717/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1952 - pos_accuracy: 0.6985 - val_loss: 0.4286 - val_pos_accuracy: 0.5150\n",
      "Epoch 718/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1949 - pos_accuracy: 0.6999 - val_loss: 0.4278 - val_pos_accuracy: 0.5200\n",
      "Epoch 719/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1946 - pos_accuracy: 0.6927 - val_loss: 0.4277 - val_pos_accuracy: 0.5200\n",
      "Epoch 720/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1940 - pos_accuracy: 0.6922 - val_loss: 0.4277 - val_pos_accuracy: 0.5300\n",
      "Epoch 721/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1940 - pos_accuracy: 0.6924 - val_loss: 0.4274 - val_pos_accuracy: 0.5225\n",
      "Epoch 722/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1935 - pos_accuracy: 0.7009 - val_loss: 0.4275 - val_pos_accuracy: 0.5200\n",
      "Epoch 723/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1937 - pos_accuracy: 0.6987 - val_loss: 0.4265 - val_pos_accuracy: 0.5225\n",
      "Epoch 724/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1939 - pos_accuracy: 0.7021 - val_loss: 0.4270 - val_pos_accuracy: 0.5175\n",
      "Epoch 725/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1930 - pos_accuracy: 0.7023 - val_loss: 0.4266 - val_pos_accuracy: 0.5125\n",
      "Epoch 726/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1928 - pos_accuracy: 0.6966 - val_loss: 0.4259 - val_pos_accuracy: 0.5250\n",
      "Epoch 727/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1931 - pos_accuracy: 0.6897 - val_loss: 0.4262 - val_pos_accuracy: 0.5175\n",
      "Epoch 728/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1925 - pos_accuracy: 0.6944 - val_loss: 0.4256 - val_pos_accuracy: 0.5200\n",
      "Epoch 729/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1917 - pos_accuracy: 0.6940 - val_loss: 0.4259 - val_pos_accuracy: 0.5200\n",
      "Epoch 730/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1915 - pos_accuracy: 0.7034 - val_loss: 0.4255 - val_pos_accuracy: 0.5250\n",
      "Epoch 731/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1914 - pos_accuracy: 0.6991 - val_loss: 0.4258 - val_pos_accuracy: 0.5225\n",
      "Epoch 732/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1913 - pos_accuracy: 0.7010 - val_loss: 0.4259 - val_pos_accuracy: 0.5250\n",
      "Epoch 733/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1909 - pos_accuracy: 0.7000 - val_loss: 0.4249 - val_pos_accuracy: 0.5250\n",
      "Epoch 734/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1902 - pos_accuracy: 0.7004 - val_loss: 0.4238 - val_pos_accuracy: 0.5150\n",
      "Epoch 735/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1904 - pos_accuracy: 0.7002 - val_loss: 0.4244 - val_pos_accuracy: 0.5150\n",
      "Epoch 736/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1901 - pos_accuracy: 0.6974 - val_loss: 0.4229 - val_pos_accuracy: 0.5200\n",
      "Epoch 737/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1896 - pos_accuracy: 0.6956 - val_loss: 0.4227 - val_pos_accuracy: 0.5325\n",
      "Epoch 738/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1892 - pos_accuracy: 0.7008 - val_loss: 0.4226 - val_pos_accuracy: 0.5150\n",
      "Epoch 739/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1891 - pos_accuracy: 0.6979 - val_loss: 0.4221 - val_pos_accuracy: 0.5250\n",
      "Epoch 740/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1889 - pos_accuracy: 0.6953 - val_loss: 0.4220 - val_pos_accuracy: 0.5150\n",
      "Epoch 741/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1889 - pos_accuracy: 0.7020 - val_loss: 0.4222 - val_pos_accuracy: 0.5300\n",
      "Epoch 742/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1888 - pos_accuracy: 0.6933 - val_loss: 0.4219 - val_pos_accuracy: 0.5125\n",
      "Epoch 743/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1881 - pos_accuracy: 0.6975 - val_loss: 0.4210 - val_pos_accuracy: 0.5225\n",
      "Epoch 744/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1878 - pos_accuracy: 0.6957 - val_loss: 0.4216 - val_pos_accuracy: 0.5125\n",
      "Epoch 745/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1878 - pos_accuracy: 0.7065 - val_loss: 0.4207 - val_pos_accuracy: 0.5250\n",
      "Epoch 746/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1873 - pos_accuracy: 0.6966 - val_loss: 0.4201 - val_pos_accuracy: 0.5175\n",
      "Epoch 747/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1875 - pos_accuracy: 0.6987 - val_loss: 0.4212 - val_pos_accuracy: 0.5125\n",
      "Epoch 748/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1869 - pos_accuracy: 0.7105 - val_loss: 0.4196 - val_pos_accuracy: 0.5275\n",
      "Epoch 749/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1867 - pos_accuracy: 0.6992 - val_loss: 0.4196 - val_pos_accuracy: 0.5250\n",
      "Epoch 750/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1863 - pos_accuracy: 0.7028 - val_loss: 0.4199 - val_pos_accuracy: 0.5275\n",
      "Epoch 751/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1861 - pos_accuracy: 0.6979 - val_loss: 0.4198 - val_pos_accuracy: 0.5200\n",
      "Epoch 752/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1858 - pos_accuracy: 0.7039 - val_loss: 0.4195 - val_pos_accuracy: 0.5275\n",
      "Epoch 753/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1857 - pos_accuracy: 0.7069 - val_loss: 0.4196 - val_pos_accuracy: 0.5200\n",
      "Epoch 754/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1853 - pos_accuracy: 0.7031 - val_loss: 0.4186 - val_pos_accuracy: 0.5200\n",
      "Epoch 755/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1850 - pos_accuracy: 0.7095 - val_loss: 0.4189 - val_pos_accuracy: 0.5175\n",
      "Epoch 756/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1847 - pos_accuracy: 0.7001 - val_loss: 0.4188 - val_pos_accuracy: 0.5250\n",
      "Epoch 757/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1845 - pos_accuracy: 0.7012 - val_loss: 0.4185 - val_pos_accuracy: 0.5225\n",
      "Epoch 758/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1844 - pos_accuracy: 0.7058 - val_loss: 0.4186 - val_pos_accuracy: 0.5275\n",
      "Epoch 759/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1840 - pos_accuracy: 0.7148 - val_loss: 0.4181 - val_pos_accuracy: 0.5200\n",
      "Epoch 760/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1838 - pos_accuracy: 0.7043 - val_loss: 0.4187 - val_pos_accuracy: 0.5225\n",
      "Epoch 761/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1837 - pos_accuracy: 0.7041 - val_loss: 0.4181 - val_pos_accuracy: 0.5300\n",
      "Epoch 762/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1836 - pos_accuracy: 0.7086 - val_loss: 0.4177 - val_pos_accuracy: 0.5200\n",
      "Epoch 763/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1830 - pos_accuracy: 0.7088 - val_loss: 0.4170 - val_pos_accuracy: 0.5175\n",
      "Epoch 764/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1828 - pos_accuracy: 0.7106 - val_loss: 0.4167 - val_pos_accuracy: 0.5300\n",
      "Epoch 765/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1832 - pos_accuracy: 0.7072 - val_loss: 0.4166 - val_pos_accuracy: 0.5325\n",
      "Epoch 766/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1829 - pos_accuracy: 0.7116 - val_loss: 0.4165 - val_pos_accuracy: 0.5300\n",
      "Epoch 767/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1822 - pos_accuracy: 0.7164 - val_loss: 0.4166 - val_pos_accuracy: 0.5275\n",
      "Epoch 768/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1817 - pos_accuracy: 0.7163 - val_loss: 0.4170 - val_pos_accuracy: 0.5200\n",
      "Epoch 769/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1817 - pos_accuracy: 0.7093 - val_loss: 0.4162 - val_pos_accuracy: 0.5250\n",
      "Epoch 770/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1813 - pos_accuracy: 0.7146 - val_loss: 0.4157 - val_pos_accuracy: 0.5250\n",
      "Epoch 771/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1811 - pos_accuracy: 0.7164 - val_loss: 0.4157 - val_pos_accuracy: 0.5225\n",
      "Epoch 772/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1811 - pos_accuracy: 0.7122 - val_loss: 0.4154 - val_pos_accuracy: 0.5225\n",
      "Epoch 773/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1809 - pos_accuracy: 0.7112 - val_loss: 0.4157 - val_pos_accuracy: 0.5250\n",
      "Epoch 774/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1807 - pos_accuracy: 0.7195 - val_loss: 0.4137 - val_pos_accuracy: 0.5250\n",
      "Epoch 775/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1798 - pos_accuracy: 0.7129 - val_loss: 0.4138 - val_pos_accuracy: 0.5300\n",
      "Epoch 776/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1798 - pos_accuracy: 0.7159 - val_loss: 0.4139 - val_pos_accuracy: 0.5250\n",
      "Epoch 777/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1797 - pos_accuracy: 0.7143 - val_loss: 0.4136 - val_pos_accuracy: 0.5275\n",
      "Epoch 778/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1795 - pos_accuracy: 0.7158 - val_loss: 0.4131 - val_pos_accuracy: 0.5250\n",
      "Epoch 779/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1792 - pos_accuracy: 0.7136 - val_loss: 0.4133 - val_pos_accuracy: 0.5275\n",
      "Epoch 780/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1791 - pos_accuracy: 0.7132 - val_loss: 0.4127 - val_pos_accuracy: 0.5250\n",
      "Epoch 781/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1788 - pos_accuracy: 0.7289 - val_loss: 0.4122 - val_pos_accuracy: 0.5375\n",
      "Epoch 782/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1792 - pos_accuracy: 0.7140 - val_loss: 0.4122 - val_pos_accuracy: 0.5400\n",
      "Epoch 783/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1786 - pos_accuracy: 0.7173 - val_loss: 0.4117 - val_pos_accuracy: 0.5350\n",
      "Epoch 784/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1779 - pos_accuracy: 0.7206 - val_loss: 0.4119 - val_pos_accuracy: 0.5275\n",
      "Epoch 785/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1779 - pos_accuracy: 0.7214 - val_loss: 0.4123 - val_pos_accuracy: 0.5350\n",
      "Epoch 786/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1777 - pos_accuracy: 0.7178 - val_loss: 0.4118 - val_pos_accuracy: 0.5300\n",
      "Epoch 787/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1774 - pos_accuracy: 0.7290 - val_loss: 0.4113 - val_pos_accuracy: 0.5350\n",
      "Epoch 788/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1770 - pos_accuracy: 0.7203 - val_loss: 0.4111 - val_pos_accuracy: 0.5425\n",
      "Epoch 789/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1770 - pos_accuracy: 0.7216 - val_loss: 0.4113 - val_pos_accuracy: 0.5350\n",
      "Epoch 790/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1770 - pos_accuracy: 0.7248 - val_loss: 0.4106 - val_pos_accuracy: 0.5475\n",
      "Epoch 791/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1766 - pos_accuracy: 0.7285 - val_loss: 0.4106 - val_pos_accuracy: 0.5375\n",
      "Epoch 792/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1761 - pos_accuracy: 0.7319 - val_loss: 0.4107 - val_pos_accuracy: 0.5325\n",
      "Epoch 793/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1759 - pos_accuracy: 0.7254 - val_loss: 0.4116 - val_pos_accuracy: 0.5250\n",
      "Epoch 794/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1757 - pos_accuracy: 0.7242 - val_loss: 0.4101 - val_pos_accuracy: 0.5350\n",
      "Epoch 795/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1755 - pos_accuracy: 0.7259 - val_loss: 0.4097 - val_pos_accuracy: 0.5400\n",
      "Epoch 796/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1753 - pos_accuracy: 0.7344 - val_loss: 0.4092 - val_pos_accuracy: 0.5375\n",
      "Epoch 797/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1749 - pos_accuracy: 0.7218 - val_loss: 0.4101 - val_pos_accuracy: 0.5375\n",
      "Epoch 798/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1749 - pos_accuracy: 0.7251 - val_loss: 0.4093 - val_pos_accuracy: 0.5325\n",
      "Epoch 799/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1747 - pos_accuracy: 0.7324 - val_loss: 0.4092 - val_pos_accuracy: 0.5350\n",
      "Epoch 800/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1743 - pos_accuracy: 0.7335 - val_loss: 0.4090 - val_pos_accuracy: 0.5275\n",
      "Epoch 801/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1746 - pos_accuracy: 0.7330 - val_loss: 0.4089 - val_pos_accuracy: 0.5300\n",
      "Epoch 802/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1740 - pos_accuracy: 0.7320 - val_loss: 0.4089 - val_pos_accuracy: 0.5375\n",
      "Epoch 803/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1740 - pos_accuracy: 0.7319 - val_loss: 0.4088 - val_pos_accuracy: 0.5400\n",
      "Epoch 804/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1735 - pos_accuracy: 0.7297 - val_loss: 0.4090 - val_pos_accuracy: 0.5300\n",
      "Epoch 805/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1733 - pos_accuracy: 0.7241 - val_loss: 0.4076 - val_pos_accuracy: 0.5375\n",
      "Epoch 806/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1736 - pos_accuracy: 0.7344 - val_loss: 0.4088 - val_pos_accuracy: 0.5300\n",
      "Epoch 807/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1732 - pos_accuracy: 0.7336 - val_loss: 0.4074 - val_pos_accuracy: 0.5375\n",
      "Epoch 808/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1725 - pos_accuracy: 0.7260 - val_loss: 0.4070 - val_pos_accuracy: 0.5375\n",
      "Epoch 809/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1727 - pos_accuracy: 0.7329 - val_loss: 0.4081 - val_pos_accuracy: 0.5300\n",
      "Epoch 810/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1724 - pos_accuracy: 0.7311 - val_loss: 0.4069 - val_pos_accuracy: 0.5325\n",
      "Epoch 811/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1722 - pos_accuracy: 0.7297 - val_loss: 0.4068 - val_pos_accuracy: 0.5325\n",
      "Epoch 812/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1719 - pos_accuracy: 0.7234 - val_loss: 0.4072 - val_pos_accuracy: 0.5325\n",
      "Epoch 813/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1719 - pos_accuracy: 0.7323 - val_loss: 0.4071 - val_pos_accuracy: 0.5325\n",
      "Epoch 814/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1714 - pos_accuracy: 0.7306 - val_loss: 0.4061 - val_pos_accuracy: 0.5350\n",
      "Epoch 815/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1711 - pos_accuracy: 0.7329 - val_loss: 0.4058 - val_pos_accuracy: 0.5500\n",
      "Epoch 816/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1708 - pos_accuracy: 0.7333 - val_loss: 0.4060 - val_pos_accuracy: 0.5325\n",
      "Epoch 817/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1706 - pos_accuracy: 0.7288 - val_loss: 0.4057 - val_pos_accuracy: 0.5500\n",
      "Epoch 818/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1706 - pos_accuracy: 0.7459 - val_loss: 0.4047 - val_pos_accuracy: 0.5375\n",
      "Epoch 819/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1702 - pos_accuracy: 0.7234 - val_loss: 0.4050 - val_pos_accuracy: 0.5475\n",
      "Epoch 820/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1699 - pos_accuracy: 0.7338 - val_loss: 0.4046 - val_pos_accuracy: 0.5375\n",
      "Epoch 821/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1696 - pos_accuracy: 0.7268 - val_loss: 0.4048 - val_pos_accuracy: 0.5475\n",
      "Epoch 822/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1696 - pos_accuracy: 0.7326 - val_loss: 0.4048 - val_pos_accuracy: 0.5475\n",
      "Epoch 823/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1692 - pos_accuracy: 0.7344 - val_loss: 0.4046 - val_pos_accuracy: 0.5450\n",
      "Epoch 824/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1691 - pos_accuracy: 0.7377 - val_loss: 0.4046 - val_pos_accuracy: 0.5425\n",
      "Epoch 825/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1687 - pos_accuracy: 0.7446 - val_loss: 0.4040 - val_pos_accuracy: 0.5475\n",
      "Epoch 826/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1687 - pos_accuracy: 0.7398 - val_loss: 0.4039 - val_pos_accuracy: 0.5475\n",
      "Epoch 827/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1685 - pos_accuracy: 0.7397 - val_loss: 0.4039 - val_pos_accuracy: 0.5500\n",
      "Epoch 828/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1681 - pos_accuracy: 0.7352 - val_loss: 0.4032 - val_pos_accuracy: 0.5500\n",
      "Epoch 829/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1680 - pos_accuracy: 0.7352 - val_loss: 0.4033 - val_pos_accuracy: 0.5475\n",
      "Epoch 830/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1676 - pos_accuracy: 0.7347 - val_loss: 0.4031 - val_pos_accuracy: 0.5500\n",
      "Epoch 831/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1675 - pos_accuracy: 0.7377 - val_loss: 0.4030 - val_pos_accuracy: 0.5500\n",
      "Epoch 832/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1674 - pos_accuracy: 0.7431 - val_loss: 0.4035 - val_pos_accuracy: 0.5375\n",
      "Epoch 833/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1672 - pos_accuracy: 0.7358 - val_loss: 0.4025 - val_pos_accuracy: 0.5400\n",
      "Epoch 834/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1671 - pos_accuracy: 0.7339 - val_loss: 0.4016 - val_pos_accuracy: 0.5425\n",
      "Epoch 835/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1667 - pos_accuracy: 0.7358 - val_loss: 0.4016 - val_pos_accuracy: 0.5500\n",
      "Epoch 836/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1667 - pos_accuracy: 0.7383 - val_loss: 0.4015 - val_pos_accuracy: 0.5525\n",
      "Epoch 837/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1662 - pos_accuracy: 0.7421 - val_loss: 0.4015 - val_pos_accuracy: 0.5450\n",
      "Epoch 838/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1663 - pos_accuracy: 0.7370 - val_loss: 0.4016 - val_pos_accuracy: 0.5425\n",
      "Epoch 839/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1660 - pos_accuracy: 0.7328 - val_loss: 0.4011 - val_pos_accuracy: 0.5450\n",
      "Epoch 840/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1659 - pos_accuracy: 0.7354 - val_loss: 0.4012 - val_pos_accuracy: 0.5450\n",
      "Epoch 841/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1654 - pos_accuracy: 0.7362 - val_loss: 0.4005 - val_pos_accuracy: 0.5550\n",
      "Epoch 842/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1651 - pos_accuracy: 0.7489 - val_loss: 0.4011 - val_pos_accuracy: 0.5475\n",
      "Epoch 843/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1653 - pos_accuracy: 0.7380 - val_loss: 0.4008 - val_pos_accuracy: 0.5450\n",
      "Epoch 844/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1649 - pos_accuracy: 0.7375 - val_loss: 0.4002 - val_pos_accuracy: 0.5550\n",
      "Epoch 845/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1648 - pos_accuracy: 0.7472 - val_loss: 0.4009 - val_pos_accuracy: 0.5475\n",
      "Epoch 846/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1646 - pos_accuracy: 0.7465 - val_loss: 0.4000 - val_pos_accuracy: 0.5525\n",
      "Epoch 847/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1642 - pos_accuracy: 0.7445 - val_loss: 0.3994 - val_pos_accuracy: 0.5550\n",
      "Epoch 848/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1640 - pos_accuracy: 0.7482 - val_loss: 0.4001 - val_pos_accuracy: 0.5525\n",
      "Epoch 849/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1641 - pos_accuracy: 0.7390 - val_loss: 0.3994 - val_pos_accuracy: 0.5525\n",
      "Epoch 850/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1635 - pos_accuracy: 0.7497 - val_loss: 0.3998 - val_pos_accuracy: 0.5525\n",
      "Epoch 851/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1636 - pos_accuracy: 0.7421 - val_loss: 0.3996 - val_pos_accuracy: 0.5525\n",
      "Epoch 852/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1632 - pos_accuracy: 0.7451 - val_loss: 0.3994 - val_pos_accuracy: 0.5500\n",
      "Epoch 853/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1631 - pos_accuracy: 0.7475 - val_loss: 0.3992 - val_pos_accuracy: 0.5500\n",
      "Epoch 854/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1629 - pos_accuracy: 0.7444 - val_loss: 0.3987 - val_pos_accuracy: 0.5575\n",
      "Epoch 855/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1628 - pos_accuracy: 0.7540 - val_loss: 0.3992 - val_pos_accuracy: 0.5500\n",
      "Epoch 856/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1625 - pos_accuracy: 0.7465 - val_loss: 0.3974 - val_pos_accuracy: 0.5525\n",
      "Epoch 857/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1622 - pos_accuracy: 0.7456 - val_loss: 0.3982 - val_pos_accuracy: 0.5525\n",
      "Epoch 858/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1620 - pos_accuracy: 0.7440 - val_loss: 0.3979 - val_pos_accuracy: 0.5525\n",
      "Epoch 859/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1618 - pos_accuracy: 0.7528 - val_loss: 0.3977 - val_pos_accuracy: 0.5550\n",
      "Epoch 860/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1616 - pos_accuracy: 0.7484 - val_loss: 0.3977 - val_pos_accuracy: 0.5500\n",
      "Epoch 861/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1614 - pos_accuracy: 0.7472 - val_loss: 0.3974 - val_pos_accuracy: 0.5525\n",
      "Epoch 862/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1613 - pos_accuracy: 0.7434 - val_loss: 0.3968 - val_pos_accuracy: 0.5500\n",
      "Epoch 863/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1611 - pos_accuracy: 0.7483 - val_loss: 0.3971 - val_pos_accuracy: 0.5525\n",
      "Epoch 864/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1610 - pos_accuracy: 0.7476 - val_loss: 0.3965 - val_pos_accuracy: 0.5600\n",
      "Epoch 865/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1617 - pos_accuracy: 0.7518 - val_loss: 0.3965 - val_pos_accuracy: 0.5525\n",
      "Epoch 866/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1608 - pos_accuracy: 0.7467 - val_loss: 0.3967 - val_pos_accuracy: 0.5550\n",
      "Epoch 867/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1603 - pos_accuracy: 0.7555 - val_loss: 0.3961 - val_pos_accuracy: 0.5600\n",
      "Epoch 868/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1600 - pos_accuracy: 0.7569 - val_loss: 0.3962 - val_pos_accuracy: 0.5575\n",
      "Epoch 869/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1599 - pos_accuracy: 0.7593 - val_loss: 0.3965 - val_pos_accuracy: 0.5500\n",
      "Epoch 870/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1597 - pos_accuracy: 0.7512 - val_loss: 0.3966 - val_pos_accuracy: 0.5525\n",
      "Epoch 871/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1597 - pos_accuracy: 0.7473 - val_loss: 0.3956 - val_pos_accuracy: 0.5450\n",
      "Epoch 872/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1596 - pos_accuracy: 0.7481 - val_loss: 0.3966 - val_pos_accuracy: 0.5525\n",
      "Epoch 873/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1590 - pos_accuracy: 0.7466 - val_loss: 0.3964 - val_pos_accuracy: 0.5450\n",
      "Epoch 874/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1588 - pos_accuracy: 0.7493 - val_loss: 0.3953 - val_pos_accuracy: 0.5525\n",
      "Epoch 875/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1586 - pos_accuracy: 0.7452 - val_loss: 0.3950 - val_pos_accuracy: 0.5500\n",
      "Epoch 876/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1582 - pos_accuracy: 0.7511 - val_loss: 0.3948 - val_pos_accuracy: 0.5525\n",
      "Epoch 877/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1582 - pos_accuracy: 0.7515 - val_loss: 0.3948 - val_pos_accuracy: 0.5575\n",
      "Epoch 878/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1582 - pos_accuracy: 0.7515 - val_loss: 0.3945 - val_pos_accuracy: 0.5550\n",
      "Epoch 879/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1581 - pos_accuracy: 0.7521 - val_loss: 0.3939 - val_pos_accuracy: 0.5475\n",
      "Epoch 880/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1577 - pos_accuracy: 0.7447 - val_loss: 0.3937 - val_pos_accuracy: 0.5550\n",
      "Epoch 881/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1575 - pos_accuracy: 0.7541 - val_loss: 0.3940 - val_pos_accuracy: 0.5475\n",
      "Epoch 882/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1573 - pos_accuracy: 0.7618 - val_loss: 0.3939 - val_pos_accuracy: 0.5525\n",
      "Epoch 883/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1576 - pos_accuracy: 0.7512 - val_loss: 0.3941 - val_pos_accuracy: 0.5525\n",
      "Epoch 884/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1574 - pos_accuracy: 0.7547 - val_loss: 0.3932 - val_pos_accuracy: 0.5600\n",
      "Epoch 885/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1568 - pos_accuracy: 0.7641 - val_loss: 0.3934 - val_pos_accuracy: 0.5500\n",
      "Epoch 886/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1565 - pos_accuracy: 0.7538 - val_loss: 0.3928 - val_pos_accuracy: 0.5625\n",
      "Epoch 887/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1562 - pos_accuracy: 0.7529 - val_loss: 0.3933 - val_pos_accuracy: 0.5575\n",
      "Epoch 888/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1562 - pos_accuracy: 0.7620 - val_loss: 0.3931 - val_pos_accuracy: 0.5625\n",
      "Epoch 889/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1559 - pos_accuracy: 0.7600 - val_loss: 0.3926 - val_pos_accuracy: 0.5625\n",
      "Epoch 890/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1559 - pos_accuracy: 0.7609 - val_loss: 0.3922 - val_pos_accuracy: 0.5675\n",
      "Epoch 891/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1555 - pos_accuracy: 0.7678 - val_loss: 0.3923 - val_pos_accuracy: 0.5625\n",
      "Epoch 892/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1556 - pos_accuracy: 0.7620 - val_loss: 0.3929 - val_pos_accuracy: 0.5625\n",
      "Epoch 893/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1551 - pos_accuracy: 0.7607 - val_loss: 0.3925 - val_pos_accuracy: 0.5675\n",
      "Epoch 894/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1550 - pos_accuracy: 0.7663 - val_loss: 0.3924 - val_pos_accuracy: 0.5650\n",
      "Epoch 895/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1549 - pos_accuracy: 0.7554 - val_loss: 0.3920 - val_pos_accuracy: 0.5550\n",
      "Epoch 896/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1549 - pos_accuracy: 0.7505 - val_loss: 0.3915 - val_pos_accuracy: 0.5600\n",
      "Epoch 897/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1547 - pos_accuracy: 0.7553 - val_loss: 0.3912 - val_pos_accuracy: 0.5675\n",
      "Epoch 898/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1544 - pos_accuracy: 0.7545 - val_loss: 0.3913 - val_pos_accuracy: 0.5700\n",
      "Epoch 899/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1541 - pos_accuracy: 0.7612 - val_loss: 0.3908 - val_pos_accuracy: 0.5650\n",
      "Epoch 900/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1539 - pos_accuracy: 0.7596 - val_loss: 0.3904 - val_pos_accuracy: 0.5650\n",
      "Epoch 901/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1537 - pos_accuracy: 0.7649 - val_loss: 0.3902 - val_pos_accuracy: 0.5625\n",
      "Epoch 902/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1535 - pos_accuracy: 0.7638 - val_loss: 0.3895 - val_pos_accuracy: 0.5675\n",
      "Epoch 903/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1534 - pos_accuracy: 0.7675 - val_loss: 0.3900 - val_pos_accuracy: 0.5700\n",
      "Epoch 904/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1534 - pos_accuracy: 0.7574 - val_loss: 0.3897 - val_pos_accuracy: 0.5625\n",
      "Epoch 905/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1531 - pos_accuracy: 0.7614 - val_loss: 0.3890 - val_pos_accuracy: 0.5650\n",
      "Epoch 906/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1527 - pos_accuracy: 0.7618 - val_loss: 0.3897 - val_pos_accuracy: 0.5625\n",
      "Epoch 907/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1527 - pos_accuracy: 0.7626 - val_loss: 0.3896 - val_pos_accuracy: 0.5675\n",
      "Epoch 908/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1525 - pos_accuracy: 0.7619 - val_loss: 0.3891 - val_pos_accuracy: 0.5700\n",
      "Epoch 909/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1522 - pos_accuracy: 0.7649 - val_loss: 0.3890 - val_pos_accuracy: 0.5675\n",
      "Epoch 910/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1520 - pos_accuracy: 0.7623 - val_loss: 0.3887 - val_pos_accuracy: 0.5675\n",
      "Epoch 911/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1520 - pos_accuracy: 0.7660 - val_loss: 0.3887 - val_pos_accuracy: 0.5675\n",
      "Epoch 912/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1519 - pos_accuracy: 0.7700 - val_loss: 0.3883 - val_pos_accuracy: 0.5675\n",
      "Epoch 913/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1517 - pos_accuracy: 0.7615 - val_loss: 0.3886 - val_pos_accuracy: 0.5650\n",
      "Epoch 914/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1515 - pos_accuracy: 0.7642 - val_loss: 0.3880 - val_pos_accuracy: 0.5675\n",
      "Epoch 915/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1515 - pos_accuracy: 0.7673 - val_loss: 0.3873 - val_pos_accuracy: 0.5650\n",
      "Epoch 916/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1510 - pos_accuracy: 0.7616 - val_loss: 0.3875 - val_pos_accuracy: 0.5675\n",
      "Epoch 917/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1510 - pos_accuracy: 0.7645 - val_loss: 0.3875 - val_pos_accuracy: 0.5725\n",
      "Epoch 918/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1512 - pos_accuracy: 0.7629 - val_loss: 0.3876 - val_pos_accuracy: 0.5675\n",
      "Epoch 919/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1512 - pos_accuracy: 0.7676 - val_loss: 0.3872 - val_pos_accuracy: 0.5725\n",
      "Epoch 920/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1513 - pos_accuracy: 0.7652 - val_loss: 0.3876 - val_pos_accuracy: 0.5700\n",
      "Epoch 921/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1503 - pos_accuracy: 0.7605 - val_loss: 0.3857 - val_pos_accuracy: 0.5750\n",
      "Epoch 922/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1505 - pos_accuracy: 0.7693 - val_loss: 0.3862 - val_pos_accuracy: 0.5700\n",
      "Epoch 923/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1499 - pos_accuracy: 0.7636 - val_loss: 0.3870 - val_pos_accuracy: 0.5675\n",
      "Epoch 924/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1497 - pos_accuracy: 0.7622 - val_loss: 0.3859 - val_pos_accuracy: 0.5750\n",
      "Epoch 925/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1498 - pos_accuracy: 0.7674 - val_loss: 0.3869 - val_pos_accuracy: 0.5650\n",
      "Epoch 926/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1495 - pos_accuracy: 0.7609 - val_loss: 0.3863 - val_pos_accuracy: 0.5650\n",
      "Epoch 927/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1490 - pos_accuracy: 0.7609 - val_loss: 0.3857 - val_pos_accuracy: 0.5725\n",
      "Epoch 928/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1489 - pos_accuracy: 0.7655 - val_loss: 0.3862 - val_pos_accuracy: 0.5700\n",
      "Epoch 929/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1486 - pos_accuracy: 0.7687 - val_loss: 0.3858 - val_pos_accuracy: 0.5725\n",
      "Epoch 930/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1487 - pos_accuracy: 0.7689 - val_loss: 0.3861 - val_pos_accuracy: 0.5700\n",
      "Epoch 931/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1485 - pos_accuracy: 0.7635 - val_loss: 0.3855 - val_pos_accuracy: 0.5700\n",
      "Epoch 932/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1485 - pos_accuracy: 0.7650 - val_loss: 0.3859 - val_pos_accuracy: 0.5675\n",
      "Epoch 933/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1481 - pos_accuracy: 0.7702 - val_loss: 0.3850 - val_pos_accuracy: 0.5700\n",
      "Epoch 934/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1479 - pos_accuracy: 0.7668 - val_loss: 0.3848 - val_pos_accuracy: 0.5700\n",
      "Epoch 935/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1477 - pos_accuracy: 0.7716 - val_loss: 0.3848 - val_pos_accuracy: 0.5725\n",
      "Epoch 936/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1474 - pos_accuracy: 0.7646 - val_loss: 0.3846 - val_pos_accuracy: 0.5750\n",
      "Epoch 937/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1474 - pos_accuracy: 0.7753 - val_loss: 0.3841 - val_pos_accuracy: 0.5775\n",
      "Epoch 938/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1471 - pos_accuracy: 0.7678 - val_loss: 0.3845 - val_pos_accuracy: 0.5750\n",
      "Epoch 939/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1470 - pos_accuracy: 0.7707 - val_loss: 0.3843 - val_pos_accuracy: 0.5750\n",
      "Epoch 940/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1470 - pos_accuracy: 0.7681 - val_loss: 0.3851 - val_pos_accuracy: 0.5725\n",
      "Epoch 941/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1467 - pos_accuracy: 0.7760 - val_loss: 0.3846 - val_pos_accuracy: 0.5700\n",
      "Epoch 942/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1466 - pos_accuracy: 0.7658 - val_loss: 0.3839 - val_pos_accuracy: 0.5750\n",
      "Epoch 943/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1467 - pos_accuracy: 0.7675 - val_loss: 0.3837 - val_pos_accuracy: 0.5725\n",
      "Epoch 944/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1465 - pos_accuracy: 0.7743 - val_loss: 0.3840 - val_pos_accuracy: 0.5625\n",
      "Epoch 945/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1462 - pos_accuracy: 0.7671 - val_loss: 0.3834 - val_pos_accuracy: 0.5725\n",
      "Epoch 946/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1458 - pos_accuracy: 0.7671 - val_loss: 0.3831 - val_pos_accuracy: 0.5775\n",
      "Epoch 947/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1459 - pos_accuracy: 0.7671 - val_loss: 0.3830 - val_pos_accuracy: 0.5750\n",
      "Epoch 948/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1456 - pos_accuracy: 0.7674 - val_loss: 0.3834 - val_pos_accuracy: 0.5725\n",
      "Epoch 949/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1452 - pos_accuracy: 0.7735 - val_loss: 0.3835 - val_pos_accuracy: 0.5725\n",
      "Epoch 950/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1453 - pos_accuracy: 0.7678 - val_loss: 0.3828 - val_pos_accuracy: 0.5750\n",
      "Epoch 951/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1448 - pos_accuracy: 0.7771 - val_loss: 0.3825 - val_pos_accuracy: 0.5750\n",
      "Epoch 952/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1451 - pos_accuracy: 0.7738 - val_loss: 0.3828 - val_pos_accuracy: 0.5725\n",
      "Epoch 953/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1447 - pos_accuracy: 0.7691 - val_loss: 0.3818 - val_pos_accuracy: 0.5725\n",
      "Epoch 954/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1444 - pos_accuracy: 0.7695 - val_loss: 0.3825 - val_pos_accuracy: 0.5800\n",
      "Epoch 955/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1441 - pos_accuracy: 0.7695 - val_loss: 0.3824 - val_pos_accuracy: 0.5675\n",
      "Epoch 956/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1442 - pos_accuracy: 0.7709 - val_loss: 0.3819 - val_pos_accuracy: 0.5725\n",
      "Epoch 957/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1439 - pos_accuracy: 0.7766 - val_loss: 0.3821 - val_pos_accuracy: 0.5750\n",
      "Epoch 958/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1439 - pos_accuracy: 0.7658 - val_loss: 0.3813 - val_pos_accuracy: 0.5775\n",
      "Epoch 959/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1435 - pos_accuracy: 0.7739 - val_loss: 0.3816 - val_pos_accuracy: 0.5675\n",
      "Epoch 960/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1436 - pos_accuracy: 0.7712 - val_loss: 0.3811 - val_pos_accuracy: 0.5675\n",
      "Epoch 961/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1434 - pos_accuracy: 0.7710 - val_loss: 0.3808 - val_pos_accuracy: 0.5700\n",
      "Epoch 962/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1433 - pos_accuracy: 0.7759 - val_loss: 0.3803 - val_pos_accuracy: 0.5725\n",
      "Epoch 963/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1432 - pos_accuracy: 0.7785 - val_loss: 0.3805 - val_pos_accuracy: 0.5725\n",
      "Epoch 964/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1427 - pos_accuracy: 0.7722 - val_loss: 0.3808 - val_pos_accuracy: 0.5825\n",
      "Epoch 965/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1428 - pos_accuracy: 0.7793 - val_loss: 0.3809 - val_pos_accuracy: 0.5775\n",
      "Epoch 966/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1427 - pos_accuracy: 0.7746 - val_loss: 0.3803 - val_pos_accuracy: 0.5725\n",
      "Epoch 967/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1423 - pos_accuracy: 0.7768 - val_loss: 0.3800 - val_pos_accuracy: 0.5775\n",
      "Epoch 968/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1420 - pos_accuracy: 0.7795 - val_loss: 0.3796 - val_pos_accuracy: 0.5800\n",
      "Epoch 969/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1420 - pos_accuracy: 0.7784 - val_loss: 0.3802 - val_pos_accuracy: 0.5750\n",
      "Epoch 970/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1419 - pos_accuracy: 0.7833 - val_loss: 0.3794 - val_pos_accuracy: 0.5800\n",
      "Epoch 971/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1418 - pos_accuracy: 0.7722 - val_loss: 0.3797 - val_pos_accuracy: 0.5825\n",
      "Epoch 972/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1417 - pos_accuracy: 0.7813 - val_loss: 0.3795 - val_pos_accuracy: 0.5800\n",
      "Epoch 973/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1415 - pos_accuracy: 0.7752 - val_loss: 0.3793 - val_pos_accuracy: 0.5900\n",
      "Epoch 974/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1414 - pos_accuracy: 0.7854 - val_loss: 0.3798 - val_pos_accuracy: 0.5725\n",
      "Epoch 975/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1414 - pos_accuracy: 0.7684 - val_loss: 0.3793 - val_pos_accuracy: 0.5750\n",
      "Epoch 976/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1409 - pos_accuracy: 0.7775 - val_loss: 0.3784 - val_pos_accuracy: 0.5750\n",
      "Epoch 977/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1406 - pos_accuracy: 0.7777 - val_loss: 0.3789 - val_pos_accuracy: 0.5825\n",
      "Epoch 978/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1407 - pos_accuracy: 0.7758 - val_loss: 0.3786 - val_pos_accuracy: 0.5825\n",
      "Epoch 979/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1403 - pos_accuracy: 0.7801 - val_loss: 0.3784 - val_pos_accuracy: 0.5825\n",
      "Epoch 980/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1402 - pos_accuracy: 0.7771 - val_loss: 0.3783 - val_pos_accuracy: 0.5850\n",
      "Epoch 981/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1405 - pos_accuracy: 0.7848 - val_loss: 0.3783 - val_pos_accuracy: 0.5825\n",
      "Epoch 982/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1399 - pos_accuracy: 0.7818 - val_loss: 0.3787 - val_pos_accuracy: 0.5750\n",
      "Epoch 983/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1398 - pos_accuracy: 0.7741 - val_loss: 0.3781 - val_pos_accuracy: 0.5825\n",
      "Epoch 984/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1395 - pos_accuracy: 0.7748 - val_loss: 0.3777 - val_pos_accuracy: 0.5875\n",
      "Epoch 985/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1394 - pos_accuracy: 0.7727 - val_loss: 0.3776 - val_pos_accuracy: 0.5850\n",
      "Epoch 986/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1393 - pos_accuracy: 0.7795 - val_loss: 0.3777 - val_pos_accuracy: 0.5950\n",
      "Epoch 987/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1396 - pos_accuracy: 0.7797 - val_loss: 0.3779 - val_pos_accuracy: 0.5975\n",
      "Epoch 988/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1391 - pos_accuracy: 0.7823 - val_loss: 0.3780 - val_pos_accuracy: 0.5800\n",
      "Epoch 989/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1388 - pos_accuracy: 0.7759 - val_loss: 0.3776 - val_pos_accuracy: 0.5825\n",
      "Epoch 990/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1388 - pos_accuracy: 0.7743 - val_loss: 0.3764 - val_pos_accuracy: 0.5925\n",
      "Epoch 991/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1384 - pos_accuracy: 0.7875 - val_loss: 0.3764 - val_pos_accuracy: 0.5900\n",
      "Epoch 992/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1382 - pos_accuracy: 0.7821 - val_loss: 0.3768 - val_pos_accuracy: 0.5875\n",
      "Epoch 993/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1382 - pos_accuracy: 0.7783 - val_loss: 0.3764 - val_pos_accuracy: 0.5900\n",
      "Epoch 994/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1379 - pos_accuracy: 0.7772 - val_loss: 0.3772 - val_pos_accuracy: 0.5825\n",
      "Epoch 995/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1378 - pos_accuracy: 0.7802 - val_loss: 0.3770 - val_pos_accuracy: 0.5775\n",
      "Epoch 996/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1378 - pos_accuracy: 0.7821 - val_loss: 0.3762 - val_pos_accuracy: 0.5850\n",
      "Epoch 997/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1376 - pos_accuracy: 0.7778 - val_loss: 0.3757 - val_pos_accuracy: 0.5875\n",
      "Epoch 998/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1377 - pos_accuracy: 0.7858 - val_loss: 0.3758 - val_pos_accuracy: 0.5950\n",
      "Epoch 999/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1372 - pos_accuracy: 0.7830 - val_loss: 0.3762 - val_pos_accuracy: 0.5825\n",
      "Epoch 1000/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1373 - pos_accuracy: 0.7804 - val_loss: 0.3761 - val_pos_accuracy: 0.5825\n",
      "Epoch 1001/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1371 - pos_accuracy: 0.7811 - val_loss: 0.3754 - val_pos_accuracy: 0.5850\n",
      "Epoch 1002/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1370 - pos_accuracy: 0.7815 - val_loss: 0.3756 - val_pos_accuracy: 0.5825\n",
      "Epoch 1003/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1366 - pos_accuracy: 0.7817 - val_loss: 0.3746 - val_pos_accuracy: 0.5900\n",
      "Epoch 1004/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1365 - pos_accuracy: 0.7841 - val_loss: 0.3752 - val_pos_accuracy: 0.5900\n",
      "Epoch 1005/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1366 - pos_accuracy: 0.7835 - val_loss: 0.3749 - val_pos_accuracy: 0.5900\n",
      "Epoch 1006/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1363 - pos_accuracy: 0.7829 - val_loss: 0.3741 - val_pos_accuracy: 0.5900\n",
      "Epoch 1007/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1363 - pos_accuracy: 0.7818 - val_loss: 0.3751 - val_pos_accuracy: 0.5900\n",
      "Epoch 1008/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1360 - pos_accuracy: 0.7848 - val_loss: 0.3751 - val_pos_accuracy: 0.5825\n",
      "Epoch 1009/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1357 - pos_accuracy: 0.7753 - val_loss: 0.3748 - val_pos_accuracy: 0.5850\n",
      "Epoch 1010/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1355 - pos_accuracy: 0.7867 - val_loss: 0.3744 - val_pos_accuracy: 0.5925\n",
      "Epoch 1011/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1355 - pos_accuracy: 0.7841 - val_loss: 0.3747 - val_pos_accuracy: 0.5900\n",
      "Epoch 1012/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1353 - pos_accuracy: 0.7836 - val_loss: 0.3745 - val_pos_accuracy: 0.5850\n",
      "Epoch 1013/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1349 - pos_accuracy: 0.7849 - val_loss: 0.3741 - val_pos_accuracy: 0.5875\n",
      "Epoch 1014/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1352 - pos_accuracy: 0.7852 - val_loss: 0.3743 - val_pos_accuracy: 0.5850\n",
      "Epoch 1015/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1350 - pos_accuracy: 0.7868 - val_loss: 0.3737 - val_pos_accuracy: 0.5975\n",
      "Epoch 1016/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1347 - pos_accuracy: 0.7879 - val_loss: 0.3737 - val_pos_accuracy: 0.5975\n",
      "Epoch 1017/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1345 - pos_accuracy: 0.7898 - val_loss: 0.3740 - val_pos_accuracy: 0.5900\n",
      "Epoch 1018/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1349 - pos_accuracy: 0.7846 - val_loss: 0.3739 - val_pos_accuracy: 0.5975\n",
      "Epoch 1019/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1347 - pos_accuracy: 0.7884 - val_loss: 0.3741 - val_pos_accuracy: 0.5975\n",
      "Epoch 1020/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1342 - pos_accuracy: 0.7933 - val_loss: 0.3729 - val_pos_accuracy: 0.6025\n",
      "Epoch 1021/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1340 - pos_accuracy: 0.7887 - val_loss: 0.3734 - val_pos_accuracy: 0.5975\n",
      "Epoch 1022/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1338 - pos_accuracy: 0.7928 - val_loss: 0.3733 - val_pos_accuracy: 0.5975\n",
      "Epoch 1023/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1337 - pos_accuracy: 0.7843 - val_loss: 0.3722 - val_pos_accuracy: 0.6025\n",
      "Epoch 1024/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1336 - pos_accuracy: 0.7860 - val_loss: 0.3720 - val_pos_accuracy: 0.5950\n",
      "Epoch 1025/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1332 - pos_accuracy: 0.7855 - val_loss: 0.3730 - val_pos_accuracy: 0.5925\n",
      "Epoch 1026/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1333 - pos_accuracy: 0.7900 - val_loss: 0.3719 - val_pos_accuracy: 0.5900\n",
      "Epoch 1027/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1329 - pos_accuracy: 0.7847 - val_loss: 0.3717 - val_pos_accuracy: 0.6000\n",
      "Epoch 1028/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1329 - pos_accuracy: 0.7852 - val_loss: 0.3718 - val_pos_accuracy: 0.5925\n",
      "Epoch 1029/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1329 - pos_accuracy: 0.7886 - val_loss: 0.3720 - val_pos_accuracy: 0.5925\n",
      "Epoch 1030/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1328 - pos_accuracy: 0.7904 - val_loss: 0.3718 - val_pos_accuracy: 0.5975\n",
      "Epoch 1031/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1324 - pos_accuracy: 0.7890 - val_loss: 0.3711 - val_pos_accuracy: 0.5950\n",
      "Epoch 1032/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1323 - pos_accuracy: 0.7909 - val_loss: 0.3712 - val_pos_accuracy: 0.6000\n",
      "Epoch 1033/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1321 - pos_accuracy: 0.7964 - val_loss: 0.3711 - val_pos_accuracy: 0.5975\n",
      "Epoch 1034/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1323 - pos_accuracy: 0.7871 - val_loss: 0.3713 - val_pos_accuracy: 0.5975\n",
      "Epoch 1035/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1319 - pos_accuracy: 0.7848 - val_loss: 0.3712 - val_pos_accuracy: 0.6000\n",
      "Epoch 1036/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1316 - pos_accuracy: 0.7929 - val_loss: 0.3710 - val_pos_accuracy: 0.5950\n",
      "Epoch 1037/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1316 - pos_accuracy: 0.7862 - val_loss: 0.3708 - val_pos_accuracy: 0.6000\n",
      "Epoch 1038/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1315 - pos_accuracy: 0.7952 - val_loss: 0.3711 - val_pos_accuracy: 0.5975\n",
      "Epoch 1039/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1313 - pos_accuracy: 0.7883 - val_loss: 0.3712 - val_pos_accuracy: 0.5875\n",
      "Epoch 1040/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1313 - pos_accuracy: 0.7913 - val_loss: 0.3701 - val_pos_accuracy: 0.6025\n",
      "Epoch 1041/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1311 - pos_accuracy: 0.7892 - val_loss: 0.3701 - val_pos_accuracy: 0.5975\n",
      "Epoch 1042/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1308 - pos_accuracy: 0.7866 - val_loss: 0.3696 - val_pos_accuracy: 0.6000\n",
      "Epoch 1043/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1308 - pos_accuracy: 0.7890 - val_loss: 0.3705 - val_pos_accuracy: 0.5925\n",
      "Epoch 1044/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1306 - pos_accuracy: 0.7807 - val_loss: 0.3701 - val_pos_accuracy: 0.5925\n",
      "Epoch 1045/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1305 - pos_accuracy: 0.7873 - val_loss: 0.3701 - val_pos_accuracy: 0.6000\n",
      "Epoch 1046/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1304 - pos_accuracy: 0.7962 - val_loss: 0.3697 - val_pos_accuracy: 0.6000\n",
      "Epoch 1047/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1301 - pos_accuracy: 0.7885 - val_loss: 0.3695 - val_pos_accuracy: 0.5950\n",
      "Epoch 1048/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1300 - pos_accuracy: 0.7891 - val_loss: 0.3694 - val_pos_accuracy: 0.6025\n",
      "Epoch 1049/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1297 - pos_accuracy: 0.7917 - val_loss: 0.3691 - val_pos_accuracy: 0.6025\n",
      "Epoch 1050/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1296 - pos_accuracy: 0.7928 - val_loss: 0.3690 - val_pos_accuracy: 0.6000\n",
      "Epoch 1051/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1295 - pos_accuracy: 0.7894 - val_loss: 0.3691 - val_pos_accuracy: 0.5950\n",
      "Epoch 1052/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1293 - pos_accuracy: 0.7935 - val_loss: 0.3681 - val_pos_accuracy: 0.5975\n",
      "Epoch 1053/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1295 - pos_accuracy: 0.7947 - val_loss: 0.3692 - val_pos_accuracy: 0.5975\n",
      "Epoch 1054/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1298 - pos_accuracy: 0.7862 - val_loss: 0.3683 - val_pos_accuracy: 0.5975\n",
      "Epoch 1055/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1292 - pos_accuracy: 0.7920 - val_loss: 0.3688 - val_pos_accuracy: 0.5975\n",
      "Epoch 1056/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1296 - pos_accuracy: 0.8060 - val_loss: 0.3680 - val_pos_accuracy: 0.5975\n",
      "Epoch 1057/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1290 - pos_accuracy: 0.7909 - val_loss: 0.3686 - val_pos_accuracy: 0.5950\n",
      "Epoch 1058/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1289 - pos_accuracy: 0.7888 - val_loss: 0.3678 - val_pos_accuracy: 0.6025\n",
      "Epoch 1059/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1286 - pos_accuracy: 0.7923 - val_loss: 0.3686 - val_pos_accuracy: 0.5975\n",
      "Epoch 1060/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1286 - pos_accuracy: 0.7965 - val_loss: 0.3679 - val_pos_accuracy: 0.5975\n",
      "Epoch 1061/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1285 - pos_accuracy: 0.7897 - val_loss: 0.3675 - val_pos_accuracy: 0.6000\n",
      "Epoch 1062/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1284 - pos_accuracy: 0.7987 - val_loss: 0.3692 - val_pos_accuracy: 0.5925\n",
      "Epoch 1063/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1281 - pos_accuracy: 0.8059 - val_loss: 0.3673 - val_pos_accuracy: 0.5950\n",
      "Epoch 1064/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1278 - pos_accuracy: 0.8004 - val_loss: 0.3670 - val_pos_accuracy: 0.6000\n",
      "Epoch 1065/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1277 - pos_accuracy: 0.7923 - val_loss: 0.3672 - val_pos_accuracy: 0.5975\n",
      "Epoch 1066/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1274 - pos_accuracy: 0.7938 - val_loss: 0.3673 - val_pos_accuracy: 0.5975\n",
      "Epoch 1067/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1274 - pos_accuracy: 0.7912 - val_loss: 0.3676 - val_pos_accuracy: 0.5950\n",
      "Epoch 1068/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1272 - pos_accuracy: 0.7929 - val_loss: 0.3675 - val_pos_accuracy: 0.5975\n",
      "Epoch 1069/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1270 - pos_accuracy: 0.7919 - val_loss: 0.3674 - val_pos_accuracy: 0.5975\n",
      "Epoch 1070/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1270 - pos_accuracy: 0.7940 - val_loss: 0.3668 - val_pos_accuracy: 0.5950\n",
      "Epoch 1071/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1269 - pos_accuracy: 0.7991 - val_loss: 0.3670 - val_pos_accuracy: 0.5950\n",
      "Epoch 1072/3000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1266 - pos_accuracy: 0.8002 - val_loss: 0.3666 - val_pos_accuracy: 0.5975\n",
      "Epoch 1073/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1265 - pos_accuracy: 0.7954 - val_loss: 0.3668 - val_pos_accuracy: 0.6000\n",
      "Epoch 1074/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1264 - pos_accuracy: 0.7987 - val_loss: 0.3669 - val_pos_accuracy: 0.5975\n",
      "Epoch 1075/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1262 - pos_accuracy: 0.8036 - val_loss: 0.3665 - val_pos_accuracy: 0.5975\n",
      "Epoch 1076/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1265 - pos_accuracy: 0.7967 - val_loss: 0.3663 - val_pos_accuracy: 0.5975\n",
      "Epoch 1077/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1261 - pos_accuracy: 0.7994 - val_loss: 0.3659 - val_pos_accuracy: 0.5975\n",
      "Epoch 1078/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1258 - pos_accuracy: 0.7969 - val_loss: 0.3655 - val_pos_accuracy: 0.5975\n",
      "Epoch 1079/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1258 - pos_accuracy: 0.8005 - val_loss: 0.3660 - val_pos_accuracy: 0.5975\n",
      "Epoch 1080/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1257 - pos_accuracy: 0.8040 - val_loss: 0.3662 - val_pos_accuracy: 0.5975\n",
      "Epoch 1081/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1255 - pos_accuracy: 0.8049 - val_loss: 0.3657 - val_pos_accuracy: 0.5975\n",
      "Epoch 1082/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1254 - pos_accuracy: 0.8033 - val_loss: 0.3664 - val_pos_accuracy: 0.5925\n",
      "Epoch 1083/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1252 - pos_accuracy: 0.8075 - val_loss: 0.3656 - val_pos_accuracy: 0.6000\n",
      "Epoch 1084/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1250 - pos_accuracy: 0.7978 - val_loss: 0.3652 - val_pos_accuracy: 0.5925\n",
      "Epoch 1085/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1252 - pos_accuracy: 0.8082 - val_loss: 0.3652 - val_pos_accuracy: 0.6025\n",
      "Epoch 1086/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1251 - pos_accuracy: 0.8016 - val_loss: 0.3649 - val_pos_accuracy: 0.6025\n",
      "Epoch 1087/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1250 - pos_accuracy: 0.8052 - val_loss: 0.3655 - val_pos_accuracy: 0.5975\n",
      "Epoch 1088/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1246 - pos_accuracy: 0.8086 - val_loss: 0.3643 - val_pos_accuracy: 0.6050\n",
      "Epoch 1089/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1246 - pos_accuracy: 0.8008 - val_loss: 0.3649 - val_pos_accuracy: 0.6000\n",
      "Epoch 1090/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1244 - pos_accuracy: 0.8006 - val_loss: 0.3650 - val_pos_accuracy: 0.6000\n",
      "Epoch 1091/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1241 - pos_accuracy: 0.8021 - val_loss: 0.3639 - val_pos_accuracy: 0.6000\n",
      "Epoch 1092/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1243 - pos_accuracy: 0.8066 - val_loss: 0.3631 - val_pos_accuracy: 0.6000\n",
      "Epoch 1093/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1240 - pos_accuracy: 0.7998 - val_loss: 0.3647 - val_pos_accuracy: 0.6000\n",
      "Epoch 1094/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1238 - pos_accuracy: 0.8072 - val_loss: 0.3635 - val_pos_accuracy: 0.6025\n",
      "Epoch 1095/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1236 - pos_accuracy: 0.8026 - val_loss: 0.3638 - val_pos_accuracy: 0.6000\n",
      "Epoch 1096/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1235 - pos_accuracy: 0.8019 - val_loss: 0.3639 - val_pos_accuracy: 0.6000\n",
      "Epoch 1097/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1233 - pos_accuracy: 0.8038 - val_loss: 0.3630 - val_pos_accuracy: 0.6025\n",
      "Epoch 1098/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1232 - pos_accuracy: 0.8110 - val_loss: 0.3631 - val_pos_accuracy: 0.6000\n",
      "Epoch 1099/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1231 - pos_accuracy: 0.8077 - val_loss: 0.3626 - val_pos_accuracy: 0.6000\n",
      "Epoch 1100/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1228 - pos_accuracy: 0.8000 - val_loss: 0.3628 - val_pos_accuracy: 0.6000\n",
      "Epoch 1101/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1226 - pos_accuracy: 0.7990 - val_loss: 0.3631 - val_pos_accuracy: 0.6000\n",
      "Epoch 1102/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1227 - pos_accuracy: 0.8024 - val_loss: 0.3629 - val_pos_accuracy: 0.6000\n",
      "Epoch 1103/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1228 - pos_accuracy: 0.8078 - val_loss: 0.3634 - val_pos_accuracy: 0.6000\n",
      "Epoch 1104/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1226 - pos_accuracy: 0.8103 - val_loss: 0.3628 - val_pos_accuracy: 0.6000\n",
      "Epoch 1105/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1222 - pos_accuracy: 0.8098 - val_loss: 0.3622 - val_pos_accuracy: 0.6025\n",
      "Epoch 1106/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1221 - pos_accuracy: 0.8102 - val_loss: 0.3624 - val_pos_accuracy: 0.6025\n",
      "Epoch 1107/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1222 - pos_accuracy: 0.8047 - val_loss: 0.3618 - val_pos_accuracy: 0.6050\n",
      "Epoch 1108/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1220 - pos_accuracy: 0.8008 - val_loss: 0.3623 - val_pos_accuracy: 0.6025\n",
      "Epoch 1109/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1216 - pos_accuracy: 0.8111 - val_loss: 0.3621 - val_pos_accuracy: 0.6025\n",
      "Epoch 1110/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1216 - pos_accuracy: 0.8106 - val_loss: 0.3626 - val_pos_accuracy: 0.6025\n",
      "Epoch 1111/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1218 - pos_accuracy: 0.8015 - val_loss: 0.3632 - val_pos_accuracy: 0.6000\n",
      "Epoch 1112/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1215 - pos_accuracy: 0.8095 - val_loss: 0.3626 - val_pos_accuracy: 0.6025\n",
      "Epoch 1113/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1214 - pos_accuracy: 0.8108 - val_loss: 0.3621 - val_pos_accuracy: 0.6025\n",
      "Epoch 1114/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1214 - pos_accuracy: 0.8047 - val_loss: 0.3633 - val_pos_accuracy: 0.5975\n",
      "Epoch 1115/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1210 - pos_accuracy: 0.8185 - val_loss: 0.3612 - val_pos_accuracy: 0.6025\n",
      "Epoch 1116/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1209 - pos_accuracy: 0.8122 - val_loss: 0.3611 - val_pos_accuracy: 0.6075\n",
      "Epoch 1117/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1209 - pos_accuracy: 0.8100 - val_loss: 0.3611 - val_pos_accuracy: 0.6025\n",
      "Epoch 1118/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1207 - pos_accuracy: 0.8071 - val_loss: 0.3618 - val_pos_accuracy: 0.6050\n",
      "Epoch 1119/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1205 - pos_accuracy: 0.8192 - val_loss: 0.3612 - val_pos_accuracy: 0.6025\n",
      "Epoch 1120/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1203 - pos_accuracy: 0.8128 - val_loss: 0.3607 - val_pos_accuracy: 0.6025\n",
      "Epoch 1121/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1206 - pos_accuracy: 0.8152 - val_loss: 0.3605 - val_pos_accuracy: 0.6050\n",
      "Epoch 1122/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1202 - pos_accuracy: 0.8111 - val_loss: 0.3611 - val_pos_accuracy: 0.6075\n",
      "Epoch 1123/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1202 - pos_accuracy: 0.8044 - val_loss: 0.3614 - val_pos_accuracy: 0.6050\n",
      "Epoch 1124/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1199 - pos_accuracy: 0.8178 - val_loss: 0.3601 - val_pos_accuracy: 0.6075\n",
      "Epoch 1125/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1199 - pos_accuracy: 0.8110 - val_loss: 0.3602 - val_pos_accuracy: 0.6075\n",
      "Epoch 1126/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1198 - pos_accuracy: 0.8164 - val_loss: 0.3597 - val_pos_accuracy: 0.6075\n",
      "Epoch 1127/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1196 - pos_accuracy: 0.8134 - val_loss: 0.3607 - val_pos_accuracy: 0.6050\n",
      "Epoch 1128/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1194 - pos_accuracy: 0.8197 - val_loss: 0.3603 - val_pos_accuracy: 0.6050\n",
      "Epoch 1129/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1193 - pos_accuracy: 0.8129 - val_loss: 0.3598 - val_pos_accuracy: 0.6050\n",
      "Epoch 1130/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1191 - pos_accuracy: 0.8200 - val_loss: 0.3597 - val_pos_accuracy: 0.6075\n",
      "Epoch 1131/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1194 - pos_accuracy: 0.8144 - val_loss: 0.3588 - val_pos_accuracy: 0.6100\n",
      "Epoch 1132/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1190 - pos_accuracy: 0.8127 - val_loss: 0.3594 - val_pos_accuracy: 0.6050\n",
      "Epoch 1133/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1187 - pos_accuracy: 0.8152 - val_loss: 0.3590 - val_pos_accuracy: 0.6050\n",
      "Epoch 1134/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1186 - pos_accuracy: 0.8169 - val_loss: 0.3592 - val_pos_accuracy: 0.6050\n",
      "Epoch 1135/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1185 - pos_accuracy: 0.8170 - val_loss: 0.3597 - val_pos_accuracy: 0.6075\n",
      "Epoch 1136/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1188 - pos_accuracy: 0.8142 - val_loss: 0.3588 - val_pos_accuracy: 0.6075\n",
      "Epoch 1137/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1184 - pos_accuracy: 0.8158 - val_loss: 0.3586 - val_pos_accuracy: 0.6075\n",
      "Epoch 1138/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1183 - pos_accuracy: 0.8143 - val_loss: 0.3595 - val_pos_accuracy: 0.6075\n",
      "Epoch 1139/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1184 - pos_accuracy: 0.8201 - val_loss: 0.3592 - val_pos_accuracy: 0.6075\n",
      "Epoch 1140/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1181 - pos_accuracy: 0.8148 - val_loss: 0.3590 - val_pos_accuracy: 0.6050\n",
      "Epoch 1141/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1176 - pos_accuracy: 0.8164 - val_loss: 0.3587 - val_pos_accuracy: 0.6050\n",
      "Epoch 1142/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1176 - pos_accuracy: 0.8217 - val_loss: 0.3586 - val_pos_accuracy: 0.6075\n",
      "Epoch 1143/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1174 - pos_accuracy: 0.8203 - val_loss: 0.3585 - val_pos_accuracy: 0.6050\n",
      "Epoch 1144/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1173 - pos_accuracy: 0.8137 - val_loss: 0.3574 - val_pos_accuracy: 0.6100\n",
      "Epoch 1145/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1176 - pos_accuracy: 0.8170 - val_loss: 0.3572 - val_pos_accuracy: 0.6075\n",
      "Epoch 1146/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1173 - pos_accuracy: 0.8183 - val_loss: 0.3575 - val_pos_accuracy: 0.6100\n",
      "Epoch 1147/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1171 - pos_accuracy: 0.8071 - val_loss: 0.3575 - val_pos_accuracy: 0.6125\n",
      "Epoch 1148/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1169 - pos_accuracy: 0.8227 - val_loss: 0.3568 - val_pos_accuracy: 0.6050\n",
      "Epoch 1149/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1167 - pos_accuracy: 0.8219 - val_loss: 0.3571 - val_pos_accuracy: 0.6050\n",
      "Epoch 1150/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1166 - pos_accuracy: 0.8173 - val_loss: 0.3569 - val_pos_accuracy: 0.6075\n",
      "Epoch 1151/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1165 - pos_accuracy: 0.8180 - val_loss: 0.3571 - val_pos_accuracy: 0.6050\n",
      "Epoch 1152/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1165 - pos_accuracy: 0.8162 - val_loss: 0.3580 - val_pos_accuracy: 0.6050\n",
      "Epoch 1153/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1163 - pos_accuracy: 0.8283 - val_loss: 0.3569 - val_pos_accuracy: 0.6075\n",
      "Epoch 1154/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1163 - pos_accuracy: 0.8197 - val_loss: 0.3568 - val_pos_accuracy: 0.6050\n",
      "Epoch 1155/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1160 - pos_accuracy: 0.8257 - val_loss: 0.3571 - val_pos_accuracy: 0.6050\n",
      "Epoch 1156/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1159 - pos_accuracy: 0.8218 - val_loss: 0.3568 - val_pos_accuracy: 0.6050\n",
      "Epoch 1157/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1161 - pos_accuracy: 0.8170 - val_loss: 0.3576 - val_pos_accuracy: 0.6100\n",
      "Epoch 1158/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1159 - pos_accuracy: 0.8252 - val_loss: 0.3565 - val_pos_accuracy: 0.6075\n",
      "Epoch 1159/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1155 - pos_accuracy: 0.8294 - val_loss: 0.3560 - val_pos_accuracy: 0.6050\n",
      "Epoch 1160/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1154 - pos_accuracy: 0.8275 - val_loss: 0.3565 - val_pos_accuracy: 0.6075\n",
      "Epoch 1161/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1154 - pos_accuracy: 0.8213 - val_loss: 0.3564 - val_pos_accuracy: 0.6050\n",
      "Epoch 1162/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1150 - pos_accuracy: 0.8224 - val_loss: 0.3555 - val_pos_accuracy: 0.6125\n",
      "Epoch 1163/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1151 - pos_accuracy: 0.8182 - val_loss: 0.3558 - val_pos_accuracy: 0.6100\n",
      "Epoch 1164/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1150 - pos_accuracy: 0.8226 - val_loss: 0.3559 - val_pos_accuracy: 0.6150\n",
      "Epoch 1165/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1155 - pos_accuracy: 0.8281 - val_loss: 0.3559 - val_pos_accuracy: 0.6125\n",
      "Epoch 1166/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1148 - pos_accuracy: 0.8258 - val_loss: 0.3551 - val_pos_accuracy: 0.6100\n",
      "Epoch 1167/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1147 - pos_accuracy: 0.8169 - val_loss: 0.3554 - val_pos_accuracy: 0.6075\n",
      "Epoch 1168/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1145 - pos_accuracy: 0.8267 - val_loss: 0.3559 - val_pos_accuracy: 0.6150\n",
      "Epoch 1169/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1143 - pos_accuracy: 0.8294 - val_loss: 0.3553 - val_pos_accuracy: 0.6100\n",
      "Epoch 1170/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1141 - pos_accuracy: 0.8208 - val_loss: 0.3553 - val_pos_accuracy: 0.6125\n",
      "Epoch 1171/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1140 - pos_accuracy: 0.8249 - val_loss: 0.3554 - val_pos_accuracy: 0.6100\n",
      "Epoch 1172/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1141 - pos_accuracy: 0.8338 - val_loss: 0.3549 - val_pos_accuracy: 0.6100\n",
      "Epoch 1173/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1140 - pos_accuracy: 0.8192 - val_loss: 0.3550 - val_pos_accuracy: 0.6125\n",
      "Epoch 1174/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1138 - pos_accuracy: 0.8331 - val_loss: 0.3541 - val_pos_accuracy: 0.6150\n",
      "Epoch 1175/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1138 - pos_accuracy: 0.8234 - val_loss: 0.3545 - val_pos_accuracy: 0.6100\n",
      "Epoch 1176/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1138 - pos_accuracy: 0.8257 - val_loss: 0.3548 - val_pos_accuracy: 0.6075\n",
      "Epoch 1177/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1132 - pos_accuracy: 0.8356 - val_loss: 0.3548 - val_pos_accuracy: 0.6100\n",
      "Epoch 1178/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1134 - pos_accuracy: 0.8254 - val_loss: 0.3545 - val_pos_accuracy: 0.6075\n",
      "Epoch 1179/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1131 - pos_accuracy: 0.8334 - val_loss: 0.3542 - val_pos_accuracy: 0.6100\n",
      "Epoch 1180/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1130 - pos_accuracy: 0.8273 - val_loss: 0.3541 - val_pos_accuracy: 0.6125\n",
      "Epoch 1181/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1130 - pos_accuracy: 0.8246 - val_loss: 0.3542 - val_pos_accuracy: 0.6125\n",
      "Epoch 1182/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1130 - pos_accuracy: 0.8260 - val_loss: 0.3544 - val_pos_accuracy: 0.6175\n",
      "Epoch 1183/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1128 - pos_accuracy: 0.8243 - val_loss: 0.3537 - val_pos_accuracy: 0.6075\n",
      "Epoch 1184/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1126 - pos_accuracy: 0.8276 - val_loss: 0.3539 - val_pos_accuracy: 0.6125\n",
      "Epoch 1185/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1126 - pos_accuracy: 0.8270 - val_loss: 0.3544 - val_pos_accuracy: 0.6150\n",
      "Epoch 1186/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1125 - pos_accuracy: 0.8292 - val_loss: 0.3532 - val_pos_accuracy: 0.6100\n",
      "Epoch 1187/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1123 - pos_accuracy: 0.8276 - val_loss: 0.3537 - val_pos_accuracy: 0.6175\n",
      "Epoch 1188/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1123 - pos_accuracy: 0.8319 - val_loss: 0.3535 - val_pos_accuracy: 0.6100\n",
      "Epoch 1189/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1121 - pos_accuracy: 0.8299 - val_loss: 0.3525 - val_pos_accuracy: 0.6125\n",
      "Epoch 1190/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1119 - pos_accuracy: 0.8252 - val_loss: 0.3538 - val_pos_accuracy: 0.6125\n",
      "Epoch 1191/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1119 - pos_accuracy: 0.8347 - val_loss: 0.3534 - val_pos_accuracy: 0.6175\n",
      "Epoch 1192/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1117 - pos_accuracy: 0.8306 - val_loss: 0.3536 - val_pos_accuracy: 0.6175\n",
      "Epoch 1193/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1115 - pos_accuracy: 0.8312 - val_loss: 0.3519 - val_pos_accuracy: 0.6100\n",
      "Epoch 1194/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1118 - pos_accuracy: 0.8223 - val_loss: 0.3542 - val_pos_accuracy: 0.6200\n",
      "Epoch 1195/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1117 - pos_accuracy: 0.8337 - val_loss: 0.3519 - val_pos_accuracy: 0.6125\n",
      "Epoch 1196/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1114 - pos_accuracy: 0.8280 - val_loss: 0.3523 - val_pos_accuracy: 0.6175\n",
      "Epoch 1197/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1112 - pos_accuracy: 0.8268 - val_loss: 0.3527 - val_pos_accuracy: 0.6225\n",
      "Epoch 1198/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1113 - pos_accuracy: 0.8359 - val_loss: 0.3528 - val_pos_accuracy: 0.6225\n",
      "Epoch 1199/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1109 - pos_accuracy: 0.8330 - val_loss: 0.3514 - val_pos_accuracy: 0.6150\n",
      "Epoch 1200/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1110 - pos_accuracy: 0.8311 - val_loss: 0.3523 - val_pos_accuracy: 0.6100\n",
      "Epoch 1201/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1109 - pos_accuracy: 0.8267 - val_loss: 0.3523 - val_pos_accuracy: 0.6175\n",
      "Epoch 1202/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1109 - pos_accuracy: 0.8299 - val_loss: 0.3529 - val_pos_accuracy: 0.6200\n",
      "Epoch 1203/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1105 - pos_accuracy: 0.8279 - val_loss: 0.3510 - val_pos_accuracy: 0.6125\n",
      "Epoch 1204/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1103 - pos_accuracy: 0.8345 - val_loss: 0.3517 - val_pos_accuracy: 0.6225\n",
      "Epoch 1205/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1102 - pos_accuracy: 0.8375 - val_loss: 0.3517 - val_pos_accuracy: 0.6250\n",
      "Epoch 1206/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1100 - pos_accuracy: 0.8367 - val_loss: 0.3515 - val_pos_accuracy: 0.6200\n",
      "Epoch 1207/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1100 - pos_accuracy: 0.8332 - val_loss: 0.3509 - val_pos_accuracy: 0.6175\n",
      "Epoch 1208/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1099 - pos_accuracy: 0.8256 - val_loss: 0.3511 - val_pos_accuracy: 0.6225\n",
      "Epoch 1209/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1098 - pos_accuracy: 0.8321 - val_loss: 0.3510 - val_pos_accuracy: 0.6250\n",
      "Epoch 1210/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1097 - pos_accuracy: 0.8362 - val_loss: 0.3509 - val_pos_accuracy: 0.6225\n",
      "Epoch 1211/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1098 - pos_accuracy: 0.8340 - val_loss: 0.3512 - val_pos_accuracy: 0.6225\n",
      "Epoch 1212/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1094 - pos_accuracy: 0.8390 - val_loss: 0.3503 - val_pos_accuracy: 0.6200\n",
      "Epoch 1213/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1093 - pos_accuracy: 0.8363 - val_loss: 0.3505 - val_pos_accuracy: 0.6200\n",
      "Epoch 1214/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1094 - pos_accuracy: 0.8417 - val_loss: 0.3511 - val_pos_accuracy: 0.6175\n",
      "Epoch 1215/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1092 - pos_accuracy: 0.8326 - val_loss: 0.3502 - val_pos_accuracy: 0.6250\n",
      "Epoch 1216/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1089 - pos_accuracy: 0.8339 - val_loss: 0.3505 - val_pos_accuracy: 0.6250\n",
      "Epoch 1217/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1090 - pos_accuracy: 0.8385 - val_loss: 0.3499 - val_pos_accuracy: 0.6225\n",
      "Epoch 1218/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1088 - pos_accuracy: 0.8332 - val_loss: 0.3500 - val_pos_accuracy: 0.6225\n",
      "Epoch 1219/3000\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1085 - pos_accuracy: 0.8317 - val_loss: 0.3503 - val_pos_accuracy: 0.6225\n",
      "Epoch 1220/3000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1085 - pos_accuracy: 0.8322 - val_loss: 0.3505 - val_pos_accuracy: 0.6200\n",
      "Epoch 1221/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1085 - pos_accuracy: 0.8338 - val_loss: 0.3499 - val_pos_accuracy: 0.6200\n",
      "Epoch 1222/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1087 - pos_accuracy: 0.8292 - val_loss: 0.3499 - val_pos_accuracy: 0.6250\n",
      "Epoch 1223/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1082 - pos_accuracy: 0.8373 - val_loss: 0.3505 - val_pos_accuracy: 0.6250\n",
      "Epoch 1224/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1082 - pos_accuracy: 0.8356 - val_loss: 0.3505 - val_pos_accuracy: 0.6250\n",
      "Epoch 1225/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1082 - pos_accuracy: 0.8348 - val_loss: 0.3497 - val_pos_accuracy: 0.6225\n",
      "Epoch 1226/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1079 - pos_accuracy: 0.8369 - val_loss: 0.3484 - val_pos_accuracy: 0.6250\n",
      "Epoch 1227/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1081 - pos_accuracy: 0.8290 - val_loss: 0.3485 - val_pos_accuracy: 0.6225\n",
      "Epoch 1228/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1077 - pos_accuracy: 0.8333 - val_loss: 0.3494 - val_pos_accuracy: 0.6250\n",
      "Epoch 1229/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1076 - pos_accuracy: 0.8379 - val_loss: 0.3486 - val_pos_accuracy: 0.6325\n",
      "Epoch 1230/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1075 - pos_accuracy: 0.8307 - val_loss: 0.3493 - val_pos_accuracy: 0.6175\n",
      "Epoch 1231/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1074 - pos_accuracy: 0.8312 - val_loss: 0.3487 - val_pos_accuracy: 0.6250\n",
      "Epoch 1232/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1074 - pos_accuracy: 0.8345 - val_loss: 0.3493 - val_pos_accuracy: 0.6225\n",
      "Epoch 1233/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1073 - pos_accuracy: 0.8375 - val_loss: 0.3489 - val_pos_accuracy: 0.6325\n",
      "Epoch 1234/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1071 - pos_accuracy: 0.8414 - val_loss: 0.3479 - val_pos_accuracy: 0.6325\n",
      "Epoch 1235/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1068 - pos_accuracy: 0.8385 - val_loss: 0.3485 - val_pos_accuracy: 0.6350\n",
      "Epoch 1236/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1066 - pos_accuracy: 0.8412 - val_loss: 0.3480 - val_pos_accuracy: 0.6350\n",
      "Epoch 1237/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1067 - pos_accuracy: 0.8362 - val_loss: 0.3484 - val_pos_accuracy: 0.6250\n",
      "Epoch 1238/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1065 - pos_accuracy: 0.8462 - val_loss: 0.3478 - val_pos_accuracy: 0.6325\n",
      "Epoch 1239/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1064 - pos_accuracy: 0.8396 - val_loss: 0.3483 - val_pos_accuracy: 0.6275\n",
      "Epoch 1240/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1063 - pos_accuracy: 0.8391 - val_loss: 0.3475 - val_pos_accuracy: 0.6300\n",
      "Epoch 1241/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1063 - pos_accuracy: 0.8369 - val_loss: 0.3479 - val_pos_accuracy: 0.6375\n",
      "Epoch 1242/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1062 - pos_accuracy: 0.8398 - val_loss: 0.3476 - val_pos_accuracy: 0.6325\n",
      "Epoch 1243/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1060 - pos_accuracy: 0.8365 - val_loss: 0.3475 - val_pos_accuracy: 0.6325\n",
      "Epoch 1244/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1058 - pos_accuracy: 0.8402 - val_loss: 0.3481 - val_pos_accuracy: 0.6375\n",
      "Epoch 1245/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1059 - pos_accuracy: 0.8369 - val_loss: 0.3487 - val_pos_accuracy: 0.6350\n",
      "Epoch 1246/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1061 - pos_accuracy: 0.8395 - val_loss: 0.3486 - val_pos_accuracy: 0.6350\n",
      "Epoch 1247/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1056 - pos_accuracy: 0.8491 - val_loss: 0.3475 - val_pos_accuracy: 0.6225\n",
      "Epoch 1248/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1054 - pos_accuracy: 0.8389 - val_loss: 0.3475 - val_pos_accuracy: 0.6325\n",
      "Epoch 1249/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1054 - pos_accuracy: 0.8406 - val_loss: 0.3475 - val_pos_accuracy: 0.6325\n",
      "Epoch 1250/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1052 - pos_accuracy: 0.8387 - val_loss: 0.3480 - val_pos_accuracy: 0.6300\n",
      "Epoch 1251/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1052 - pos_accuracy: 0.8442 - val_loss: 0.3476 - val_pos_accuracy: 0.6325\n",
      "Epoch 1252/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1053 - pos_accuracy: 0.8397 - val_loss: 0.3474 - val_pos_accuracy: 0.6300\n",
      "Epoch 1253/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1051 - pos_accuracy: 0.8390 - val_loss: 0.3470 - val_pos_accuracy: 0.6350\n",
      "Epoch 1254/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1049 - pos_accuracy: 0.8339 - val_loss: 0.3468 - val_pos_accuracy: 0.6325\n",
      "Epoch 1255/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1047 - pos_accuracy: 0.8404 - val_loss: 0.3468 - val_pos_accuracy: 0.6325\n",
      "Epoch 1256/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1048 - pos_accuracy: 0.8384 - val_loss: 0.3470 - val_pos_accuracy: 0.6325\n",
      "Epoch 1257/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1048 - pos_accuracy: 0.8377 - val_loss: 0.3458 - val_pos_accuracy: 0.6350\n",
      "Epoch 1258/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1045 - pos_accuracy: 0.8419 - val_loss: 0.3465 - val_pos_accuracy: 0.6350\n",
      "Epoch 1259/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1042 - pos_accuracy: 0.8385 - val_loss: 0.3469 - val_pos_accuracy: 0.6350\n",
      "Epoch 1260/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1044 - pos_accuracy: 0.8379 - val_loss: 0.3468 - val_pos_accuracy: 0.6425\n",
      "Epoch 1261/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1041 - pos_accuracy: 0.8471 - val_loss: 0.3457 - val_pos_accuracy: 0.6350\n",
      "Epoch 1262/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1040 - pos_accuracy: 0.8392 - val_loss: 0.3467 - val_pos_accuracy: 0.6475\n",
      "Epoch 1263/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1042 - pos_accuracy: 0.8397 - val_loss: 0.3466 - val_pos_accuracy: 0.6425\n",
      "Epoch 1264/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1040 - pos_accuracy: 0.8416 - val_loss: 0.3467 - val_pos_accuracy: 0.6475\n",
      "Epoch 1265/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1039 - pos_accuracy: 0.8443 - val_loss: 0.3466 - val_pos_accuracy: 0.6425\n",
      "Epoch 1266/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1037 - pos_accuracy: 0.8478 - val_loss: 0.3458 - val_pos_accuracy: 0.6350\n",
      "Epoch 1267/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1035 - pos_accuracy: 0.8404 - val_loss: 0.3465 - val_pos_accuracy: 0.6400\n",
      "Epoch 1268/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1033 - pos_accuracy: 0.8473 - val_loss: 0.3460 - val_pos_accuracy: 0.6350\n",
      "Epoch 1269/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1032 - pos_accuracy: 0.8390 - val_loss: 0.3463 - val_pos_accuracy: 0.6350\n",
      "Epoch 1270/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1033 - pos_accuracy: 0.8428 - val_loss: 0.3457 - val_pos_accuracy: 0.6350\n",
      "Epoch 1271/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1032 - pos_accuracy: 0.8464 - val_loss: 0.3453 - val_pos_accuracy: 0.6350\n",
      "Epoch 1272/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1031 - pos_accuracy: 0.8449 - val_loss: 0.3452 - val_pos_accuracy: 0.6350\n",
      "Epoch 1273/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1029 - pos_accuracy: 0.8463 - val_loss: 0.3453 - val_pos_accuracy: 0.6350\n",
      "Epoch 1274/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1028 - pos_accuracy: 0.8474 - val_loss: 0.3450 - val_pos_accuracy: 0.6425\n",
      "Epoch 1275/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1028 - pos_accuracy: 0.8440 - val_loss: 0.3458 - val_pos_accuracy: 0.6425\n",
      "Epoch 1276/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1027 - pos_accuracy: 0.8429 - val_loss: 0.3455 - val_pos_accuracy: 0.6425\n",
      "Epoch 1277/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1027 - pos_accuracy: 0.8482 - val_loss: 0.3446 - val_pos_accuracy: 0.6325\n",
      "Epoch 1278/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1025 - pos_accuracy: 0.8481 - val_loss: 0.3441 - val_pos_accuracy: 0.6350\n",
      "Epoch 1279/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1024 - pos_accuracy: 0.8408 - val_loss: 0.3449 - val_pos_accuracy: 0.6350\n",
      "Epoch 1280/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1022 - pos_accuracy: 0.8473 - val_loss: 0.3449 - val_pos_accuracy: 0.6375\n",
      "Epoch 1281/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1021 - pos_accuracy: 0.8435 - val_loss: 0.3447 - val_pos_accuracy: 0.6350\n",
      "Epoch 1282/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1024 - pos_accuracy: 0.8488 - val_loss: 0.3442 - val_pos_accuracy: 0.6350\n",
      "Epoch 1283/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1021 - pos_accuracy: 0.8446 - val_loss: 0.3439 - val_pos_accuracy: 0.6375\n",
      "Epoch 1284/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1018 - pos_accuracy: 0.8424 - val_loss: 0.3446 - val_pos_accuracy: 0.6425\n",
      "Epoch 1285/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1018 - pos_accuracy: 0.8551 - val_loss: 0.3440 - val_pos_accuracy: 0.6350\n",
      "Epoch 1286/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1022 - pos_accuracy: 0.8471 - val_loss: 0.3442 - val_pos_accuracy: 0.6375\n",
      "Epoch 1287/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1016 - pos_accuracy: 0.8466 - val_loss: 0.3446 - val_pos_accuracy: 0.6375\n",
      "Epoch 1288/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1014 - pos_accuracy: 0.8532 - val_loss: 0.3447 - val_pos_accuracy: 0.6425\n",
      "Epoch 1289/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1013 - pos_accuracy: 0.8469 - val_loss: 0.3447 - val_pos_accuracy: 0.6375\n",
      "Epoch 1290/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1014 - pos_accuracy: 0.8470 - val_loss: 0.3440 - val_pos_accuracy: 0.6375\n",
      "Epoch 1291/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1012 - pos_accuracy: 0.8420 - val_loss: 0.3443 - val_pos_accuracy: 0.6425\n",
      "Epoch 1292/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1011 - pos_accuracy: 0.8466 - val_loss: 0.3446 - val_pos_accuracy: 0.6450\n",
      "Epoch 1293/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1009 - pos_accuracy: 0.8454 - val_loss: 0.3438 - val_pos_accuracy: 0.6425\n",
      "Epoch 1294/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1008 - pos_accuracy: 0.8455 - val_loss: 0.3441 - val_pos_accuracy: 0.6375\n",
      "Epoch 1295/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1009 - pos_accuracy: 0.8447 - val_loss: 0.3448 - val_pos_accuracy: 0.6525\n",
      "Epoch 1296/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1007 - pos_accuracy: 0.8484 - val_loss: 0.3437 - val_pos_accuracy: 0.6400\n",
      "Epoch 1297/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1006 - pos_accuracy: 0.8503 - val_loss: 0.3435 - val_pos_accuracy: 0.6375\n",
      "Epoch 1298/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1006 - pos_accuracy: 0.8470 - val_loss: 0.3428 - val_pos_accuracy: 0.6350\n",
      "Epoch 1299/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1006 - pos_accuracy: 0.8378 - val_loss: 0.3432 - val_pos_accuracy: 0.6350\n",
      "Epoch 1300/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1004 - pos_accuracy: 0.8428 - val_loss: 0.3431 - val_pos_accuracy: 0.6375\n",
      "Epoch 1301/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1004 - pos_accuracy: 0.8480 - val_loss: 0.3431 - val_pos_accuracy: 0.6375\n",
      "Epoch 1302/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1003 - pos_accuracy: 0.8465 - val_loss: 0.3427 - val_pos_accuracy: 0.6350\n",
      "Epoch 1303/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1000 - pos_accuracy: 0.8403 - val_loss: 0.3434 - val_pos_accuracy: 0.6450\n",
      "Epoch 1304/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0998 - pos_accuracy: 0.8497 - val_loss: 0.3431 - val_pos_accuracy: 0.6525\n",
      "Epoch 1305/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0997 - pos_accuracy: 0.8521 - val_loss: 0.3428 - val_pos_accuracy: 0.6375\n",
      "Epoch 1306/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0998 - pos_accuracy: 0.8490 - val_loss: 0.3432 - val_pos_accuracy: 0.6525\n",
      "Epoch 1307/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0996 - pos_accuracy: 0.8473 - val_loss: 0.3436 - val_pos_accuracy: 0.6425\n",
      "Epoch 1308/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0995 - pos_accuracy: 0.8492 - val_loss: 0.3437 - val_pos_accuracy: 0.6400\n",
      "Epoch 1309/3000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0994 - pos_accuracy: 0.8497 - val_loss: 0.3429 - val_pos_accuracy: 0.6500\n",
      "Epoch 1310/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0992 - pos_accuracy: 0.8562 - val_loss: 0.3428 - val_pos_accuracy: 0.6475\n",
      "Epoch 1311/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0992 - pos_accuracy: 0.8507 - val_loss: 0.3432 - val_pos_accuracy: 0.6525\n",
      "Epoch 1312/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0995 - pos_accuracy: 0.8531 - val_loss: 0.3434 - val_pos_accuracy: 0.6550\n",
      "Epoch 1313/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0992 - pos_accuracy: 0.8562 - val_loss: 0.3419 - val_pos_accuracy: 0.6475\n",
      "Epoch 1314/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0991 - pos_accuracy: 0.8496 - val_loss: 0.3417 - val_pos_accuracy: 0.6425\n",
      "Epoch 1315/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0989 - pos_accuracy: 0.8473 - val_loss: 0.3426 - val_pos_accuracy: 0.6475\n",
      "Epoch 1316/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0987 - pos_accuracy: 0.8500 - val_loss: 0.3419 - val_pos_accuracy: 0.6400\n",
      "Epoch 1317/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0987 - pos_accuracy: 0.8548 - val_loss: 0.3417 - val_pos_accuracy: 0.6425\n",
      "Epoch 1318/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0986 - pos_accuracy: 0.8506 - val_loss: 0.3422 - val_pos_accuracy: 0.6475\n",
      "Epoch 1319/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0984 - pos_accuracy: 0.8560 - val_loss: 0.3417 - val_pos_accuracy: 0.6400\n",
      "Epoch 1320/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0983 - pos_accuracy: 0.8516 - val_loss: 0.3415 - val_pos_accuracy: 0.6375\n",
      "Epoch 1321/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0984 - pos_accuracy: 0.8524 - val_loss: 0.3411 - val_pos_accuracy: 0.6375\n",
      "Epoch 1322/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0984 - pos_accuracy: 0.8534 - val_loss: 0.3412 - val_pos_accuracy: 0.6375\n",
      "Epoch 1323/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0980 - pos_accuracy: 0.8502 - val_loss: 0.3418 - val_pos_accuracy: 0.6450\n",
      "Epoch 1324/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0980 - pos_accuracy: 0.8527 - val_loss: 0.3420 - val_pos_accuracy: 0.6475\n",
      "Epoch 1325/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0978 - pos_accuracy: 0.8500 - val_loss: 0.3406 - val_pos_accuracy: 0.6350\n",
      "Epoch 1326/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0978 - pos_accuracy: 0.8498 - val_loss: 0.3415 - val_pos_accuracy: 0.6475\n",
      "Epoch 1327/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0976 - pos_accuracy: 0.8516 - val_loss: 0.3414 - val_pos_accuracy: 0.6525\n",
      "Epoch 1328/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0976 - pos_accuracy: 0.8551 - val_loss: 0.3415 - val_pos_accuracy: 0.6475\n",
      "Epoch 1329/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0974 - pos_accuracy: 0.8557 - val_loss: 0.3411 - val_pos_accuracy: 0.6400\n",
      "Epoch 1330/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0974 - pos_accuracy: 0.8634 - val_loss: 0.3414 - val_pos_accuracy: 0.6525\n",
      "Epoch 1331/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0972 - pos_accuracy: 0.8611 - val_loss: 0.3416 - val_pos_accuracy: 0.6525\n",
      "Epoch 1332/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0972 - pos_accuracy: 0.8588 - val_loss: 0.3415 - val_pos_accuracy: 0.6525\n",
      "Epoch 1333/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0971 - pos_accuracy: 0.8614 - val_loss: 0.3406 - val_pos_accuracy: 0.6475\n",
      "Epoch 1334/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0971 - pos_accuracy: 0.8573 - val_loss: 0.3407 - val_pos_accuracy: 0.6425\n",
      "Epoch 1335/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0969 - pos_accuracy: 0.8509 - val_loss: 0.3413 - val_pos_accuracy: 0.6550\n",
      "Epoch 1336/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0968 - pos_accuracy: 0.8524 - val_loss: 0.3414 - val_pos_accuracy: 0.6550\n",
      "Epoch 1337/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0969 - pos_accuracy: 0.8560 - val_loss: 0.3410 - val_pos_accuracy: 0.6550\n",
      "Epoch 1338/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0967 - pos_accuracy: 0.8569 - val_loss: 0.3409 - val_pos_accuracy: 0.6525\n",
      "Epoch 1339/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0967 - pos_accuracy: 0.8529 - val_loss: 0.3413 - val_pos_accuracy: 0.6550\n",
      "Epoch 1340/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0964 - pos_accuracy: 0.8561 - val_loss: 0.3403 - val_pos_accuracy: 0.6475\n",
      "Epoch 1341/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0964 - pos_accuracy: 0.8497 - val_loss: 0.3412 - val_pos_accuracy: 0.6600\n",
      "Epoch 1342/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0965 - pos_accuracy: 0.8525 - val_loss: 0.3404 - val_pos_accuracy: 0.6525\n",
      "Epoch 1343/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0962 - pos_accuracy: 0.8525 - val_loss: 0.3406 - val_pos_accuracy: 0.6500\n",
      "Epoch 1344/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0962 - pos_accuracy: 0.8635 - val_loss: 0.3397 - val_pos_accuracy: 0.6525\n",
      "Epoch 1345/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0958 - pos_accuracy: 0.8548 - val_loss: 0.3394 - val_pos_accuracy: 0.6475\n",
      "Epoch 1346/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0958 - pos_accuracy: 0.8558 - val_loss: 0.3399 - val_pos_accuracy: 0.6450\n",
      "Epoch 1347/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0961 - pos_accuracy: 0.8528 - val_loss: 0.3412 - val_pos_accuracy: 0.6525\n",
      "Epoch 1348/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0958 - pos_accuracy: 0.8628 - val_loss: 0.3393 - val_pos_accuracy: 0.6600\n",
      "Epoch 1349/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0957 - pos_accuracy: 0.8589 - val_loss: 0.3401 - val_pos_accuracy: 0.6525\n",
      "Epoch 1350/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0956 - pos_accuracy: 0.8598 - val_loss: 0.3385 - val_pos_accuracy: 0.6450\n",
      "Epoch 1351/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0955 - pos_accuracy: 0.8532 - val_loss: 0.3387 - val_pos_accuracy: 0.6550\n",
      "Epoch 1352/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0953 - pos_accuracy: 0.8599 - val_loss: 0.3392 - val_pos_accuracy: 0.6600\n",
      "Epoch 1353/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0951 - pos_accuracy: 0.8636 - val_loss: 0.3394 - val_pos_accuracy: 0.6500\n",
      "Epoch 1354/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0953 - pos_accuracy: 0.8613 - val_loss: 0.3386 - val_pos_accuracy: 0.6500\n",
      "Epoch 1355/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0951 - pos_accuracy: 0.8595 - val_loss: 0.3390 - val_pos_accuracy: 0.6500\n",
      "Epoch 1356/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0950 - pos_accuracy: 0.8600 - val_loss: 0.3385 - val_pos_accuracy: 0.6500\n",
      "Epoch 1357/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0949 - pos_accuracy: 0.8623 - val_loss: 0.3383 - val_pos_accuracy: 0.6550\n",
      "Epoch 1358/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0947 - pos_accuracy: 0.8620 - val_loss: 0.3383 - val_pos_accuracy: 0.6550\n",
      "Epoch 1359/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0947 - pos_accuracy: 0.8621 - val_loss: 0.3385 - val_pos_accuracy: 0.6600\n",
      "Epoch 1360/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0947 - pos_accuracy: 0.8621 - val_loss: 0.3389 - val_pos_accuracy: 0.6600\n",
      "Epoch 1361/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0946 - pos_accuracy: 0.8655 - val_loss: 0.3380 - val_pos_accuracy: 0.6600\n",
      "Epoch 1362/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0943 - pos_accuracy: 0.8625 - val_loss: 0.3386 - val_pos_accuracy: 0.6600\n",
      "Epoch 1363/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0944 - pos_accuracy: 0.8590 - val_loss: 0.3381 - val_pos_accuracy: 0.6600\n",
      "Epoch 1364/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0942 - pos_accuracy: 0.8615 - val_loss: 0.3382 - val_pos_accuracy: 0.6650\n",
      "Epoch 1365/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0941 - pos_accuracy: 0.8573 - val_loss: 0.3383 - val_pos_accuracy: 0.6575\n",
      "Epoch 1366/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0943 - pos_accuracy: 0.8624 - val_loss: 0.3384 - val_pos_accuracy: 0.6525\n",
      "Epoch 1367/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0940 - pos_accuracy: 0.8614 - val_loss: 0.3380 - val_pos_accuracy: 0.6650\n",
      "Epoch 1368/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0940 - pos_accuracy: 0.8602 - val_loss: 0.3378 - val_pos_accuracy: 0.6600\n",
      "Epoch 1369/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0938 - pos_accuracy: 0.8621 - val_loss: 0.3382 - val_pos_accuracy: 0.6600\n",
      "Epoch 1370/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0938 - pos_accuracy: 0.8588 - val_loss: 0.3366 - val_pos_accuracy: 0.6575\n",
      "Epoch 1371/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0936 - pos_accuracy: 0.8647 - val_loss: 0.3373 - val_pos_accuracy: 0.6675\n",
      "Epoch 1372/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0943 - pos_accuracy: 0.8647 - val_loss: 0.3364 - val_pos_accuracy: 0.6675\n",
      "Epoch 1373/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0938 - pos_accuracy: 0.8567 - val_loss: 0.3368 - val_pos_accuracy: 0.6625\n",
      "Epoch 1374/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0933 - pos_accuracy: 0.8621 - val_loss: 0.3379 - val_pos_accuracy: 0.6575\n",
      "Epoch 1375/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0935 - pos_accuracy: 0.8596 - val_loss: 0.3372 - val_pos_accuracy: 0.6500\n",
      "Epoch 1376/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0932 - pos_accuracy: 0.8627 - val_loss: 0.3367 - val_pos_accuracy: 0.6600\n",
      "Epoch 1377/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0932 - pos_accuracy: 0.8558 - val_loss: 0.3366 - val_pos_accuracy: 0.6575\n",
      "Epoch 1378/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0930 - pos_accuracy: 0.8632 - val_loss: 0.3379 - val_pos_accuracy: 0.6675\n",
      "Epoch 1379/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0929 - pos_accuracy: 0.8637 - val_loss: 0.3368 - val_pos_accuracy: 0.6550\n",
      "Epoch 1380/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0929 - pos_accuracy: 0.8597 - val_loss: 0.3366 - val_pos_accuracy: 0.6600\n",
      "Epoch 1381/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0926 - pos_accuracy: 0.8677 - val_loss: 0.3373 - val_pos_accuracy: 0.6625\n",
      "Epoch 1382/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0926 - pos_accuracy: 0.8625 - val_loss: 0.3368 - val_pos_accuracy: 0.6600\n",
      "Epoch 1383/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0924 - pos_accuracy: 0.8641 - val_loss: 0.3367 - val_pos_accuracy: 0.6600\n",
      "Epoch 1384/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0925 - pos_accuracy: 0.8683 - val_loss: 0.3358 - val_pos_accuracy: 0.6550\n",
      "Epoch 1385/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0924 - pos_accuracy: 0.8610 - val_loss: 0.3365 - val_pos_accuracy: 0.6625\n",
      "Epoch 1386/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0923 - pos_accuracy: 0.8664 - val_loss: 0.3365 - val_pos_accuracy: 0.6600\n",
      "Epoch 1387/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0922 - pos_accuracy: 0.8644 - val_loss: 0.3366 - val_pos_accuracy: 0.6625\n",
      "Epoch 1388/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0921 - pos_accuracy: 0.8628 - val_loss: 0.3360 - val_pos_accuracy: 0.6625\n",
      "Epoch 1389/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0920 - pos_accuracy: 0.8662 - val_loss: 0.3363 - val_pos_accuracy: 0.6575\n",
      "Epoch 1390/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0920 - pos_accuracy: 0.8627 - val_loss: 0.3371 - val_pos_accuracy: 0.6650\n",
      "Epoch 1391/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0920 - pos_accuracy: 0.8631 - val_loss: 0.3357 - val_pos_accuracy: 0.6625\n",
      "Epoch 1392/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0918 - pos_accuracy: 0.8636 - val_loss: 0.3356 - val_pos_accuracy: 0.6650\n",
      "Epoch 1393/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0916 - pos_accuracy: 0.8591 - val_loss: 0.3356 - val_pos_accuracy: 0.6625\n",
      "Epoch 1394/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0915 - pos_accuracy: 0.8675 - val_loss: 0.3372 - val_pos_accuracy: 0.6675\n",
      "Epoch 1395/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0917 - pos_accuracy: 0.8616 - val_loss: 0.3358 - val_pos_accuracy: 0.6700\n",
      "Epoch 1396/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0916 - pos_accuracy: 0.8649 - val_loss: 0.3351 - val_pos_accuracy: 0.6725\n",
      "Epoch 1397/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0917 - pos_accuracy: 0.8691 - val_loss: 0.3347 - val_pos_accuracy: 0.6725\n",
      "Epoch 1398/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0913 - pos_accuracy: 0.8677 - val_loss: 0.3349 - val_pos_accuracy: 0.6725\n",
      "Epoch 1399/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0912 - pos_accuracy: 0.8665 - val_loss: 0.3348 - val_pos_accuracy: 0.6625\n",
      "Epoch 1400/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0912 - pos_accuracy: 0.8675 - val_loss: 0.3358 - val_pos_accuracy: 0.6700\n",
      "Epoch 1401/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0909 - pos_accuracy: 0.8738 - val_loss: 0.3347 - val_pos_accuracy: 0.6600\n",
      "Epoch 1402/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0909 - pos_accuracy: 0.8638 - val_loss: 0.3348 - val_pos_accuracy: 0.6675\n",
      "Epoch 1403/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0908 - pos_accuracy: 0.8674 - val_loss: 0.3347 - val_pos_accuracy: 0.6650\n",
      "Epoch 1404/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0906 - pos_accuracy: 0.8668 - val_loss: 0.3346 - val_pos_accuracy: 0.6650\n",
      "Epoch 1405/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0905 - pos_accuracy: 0.8691 - val_loss: 0.3342 - val_pos_accuracy: 0.6650\n",
      "Epoch 1406/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0905 - pos_accuracy: 0.8713 - val_loss: 0.3346 - val_pos_accuracy: 0.6650\n",
      "Epoch 1407/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0904 - pos_accuracy: 0.8726 - val_loss: 0.3343 - val_pos_accuracy: 0.6625\n",
      "Epoch 1408/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0904 - pos_accuracy: 0.8696 - val_loss: 0.3348 - val_pos_accuracy: 0.6625\n",
      "Epoch 1409/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0904 - pos_accuracy: 0.8676 - val_loss: 0.3356 - val_pos_accuracy: 0.6675\n",
      "Epoch 1410/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0903 - pos_accuracy: 0.8691 - val_loss: 0.3338 - val_pos_accuracy: 0.6625\n",
      "Epoch 1411/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0902 - pos_accuracy: 0.8625 - val_loss: 0.3343 - val_pos_accuracy: 0.6650\n",
      "Epoch 1412/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0900 - pos_accuracy: 0.8662 - val_loss: 0.3333 - val_pos_accuracy: 0.6700\n",
      "Epoch 1413/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0900 - pos_accuracy: 0.8641 - val_loss: 0.3339 - val_pos_accuracy: 0.6675\n",
      "Epoch 1414/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0899 - pos_accuracy: 0.8666 - val_loss: 0.3348 - val_pos_accuracy: 0.6675\n",
      "Epoch 1415/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0899 - pos_accuracy: 0.8650 - val_loss: 0.3340 - val_pos_accuracy: 0.6800\n",
      "Epoch 1416/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0897 - pos_accuracy: 0.8719 - val_loss: 0.3332 - val_pos_accuracy: 0.6650\n",
      "Epoch 1417/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0898 - pos_accuracy: 0.8646 - val_loss: 0.3329 - val_pos_accuracy: 0.6700\n",
      "Epoch 1418/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0896 - pos_accuracy: 0.8650 - val_loss: 0.3339 - val_pos_accuracy: 0.6750\n",
      "Epoch 1419/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0896 - pos_accuracy: 0.8706 - val_loss: 0.3338 - val_pos_accuracy: 0.6750\n",
      "Epoch 1420/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0894 - pos_accuracy: 0.8696 - val_loss: 0.3336 - val_pos_accuracy: 0.6800\n",
      "Epoch 1421/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0893 - pos_accuracy: 0.8677 - val_loss: 0.3328 - val_pos_accuracy: 0.6700\n",
      "Epoch 1422/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0893 - pos_accuracy: 0.8693 - val_loss: 0.3340 - val_pos_accuracy: 0.6700\n",
      "Epoch 1423/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0891 - pos_accuracy: 0.8695 - val_loss: 0.3324 - val_pos_accuracy: 0.6700\n",
      "Epoch 1424/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0892 - pos_accuracy: 0.8655 - val_loss: 0.3333 - val_pos_accuracy: 0.6700\n",
      "Epoch 1425/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0891 - pos_accuracy: 0.8698 - val_loss: 0.3321 - val_pos_accuracy: 0.6750\n",
      "Epoch 1426/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0892 - pos_accuracy: 0.8714 - val_loss: 0.3322 - val_pos_accuracy: 0.6700\n",
      "Epoch 1427/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0890 - pos_accuracy: 0.8701 - val_loss: 0.3330 - val_pos_accuracy: 0.6675\n",
      "Epoch 1428/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0886 - pos_accuracy: 0.8669 - val_loss: 0.3339 - val_pos_accuracy: 0.6700\n",
      "Epoch 1429/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0886 - pos_accuracy: 0.8679 - val_loss: 0.3329 - val_pos_accuracy: 0.6625\n",
      "Epoch 1430/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0885 - pos_accuracy: 0.8653 - val_loss: 0.3325 - val_pos_accuracy: 0.6675\n",
      "Epoch 1431/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0886 - pos_accuracy: 0.8682 - val_loss: 0.3339 - val_pos_accuracy: 0.6700\n",
      "Epoch 1432/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0885 - pos_accuracy: 0.8673 - val_loss: 0.3331 - val_pos_accuracy: 0.6700\n",
      "Epoch 1433/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0882 - pos_accuracy: 0.8728 - val_loss: 0.3331 - val_pos_accuracy: 0.6825\n",
      "Epoch 1434/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0883 - pos_accuracy: 0.8717 - val_loss: 0.3331 - val_pos_accuracy: 0.6800\n",
      "Epoch 1435/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0883 - pos_accuracy: 0.8676 - val_loss: 0.3335 - val_pos_accuracy: 0.6750\n",
      "Epoch 1436/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0879 - pos_accuracy: 0.8708 - val_loss: 0.3326 - val_pos_accuracy: 0.6750\n",
      "Epoch 1437/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0881 - pos_accuracy: 0.8649 - val_loss: 0.3326 - val_pos_accuracy: 0.6800\n",
      "Epoch 1438/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0879 - pos_accuracy: 0.8738 - val_loss: 0.3325 - val_pos_accuracy: 0.6825\n",
      "Epoch 1439/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0878 - pos_accuracy: 0.8675 - val_loss: 0.3332 - val_pos_accuracy: 0.6775\n",
      "Epoch 1440/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0878 - pos_accuracy: 0.8745 - val_loss: 0.3319 - val_pos_accuracy: 0.6775\n",
      "Epoch 1441/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0877 - pos_accuracy: 0.8719 - val_loss: 0.3325 - val_pos_accuracy: 0.6750\n",
      "Epoch 1442/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0876 - pos_accuracy: 0.8715 - val_loss: 0.3323 - val_pos_accuracy: 0.6700\n",
      "Epoch 1443/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0876 - pos_accuracy: 0.8706 - val_loss: 0.3323 - val_pos_accuracy: 0.6775\n",
      "Epoch 1444/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0875 - pos_accuracy: 0.8726 - val_loss: 0.3322 - val_pos_accuracy: 0.6750\n",
      "Epoch 1445/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0874 - pos_accuracy: 0.8720 - val_loss: 0.3319 - val_pos_accuracy: 0.6750\n",
      "Epoch 1446/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0872 - pos_accuracy: 0.8725 - val_loss: 0.3317 - val_pos_accuracy: 0.6800\n",
      "Epoch 1447/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0872 - pos_accuracy: 0.8721 - val_loss: 0.3317 - val_pos_accuracy: 0.6800\n",
      "Epoch 1448/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0874 - pos_accuracy: 0.8735 - val_loss: 0.3311 - val_pos_accuracy: 0.6825\n",
      "Epoch 1449/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0872 - pos_accuracy: 0.8704 - val_loss: 0.3315 - val_pos_accuracy: 0.6825\n",
      "Epoch 1450/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0869 - pos_accuracy: 0.8683 - val_loss: 0.3320 - val_pos_accuracy: 0.6850\n",
      "Epoch 1451/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0874 - pos_accuracy: 0.8728 - val_loss: 0.3333 - val_pos_accuracy: 0.6850\n",
      "Epoch 1452/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0870 - pos_accuracy: 0.8733 - val_loss: 0.3311 - val_pos_accuracy: 0.6750\n",
      "Epoch 1453/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0872 - pos_accuracy: 0.8734 - val_loss: 0.3313 - val_pos_accuracy: 0.6775\n",
      "Epoch 1454/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0868 - pos_accuracy: 0.8716 - val_loss: 0.3318 - val_pos_accuracy: 0.6800\n",
      "Epoch 1455/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0866 - pos_accuracy: 0.8738 - val_loss: 0.3314 - val_pos_accuracy: 0.6775\n",
      "Epoch 1456/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0864 - pos_accuracy: 0.8753 - val_loss: 0.3315 - val_pos_accuracy: 0.6775\n",
      "Epoch 1457/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0863 - pos_accuracy: 0.8833 - val_loss: 0.3306 - val_pos_accuracy: 0.6800\n",
      "Epoch 1458/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0865 - pos_accuracy: 0.8727 - val_loss: 0.3320 - val_pos_accuracy: 0.6825\n",
      "Epoch 1459/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0864 - pos_accuracy: 0.8766 - val_loss: 0.3301 - val_pos_accuracy: 0.6875\n",
      "Epoch 1460/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0864 - pos_accuracy: 0.8771 - val_loss: 0.3303 - val_pos_accuracy: 0.6825\n",
      "Epoch 1461/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0861 - pos_accuracy: 0.8773 - val_loss: 0.3295 - val_pos_accuracy: 0.6825\n",
      "Epoch 1462/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0863 - pos_accuracy: 0.8825 - val_loss: 0.3305 - val_pos_accuracy: 0.6775\n",
      "Epoch 1463/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0859 - pos_accuracy: 0.8802 - val_loss: 0.3302 - val_pos_accuracy: 0.6825\n",
      "Epoch 1464/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0858 - pos_accuracy: 0.8779 - val_loss: 0.3311 - val_pos_accuracy: 0.6750\n",
      "Epoch 1465/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0859 - pos_accuracy: 0.8798 - val_loss: 0.3305 - val_pos_accuracy: 0.6775\n",
      "Epoch 1466/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0856 - pos_accuracy: 0.8795 - val_loss: 0.3309 - val_pos_accuracy: 0.6775\n",
      "Epoch 1467/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0857 - pos_accuracy: 0.8756 - val_loss: 0.3308 - val_pos_accuracy: 0.6775\n",
      "Epoch 1468/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0857 - pos_accuracy: 0.8741 - val_loss: 0.3310 - val_pos_accuracy: 0.6800\n",
      "Epoch 1469/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0854 - pos_accuracy: 0.8774 - val_loss: 0.3301 - val_pos_accuracy: 0.6775\n",
      "Epoch 1470/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0854 - pos_accuracy: 0.8729 - val_loss: 0.3302 - val_pos_accuracy: 0.6775\n",
      "Epoch 1471/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0853 - pos_accuracy: 0.8806 - val_loss: 0.3294 - val_pos_accuracy: 0.6725\n",
      "Epoch 1472/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0851 - pos_accuracy: 0.8743 - val_loss: 0.3302 - val_pos_accuracy: 0.6825\n",
      "Epoch 1473/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0852 - pos_accuracy: 0.8812 - val_loss: 0.3299 - val_pos_accuracy: 0.6825\n",
      "Epoch 1474/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0850 - pos_accuracy: 0.8721 - val_loss: 0.3296 - val_pos_accuracy: 0.6825\n",
      "Epoch 1475/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0851 - pos_accuracy: 0.8813 - val_loss: 0.3301 - val_pos_accuracy: 0.6825\n",
      "Epoch 1476/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0853 - pos_accuracy: 0.8781 - val_loss: 0.3293 - val_pos_accuracy: 0.6825\n",
      "Epoch 1477/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0853 - pos_accuracy: 0.8751 - val_loss: 0.3295 - val_pos_accuracy: 0.6750\n",
      "Epoch 1478/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0848 - pos_accuracy: 0.8764 - val_loss: 0.3295 - val_pos_accuracy: 0.6825\n",
      "Epoch 1479/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0852 - pos_accuracy: 0.8814 - val_loss: 0.3292 - val_pos_accuracy: 0.6775\n",
      "Epoch 1480/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0847 - pos_accuracy: 0.8793 - val_loss: 0.3303 - val_pos_accuracy: 0.6800\n",
      "Epoch 1481/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0845 - pos_accuracy: 0.8812 - val_loss: 0.3297 - val_pos_accuracy: 0.6850\n",
      "Epoch 1482/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0846 - pos_accuracy: 0.8843 - val_loss: 0.3304 - val_pos_accuracy: 0.6875\n",
      "Epoch 1483/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0845 - pos_accuracy: 0.8831 - val_loss: 0.3297 - val_pos_accuracy: 0.6825\n",
      "Epoch 1484/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0844 - pos_accuracy: 0.8780 - val_loss: 0.3298 - val_pos_accuracy: 0.6825\n",
      "Epoch 1485/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0843 - pos_accuracy: 0.8816 - val_loss: 0.3290 - val_pos_accuracy: 0.6825\n",
      "Epoch 1486/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0841 - pos_accuracy: 0.8815 - val_loss: 0.3290 - val_pos_accuracy: 0.6775\n",
      "Epoch 1487/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0841 - pos_accuracy: 0.8801 - val_loss: 0.3281 - val_pos_accuracy: 0.6775\n",
      "Epoch 1488/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0844 - pos_accuracy: 0.8746 - val_loss: 0.3289 - val_pos_accuracy: 0.6800\n",
      "Epoch 1489/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0842 - pos_accuracy: 0.8833 - val_loss: 0.3283 - val_pos_accuracy: 0.6825\n",
      "Epoch 1490/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0840 - pos_accuracy: 0.8755 - val_loss: 0.3288 - val_pos_accuracy: 0.6800\n",
      "Epoch 1491/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0837 - pos_accuracy: 0.8775 - val_loss: 0.3299 - val_pos_accuracy: 0.6800\n",
      "Epoch 1492/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0836 - pos_accuracy: 0.8751 - val_loss: 0.3285 - val_pos_accuracy: 0.6775\n",
      "Epoch 1493/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0838 - pos_accuracy: 0.8783 - val_loss: 0.3284 - val_pos_accuracy: 0.6825\n",
      "Epoch 1494/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0836 - pos_accuracy: 0.8763 - val_loss: 0.3288 - val_pos_accuracy: 0.6775\n",
      "Epoch 1495/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0836 - pos_accuracy: 0.8794 - val_loss: 0.3282 - val_pos_accuracy: 0.6825\n",
      "Epoch 1496/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0835 - pos_accuracy: 0.8732 - val_loss: 0.3284 - val_pos_accuracy: 0.6875\n",
      "Epoch 1497/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0835 - pos_accuracy: 0.8760 - val_loss: 0.3284 - val_pos_accuracy: 0.6850\n",
      "Epoch 1498/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0834 - pos_accuracy: 0.8779 - val_loss: 0.3283 - val_pos_accuracy: 0.6875\n",
      "Epoch 1499/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0833 - pos_accuracy: 0.8802 - val_loss: 0.3294 - val_pos_accuracy: 0.6900\n",
      "Epoch 1500/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0835 - pos_accuracy: 0.8831 - val_loss: 0.3295 - val_pos_accuracy: 0.6850\n",
      "Epoch 1501/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0831 - pos_accuracy: 0.8834 - val_loss: 0.3286 - val_pos_accuracy: 0.6850\n",
      "Epoch 1502/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0828 - pos_accuracy: 0.8815 - val_loss: 0.3283 - val_pos_accuracy: 0.6850\n",
      "Epoch 1503/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0828 - pos_accuracy: 0.8811 - val_loss: 0.3275 - val_pos_accuracy: 0.6825\n",
      "Epoch 1504/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0828 - pos_accuracy: 0.8783 - val_loss: 0.3287 - val_pos_accuracy: 0.6900\n",
      "Epoch 1505/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0830 - pos_accuracy: 0.8754 - val_loss: 0.3284 - val_pos_accuracy: 0.6900\n",
      "Epoch 1506/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0828 - pos_accuracy: 0.8830 - val_loss: 0.3270 - val_pos_accuracy: 0.6925\n",
      "Epoch 1507/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0826 - pos_accuracy: 0.8799 - val_loss: 0.3282 - val_pos_accuracy: 0.6900\n",
      "Epoch 1508/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0824 - pos_accuracy: 0.8825 - val_loss: 0.3273 - val_pos_accuracy: 0.6900\n",
      "Epoch 1509/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0826 - pos_accuracy: 0.8820 - val_loss: 0.3268 - val_pos_accuracy: 0.6850\n",
      "Epoch 1510/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0826 - pos_accuracy: 0.8835 - val_loss: 0.3273 - val_pos_accuracy: 0.6825\n",
      "Epoch 1511/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0824 - pos_accuracy: 0.8779 - val_loss: 0.3283 - val_pos_accuracy: 0.6900\n",
      "Epoch 1512/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0822 - pos_accuracy: 0.8846 - val_loss: 0.3274 - val_pos_accuracy: 0.6825\n",
      "Epoch 1513/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0821 - pos_accuracy: 0.8825 - val_loss: 0.3268 - val_pos_accuracy: 0.6825\n",
      "Epoch 1514/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0820 - pos_accuracy: 0.8832 - val_loss: 0.3269 - val_pos_accuracy: 0.6850\n",
      "Epoch 1515/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0820 - pos_accuracy: 0.8826 - val_loss: 0.3267 - val_pos_accuracy: 0.6825\n",
      "Epoch 1516/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0819 - pos_accuracy: 0.8843 - val_loss: 0.3271 - val_pos_accuracy: 0.6900\n",
      "Epoch 1517/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0818 - pos_accuracy: 0.8841 - val_loss: 0.3267 - val_pos_accuracy: 0.6825\n",
      "Epoch 1518/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0817 - pos_accuracy: 0.8823 - val_loss: 0.3264 - val_pos_accuracy: 0.6900\n",
      "Epoch 1519/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0815 - pos_accuracy: 0.8837 - val_loss: 0.3267 - val_pos_accuracy: 0.6900\n",
      "Epoch 1520/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0815 - pos_accuracy: 0.8834 - val_loss: 0.3266 - val_pos_accuracy: 0.6875\n",
      "Epoch 1521/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0815 - pos_accuracy: 0.8823 - val_loss: 0.3269 - val_pos_accuracy: 0.6875\n",
      "Epoch 1522/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0814 - pos_accuracy: 0.8836 - val_loss: 0.3274 - val_pos_accuracy: 0.6900\n",
      "Epoch 1523/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0814 - pos_accuracy: 0.8813 - val_loss: 0.3269 - val_pos_accuracy: 0.6900\n",
      "Epoch 1524/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0814 - pos_accuracy: 0.8831 - val_loss: 0.3271 - val_pos_accuracy: 0.6975\n",
      "Epoch 1525/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0812 - pos_accuracy: 0.8849 - val_loss: 0.3260 - val_pos_accuracy: 0.6875\n",
      "Epoch 1526/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0813 - pos_accuracy: 0.8860 - val_loss: 0.3263 - val_pos_accuracy: 0.6900\n",
      "Epoch 1527/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0812 - pos_accuracy: 0.8849 - val_loss: 0.3257 - val_pos_accuracy: 0.6825\n",
      "Epoch 1528/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0812 - pos_accuracy: 0.8796 - val_loss: 0.3264 - val_pos_accuracy: 0.6875\n",
      "Epoch 1529/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0809 - pos_accuracy: 0.8842 - val_loss: 0.3262 - val_pos_accuracy: 0.6900\n",
      "Epoch 1530/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0810 - pos_accuracy: 0.8881 - val_loss: 0.3251 - val_pos_accuracy: 0.6875\n",
      "Epoch 1531/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0811 - pos_accuracy: 0.8833 - val_loss: 0.3257 - val_pos_accuracy: 0.6900\n",
      "Epoch 1532/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0809 - pos_accuracy: 0.8835 - val_loss: 0.3250 - val_pos_accuracy: 0.6950\n",
      "Epoch 1533/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0809 - pos_accuracy: 0.8860 - val_loss: 0.3250 - val_pos_accuracy: 0.6975\n",
      "Epoch 1534/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0808 - pos_accuracy: 0.8855 - val_loss: 0.3251 - val_pos_accuracy: 0.6875\n",
      "Epoch 1535/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0806 - pos_accuracy: 0.8804 - val_loss: 0.3257 - val_pos_accuracy: 0.7025\n",
      "Epoch 1536/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0806 - pos_accuracy: 0.8817 - val_loss: 0.3249 - val_pos_accuracy: 0.6900\n",
      "Epoch 1537/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0805 - pos_accuracy: 0.8846 - val_loss: 0.3248 - val_pos_accuracy: 0.6975\n",
      "Epoch 1538/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0804 - pos_accuracy: 0.8800 - val_loss: 0.3247 - val_pos_accuracy: 0.6950\n",
      "Epoch 1539/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0805 - pos_accuracy: 0.8848 - val_loss: 0.3246 - val_pos_accuracy: 0.6950\n",
      "Epoch 1540/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0805 - pos_accuracy: 0.8837 - val_loss: 0.3242 - val_pos_accuracy: 0.6950\n",
      "Epoch 1541/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0805 - pos_accuracy: 0.8841 - val_loss: 0.3243 - val_pos_accuracy: 0.6950\n",
      "Epoch 1542/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0803 - pos_accuracy: 0.8869 - val_loss: 0.3249 - val_pos_accuracy: 0.6925\n",
      "Epoch 1543/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0799 - pos_accuracy: 0.8848 - val_loss: 0.3248 - val_pos_accuracy: 0.6900\n",
      "Epoch 1544/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0800 - pos_accuracy: 0.8821 - val_loss: 0.3258 - val_pos_accuracy: 0.6950\n",
      "Epoch 1545/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0798 - pos_accuracy: 0.8850 - val_loss: 0.3247 - val_pos_accuracy: 0.6900\n",
      "Epoch 1546/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0797 - pos_accuracy: 0.8841 - val_loss: 0.3247 - val_pos_accuracy: 0.6825\n",
      "Epoch 1547/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0797 - pos_accuracy: 0.8848 - val_loss: 0.3250 - val_pos_accuracy: 0.6950\n",
      "Epoch 1548/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0795 - pos_accuracy: 0.8855 - val_loss: 0.3243 - val_pos_accuracy: 0.6900\n",
      "Epoch 1549/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0796 - pos_accuracy: 0.8864 - val_loss: 0.3237 - val_pos_accuracy: 0.7000\n",
      "Epoch 1550/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0795 - pos_accuracy: 0.8858 - val_loss: 0.3244 - val_pos_accuracy: 0.6950\n",
      "Epoch 1551/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0795 - pos_accuracy: 0.8918 - val_loss: 0.3238 - val_pos_accuracy: 0.6925\n",
      "Epoch 1552/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0794 - pos_accuracy: 0.8870 - val_loss: 0.3243 - val_pos_accuracy: 0.6925\n",
      "Epoch 1553/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0793 - pos_accuracy: 0.8900 - val_loss: 0.3231 - val_pos_accuracy: 0.6925\n",
      "Epoch 1554/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0792 - pos_accuracy: 0.8893 - val_loss: 0.3241 - val_pos_accuracy: 0.6875\n",
      "Epoch 1555/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0792 - pos_accuracy: 0.8867 - val_loss: 0.3234 - val_pos_accuracy: 0.6900\n",
      "Epoch 1556/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0791 - pos_accuracy: 0.8824 - val_loss: 0.3243 - val_pos_accuracy: 0.6950\n",
      "Epoch 1557/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0789 - pos_accuracy: 0.8877 - val_loss: 0.3237 - val_pos_accuracy: 0.6875\n",
      "Epoch 1558/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0788 - pos_accuracy: 0.8841 - val_loss: 0.3235 - val_pos_accuracy: 0.7000\n",
      "Epoch 1559/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0789 - pos_accuracy: 0.8860 - val_loss: 0.3239 - val_pos_accuracy: 0.6950\n",
      "Epoch 1560/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0788 - pos_accuracy: 0.8864 - val_loss: 0.3235 - val_pos_accuracy: 0.6875\n",
      "Epoch 1561/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0786 - pos_accuracy: 0.8818 - val_loss: 0.3234 - val_pos_accuracy: 0.6950\n",
      "Epoch 1562/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0786 - pos_accuracy: 0.8830 - val_loss: 0.3233 - val_pos_accuracy: 0.6925\n",
      "Epoch 1563/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0786 - pos_accuracy: 0.8892 - val_loss: 0.3238 - val_pos_accuracy: 0.6975\n",
      "Epoch 1564/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0784 - pos_accuracy: 0.8880 - val_loss: 0.3238 - val_pos_accuracy: 0.6925\n",
      "Epoch 1565/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0784 - pos_accuracy: 0.8888 - val_loss: 0.3240 - val_pos_accuracy: 0.7000\n",
      "Epoch 1566/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0783 - pos_accuracy: 0.8902 - val_loss: 0.3232 - val_pos_accuracy: 0.6900\n",
      "Epoch 1567/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0784 - pos_accuracy: 0.8891 - val_loss: 0.3230 - val_pos_accuracy: 0.6900\n",
      "Epoch 1568/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0784 - pos_accuracy: 0.8834 - val_loss: 0.3244 - val_pos_accuracy: 0.7000\n",
      "Epoch 1569/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0782 - pos_accuracy: 0.8908 - val_loss: 0.3227 - val_pos_accuracy: 0.6875\n",
      "Epoch 1570/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0784 - pos_accuracy: 0.8879 - val_loss: 0.3227 - val_pos_accuracy: 0.6850\n",
      "Epoch 1571/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0783 - pos_accuracy: 0.8877 - val_loss: 0.3229 - val_pos_accuracy: 0.6900\n",
      "Epoch 1572/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0784 - pos_accuracy: 0.8938 - val_loss: 0.3221 - val_pos_accuracy: 0.6950\n",
      "Epoch 1573/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0782 - pos_accuracy: 0.8891 - val_loss: 0.3235 - val_pos_accuracy: 0.6925\n",
      "Epoch 1574/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0778 - pos_accuracy: 0.8851 - val_loss: 0.3226 - val_pos_accuracy: 0.6925\n",
      "Epoch 1575/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0778 - pos_accuracy: 0.8904 - val_loss: 0.3240 - val_pos_accuracy: 0.6975\n",
      "Epoch 1576/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0781 - pos_accuracy: 0.8886 - val_loss: 0.3246 - val_pos_accuracy: 0.7025\n",
      "Epoch 1577/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0778 - pos_accuracy: 0.9001 - val_loss: 0.3217 - val_pos_accuracy: 0.7000\n",
      "Epoch 1578/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0778 - pos_accuracy: 0.8879 - val_loss: 0.3228 - val_pos_accuracy: 0.6950\n",
      "Epoch 1579/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0773 - pos_accuracy: 0.8870 - val_loss: 0.3224 - val_pos_accuracy: 0.6975\n",
      "Epoch 1580/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0778 - pos_accuracy: 0.8926 - val_loss: 0.3225 - val_pos_accuracy: 0.7025\n",
      "Epoch 1581/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0774 - pos_accuracy: 0.8896 - val_loss: 0.3224 - val_pos_accuracy: 0.7025\n",
      "Epoch 1582/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0772 - pos_accuracy: 0.8895 - val_loss: 0.3230 - val_pos_accuracy: 0.7000\n",
      "Epoch 1583/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0773 - pos_accuracy: 0.8942 - val_loss: 0.3212 - val_pos_accuracy: 0.7025\n",
      "Epoch 1584/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0772 - pos_accuracy: 0.8875 - val_loss: 0.3227 - val_pos_accuracy: 0.7000\n",
      "Epoch 1585/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0772 - pos_accuracy: 0.8939 - val_loss: 0.3217 - val_pos_accuracy: 0.7050\n",
      "Epoch 1586/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0770 - pos_accuracy: 0.8932 - val_loss: 0.3221 - val_pos_accuracy: 0.7025\n",
      "Epoch 1587/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0769 - pos_accuracy: 0.8912 - val_loss: 0.3216 - val_pos_accuracy: 0.7025\n",
      "Epoch 1588/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0768 - pos_accuracy: 0.8925 - val_loss: 0.3219 - val_pos_accuracy: 0.6950\n",
      "Epoch 1589/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0767 - pos_accuracy: 0.8917 - val_loss: 0.3215 - val_pos_accuracy: 0.6950\n",
      "Epoch 1590/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0766 - pos_accuracy: 0.8921 - val_loss: 0.3224 - val_pos_accuracy: 0.7000\n",
      "Epoch 1591/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0767 - pos_accuracy: 0.8907 - val_loss: 0.3227 - val_pos_accuracy: 0.6950\n",
      "Epoch 1592/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0769 - pos_accuracy: 0.8923 - val_loss: 0.3220 - val_pos_accuracy: 0.6975\n",
      "Epoch 1593/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0765 - pos_accuracy: 0.8939 - val_loss: 0.3216 - val_pos_accuracy: 0.6950\n",
      "Epoch 1594/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0764 - pos_accuracy: 0.8894 - val_loss: 0.3216 - val_pos_accuracy: 0.6950\n",
      "Epoch 1595/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0763 - pos_accuracy: 0.8902 - val_loss: 0.3217 - val_pos_accuracy: 0.7025\n",
      "Epoch 1596/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0765 - pos_accuracy: 0.8882 - val_loss: 0.3218 - val_pos_accuracy: 0.7050\n",
      "Epoch 1597/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0763 - pos_accuracy: 0.8975 - val_loss: 0.3212 - val_pos_accuracy: 0.6975\n",
      "Epoch 1598/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0764 - pos_accuracy: 0.8878 - val_loss: 0.3213 - val_pos_accuracy: 0.7025\n",
      "Epoch 1599/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0762 - pos_accuracy: 0.8971 - val_loss: 0.3206 - val_pos_accuracy: 0.7000\n",
      "Epoch 1600/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0761 - pos_accuracy: 0.8912 - val_loss: 0.3217 - val_pos_accuracy: 0.7000\n",
      "Epoch 1601/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0761 - pos_accuracy: 0.8965 - val_loss: 0.3211 - val_pos_accuracy: 0.6950\n",
      "Epoch 1602/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0761 - pos_accuracy: 0.8943 - val_loss: 0.3219 - val_pos_accuracy: 0.7000\n",
      "Epoch 1603/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0760 - pos_accuracy: 0.8907 - val_loss: 0.3214 - val_pos_accuracy: 0.7025\n",
      "Epoch 1604/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0757 - pos_accuracy: 0.8892 - val_loss: 0.3215 - val_pos_accuracy: 0.7025\n",
      "Epoch 1605/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0758 - pos_accuracy: 0.8967 - val_loss: 0.3213 - val_pos_accuracy: 0.6925\n",
      "Epoch 1606/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0756 - pos_accuracy: 0.8921 - val_loss: 0.3206 - val_pos_accuracy: 0.7025\n",
      "Epoch 1607/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0758 - pos_accuracy: 0.8919 - val_loss: 0.3205 - val_pos_accuracy: 0.7000\n",
      "Epoch 1608/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0761 - pos_accuracy: 0.8914 - val_loss: 0.3208 - val_pos_accuracy: 0.7025\n",
      "Epoch 1609/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0755 - pos_accuracy: 0.8927 - val_loss: 0.3217 - val_pos_accuracy: 0.7025\n",
      "Epoch 1610/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0756 - pos_accuracy: 0.8894 - val_loss: 0.3216 - val_pos_accuracy: 0.7025\n",
      "Epoch 1611/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0753 - pos_accuracy: 0.8966 - val_loss: 0.3211 - val_pos_accuracy: 0.7050\n",
      "Epoch 1612/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0752 - pos_accuracy: 0.8955 - val_loss: 0.3203 - val_pos_accuracy: 0.7025\n",
      "Epoch 1613/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0752 - pos_accuracy: 0.8907 - val_loss: 0.3198 - val_pos_accuracy: 0.7000\n",
      "Epoch 1614/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0751 - pos_accuracy: 0.8951 - val_loss: 0.3204 - val_pos_accuracy: 0.6975\n",
      "Epoch 1615/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0749 - pos_accuracy: 0.8933 - val_loss: 0.3206 - val_pos_accuracy: 0.7025\n",
      "Epoch 1616/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0750 - pos_accuracy: 0.8950 - val_loss: 0.3201 - val_pos_accuracy: 0.6975\n",
      "Epoch 1617/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0749 - pos_accuracy: 0.8893 - val_loss: 0.3200 - val_pos_accuracy: 0.7000\n",
      "Epoch 1618/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0748 - pos_accuracy: 0.8926 - val_loss: 0.3209 - val_pos_accuracy: 0.7050\n",
      "Epoch 1619/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0748 - pos_accuracy: 0.8928 - val_loss: 0.3196 - val_pos_accuracy: 0.7075\n",
      "Epoch 1620/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0748 - pos_accuracy: 0.8960 - val_loss: 0.3201 - val_pos_accuracy: 0.7050\n",
      "Epoch 1621/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0747 - pos_accuracy: 0.8934 - val_loss: 0.3206 - val_pos_accuracy: 0.7025\n",
      "Epoch 1622/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0747 - pos_accuracy: 0.8902 - val_loss: 0.3210 - val_pos_accuracy: 0.7125\n",
      "Epoch 1623/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0747 - pos_accuracy: 0.8946 - val_loss: 0.3208 - val_pos_accuracy: 0.7075\n",
      "Epoch 1624/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0746 - pos_accuracy: 0.8953 - val_loss: 0.3191 - val_pos_accuracy: 0.7050\n",
      "Epoch 1625/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0746 - pos_accuracy: 0.8927 - val_loss: 0.3200 - val_pos_accuracy: 0.7100\n",
      "Epoch 1626/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0744 - pos_accuracy: 0.8957 - val_loss: 0.3202 - val_pos_accuracy: 0.7075\n",
      "Epoch 1627/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0743 - pos_accuracy: 0.8987 - val_loss: 0.3188 - val_pos_accuracy: 0.7025\n",
      "Epoch 1628/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0743 - pos_accuracy: 0.8916 - val_loss: 0.3205 - val_pos_accuracy: 0.7125\n",
      "Epoch 1629/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0743 - pos_accuracy: 0.8990 - val_loss: 0.3190 - val_pos_accuracy: 0.7025\n",
      "Epoch 1630/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0742 - pos_accuracy: 0.8933 - val_loss: 0.3190 - val_pos_accuracy: 0.7100\n",
      "Epoch 1631/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0741 - pos_accuracy: 0.8934 - val_loss: 0.3188 - val_pos_accuracy: 0.7075\n",
      "Epoch 1632/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0739 - pos_accuracy: 0.8946 - val_loss: 0.3193 - val_pos_accuracy: 0.7050\n",
      "Epoch 1633/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0739 - pos_accuracy: 0.8920 - val_loss: 0.3189 - val_pos_accuracy: 0.7125\n",
      "Epoch 1634/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0738 - pos_accuracy: 0.8957 - val_loss: 0.3190 - val_pos_accuracy: 0.7075\n",
      "Epoch 1635/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0739 - pos_accuracy: 0.8876 - val_loss: 0.3194 - val_pos_accuracy: 0.7050\n",
      "Epoch 1636/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0738 - pos_accuracy: 0.8918 - val_loss: 0.3190 - val_pos_accuracy: 0.7025\n",
      "Epoch 1637/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0738 - pos_accuracy: 0.8940 - val_loss: 0.3189 - val_pos_accuracy: 0.7050\n",
      "Epoch 1638/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0737 - pos_accuracy: 0.8911 - val_loss: 0.3187 - val_pos_accuracy: 0.7150\n",
      "Epoch 1639/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0737 - pos_accuracy: 0.8953 - val_loss: 0.3187 - val_pos_accuracy: 0.7150\n",
      "Epoch 1640/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0735 - pos_accuracy: 0.8958 - val_loss: 0.3186 - val_pos_accuracy: 0.7100\n",
      "Epoch 1641/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0733 - pos_accuracy: 0.8950 - val_loss: 0.3189 - val_pos_accuracy: 0.7075\n",
      "Epoch 1642/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0733 - pos_accuracy: 0.8957 - val_loss: 0.3186 - val_pos_accuracy: 0.7075\n",
      "Epoch 1643/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0733 - pos_accuracy: 0.8940 - val_loss: 0.3193 - val_pos_accuracy: 0.7125\n",
      "Epoch 1644/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0733 - pos_accuracy: 0.8981 - val_loss: 0.3191 - val_pos_accuracy: 0.7025\n",
      "Epoch 1645/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0731 - pos_accuracy: 0.8917 - val_loss: 0.3190 - val_pos_accuracy: 0.7025\n",
      "Epoch 1646/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0732 - pos_accuracy: 0.8930 - val_loss: 0.3185 - val_pos_accuracy: 0.7025\n",
      "Epoch 1647/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0729 - pos_accuracy: 0.8995 - val_loss: 0.3189 - val_pos_accuracy: 0.7050\n",
      "Epoch 1648/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0730 - pos_accuracy: 0.8908 - val_loss: 0.3182 - val_pos_accuracy: 0.7050\n",
      "Epoch 1649/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0730 - pos_accuracy: 0.8956 - val_loss: 0.3181 - val_pos_accuracy: 0.7100\n",
      "Epoch 1650/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0728 - pos_accuracy: 0.8984 - val_loss: 0.3180 - val_pos_accuracy: 0.7125\n",
      "Epoch 1651/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0727 - pos_accuracy: 0.8930 - val_loss: 0.3184 - val_pos_accuracy: 0.7125\n",
      "Epoch 1652/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0728 - pos_accuracy: 0.8932 - val_loss: 0.3175 - val_pos_accuracy: 0.7075\n",
      "Epoch 1653/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0728 - pos_accuracy: 0.8946 - val_loss: 0.3182 - val_pos_accuracy: 0.7100\n",
      "Epoch 1654/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0727 - pos_accuracy: 0.8984 - val_loss: 0.3181 - val_pos_accuracy: 0.7075\n",
      "Epoch 1655/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0727 - pos_accuracy: 0.8975 - val_loss: 0.3175 - val_pos_accuracy: 0.7050\n",
      "Epoch 1656/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0725 - pos_accuracy: 0.8971 - val_loss: 0.3180 - val_pos_accuracy: 0.7075\n",
      "Epoch 1657/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0725 - pos_accuracy: 0.8966 - val_loss: 0.3179 - val_pos_accuracy: 0.7075\n",
      "Epoch 1658/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0722 - pos_accuracy: 0.9025 - val_loss: 0.3178 - val_pos_accuracy: 0.7050\n",
      "Epoch 1659/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0724 - pos_accuracy: 0.8944 - val_loss: 0.3175 - val_pos_accuracy: 0.7075\n",
      "Epoch 1660/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0727 - pos_accuracy: 0.8966 - val_loss: 0.3194 - val_pos_accuracy: 0.7175\n",
      "Epoch 1661/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0724 - pos_accuracy: 0.9002 - val_loss: 0.3173 - val_pos_accuracy: 0.7075\n",
      "Epoch 1662/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0721 - pos_accuracy: 0.9008 - val_loss: 0.3180 - val_pos_accuracy: 0.7200\n",
      "Epoch 1663/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0720 - pos_accuracy: 0.8981 - val_loss: 0.3172 - val_pos_accuracy: 0.7125\n",
      "Epoch 1664/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0719 - pos_accuracy: 0.8988 - val_loss: 0.3177 - val_pos_accuracy: 0.7225\n",
      "Epoch 1665/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0721 - pos_accuracy: 0.8976 - val_loss: 0.3176 - val_pos_accuracy: 0.7225\n",
      "Epoch 1666/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0720 - pos_accuracy: 0.8994 - val_loss: 0.3182 - val_pos_accuracy: 0.7225\n",
      "Epoch 1667/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0718 - pos_accuracy: 0.9048 - val_loss: 0.3165 - val_pos_accuracy: 0.7100\n",
      "Epoch 1668/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0719 - pos_accuracy: 0.8976 - val_loss: 0.3160 - val_pos_accuracy: 0.7125\n",
      "Epoch 1669/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0723 - pos_accuracy: 0.8976 - val_loss: 0.3159 - val_pos_accuracy: 0.7100\n",
      "Epoch 1670/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0721 - pos_accuracy: 0.8993 - val_loss: 0.3163 - val_pos_accuracy: 0.7125\n",
      "Epoch 1671/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0716 - pos_accuracy: 0.8932 - val_loss: 0.3164 - val_pos_accuracy: 0.7075\n",
      "Epoch 1672/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0715 - pos_accuracy: 0.8944 - val_loss: 0.3159 - val_pos_accuracy: 0.7100\n",
      "Epoch 1673/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0714 - pos_accuracy: 0.8961 - val_loss: 0.3165 - val_pos_accuracy: 0.7100\n",
      "Epoch 1674/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0713 - pos_accuracy: 0.8958 - val_loss: 0.3164 - val_pos_accuracy: 0.7175\n",
      "Epoch 1675/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0714 - pos_accuracy: 0.8991 - val_loss: 0.3162 - val_pos_accuracy: 0.7125\n",
      "Epoch 1676/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0712 - pos_accuracy: 0.8987 - val_loss: 0.3162 - val_pos_accuracy: 0.7125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1677/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0711 - pos_accuracy: 0.8992 - val_loss: 0.3161 - val_pos_accuracy: 0.7150\n",
      "Epoch 1678/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0711 - pos_accuracy: 0.8987 - val_loss: 0.3161 - val_pos_accuracy: 0.7100\n",
      "Epoch 1679/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0712 - pos_accuracy: 0.8982 - val_loss: 0.3165 - val_pos_accuracy: 0.7100\n",
      "Epoch 1680/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0711 - pos_accuracy: 0.8959 - val_loss: 0.3172 - val_pos_accuracy: 0.7175\n",
      "Epoch 1681/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0710 - pos_accuracy: 0.8985 - val_loss: 0.3157 - val_pos_accuracy: 0.7125\n",
      "Epoch 1682/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0709 - pos_accuracy: 0.8973 - val_loss: 0.3160 - val_pos_accuracy: 0.7125\n",
      "Epoch 1683/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0713 - pos_accuracy: 0.8947 - val_loss: 0.3153 - val_pos_accuracy: 0.7075\n",
      "Epoch 1684/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0711 - pos_accuracy: 0.8994 - val_loss: 0.3153 - val_pos_accuracy: 0.7100\n",
      "Epoch 1685/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0708 - pos_accuracy: 0.8964 - val_loss: 0.3163 - val_pos_accuracy: 0.7275\n",
      "Epoch 1686/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0709 - pos_accuracy: 0.9025 - val_loss: 0.3167 - val_pos_accuracy: 0.7250\n",
      "Epoch 1687/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0707 - pos_accuracy: 0.9033 - val_loss: 0.3164 - val_pos_accuracy: 0.7175\n",
      "Epoch 1688/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0707 - pos_accuracy: 0.9036 - val_loss: 0.3163 - val_pos_accuracy: 0.7225\n",
      "Epoch 1689/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0705 - pos_accuracy: 0.9021 - val_loss: 0.3152 - val_pos_accuracy: 0.7150\n",
      "Epoch 1690/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0705 - pos_accuracy: 0.9016 - val_loss: 0.3161 - val_pos_accuracy: 0.7200\n",
      "Epoch 1691/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0704 - pos_accuracy: 0.9027 - val_loss: 0.3155 - val_pos_accuracy: 0.7200\n",
      "Epoch 1692/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0704 - pos_accuracy: 0.8975 - val_loss: 0.3163 - val_pos_accuracy: 0.7175\n",
      "Epoch 1693/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0703 - pos_accuracy: 0.9032 - val_loss: 0.3159 - val_pos_accuracy: 0.7200\n",
      "Epoch 1694/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0702 - pos_accuracy: 0.8998 - val_loss: 0.3159 - val_pos_accuracy: 0.7200\n",
      "Epoch 1695/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0702 - pos_accuracy: 0.9058 - val_loss: 0.3158 - val_pos_accuracy: 0.7200\n",
      "Epoch 1696/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0701 - pos_accuracy: 0.9068 - val_loss: 0.3150 - val_pos_accuracy: 0.7175\n",
      "Epoch 1697/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0701 - pos_accuracy: 0.8998 - val_loss: 0.3152 - val_pos_accuracy: 0.7150\n",
      "Epoch 1698/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0700 - pos_accuracy: 0.9011 - val_loss: 0.3152 - val_pos_accuracy: 0.7200\n",
      "Epoch 1699/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0698 - pos_accuracy: 0.9025 - val_loss: 0.3157 - val_pos_accuracy: 0.7250\n",
      "Epoch 1700/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0698 - pos_accuracy: 0.9058 - val_loss: 0.3153 - val_pos_accuracy: 0.7200\n",
      "Epoch 1701/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0698 - pos_accuracy: 0.9064 - val_loss: 0.3150 - val_pos_accuracy: 0.7225\n",
      "Epoch 1702/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0696 - pos_accuracy: 0.9060 - val_loss: 0.3143 - val_pos_accuracy: 0.7200\n",
      "Epoch 1703/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0697 - pos_accuracy: 0.9009 - val_loss: 0.3150 - val_pos_accuracy: 0.7200\n",
      "Epoch 1704/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0697 - pos_accuracy: 0.9025 - val_loss: 0.3151 - val_pos_accuracy: 0.7225\n",
      "Epoch 1705/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0696 - pos_accuracy: 0.9045 - val_loss: 0.3148 - val_pos_accuracy: 0.7200\n",
      "Epoch 1706/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0694 - pos_accuracy: 0.9065 - val_loss: 0.3145 - val_pos_accuracy: 0.7150\n",
      "Epoch 1707/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0698 - pos_accuracy: 0.8991 - val_loss: 0.3144 - val_pos_accuracy: 0.7175\n",
      "Epoch 1708/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0696 - pos_accuracy: 0.8996 - val_loss: 0.3147 - val_pos_accuracy: 0.7175\n",
      "Epoch 1709/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0693 - pos_accuracy: 0.8990 - val_loss: 0.3157 - val_pos_accuracy: 0.7250\n",
      "Epoch 1710/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0693 - pos_accuracy: 0.9055 - val_loss: 0.3152 - val_pos_accuracy: 0.7250\n",
      "Epoch 1711/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0692 - pos_accuracy: 0.9036 - val_loss: 0.3145 - val_pos_accuracy: 0.7225\n",
      "Epoch 1712/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0692 - pos_accuracy: 0.9048 - val_loss: 0.3147 - val_pos_accuracy: 0.7250\n",
      "Epoch 1713/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0693 - pos_accuracy: 0.9065 - val_loss: 0.3134 - val_pos_accuracy: 0.7250\n",
      "Epoch 1714/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0691 - pos_accuracy: 0.9048 - val_loss: 0.3142 - val_pos_accuracy: 0.7250\n",
      "Epoch 1715/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0691 - pos_accuracy: 0.9081 - val_loss: 0.3140 - val_pos_accuracy: 0.7300\n",
      "Epoch 1716/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0691 - pos_accuracy: 0.9043 - val_loss: 0.3136 - val_pos_accuracy: 0.7200\n",
      "Epoch 1717/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0694 - pos_accuracy: 0.9006 - val_loss: 0.3131 - val_pos_accuracy: 0.7175\n",
      "Epoch 1718/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0691 - pos_accuracy: 0.9056 - val_loss: 0.3147 - val_pos_accuracy: 0.7300\n",
      "Epoch 1719/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0688 - pos_accuracy: 0.9039 - val_loss: 0.3134 - val_pos_accuracy: 0.7200\n",
      "Epoch 1720/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0689 - pos_accuracy: 0.9041 - val_loss: 0.3138 - val_pos_accuracy: 0.7225\n",
      "Epoch 1721/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0688 - pos_accuracy: 0.9086 - val_loss: 0.3130 - val_pos_accuracy: 0.7300\n",
      "Epoch 1722/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0689 - pos_accuracy: 0.9089 - val_loss: 0.3135 - val_pos_accuracy: 0.7225\n",
      "Epoch 1723/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0685 - pos_accuracy: 0.9042 - val_loss: 0.3141 - val_pos_accuracy: 0.7225\n",
      "Epoch 1724/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0685 - pos_accuracy: 0.9073 - val_loss: 0.3146 - val_pos_accuracy: 0.7200\n",
      "Epoch 1725/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0684 - pos_accuracy: 0.9090 - val_loss: 0.3139 - val_pos_accuracy: 0.7200\n",
      "Epoch 1726/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0683 - pos_accuracy: 0.9066 - val_loss: 0.3130 - val_pos_accuracy: 0.7200\n",
      "Epoch 1727/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0684 - pos_accuracy: 0.9066 - val_loss: 0.3130 - val_pos_accuracy: 0.7225\n",
      "Epoch 1728/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0683 - pos_accuracy: 0.9066 - val_loss: 0.3133 - val_pos_accuracy: 0.7200\n",
      "Epoch 1729/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0682 - pos_accuracy: 0.9080 - val_loss: 0.3133 - val_pos_accuracy: 0.7275\n",
      "Epoch 1730/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0681 - pos_accuracy: 0.9041 - val_loss: 0.3135 - val_pos_accuracy: 0.7300\n",
      "Epoch 1731/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0681 - pos_accuracy: 0.9049 - val_loss: 0.3135 - val_pos_accuracy: 0.7300\n",
      "Epoch 1732/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0681 - pos_accuracy: 0.9076 - val_loss: 0.3134 - val_pos_accuracy: 0.7275\n",
      "Epoch 1733/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0681 - pos_accuracy: 0.9064 - val_loss: 0.3133 - val_pos_accuracy: 0.7275\n",
      "Epoch 1734/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0680 - pos_accuracy: 0.9083 - val_loss: 0.3133 - val_pos_accuracy: 0.7275\n",
      "Epoch 1735/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0678 - pos_accuracy: 0.9077 - val_loss: 0.3136 - val_pos_accuracy: 0.7300\n",
      "Epoch 1736/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0678 - pos_accuracy: 0.9121 - val_loss: 0.3138 - val_pos_accuracy: 0.7325\n",
      "Epoch 1737/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0677 - pos_accuracy: 0.9106 - val_loss: 0.3133 - val_pos_accuracy: 0.7275\n",
      "Epoch 1738/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0677 - pos_accuracy: 0.9012 - val_loss: 0.3138 - val_pos_accuracy: 0.7300\n",
      "Epoch 1739/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0677 - pos_accuracy: 0.9038 - val_loss: 0.3130 - val_pos_accuracy: 0.7300\n",
      "Epoch 1740/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0676 - pos_accuracy: 0.9076 - val_loss: 0.3129 - val_pos_accuracy: 0.7325\n",
      "Epoch 1741/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0676 - pos_accuracy: 0.9074 - val_loss: 0.3125 - val_pos_accuracy: 0.7250\n",
      "Epoch 1742/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0675 - pos_accuracy: 0.9030 - val_loss: 0.3132 - val_pos_accuracy: 0.7275\n",
      "Epoch 1743/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0676 - pos_accuracy: 0.9062 - val_loss: 0.3124 - val_pos_accuracy: 0.7300\n",
      "Epoch 1744/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0674 - pos_accuracy: 0.9026 - val_loss: 0.3140 - val_pos_accuracy: 0.7325\n",
      "Epoch 1745/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0675 - pos_accuracy: 0.9085 - val_loss: 0.3133 - val_pos_accuracy: 0.7300\n",
      "Epoch 1746/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0675 - pos_accuracy: 0.9064 - val_loss: 0.3136 - val_pos_accuracy: 0.7250\n",
      "Epoch 1747/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0675 - pos_accuracy: 0.9044 - val_loss: 0.3136 - val_pos_accuracy: 0.7350\n",
      "Epoch 1748/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0673 - pos_accuracy: 0.9083 - val_loss: 0.3125 - val_pos_accuracy: 0.7325\n",
      "Epoch 1749/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0671 - pos_accuracy: 0.9096 - val_loss: 0.3122 - val_pos_accuracy: 0.7375\n",
      "Epoch 1750/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0672 - pos_accuracy: 0.9128 - val_loss: 0.3123 - val_pos_accuracy: 0.7325\n",
      "Epoch 1751/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0670 - pos_accuracy: 0.9113 - val_loss: 0.3124 - val_pos_accuracy: 0.7375\n",
      "Epoch 1752/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0669 - pos_accuracy: 0.9111 - val_loss: 0.3133 - val_pos_accuracy: 0.7325\n",
      "Epoch 1753/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0670 - pos_accuracy: 0.9076 - val_loss: 0.3131 - val_pos_accuracy: 0.7350\n",
      "Epoch 1754/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0669 - pos_accuracy: 0.9088 - val_loss: 0.3131 - val_pos_accuracy: 0.7350\n",
      "Epoch 1755/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0670 - pos_accuracy: 0.9095 - val_loss: 0.3134 - val_pos_accuracy: 0.7275\n",
      "Epoch 1756/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0668 - pos_accuracy: 0.9148 - val_loss: 0.3126 - val_pos_accuracy: 0.7350\n",
      "Epoch 1757/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0668 - pos_accuracy: 0.9151 - val_loss: 0.3129 - val_pos_accuracy: 0.7350\n",
      "Epoch 1758/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0668 - pos_accuracy: 0.9090 - val_loss: 0.3111 - val_pos_accuracy: 0.7325\n",
      "Epoch 1759/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0668 - pos_accuracy: 0.9118 - val_loss: 0.3120 - val_pos_accuracy: 0.7350\n",
      "Epoch 1760/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0664 - pos_accuracy: 0.9083 - val_loss: 0.3113 - val_pos_accuracy: 0.7375\n",
      "Epoch 1761/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0665 - pos_accuracy: 0.9111 - val_loss: 0.3124 - val_pos_accuracy: 0.7400\n",
      "Epoch 1762/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0664 - pos_accuracy: 0.9106 - val_loss: 0.3114 - val_pos_accuracy: 0.7400\n",
      "Epoch 1763/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0663 - pos_accuracy: 0.9111 - val_loss: 0.3109 - val_pos_accuracy: 0.7375\n",
      "Epoch 1764/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0662 - pos_accuracy: 0.9127 - val_loss: 0.3114 - val_pos_accuracy: 0.7400\n",
      "Epoch 1765/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0662 - pos_accuracy: 0.9094 - val_loss: 0.3114 - val_pos_accuracy: 0.7400\n",
      "Epoch 1766/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0662 - pos_accuracy: 0.9157 - val_loss: 0.3116 - val_pos_accuracy: 0.7400\n",
      "Epoch 1767/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0660 - pos_accuracy: 0.9162 - val_loss: 0.3114 - val_pos_accuracy: 0.7400\n",
      "Epoch 1768/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0659 - pos_accuracy: 0.9086 - val_loss: 0.3113 - val_pos_accuracy: 0.7375\n",
      "Epoch 1769/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0660 - pos_accuracy: 0.9119 - val_loss: 0.3119 - val_pos_accuracy: 0.7375\n",
      "Epoch 1770/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0659 - pos_accuracy: 0.9106 - val_loss: 0.3117 - val_pos_accuracy: 0.7325\n",
      "Epoch 1771/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0660 - pos_accuracy: 0.9117 - val_loss: 0.3108 - val_pos_accuracy: 0.7425\n",
      "Epoch 1772/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0659 - pos_accuracy: 0.9092 - val_loss: 0.3116 - val_pos_accuracy: 0.7400\n",
      "Epoch 1773/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0658 - pos_accuracy: 0.9104 - val_loss: 0.3116 - val_pos_accuracy: 0.7400\n",
      "Epoch 1774/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0657 - pos_accuracy: 0.9139 - val_loss: 0.3113 - val_pos_accuracy: 0.7350\n",
      "Epoch 1775/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0655 - pos_accuracy: 0.9135 - val_loss: 0.3112 - val_pos_accuracy: 0.7425\n",
      "Epoch 1776/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0656 - pos_accuracy: 0.9138 - val_loss: 0.3106 - val_pos_accuracy: 0.7425\n",
      "Epoch 1777/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0657 - pos_accuracy: 0.9201 - val_loss: 0.3106 - val_pos_accuracy: 0.7425\n",
      "Epoch 1778/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0656 - pos_accuracy: 0.9144 - val_loss: 0.3101 - val_pos_accuracy: 0.7450\n",
      "Epoch 1779/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0655 - pos_accuracy: 0.9126 - val_loss: 0.3103 - val_pos_accuracy: 0.7425\n",
      "Epoch 1780/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0653 - pos_accuracy: 0.9140 - val_loss: 0.3108 - val_pos_accuracy: 0.7400\n",
      "Epoch 1781/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0655 - pos_accuracy: 0.9130 - val_loss: 0.3103 - val_pos_accuracy: 0.7475\n",
      "Epoch 1782/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0654 - pos_accuracy: 0.9092 - val_loss: 0.3102 - val_pos_accuracy: 0.7425\n",
      "Epoch 1783/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0652 - pos_accuracy: 0.9161 - val_loss: 0.3109 - val_pos_accuracy: 0.7400\n",
      "Epoch 1784/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0652 - pos_accuracy: 0.9151 - val_loss: 0.3105 - val_pos_accuracy: 0.7425\n",
      "Epoch 1785/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0651 - pos_accuracy: 0.9137 - val_loss: 0.3104 - val_pos_accuracy: 0.7400\n",
      "Epoch 1786/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0651 - pos_accuracy: 0.9169 - val_loss: 0.3099 - val_pos_accuracy: 0.7375\n",
      "Epoch 1787/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0650 - pos_accuracy: 0.9162 - val_loss: 0.3107 - val_pos_accuracy: 0.7425\n",
      "Epoch 1788/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0650 - pos_accuracy: 0.9149 - val_loss: 0.3106 - val_pos_accuracy: 0.7400\n",
      "Epoch 1789/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0650 - pos_accuracy: 0.9127 - val_loss: 0.3104 - val_pos_accuracy: 0.7450\n",
      "Epoch 1790/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0649 - pos_accuracy: 0.9180 - val_loss: 0.3098 - val_pos_accuracy: 0.7425\n",
      "Epoch 1791/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0649 - pos_accuracy: 0.9127 - val_loss: 0.3102 - val_pos_accuracy: 0.7350\n",
      "Epoch 1792/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0648 - pos_accuracy: 0.9182 - val_loss: 0.3097 - val_pos_accuracy: 0.7350\n",
      "Epoch 1793/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0648 - pos_accuracy: 0.9194 - val_loss: 0.3095 - val_pos_accuracy: 0.7425\n",
      "Epoch 1794/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0648 - pos_accuracy: 0.9170 - val_loss: 0.3093 - val_pos_accuracy: 0.7450\n",
      "Epoch 1795/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0646 - pos_accuracy: 0.9142 - val_loss: 0.3100 - val_pos_accuracy: 0.7450\n",
      "Epoch 1796/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0647 - pos_accuracy: 0.9177 - val_loss: 0.3099 - val_pos_accuracy: 0.7425\n",
      "Epoch 1797/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0647 - pos_accuracy: 0.9147 - val_loss: 0.3092 - val_pos_accuracy: 0.7475\n",
      "Epoch 1798/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0645 - pos_accuracy: 0.9205 - val_loss: 0.3105 - val_pos_accuracy: 0.7325\n",
      "Epoch 1799/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0645 - pos_accuracy: 0.9134 - val_loss: 0.3088 - val_pos_accuracy: 0.7450\n",
      "Epoch 1800/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0646 - pos_accuracy: 0.9181 - val_loss: 0.3090 - val_pos_accuracy: 0.7450\n",
      "Epoch 1801/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0643 - pos_accuracy: 0.9163 - val_loss: 0.3105 - val_pos_accuracy: 0.7350\n",
      "Epoch 1802/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0644 - pos_accuracy: 0.9163 - val_loss: 0.3098 - val_pos_accuracy: 0.7375\n",
      "Epoch 1803/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0642 - pos_accuracy: 0.9141 - val_loss: 0.3094 - val_pos_accuracy: 0.7425\n",
      "Epoch 1804/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0640 - pos_accuracy: 0.9214 - val_loss: 0.3093 - val_pos_accuracy: 0.7450\n",
      "Epoch 1805/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0641 - pos_accuracy: 0.9157 - val_loss: 0.3096 - val_pos_accuracy: 0.7450\n",
      "Epoch 1806/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0641 - pos_accuracy: 0.9174 - val_loss: 0.3090 - val_pos_accuracy: 0.7450\n",
      "Epoch 1807/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0639 - pos_accuracy: 0.9195 - val_loss: 0.3093 - val_pos_accuracy: 0.7450\n",
      "Epoch 1808/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0639 - pos_accuracy: 0.9151 - val_loss: 0.3097 - val_pos_accuracy: 0.7450\n",
      "Epoch 1809/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0639 - pos_accuracy: 0.9171 - val_loss: 0.3099 - val_pos_accuracy: 0.7475\n",
      "Epoch 1810/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0639 - pos_accuracy: 0.9225 - val_loss: 0.3098 - val_pos_accuracy: 0.7425\n",
      "Epoch 1811/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0639 - pos_accuracy: 0.9149 - val_loss: 0.3098 - val_pos_accuracy: 0.7400\n",
      "Epoch 1812/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0637 - pos_accuracy: 0.9189 - val_loss: 0.3091 - val_pos_accuracy: 0.7450\n",
      "Epoch 1813/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0636 - pos_accuracy: 0.9168 - val_loss: 0.3088 - val_pos_accuracy: 0.7550\n",
      "Epoch 1814/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0637 - pos_accuracy: 0.9164 - val_loss: 0.3084 - val_pos_accuracy: 0.7550\n",
      "Epoch 1815/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0637 - pos_accuracy: 0.9183 - val_loss: 0.3100 - val_pos_accuracy: 0.7400\n",
      "Epoch 1816/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0636 - pos_accuracy: 0.9153 - val_loss: 0.3086 - val_pos_accuracy: 0.7450\n",
      "Epoch 1817/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0634 - pos_accuracy: 0.9182 - val_loss: 0.3085 - val_pos_accuracy: 0.7475\n",
      "Epoch 1818/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0634 - pos_accuracy: 0.9186 - val_loss: 0.3095 - val_pos_accuracy: 0.7450\n",
      "Epoch 1819/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0634 - pos_accuracy: 0.9198 - val_loss: 0.3092 - val_pos_accuracy: 0.7450\n",
      "Epoch 1820/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0633 - pos_accuracy: 0.9205 - val_loss: 0.3084 - val_pos_accuracy: 0.7550\n",
      "Epoch 1821/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0632 - pos_accuracy: 0.9227 - val_loss: 0.3085 - val_pos_accuracy: 0.7475\n",
      "Epoch 1822/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0632 - pos_accuracy: 0.9204 - val_loss: 0.3088 - val_pos_accuracy: 0.7450\n",
      "Epoch 1823/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0631 - pos_accuracy: 0.9193 - val_loss: 0.3088 - val_pos_accuracy: 0.7450\n",
      "Epoch 1824/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0631 - pos_accuracy: 0.9169 - val_loss: 0.3082 - val_pos_accuracy: 0.7475\n",
      "Epoch 1825/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0631 - pos_accuracy: 0.9219 - val_loss: 0.3081 - val_pos_accuracy: 0.7450\n",
      "Epoch 1826/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0630 - pos_accuracy: 0.9182 - val_loss: 0.3082 - val_pos_accuracy: 0.7450\n",
      "Epoch 1827/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0631 - pos_accuracy: 0.9202 - val_loss: 0.3077 - val_pos_accuracy: 0.7425\n",
      "Epoch 1828/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0629 - pos_accuracy: 0.9185 - val_loss: 0.3081 - val_pos_accuracy: 0.7425\n",
      "Epoch 1829/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0630 - pos_accuracy: 0.9169 - val_loss: 0.3078 - val_pos_accuracy: 0.7525\n",
      "Epoch 1830/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0630 - pos_accuracy: 0.9159 - val_loss: 0.3075 - val_pos_accuracy: 0.7450\n",
      "Epoch 1831/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0629 - pos_accuracy: 0.9155 - val_loss: 0.3080 - val_pos_accuracy: 0.7450\n",
      "Epoch 1832/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0627 - pos_accuracy: 0.9159 - val_loss: 0.3071 - val_pos_accuracy: 0.7450\n",
      "Epoch 1833/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0629 - pos_accuracy: 0.9131 - val_loss: 0.3071 - val_pos_accuracy: 0.7500\n",
      "Epoch 1834/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0628 - pos_accuracy: 0.9252 - val_loss: 0.3080 - val_pos_accuracy: 0.7375\n",
      "Epoch 1835/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0626 - pos_accuracy: 0.9178 - val_loss: 0.3078 - val_pos_accuracy: 0.7550\n",
      "Epoch 1836/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0627 - pos_accuracy: 0.9149 - val_loss: 0.3075 - val_pos_accuracy: 0.7550\n",
      "Epoch 1837/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0625 - pos_accuracy: 0.9222 - val_loss: 0.3077 - val_pos_accuracy: 0.7500\n",
      "Epoch 1838/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0626 - pos_accuracy: 0.9244 - val_loss: 0.3086 - val_pos_accuracy: 0.7475\n",
      "Epoch 1839/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0625 - pos_accuracy: 0.9203 - val_loss: 0.3078 - val_pos_accuracy: 0.7500\n",
      "Epoch 1840/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0625 - pos_accuracy: 0.9240 - val_loss: 0.3087 - val_pos_accuracy: 0.7425\n",
      "Epoch 1841/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0625 - pos_accuracy: 0.9212 - val_loss: 0.3082 - val_pos_accuracy: 0.7475\n",
      "Epoch 1842/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0622 - pos_accuracy: 0.9214 - val_loss: 0.3082 - val_pos_accuracy: 0.7475\n",
      "Epoch 1843/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0623 - pos_accuracy: 0.9220 - val_loss: 0.3077 - val_pos_accuracy: 0.7475\n",
      "Epoch 1844/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0621 - pos_accuracy: 0.9220 - val_loss: 0.3075 - val_pos_accuracy: 0.7525\n",
      "Epoch 1845/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0621 - pos_accuracy: 0.9229 - val_loss: 0.3078 - val_pos_accuracy: 0.7525\n",
      "Epoch 1846/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0620 - pos_accuracy: 0.9210 - val_loss: 0.3063 - val_pos_accuracy: 0.7550\n",
      "Epoch 1847/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0622 - pos_accuracy: 0.9175 - val_loss: 0.3065 - val_pos_accuracy: 0.7550\n",
      "Epoch 1848/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0619 - pos_accuracy: 0.9248 - val_loss: 0.3078 - val_pos_accuracy: 0.7525\n",
      "Epoch 1849/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0618 - pos_accuracy: 0.9246 - val_loss: 0.3076 - val_pos_accuracy: 0.7525\n",
      "Epoch 1850/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0617 - pos_accuracy: 0.9275 - val_loss: 0.3075 - val_pos_accuracy: 0.7525\n",
      "Epoch 1851/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0618 - pos_accuracy: 0.9187 - val_loss: 0.3072 - val_pos_accuracy: 0.7525\n",
      "Epoch 1852/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0617 - pos_accuracy: 0.9243 - val_loss: 0.3069 - val_pos_accuracy: 0.7525\n",
      "Epoch 1853/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0617 - pos_accuracy: 0.9233 - val_loss: 0.3069 - val_pos_accuracy: 0.7525\n",
      "Epoch 1854/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0618 - pos_accuracy: 0.9217 - val_loss: 0.3076 - val_pos_accuracy: 0.7525\n",
      "Epoch 1855/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0618 - pos_accuracy: 0.9200 - val_loss: 0.3072 - val_pos_accuracy: 0.7525\n",
      "Epoch 1856/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0615 - pos_accuracy: 0.9258 - val_loss: 0.3072 - val_pos_accuracy: 0.7525\n",
      "Epoch 1857/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0616 - pos_accuracy: 0.9217 - val_loss: 0.3064 - val_pos_accuracy: 0.7550\n",
      "Epoch 1858/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0615 - pos_accuracy: 0.9252 - val_loss: 0.3063 - val_pos_accuracy: 0.7550\n",
      "Epoch 1859/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0614 - pos_accuracy: 0.9250 - val_loss: 0.3073 - val_pos_accuracy: 0.7550\n",
      "Epoch 1860/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0613 - pos_accuracy: 0.9255 - val_loss: 0.3065 - val_pos_accuracy: 0.7550\n",
      "Epoch 1861/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0614 - pos_accuracy: 0.9257 - val_loss: 0.3062 - val_pos_accuracy: 0.7575\n",
      "Epoch 1862/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0612 - pos_accuracy: 0.9272 - val_loss: 0.3074 - val_pos_accuracy: 0.7550\n",
      "Epoch 1863/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0612 - pos_accuracy: 0.9240 - val_loss: 0.3073 - val_pos_accuracy: 0.7525\n",
      "Epoch 1864/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0612 - pos_accuracy: 0.9213 - val_loss: 0.3066 - val_pos_accuracy: 0.7550\n",
      "Epoch 1865/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0610 - pos_accuracy: 0.9266 - val_loss: 0.3061 - val_pos_accuracy: 0.7575\n",
      "Epoch 1866/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0610 - pos_accuracy: 0.9287 - val_loss: 0.3064 - val_pos_accuracy: 0.7550\n",
      "Epoch 1867/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0611 - pos_accuracy: 0.9267 - val_loss: 0.3061 - val_pos_accuracy: 0.7525\n",
      "Epoch 1868/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0611 - pos_accuracy: 0.9261 - val_loss: 0.3065 - val_pos_accuracy: 0.7550\n",
      "Epoch 1869/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0609 - pos_accuracy: 0.9218 - val_loss: 0.3068 - val_pos_accuracy: 0.7525\n",
      "Epoch 1870/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0609 - pos_accuracy: 0.9215 - val_loss: 0.3066 - val_pos_accuracy: 0.7525\n",
      "Epoch 1871/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0608 - pos_accuracy: 0.9180 - val_loss: 0.3063 - val_pos_accuracy: 0.7550\n",
      "Epoch 1872/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0608 - pos_accuracy: 0.9233 - val_loss: 0.3056 - val_pos_accuracy: 0.7550\n",
      "Epoch 1873/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0607 - pos_accuracy: 0.9271 - val_loss: 0.3059 - val_pos_accuracy: 0.7525\n",
      "Epoch 1874/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0606 - pos_accuracy: 0.9258 - val_loss: 0.3053 - val_pos_accuracy: 0.7550\n",
      "Epoch 1875/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0608 - pos_accuracy: 0.9246 - val_loss: 0.3051 - val_pos_accuracy: 0.7550\n",
      "Epoch 1876/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0605 - pos_accuracy: 0.9276 - val_loss: 0.3055 - val_pos_accuracy: 0.7525\n",
      "Epoch 1877/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0605 - pos_accuracy: 0.9185 - val_loss: 0.3051 - val_pos_accuracy: 0.7575\n",
      "Epoch 1878/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0606 - pos_accuracy: 0.9268 - val_loss: 0.3056 - val_pos_accuracy: 0.7525\n",
      "Epoch 1879/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0603 - pos_accuracy: 0.9290 - val_loss: 0.3054 - val_pos_accuracy: 0.7550\n",
      "Epoch 1880/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0604 - pos_accuracy: 0.9312 - val_loss: 0.3052 - val_pos_accuracy: 0.7550\n",
      "Epoch 1881/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0604 - pos_accuracy: 0.9221 - val_loss: 0.3049 - val_pos_accuracy: 0.7575\n",
      "Epoch 1882/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0604 - pos_accuracy: 0.9253 - val_loss: 0.3050 - val_pos_accuracy: 0.7550\n",
      "Epoch 1883/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0603 - pos_accuracy: 0.9248 - val_loss: 0.3069 - val_pos_accuracy: 0.7600\n",
      "Epoch 1884/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0603 - pos_accuracy: 0.9227 - val_loss: 0.3052 - val_pos_accuracy: 0.7550\n",
      "Epoch 1885/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0600 - pos_accuracy: 0.9269 - val_loss: 0.3054 - val_pos_accuracy: 0.7550\n",
      "Epoch 1886/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0601 - pos_accuracy: 0.9249 - val_loss: 0.3055 - val_pos_accuracy: 0.7550\n",
      "Epoch 1887/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0601 - pos_accuracy: 0.9280 - val_loss: 0.3059 - val_pos_accuracy: 0.7600\n",
      "Epoch 1888/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0600 - pos_accuracy: 0.9231 - val_loss: 0.3048 - val_pos_accuracy: 0.7575\n",
      "Epoch 1889/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0600 - pos_accuracy: 0.9212 - val_loss: 0.3044 - val_pos_accuracy: 0.7575\n",
      "Epoch 1890/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0600 - pos_accuracy: 0.9269 - val_loss: 0.3066 - val_pos_accuracy: 0.7575\n",
      "Epoch 1891/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0599 - pos_accuracy: 0.9247 - val_loss: 0.3050 - val_pos_accuracy: 0.7550\n",
      "Epoch 1892/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0599 - pos_accuracy: 0.9278 - val_loss: 0.3050 - val_pos_accuracy: 0.7550\n",
      "Epoch 1893/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0599 - pos_accuracy: 0.9275 - val_loss: 0.3042 - val_pos_accuracy: 0.7550\n",
      "Epoch 1894/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0599 - pos_accuracy: 0.9240 - val_loss: 0.3050 - val_pos_accuracy: 0.7550\n",
      "Epoch 1895/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0598 - pos_accuracy: 0.9253 - val_loss: 0.3058 - val_pos_accuracy: 0.7600\n",
      "Epoch 1896/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0599 - pos_accuracy: 0.9284 - val_loss: 0.3068 - val_pos_accuracy: 0.7550\n",
      "Epoch 1897/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0597 - pos_accuracy: 0.9274 - val_loss: 0.3044 - val_pos_accuracy: 0.7550\n",
      "Epoch 1898/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0596 - pos_accuracy: 0.9300 - val_loss: 0.3050 - val_pos_accuracy: 0.7550\n",
      "Epoch 1899/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0595 - pos_accuracy: 0.9221 - val_loss: 0.3047 - val_pos_accuracy: 0.7550\n",
      "Epoch 1900/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0595 - pos_accuracy: 0.9257 - val_loss: 0.3048 - val_pos_accuracy: 0.7550\n",
      "Epoch 1901/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0594 - pos_accuracy: 0.9254 - val_loss: 0.3047 - val_pos_accuracy: 0.7550\n",
      "Epoch 1902/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0593 - pos_accuracy: 0.9287 - val_loss: 0.3041 - val_pos_accuracy: 0.7550\n",
      "Epoch 1903/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0592 - pos_accuracy: 0.9248 - val_loss: 0.3053 - val_pos_accuracy: 0.7600\n",
      "Epoch 1904/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0593 - pos_accuracy: 0.9254 - val_loss: 0.3041 - val_pos_accuracy: 0.7575\n",
      "Epoch 1905/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0592 - pos_accuracy: 0.9268 - val_loss: 0.3039 - val_pos_accuracy: 0.7575\n",
      "Epoch 1906/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0593 - pos_accuracy: 0.9267 - val_loss: 0.3036 - val_pos_accuracy: 0.7575\n",
      "Epoch 1907/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0592 - pos_accuracy: 0.9295 - val_loss: 0.3040 - val_pos_accuracy: 0.7550\n",
      "Epoch 1908/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0590 - pos_accuracy: 0.9281 - val_loss: 0.3052 - val_pos_accuracy: 0.7625\n",
      "Epoch 1909/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0591 - pos_accuracy: 0.9255 - val_loss: 0.3050 - val_pos_accuracy: 0.7600\n",
      "Epoch 1910/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0590 - pos_accuracy: 0.9284 - val_loss: 0.3041 - val_pos_accuracy: 0.7575\n",
      "Epoch 1911/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0589 - pos_accuracy: 0.9288 - val_loss: 0.3043 - val_pos_accuracy: 0.7625\n",
      "Epoch 1912/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0590 - pos_accuracy: 0.9319 - val_loss: 0.3040 - val_pos_accuracy: 0.7575\n",
      "Epoch 1913/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0588 - pos_accuracy: 0.9251 - val_loss: 0.3041 - val_pos_accuracy: 0.7575\n",
      "Epoch 1914/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0588 - pos_accuracy: 0.9293 - val_loss: 0.3047 - val_pos_accuracy: 0.7600\n",
      "Epoch 1915/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0588 - pos_accuracy: 0.9270 - val_loss: 0.3037 - val_pos_accuracy: 0.7575\n",
      "Epoch 1916/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0587 - pos_accuracy: 0.9275 - val_loss: 0.3038 - val_pos_accuracy: 0.7600\n",
      "Epoch 1917/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0586 - pos_accuracy: 0.9246 - val_loss: 0.3035 - val_pos_accuracy: 0.7575\n",
      "Epoch 1918/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0586 - pos_accuracy: 0.9266 - val_loss: 0.3040 - val_pos_accuracy: 0.7575\n",
      "Epoch 1919/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0586 - pos_accuracy: 0.9295 - val_loss: 0.3030 - val_pos_accuracy: 0.7575\n",
      "Epoch 1920/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0587 - pos_accuracy: 0.9297 - val_loss: 0.3034 - val_pos_accuracy: 0.7625\n",
      "Epoch 1921/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0586 - pos_accuracy: 0.9288 - val_loss: 0.3038 - val_pos_accuracy: 0.7550\n",
      "Epoch 1922/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0587 - pos_accuracy: 0.9268 - val_loss: 0.3041 - val_pos_accuracy: 0.7600\n",
      "Epoch 1923/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0584 - pos_accuracy: 0.9232 - val_loss: 0.3027 - val_pos_accuracy: 0.7575\n",
      "Epoch 1924/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0584 - pos_accuracy: 0.9298 - val_loss: 0.3031 - val_pos_accuracy: 0.7575\n",
      "Epoch 1925/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0584 - pos_accuracy: 0.9303 - val_loss: 0.3031 - val_pos_accuracy: 0.7575\n",
      "Epoch 1926/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0583 - pos_accuracy: 0.9253 - val_loss: 0.3028 - val_pos_accuracy: 0.7625\n",
      "Epoch 1927/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0584 - pos_accuracy: 0.9263 - val_loss: 0.3030 - val_pos_accuracy: 0.7575\n",
      "Epoch 1928/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0582 - pos_accuracy: 0.9300 - val_loss: 0.3036 - val_pos_accuracy: 0.7625\n",
      "Epoch 1929/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0580 - pos_accuracy: 0.9303 - val_loss: 0.3036 - val_pos_accuracy: 0.7625\n",
      "Epoch 1930/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0580 - pos_accuracy: 0.9309 - val_loss: 0.3029 - val_pos_accuracy: 0.7625\n",
      "Epoch 1931/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0580 - pos_accuracy: 0.9288 - val_loss: 0.3036 - val_pos_accuracy: 0.7600\n",
      "Epoch 1932/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0579 - pos_accuracy: 0.9300 - val_loss: 0.3022 - val_pos_accuracy: 0.7575\n",
      "Epoch 1933/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0580 - pos_accuracy: 0.9230 - val_loss: 0.3034 - val_pos_accuracy: 0.7600\n",
      "Epoch 1934/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0579 - pos_accuracy: 0.9280 - val_loss: 0.3039 - val_pos_accuracy: 0.7625\n",
      "Epoch 1935/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0578 - pos_accuracy: 0.9277 - val_loss: 0.3020 - val_pos_accuracy: 0.7575\n",
      "Epoch 1936/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0578 - pos_accuracy: 0.9276 - val_loss: 0.3026 - val_pos_accuracy: 0.7575\n",
      "Epoch 1937/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0577 - pos_accuracy: 0.9245 - val_loss: 0.3033 - val_pos_accuracy: 0.7625\n",
      "Epoch 1938/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0577 - pos_accuracy: 0.9289 - val_loss: 0.3029 - val_pos_accuracy: 0.7600\n",
      "Epoch 1939/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0576 - pos_accuracy: 0.9279 - val_loss: 0.3034 - val_pos_accuracy: 0.7625\n",
      "Epoch 1940/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0576 - pos_accuracy: 0.9314 - val_loss: 0.3019 - val_pos_accuracy: 0.7575\n",
      "Epoch 1941/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0576 - pos_accuracy: 0.9264 - val_loss: 0.3038 - val_pos_accuracy: 0.7625\n",
      "Epoch 1942/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0576 - pos_accuracy: 0.9313 - val_loss: 0.3024 - val_pos_accuracy: 0.7575\n",
      "Epoch 1943/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0574 - pos_accuracy: 0.9270 - val_loss: 0.3032 - val_pos_accuracy: 0.7650\n",
      "Epoch 1944/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0575 - pos_accuracy: 0.9272 - val_loss: 0.3032 - val_pos_accuracy: 0.7625\n",
      "Epoch 1945/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0574 - pos_accuracy: 0.9290 - val_loss: 0.3018 - val_pos_accuracy: 0.7575\n",
      "Epoch 1946/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0574 - pos_accuracy: 0.9261 - val_loss: 0.3027 - val_pos_accuracy: 0.7650\n",
      "Epoch 1947/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0572 - pos_accuracy: 0.9274 - val_loss: 0.3022 - val_pos_accuracy: 0.7625\n",
      "Epoch 1948/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0572 - pos_accuracy: 0.9291 - val_loss: 0.3033 - val_pos_accuracy: 0.7650\n",
      "Epoch 1949/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0572 - pos_accuracy: 0.9284 - val_loss: 0.3025 - val_pos_accuracy: 0.7650\n",
      "Epoch 1950/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0572 - pos_accuracy: 0.9297 - val_loss: 0.3034 - val_pos_accuracy: 0.7625\n",
      "Epoch 1951/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0572 - pos_accuracy: 0.9255 - val_loss: 0.3028 - val_pos_accuracy: 0.7625\n",
      "Epoch 1952/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0571 - pos_accuracy: 0.9304 - val_loss: 0.3025 - val_pos_accuracy: 0.7650\n",
      "Epoch 1953/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0570 - pos_accuracy: 0.9290 - val_loss: 0.3023 - val_pos_accuracy: 0.7650\n",
      "Epoch 1954/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0571 - pos_accuracy: 0.9303 - val_loss: 0.3010 - val_pos_accuracy: 0.7575\n",
      "Epoch 1955/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0570 - pos_accuracy: 0.9259 - val_loss: 0.3011 - val_pos_accuracy: 0.7575\n",
      "Epoch 1956/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0570 - pos_accuracy: 0.9327 - val_loss: 0.3016 - val_pos_accuracy: 0.7625\n",
      "Epoch 1957/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0568 - pos_accuracy: 0.9322 - val_loss: 0.3019 - val_pos_accuracy: 0.7650\n",
      "Epoch 1958/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0570 - pos_accuracy: 0.9275 - val_loss: 0.3027 - val_pos_accuracy: 0.7625\n",
      "Epoch 1959/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0569 - pos_accuracy: 0.9301 - val_loss: 0.3017 - val_pos_accuracy: 0.7650\n",
      "Epoch 1960/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0568 - pos_accuracy: 0.9317 - val_loss: 0.3016 - val_pos_accuracy: 0.7625\n",
      "Epoch 1961/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0568 - pos_accuracy: 0.9302 - val_loss: 0.3017 - val_pos_accuracy: 0.7600\n",
      "Epoch 1962/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0567 - pos_accuracy: 0.9271 - val_loss: 0.3023 - val_pos_accuracy: 0.7650\n",
      "Epoch 1963/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0566 - pos_accuracy: 0.9322 - val_loss: 0.3010 - val_pos_accuracy: 0.7625\n",
      "Epoch 1964/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0565 - pos_accuracy: 0.9307 - val_loss: 0.3023 - val_pos_accuracy: 0.7650\n",
      "Epoch 1965/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0564 - pos_accuracy: 0.9305 - val_loss: 0.3019 - val_pos_accuracy: 0.7650\n",
      "Epoch 1966/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0565 - pos_accuracy: 0.9290 - val_loss: 0.3019 - val_pos_accuracy: 0.7600\n",
      "Epoch 1967/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0564 - pos_accuracy: 0.9298 - val_loss: 0.3016 - val_pos_accuracy: 0.7600\n",
      "Epoch 1968/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0563 - pos_accuracy: 0.9314 - val_loss: 0.3015 - val_pos_accuracy: 0.7650\n",
      "Epoch 1969/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0563 - pos_accuracy: 0.9297 - val_loss: 0.3015 - val_pos_accuracy: 0.7650\n",
      "Epoch 1970/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0562 - pos_accuracy: 0.9314 - val_loss: 0.3011 - val_pos_accuracy: 0.7650\n",
      "Epoch 1971/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0563 - pos_accuracy: 0.9291 - val_loss: 0.3018 - val_pos_accuracy: 0.7650\n",
      "Epoch 1972/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0562 - pos_accuracy: 0.9335 - val_loss: 0.3006 - val_pos_accuracy: 0.7625\n",
      "Epoch 1973/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0562 - pos_accuracy: 0.9328 - val_loss: 0.3022 - val_pos_accuracy: 0.7625\n",
      "Epoch 1974/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0561 - pos_accuracy: 0.9283 - val_loss: 0.3006 - val_pos_accuracy: 0.7600\n",
      "Epoch 1975/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0561 - pos_accuracy: 0.9278 - val_loss: 0.3013 - val_pos_accuracy: 0.7650\n",
      "Epoch 1976/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0561 - pos_accuracy: 0.9316 - val_loss: 0.3025 - val_pos_accuracy: 0.7600\n",
      "Epoch 1977/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0561 - pos_accuracy: 0.9294 - val_loss: 0.3004 - val_pos_accuracy: 0.7575\n",
      "Epoch 1978/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0558 - pos_accuracy: 0.9280 - val_loss: 0.3020 - val_pos_accuracy: 0.7625\n",
      "Epoch 1979/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0560 - pos_accuracy: 0.9302 - val_loss: 0.3005 - val_pos_accuracy: 0.7575\n",
      "Epoch 1980/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0559 - pos_accuracy: 0.9327 - val_loss: 0.3003 - val_pos_accuracy: 0.7575\n",
      "Epoch 1981/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0558 - pos_accuracy: 0.9290 - val_loss: 0.3003 - val_pos_accuracy: 0.7625\n",
      "Epoch 1982/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0560 - pos_accuracy: 0.9323 - val_loss: 0.2999 - val_pos_accuracy: 0.7575\n",
      "Epoch 1983/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0558 - pos_accuracy: 0.9268 - val_loss: 0.3001 - val_pos_accuracy: 0.7575\n",
      "Epoch 1984/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0558 - pos_accuracy: 0.9330 - val_loss: 0.2998 - val_pos_accuracy: 0.7575\n",
      "Epoch 1985/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0556 - pos_accuracy: 0.9308 - val_loss: 0.3013 - val_pos_accuracy: 0.7625\n",
      "Epoch 1986/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0557 - pos_accuracy: 0.9333 - val_loss: 0.3016 - val_pos_accuracy: 0.7625\n",
      "Epoch 1987/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0558 - pos_accuracy: 0.9293 - val_loss: 0.3015 - val_pos_accuracy: 0.7650\n",
      "Epoch 1988/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0555 - pos_accuracy: 0.9340 - val_loss: 0.3009 - val_pos_accuracy: 0.7650\n",
      "Epoch 1989/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0554 - pos_accuracy: 0.9349 - val_loss: 0.2991 - val_pos_accuracy: 0.7575\n",
      "Epoch 1990/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0557 - pos_accuracy: 0.9296 - val_loss: 0.3000 - val_pos_accuracy: 0.7575\n",
      "Epoch 1991/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0556 - pos_accuracy: 0.9300 - val_loss: 0.3002 - val_pos_accuracy: 0.7575\n",
      "Epoch 1992/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0553 - pos_accuracy: 0.9299 - val_loss: 0.3006 - val_pos_accuracy: 0.7650\n",
      "Epoch 1993/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0552 - pos_accuracy: 0.9315 - val_loss: 0.3005 - val_pos_accuracy: 0.7650\n",
      "Epoch 1994/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0551 - pos_accuracy: 0.9311 - val_loss: 0.3005 - val_pos_accuracy: 0.7650\n",
      "Epoch 1995/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0551 - pos_accuracy: 0.9315 - val_loss: 0.2987 - val_pos_accuracy: 0.7575\n",
      "Epoch 1996/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0555 - pos_accuracy: 0.9283 - val_loss: 0.2995 - val_pos_accuracy: 0.7650\n",
      "Epoch 1997/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0552 - pos_accuracy: 0.9281 - val_loss: 0.3002 - val_pos_accuracy: 0.7650\n",
      "Epoch 1998/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0549 - pos_accuracy: 0.9315 - val_loss: 0.3002 - val_pos_accuracy: 0.7650\n",
      "Epoch 1999/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0550 - pos_accuracy: 0.9296 - val_loss: 0.2998 - val_pos_accuracy: 0.7650\n",
      "Epoch 2000/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0550 - pos_accuracy: 0.9288 - val_loss: 0.2992 - val_pos_accuracy: 0.7650\n",
      "Epoch 2001/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0550 - pos_accuracy: 0.9307 - val_loss: 0.2992 - val_pos_accuracy: 0.7650\n",
      "Epoch 2002/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0548 - pos_accuracy: 0.9338 - val_loss: 0.2997 - val_pos_accuracy: 0.7650\n",
      "Epoch 2003/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0548 - pos_accuracy: 0.9315 - val_loss: 0.2998 - val_pos_accuracy: 0.7650\n",
      "Epoch 2004/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0548 - pos_accuracy: 0.9321 - val_loss: 0.3001 - val_pos_accuracy: 0.7650\n",
      "Epoch 2005/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0548 - pos_accuracy: 0.9323 - val_loss: 0.2997 - val_pos_accuracy: 0.7650\n",
      "Epoch 2006/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0547 - pos_accuracy: 0.9346 - val_loss: 0.2997 - val_pos_accuracy: 0.7650\n",
      "Epoch 2007/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0547 - pos_accuracy: 0.9331 - val_loss: 0.2997 - val_pos_accuracy: 0.7650\n",
      "Epoch 2008/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0546 - pos_accuracy: 0.9321 - val_loss: 0.2996 - val_pos_accuracy: 0.7650\n",
      "Epoch 2009/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0546 - pos_accuracy: 0.9320 - val_loss: 0.3004 - val_pos_accuracy: 0.7650\n",
      "Epoch 2010/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0546 - pos_accuracy: 0.9315 - val_loss: 0.2999 - val_pos_accuracy: 0.7650\n",
      "Epoch 2011/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0547 - pos_accuracy: 0.9321 - val_loss: 0.2994 - val_pos_accuracy: 0.7600\n",
      "Epoch 2012/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0545 - pos_accuracy: 0.9330 - val_loss: 0.3004 - val_pos_accuracy: 0.7650\n",
      "Epoch 2013/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0547 - pos_accuracy: 0.9314 - val_loss: 0.2997 - val_pos_accuracy: 0.7650\n",
      "Epoch 2014/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0544 - pos_accuracy: 0.9344 - val_loss: 0.2990 - val_pos_accuracy: 0.7650\n",
      "Epoch 2015/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0543 - pos_accuracy: 0.9336 - val_loss: 0.2996 - val_pos_accuracy: 0.7650\n",
      "Epoch 2016/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0545 - pos_accuracy: 0.9312 - val_loss: 0.2985 - val_pos_accuracy: 0.7625\n",
      "Epoch 2017/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0543 - pos_accuracy: 0.9315 - val_loss: 0.2986 - val_pos_accuracy: 0.7650\n",
      "Epoch 2018/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0544 - pos_accuracy: 0.9306 - val_loss: 0.2991 - val_pos_accuracy: 0.7650\n",
      "Epoch 2019/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0542 - pos_accuracy: 0.9341 - val_loss: 0.2999 - val_pos_accuracy: 0.7650\n",
      "Epoch 2020/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0542 - pos_accuracy: 0.9311 - val_loss: 0.2986 - val_pos_accuracy: 0.7625\n",
      "Epoch 2021/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0540 - pos_accuracy: 0.9303 - val_loss: 0.2993 - val_pos_accuracy: 0.7650\n",
      "Epoch 2022/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0539 - pos_accuracy: 0.9306 - val_loss: 0.2990 - val_pos_accuracy: 0.7650\n",
      "Epoch 2023/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0540 - pos_accuracy: 0.9320 - val_loss: 0.2987 - val_pos_accuracy: 0.7650\n",
      "Epoch 2024/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0539 - pos_accuracy: 0.9323 - val_loss: 0.2984 - val_pos_accuracy: 0.7600\n",
      "Epoch 2025/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0539 - pos_accuracy: 0.9288 - val_loss: 0.2989 - val_pos_accuracy: 0.7650\n",
      "Epoch 2026/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0538 - pos_accuracy: 0.9321 - val_loss: 0.2988 - val_pos_accuracy: 0.7650\n",
      "Epoch 2027/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0538 - pos_accuracy: 0.9313 - val_loss: 0.2987 - val_pos_accuracy: 0.7650\n",
      "Epoch 2028/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0537 - pos_accuracy: 0.9334 - val_loss: 0.2988 - val_pos_accuracy: 0.7650\n",
      "Epoch 2029/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0538 - pos_accuracy: 0.9348 - val_loss: 0.2987 - val_pos_accuracy: 0.7575\n",
      "Epoch 2030/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0538 - pos_accuracy: 0.9309 - val_loss: 0.2985 - val_pos_accuracy: 0.7600\n",
      "Epoch 2031/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0537 - pos_accuracy: 0.9336 - val_loss: 0.2994 - val_pos_accuracy: 0.7625\n",
      "Epoch 2032/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0537 - pos_accuracy: 0.9295 - val_loss: 0.2985 - val_pos_accuracy: 0.7600\n",
      "Epoch 2033/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0536 - pos_accuracy: 0.9340 - val_loss: 0.2982 - val_pos_accuracy: 0.7600\n",
      "Epoch 2034/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0536 - pos_accuracy: 0.9316 - val_loss: 0.2981 - val_pos_accuracy: 0.7600\n",
      "Epoch 2035/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0536 - pos_accuracy: 0.9283 - val_loss: 0.2991 - val_pos_accuracy: 0.7650\n",
      "Epoch 2036/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0534 - pos_accuracy: 0.9282 - val_loss: 0.2987 - val_pos_accuracy: 0.7700\n",
      "Epoch 2037/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0534 - pos_accuracy: 0.9354 - val_loss: 0.2988 - val_pos_accuracy: 0.7650\n",
      "Epoch 2038/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0534 - pos_accuracy: 0.9355 - val_loss: 0.2976 - val_pos_accuracy: 0.7625\n",
      "Epoch 2039/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0534 - pos_accuracy: 0.9302 - val_loss: 0.2975 - val_pos_accuracy: 0.7625\n",
      "Epoch 2040/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0535 - pos_accuracy: 0.9325 - val_loss: 0.2978 - val_pos_accuracy: 0.7625\n",
      "Epoch 2041/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0534 - pos_accuracy: 0.9288 - val_loss: 0.2975 - val_pos_accuracy: 0.7625\n",
      "Epoch 2042/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0532 - pos_accuracy: 0.9357 - val_loss: 0.2982 - val_pos_accuracy: 0.7650\n",
      "Epoch 2043/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0532 - pos_accuracy: 0.9362 - val_loss: 0.2980 - val_pos_accuracy: 0.7650\n",
      "Epoch 2044/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0531 - pos_accuracy: 0.9359 - val_loss: 0.2988 - val_pos_accuracy: 0.7650\n",
      "Epoch 2045/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0532 - pos_accuracy: 0.9346 - val_loss: 0.2982 - val_pos_accuracy: 0.7575\n",
      "Epoch 2046/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0531 - pos_accuracy: 0.9330 - val_loss: 0.2983 - val_pos_accuracy: 0.7700\n",
      "Epoch 2047/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0530 - pos_accuracy: 0.9339 - val_loss: 0.2981 - val_pos_accuracy: 0.7700\n",
      "Epoch 2048/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0530 - pos_accuracy: 0.9320 - val_loss: 0.2972 - val_pos_accuracy: 0.7625\n",
      "Epoch 2049/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0530 - pos_accuracy: 0.9296 - val_loss: 0.2972 - val_pos_accuracy: 0.7700\n",
      "Epoch 2050/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0530 - pos_accuracy: 0.9354 - val_loss: 0.2977 - val_pos_accuracy: 0.7650\n",
      "Epoch 2051/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0528 - pos_accuracy: 0.9365 - val_loss: 0.2976 - val_pos_accuracy: 0.7700\n",
      "Epoch 2052/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0528 - pos_accuracy: 0.9336 - val_loss: 0.2979 - val_pos_accuracy: 0.7700\n",
      "Epoch 2053/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0527 - pos_accuracy: 0.9322 - val_loss: 0.2969 - val_pos_accuracy: 0.7700\n",
      "Epoch 2054/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0528 - pos_accuracy: 0.9345 - val_loss: 0.2973 - val_pos_accuracy: 0.7700\n",
      "Epoch 2055/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0527 - pos_accuracy: 0.9359 - val_loss: 0.2982 - val_pos_accuracy: 0.7700\n",
      "Epoch 2056/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0526 - pos_accuracy: 0.9334 - val_loss: 0.2970 - val_pos_accuracy: 0.7700\n",
      "Epoch 2057/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0526 - pos_accuracy: 0.9363 - val_loss: 0.2975 - val_pos_accuracy: 0.7650\n",
      "Epoch 2058/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0526 - pos_accuracy: 0.9367 - val_loss: 0.2968 - val_pos_accuracy: 0.7700\n",
      "Epoch 2059/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0525 - pos_accuracy: 0.9339 - val_loss: 0.2970 - val_pos_accuracy: 0.7700\n",
      "Epoch 2060/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0524 - pos_accuracy: 0.9348 - val_loss: 0.2979 - val_pos_accuracy: 0.7700\n",
      "Epoch 2061/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0524 - pos_accuracy: 0.9312 - val_loss: 0.2966 - val_pos_accuracy: 0.7700\n",
      "Epoch 2062/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0524 - pos_accuracy: 0.9309 - val_loss: 0.2973 - val_pos_accuracy: 0.7700\n",
      "Epoch 2063/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0524 - pos_accuracy: 0.9366 - val_loss: 0.2967 - val_pos_accuracy: 0.7675\n",
      "Epoch 2064/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0523 - pos_accuracy: 0.9342 - val_loss: 0.2967 - val_pos_accuracy: 0.7700\n",
      "Epoch 2065/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0523 - pos_accuracy: 0.9340 - val_loss: 0.2967 - val_pos_accuracy: 0.7675\n",
      "Epoch 2066/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0522 - pos_accuracy: 0.9338 - val_loss: 0.2964 - val_pos_accuracy: 0.7700\n",
      "Epoch 2067/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0522 - pos_accuracy: 0.9344 - val_loss: 0.2968 - val_pos_accuracy: 0.7700\n",
      "Epoch 2068/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0521 - pos_accuracy: 0.9377 - val_loss: 0.2964 - val_pos_accuracy: 0.7700\n",
      "Epoch 2069/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0521 - pos_accuracy: 0.9336 - val_loss: 0.2964 - val_pos_accuracy: 0.7700\n",
      "Epoch 2070/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0521 - pos_accuracy: 0.9371 - val_loss: 0.2964 - val_pos_accuracy: 0.7700\n",
      "Epoch 2071/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0520 - pos_accuracy: 0.9359 - val_loss: 0.2964 - val_pos_accuracy: 0.7700\n",
      "Epoch 2072/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0520 - pos_accuracy: 0.9342 - val_loss: 0.2973 - val_pos_accuracy: 0.7700\n",
      "Epoch 2073/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0520 - pos_accuracy: 0.9354 - val_loss: 0.2966 - val_pos_accuracy: 0.7700\n",
      "Epoch 2074/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0519 - pos_accuracy: 0.9342 - val_loss: 0.2964 - val_pos_accuracy: 0.7700\n",
      "Epoch 2075/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0520 - pos_accuracy: 0.9325 - val_loss: 0.2965 - val_pos_accuracy: 0.7700\n",
      "Epoch 2076/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0519 - pos_accuracy: 0.9329 - val_loss: 0.2961 - val_pos_accuracy: 0.7700\n",
      "Epoch 2077/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0518 - pos_accuracy: 0.9367 - val_loss: 0.2967 - val_pos_accuracy: 0.7650\n",
      "Epoch 2078/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0519 - pos_accuracy: 0.9355 - val_loss: 0.2963 - val_pos_accuracy: 0.7700\n",
      "Epoch 2079/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0518 - pos_accuracy: 0.9358 - val_loss: 0.2963 - val_pos_accuracy: 0.7700\n",
      "Epoch 2080/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0517 - pos_accuracy: 0.9348 - val_loss: 0.2964 - val_pos_accuracy: 0.7700\n",
      "Epoch 2081/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0517 - pos_accuracy: 0.9332 - val_loss: 0.2956 - val_pos_accuracy: 0.7725\n",
      "Epoch 2082/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0517 - pos_accuracy: 0.9349 - val_loss: 0.2961 - val_pos_accuracy: 0.7700\n",
      "Epoch 2083/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0517 - pos_accuracy: 0.9350 - val_loss: 0.2964 - val_pos_accuracy: 0.7700\n",
      "Epoch 2084/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0516 - pos_accuracy: 0.9366 - val_loss: 0.2955 - val_pos_accuracy: 0.7725\n",
      "Epoch 2085/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0516 - pos_accuracy: 0.9372 - val_loss: 0.2965 - val_pos_accuracy: 0.7700\n",
      "Epoch 2086/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0517 - pos_accuracy: 0.9347 - val_loss: 0.2966 - val_pos_accuracy: 0.7700\n",
      "Epoch 2087/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0515 - pos_accuracy: 0.9315 - val_loss: 0.2973 - val_pos_accuracy: 0.7675\n",
      "Epoch 2088/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0514 - pos_accuracy: 0.9338 - val_loss: 0.2955 - val_pos_accuracy: 0.7675\n",
      "Epoch 2089/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0514 - pos_accuracy: 0.9352 - val_loss: 0.2964 - val_pos_accuracy: 0.7700\n",
      "Epoch 2090/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0513 - pos_accuracy: 0.9352 - val_loss: 0.2950 - val_pos_accuracy: 0.7700\n",
      "Epoch 2091/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0513 - pos_accuracy: 0.9362 - val_loss: 0.2952 - val_pos_accuracy: 0.7700\n",
      "Epoch 2092/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0513 - pos_accuracy: 0.9354 - val_loss: 0.2950 - val_pos_accuracy: 0.7700\n",
      "Epoch 2093/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0513 - pos_accuracy: 0.9334 - val_loss: 0.2954 - val_pos_accuracy: 0.7700\n",
      "Epoch 2094/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0511 - pos_accuracy: 0.9374 - val_loss: 0.2959 - val_pos_accuracy: 0.7700\n",
      "Epoch 2095/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0511 - pos_accuracy: 0.9340 - val_loss: 0.2956 - val_pos_accuracy: 0.7700\n",
      "Epoch 2096/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0511 - pos_accuracy: 0.9322 - val_loss: 0.2951 - val_pos_accuracy: 0.7700\n",
      "Epoch 2097/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0511 - pos_accuracy: 0.9357 - val_loss: 0.2946 - val_pos_accuracy: 0.7625\n",
      "Epoch 2098/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0511 - pos_accuracy: 0.9322 - val_loss: 0.2954 - val_pos_accuracy: 0.7700\n",
      "Epoch 2099/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0510 - pos_accuracy: 0.9379 - val_loss: 0.2960 - val_pos_accuracy: 0.7700\n",
      "Epoch 2100/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0509 - pos_accuracy: 0.9349 - val_loss: 0.2947 - val_pos_accuracy: 0.7700\n",
      "Epoch 2101/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0508 - pos_accuracy: 0.9358 - val_loss: 0.2957 - val_pos_accuracy: 0.7700\n",
      "Epoch 2102/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0509 - pos_accuracy: 0.9324 - val_loss: 0.2955 - val_pos_accuracy: 0.7700\n",
      "Epoch 2103/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0508 - pos_accuracy: 0.9343 - val_loss: 0.2951 - val_pos_accuracy: 0.7700\n",
      "Epoch 2104/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0508 - pos_accuracy: 0.9345 - val_loss: 0.2961 - val_pos_accuracy: 0.7700\n",
      "Epoch 2105/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0508 - pos_accuracy: 0.9335 - val_loss: 0.2948 - val_pos_accuracy: 0.7725\n",
      "Epoch 2106/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0507 - pos_accuracy: 0.9352 - val_loss: 0.2943 - val_pos_accuracy: 0.7700\n",
      "Epoch 2107/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0507 - pos_accuracy: 0.9359 - val_loss: 0.2956 - val_pos_accuracy: 0.7700\n",
      "Epoch 2108/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0506 - pos_accuracy: 0.9361 - val_loss: 0.2945 - val_pos_accuracy: 0.7700\n",
      "Epoch 2109/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0505 - pos_accuracy: 0.9332 - val_loss: 0.2955 - val_pos_accuracy: 0.7700\n",
      "Epoch 2110/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0507 - pos_accuracy: 0.9345 - val_loss: 0.2941 - val_pos_accuracy: 0.7700\n",
      "Epoch 2111/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0506 - pos_accuracy: 0.9378 - val_loss: 0.2953 - val_pos_accuracy: 0.7700\n",
      "Epoch 2112/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0506 - pos_accuracy: 0.9398 - val_loss: 0.2954 - val_pos_accuracy: 0.7700\n",
      "Epoch 2113/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0505 - pos_accuracy: 0.9353 - val_loss: 0.2948 - val_pos_accuracy: 0.7700\n",
      "Epoch 2114/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0504 - pos_accuracy: 0.9376 - val_loss: 0.2950 - val_pos_accuracy: 0.7700\n",
      "Epoch 2115/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0505 - pos_accuracy: 0.9359 - val_loss: 0.2948 - val_pos_accuracy: 0.7700\n",
      "Epoch 2116/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0505 - pos_accuracy: 0.9347 - val_loss: 0.2949 - val_pos_accuracy: 0.7700\n",
      "Epoch 2117/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0504 - pos_accuracy: 0.9363 - val_loss: 0.2942 - val_pos_accuracy: 0.7725\n",
      "Epoch 2118/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0503 - pos_accuracy: 0.9364 - val_loss: 0.2948 - val_pos_accuracy: 0.7700\n",
      "Epoch 2119/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0502 - pos_accuracy: 0.9366 - val_loss: 0.2934 - val_pos_accuracy: 0.7675\n",
      "Epoch 2120/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0502 - pos_accuracy: 0.9315 - val_loss: 0.2944 - val_pos_accuracy: 0.7725\n",
      "Epoch 2121/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0501 - pos_accuracy: 0.9359 - val_loss: 0.2944 - val_pos_accuracy: 0.7725\n",
      "Epoch 2122/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0501 - pos_accuracy: 0.9374 - val_loss: 0.2942 - val_pos_accuracy: 0.7725\n",
      "Epoch 2123/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0500 - pos_accuracy: 0.9340 - val_loss: 0.2937 - val_pos_accuracy: 0.7725\n",
      "Epoch 2124/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0499 - pos_accuracy: 0.9374 - val_loss: 0.2944 - val_pos_accuracy: 0.7725\n",
      "Epoch 2125/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0500 - pos_accuracy: 0.9369 - val_loss: 0.2937 - val_pos_accuracy: 0.7725\n",
      "Epoch 2126/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0499 - pos_accuracy: 0.9361 - val_loss: 0.2942 - val_pos_accuracy: 0.7725\n",
      "Epoch 2127/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0499 - pos_accuracy: 0.9382 - val_loss: 0.2942 - val_pos_accuracy: 0.7725\n",
      "Epoch 2128/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0499 - pos_accuracy: 0.9380 - val_loss: 0.2944 - val_pos_accuracy: 0.7725\n",
      "Epoch 2129/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0498 - pos_accuracy: 0.9364 - val_loss: 0.2940 - val_pos_accuracy: 0.7725\n",
      "Epoch 2130/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0498 - pos_accuracy: 0.9336 - val_loss: 0.2935 - val_pos_accuracy: 0.7725\n",
      "Epoch 2131/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0498 - pos_accuracy: 0.9383 - val_loss: 0.2937 - val_pos_accuracy: 0.7725\n",
      "Epoch 2132/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0496 - pos_accuracy: 0.9391 - val_loss: 0.2949 - val_pos_accuracy: 0.7700\n",
      "Epoch 2133/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0497 - pos_accuracy: 0.9379 - val_loss: 0.2932 - val_pos_accuracy: 0.7725\n",
      "Epoch 2134/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0496 - pos_accuracy: 0.9356 - val_loss: 0.2953 - val_pos_accuracy: 0.7700\n",
      "Epoch 2135/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0496 - pos_accuracy: 0.9372 - val_loss: 0.2944 - val_pos_accuracy: 0.7725\n",
      "Epoch 2136/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0497 - pos_accuracy: 0.9367 - val_loss: 0.2948 - val_pos_accuracy: 0.7675\n",
      "Epoch 2137/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0496 - pos_accuracy: 0.9410 - val_loss: 0.2943 - val_pos_accuracy: 0.7725\n",
      "Epoch 2138/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0494 - pos_accuracy: 0.9364 - val_loss: 0.2929 - val_pos_accuracy: 0.7725\n",
      "Epoch 2139/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0496 - pos_accuracy: 0.9383 - val_loss: 0.2928 - val_pos_accuracy: 0.7725\n",
      "Epoch 2140/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0494 - pos_accuracy: 0.9378 - val_loss: 0.2940 - val_pos_accuracy: 0.7725\n",
      "Epoch 2141/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0495 - pos_accuracy: 0.9355 - val_loss: 0.2941 - val_pos_accuracy: 0.7725\n",
      "Epoch 2142/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0493 - pos_accuracy: 0.9351 - val_loss: 0.2943 - val_pos_accuracy: 0.7700\n",
      "Epoch 2143/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0493 - pos_accuracy: 0.9342 - val_loss: 0.2928 - val_pos_accuracy: 0.7725\n",
      "Epoch 2144/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0493 - pos_accuracy: 0.9336 - val_loss: 0.2937 - val_pos_accuracy: 0.7725\n",
      "Epoch 2145/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0495 - pos_accuracy: 0.9394 - val_loss: 0.2931 - val_pos_accuracy: 0.7725\n",
      "Epoch 2146/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0491 - pos_accuracy: 0.9406 - val_loss: 0.2931 - val_pos_accuracy: 0.7700\n",
      "Epoch 2147/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0492 - pos_accuracy: 0.9356 - val_loss: 0.2929 - val_pos_accuracy: 0.7700\n",
      "Epoch 2148/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0491 - pos_accuracy: 0.9361 - val_loss: 0.2931 - val_pos_accuracy: 0.7700\n",
      "Epoch 2149/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0492 - pos_accuracy: 0.9332 - val_loss: 0.2943 - val_pos_accuracy: 0.7700\n",
      "Epoch 2150/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0492 - pos_accuracy: 0.9384 - val_loss: 0.2935 - val_pos_accuracy: 0.7725\n",
      "Epoch 2151/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0490 - pos_accuracy: 0.9383 - val_loss: 0.2934 - val_pos_accuracy: 0.7725\n",
      "Epoch 2152/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0489 - pos_accuracy: 0.9396 - val_loss: 0.2927 - val_pos_accuracy: 0.7725\n",
      "Epoch 2153/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0489 - pos_accuracy: 0.9365 - val_loss: 0.2920 - val_pos_accuracy: 0.7700\n",
      "Epoch 2154/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0490 - pos_accuracy: 0.9380 - val_loss: 0.2930 - val_pos_accuracy: 0.7725\n",
      "Epoch 2155/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0488 - pos_accuracy: 0.9382 - val_loss: 0.2925 - val_pos_accuracy: 0.7725\n",
      "Epoch 2156/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0488 - pos_accuracy: 0.9406 - val_loss: 0.2921 - val_pos_accuracy: 0.7775\n",
      "Epoch 2157/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0489 - pos_accuracy: 0.9396 - val_loss: 0.2926 - val_pos_accuracy: 0.7725\n",
      "Epoch 2158/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0487 - pos_accuracy: 0.9437 - val_loss: 0.2927 - val_pos_accuracy: 0.7725\n",
      "Epoch 2159/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0487 - pos_accuracy: 0.9390 - val_loss: 0.2922 - val_pos_accuracy: 0.7725\n",
      "Epoch 2160/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0487 - pos_accuracy: 0.9387 - val_loss: 0.2926 - val_pos_accuracy: 0.7725\n",
      "Epoch 2161/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0487 - pos_accuracy: 0.9377 - val_loss: 0.2936 - val_pos_accuracy: 0.7725\n",
      "Epoch 2162/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0486 - pos_accuracy: 0.9420 - val_loss: 0.2929 - val_pos_accuracy: 0.7725\n",
      "Epoch 2163/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0486 - pos_accuracy: 0.9401 - val_loss: 0.2924 - val_pos_accuracy: 0.7725\n",
      "Epoch 2164/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0485 - pos_accuracy: 0.9376 - val_loss: 0.2923 - val_pos_accuracy: 0.7725\n",
      "Epoch 2165/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0485 - pos_accuracy: 0.9383 - val_loss: 0.2927 - val_pos_accuracy: 0.7725\n",
      "Epoch 2166/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0484 - pos_accuracy: 0.9434 - val_loss: 0.2927 - val_pos_accuracy: 0.7725\n",
      "Epoch 2167/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0484 - pos_accuracy: 0.9393 - val_loss: 0.2923 - val_pos_accuracy: 0.7775\n",
      "Epoch 2168/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0484 - pos_accuracy: 0.9410 - val_loss: 0.2925 - val_pos_accuracy: 0.7775\n",
      "Epoch 2169/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0484 - pos_accuracy: 0.9441 - val_loss: 0.2927 - val_pos_accuracy: 0.7725\n",
      "Epoch 2170/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0483 - pos_accuracy: 0.9419 - val_loss: 0.2930 - val_pos_accuracy: 0.7725\n",
      "Epoch 2171/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0484 - pos_accuracy: 0.9427 - val_loss: 0.2929 - val_pos_accuracy: 0.7725\n",
      "Epoch 2172/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0482 - pos_accuracy: 0.9397 - val_loss: 0.2931 - val_pos_accuracy: 0.7725\n",
      "Epoch 2173/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0482 - pos_accuracy: 0.9434 - val_loss: 0.2920 - val_pos_accuracy: 0.7775\n",
      "Epoch 2174/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0483 - pos_accuracy: 0.9415 - val_loss: 0.2922 - val_pos_accuracy: 0.7775\n",
      "Epoch 2175/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0481 - pos_accuracy: 0.9433 - val_loss: 0.2927 - val_pos_accuracy: 0.7725\n",
      "Epoch 2176/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0483 - pos_accuracy: 0.9412 - val_loss: 0.2924 - val_pos_accuracy: 0.7775\n",
      "Epoch 2177/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0480 - pos_accuracy: 0.9408 - val_loss: 0.2929 - val_pos_accuracy: 0.7750\n",
      "Epoch 2178/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0480 - pos_accuracy: 0.9396 - val_loss: 0.2914 - val_pos_accuracy: 0.7775\n",
      "Epoch 2179/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0481 - pos_accuracy: 0.9406 - val_loss: 0.2911 - val_pos_accuracy: 0.7775\n",
      "Epoch 2180/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0480 - pos_accuracy: 0.9402 - val_loss: 0.2924 - val_pos_accuracy: 0.7775\n",
      "Epoch 2181/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0480 - pos_accuracy: 0.9417 - val_loss: 0.2918 - val_pos_accuracy: 0.7775\n",
      "Epoch 2182/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0479 - pos_accuracy: 0.9397 - val_loss: 0.2918 - val_pos_accuracy: 0.7775\n",
      "Epoch 2183/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0480 - pos_accuracy: 0.9454 - val_loss: 0.2915 - val_pos_accuracy: 0.7775\n",
      "Epoch 2184/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0479 - pos_accuracy: 0.9423 - val_loss: 0.2916 - val_pos_accuracy: 0.7775\n",
      "Epoch 2185/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0478 - pos_accuracy: 0.9413 - val_loss: 0.2931 - val_pos_accuracy: 0.7775\n",
      "Epoch 2186/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0478 - pos_accuracy: 0.9453 - val_loss: 0.2925 - val_pos_accuracy: 0.7775\n",
      "Epoch 2187/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0477 - pos_accuracy: 0.9423 - val_loss: 0.2913 - val_pos_accuracy: 0.7775\n",
      "Epoch 2188/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0476 - pos_accuracy: 0.9432 - val_loss: 0.2917 - val_pos_accuracy: 0.7775\n",
      "Epoch 2189/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0476 - pos_accuracy: 0.9433 - val_loss: 0.2906 - val_pos_accuracy: 0.7775\n",
      "Epoch 2190/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0477 - pos_accuracy: 0.9432 - val_loss: 0.2919 - val_pos_accuracy: 0.7775\n",
      "Epoch 2191/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0475 - pos_accuracy: 0.9432 - val_loss: 0.2906 - val_pos_accuracy: 0.7775\n",
      "Epoch 2192/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0477 - pos_accuracy: 0.9391 - val_loss: 0.2923 - val_pos_accuracy: 0.7775\n",
      "Epoch 2193/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0476 - pos_accuracy: 0.9410 - val_loss: 0.2924 - val_pos_accuracy: 0.7775\n",
      "Epoch 2194/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0475 - pos_accuracy: 0.9428 - val_loss: 0.2912 - val_pos_accuracy: 0.7775\n",
      "Epoch 2195/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0473 - pos_accuracy: 0.9390 - val_loss: 0.2909 - val_pos_accuracy: 0.7775\n",
      "Epoch 2196/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0474 - pos_accuracy: 0.9395 - val_loss: 0.2917 - val_pos_accuracy: 0.7775\n",
      "Epoch 2197/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0473 - pos_accuracy: 0.9415 - val_loss: 0.2916 - val_pos_accuracy: 0.7775\n",
      "Epoch 2198/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0473 - pos_accuracy: 0.9438 - val_loss: 0.2914 - val_pos_accuracy: 0.7775\n",
      "Epoch 2199/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0473 - pos_accuracy: 0.9403 - val_loss: 0.2907 - val_pos_accuracy: 0.7775\n",
      "Epoch 2200/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0472 - pos_accuracy: 0.9453 - val_loss: 0.2909 - val_pos_accuracy: 0.7775\n",
      "Epoch 2201/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0471 - pos_accuracy: 0.9400 - val_loss: 0.2911 - val_pos_accuracy: 0.7775\n",
      "Epoch 2202/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0471 - pos_accuracy: 0.9423 - val_loss: 0.2908 - val_pos_accuracy: 0.7775\n",
      "Epoch 2203/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0472 - pos_accuracy: 0.9420 - val_loss: 0.2904 - val_pos_accuracy: 0.7775\n",
      "Epoch 2204/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0471 - pos_accuracy: 0.9439 - val_loss: 0.2902 - val_pos_accuracy: 0.7775\n",
      "Epoch 2205/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0471 - pos_accuracy: 0.9417 - val_loss: 0.2905 - val_pos_accuracy: 0.7775\n",
      "Epoch 2206/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0470 - pos_accuracy: 0.9434 - val_loss: 0.2905 - val_pos_accuracy: 0.7775\n",
      "Epoch 2207/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0471 - pos_accuracy: 0.9409 - val_loss: 0.2918 - val_pos_accuracy: 0.7775\n",
      "Epoch 2208/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0470 - pos_accuracy: 0.9441 - val_loss: 0.2905 - val_pos_accuracy: 0.7775\n",
      "Epoch 2209/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0469 - pos_accuracy: 0.9447 - val_loss: 0.2909 - val_pos_accuracy: 0.7775\n",
      "Epoch 2210/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0470 - pos_accuracy: 0.9434 - val_loss: 0.2904 - val_pos_accuracy: 0.7775\n",
      "Epoch 2211/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0469 - pos_accuracy: 0.9411 - val_loss: 0.2917 - val_pos_accuracy: 0.7775\n",
      "Epoch 2212/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0469 - pos_accuracy: 0.9427 - val_loss: 0.2908 - val_pos_accuracy: 0.7775\n",
      "Epoch 2213/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0469 - pos_accuracy: 0.9409 - val_loss: 0.2908 - val_pos_accuracy: 0.7775\n",
      "Epoch 2214/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0467 - pos_accuracy: 0.9413 - val_loss: 0.2911 - val_pos_accuracy: 0.7775\n",
      "Epoch 2215/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0468 - pos_accuracy: 0.9405 - val_loss: 0.2897 - val_pos_accuracy: 0.7775\n",
      "Epoch 2216/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0467 - pos_accuracy: 0.9430 - val_loss: 0.2904 - val_pos_accuracy: 0.7775\n",
      "Epoch 2217/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0467 - pos_accuracy: 0.9442 - val_loss: 0.2903 - val_pos_accuracy: 0.7775\n",
      "Epoch 2218/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0466 - pos_accuracy: 0.9434 - val_loss: 0.2903 - val_pos_accuracy: 0.7775\n",
      "Epoch 2219/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0467 - pos_accuracy: 0.9414 - val_loss: 0.2907 - val_pos_accuracy: 0.7775\n",
      "Epoch 2220/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0465 - pos_accuracy: 0.9465 - val_loss: 0.2902 - val_pos_accuracy: 0.7775\n",
      "Epoch 2221/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0465 - pos_accuracy: 0.9424 - val_loss: 0.2909 - val_pos_accuracy: 0.7775\n",
      "Epoch 2222/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0466 - pos_accuracy: 0.9415 - val_loss: 0.2909 - val_pos_accuracy: 0.7775\n",
      "Epoch 2223/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0465 - pos_accuracy: 0.9450 - val_loss: 0.2905 - val_pos_accuracy: 0.7775\n",
      "Epoch 2224/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0465 - pos_accuracy: 0.9423 - val_loss: 0.2901 - val_pos_accuracy: 0.7775\n",
      "Epoch 2225/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0464 - pos_accuracy: 0.9451 - val_loss: 0.2897 - val_pos_accuracy: 0.7775\n",
      "Epoch 2226/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0463 - pos_accuracy: 0.9415 - val_loss: 0.2905 - val_pos_accuracy: 0.7775\n",
      "Epoch 2227/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0463 - pos_accuracy: 0.9389 - val_loss: 0.2898 - val_pos_accuracy: 0.7775\n",
      "Epoch 2228/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0462 - pos_accuracy: 0.9474 - val_loss: 0.2897 - val_pos_accuracy: 0.7775\n",
      "Epoch 2229/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0462 - pos_accuracy: 0.9458 - val_loss: 0.2900 - val_pos_accuracy: 0.7775\n",
      "Epoch 2230/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0462 - pos_accuracy: 0.9366 - val_loss: 0.2896 - val_pos_accuracy: 0.7775\n",
      "Epoch 2231/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0463 - pos_accuracy: 0.9430 - val_loss: 0.2902 - val_pos_accuracy: 0.7775\n",
      "Epoch 2232/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0462 - pos_accuracy: 0.9404 - val_loss: 0.2897 - val_pos_accuracy: 0.7775\n",
      "Epoch 2233/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0461 - pos_accuracy: 0.9410 - val_loss: 0.2900 - val_pos_accuracy: 0.7775\n",
      "Epoch 2234/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0461 - pos_accuracy: 0.9403 - val_loss: 0.2900 - val_pos_accuracy: 0.7775\n",
      "Epoch 2235/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0460 - pos_accuracy: 0.9408 - val_loss: 0.2907 - val_pos_accuracy: 0.7775\n",
      "Epoch 2236/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0462 - pos_accuracy: 0.9420 - val_loss: 0.2887 - val_pos_accuracy: 0.7775\n",
      "Epoch 2237/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0460 - pos_accuracy: 0.9439 - val_loss: 0.2903 - val_pos_accuracy: 0.7775\n",
      "Epoch 2238/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0459 - pos_accuracy: 0.9440 - val_loss: 0.2901 - val_pos_accuracy: 0.7775\n",
      "Epoch 2239/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0459 - pos_accuracy: 0.9405 - val_loss: 0.2893 - val_pos_accuracy: 0.7775\n",
      "Epoch 2240/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0458 - pos_accuracy: 0.9459 - val_loss: 0.2892 - val_pos_accuracy: 0.7775\n",
      "Epoch 2241/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0460 - pos_accuracy: 0.9427 - val_loss: 0.2888 - val_pos_accuracy: 0.7775\n",
      "Epoch 2242/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0460 - pos_accuracy: 0.9431 - val_loss: 0.2889 - val_pos_accuracy: 0.7775\n",
      "Epoch 2243/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0458 - pos_accuracy: 0.9446 - val_loss: 0.2900 - val_pos_accuracy: 0.7775\n",
      "Epoch 2244/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0458 - pos_accuracy: 0.9438 - val_loss: 0.2883 - val_pos_accuracy: 0.7775\n",
      "Epoch 2245/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0458 - pos_accuracy: 0.9463 - val_loss: 0.2900 - val_pos_accuracy: 0.7775\n",
      "Epoch 2246/3000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0457 - pos_accuracy: 0.9434 - val_loss: 0.2898 - val_pos_accuracy: 0.7775\n",
      "Epoch 2247/3000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0458 - pos_accuracy: 0.9444 - val_loss: 0.2905 - val_pos_accuracy: 0.7775\n",
      "Epoch 2248/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0457 - pos_accuracy: 0.9463 - val_loss: 0.2888 - val_pos_accuracy: 0.7775\n",
      "Epoch 2249/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0456 - pos_accuracy: 0.9444 - val_loss: 0.2891 - val_pos_accuracy: 0.7775\n",
      "Epoch 2250/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0455 - pos_accuracy: 0.9427 - val_loss: 0.2891 - val_pos_accuracy: 0.7775\n",
      "Epoch 2251/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0455 - pos_accuracy: 0.9443 - val_loss: 0.2884 - val_pos_accuracy: 0.7800\n",
      "Epoch 2252/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0454 - pos_accuracy: 0.9451 - val_loss: 0.2893 - val_pos_accuracy: 0.7775\n",
      "Epoch 2253/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0454 - pos_accuracy: 0.9442 - val_loss: 0.2889 - val_pos_accuracy: 0.7775\n",
      "Epoch 2254/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0454 - pos_accuracy: 0.9423 - val_loss: 0.2891 - val_pos_accuracy: 0.7775\n",
      "Epoch 2255/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0454 - pos_accuracy: 0.9444 - val_loss: 0.2888 - val_pos_accuracy: 0.7775\n",
      "Epoch 2256/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0453 - pos_accuracy: 0.9449 - val_loss: 0.2885 - val_pos_accuracy: 0.7775\n",
      "Epoch 2257/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0452 - pos_accuracy: 0.9461 - val_loss: 0.2893 - val_pos_accuracy: 0.7775\n",
      "Epoch 2258/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0453 - pos_accuracy: 0.9468 - val_loss: 0.2897 - val_pos_accuracy: 0.7775\n",
      "Epoch 2259/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0452 - pos_accuracy: 0.9461 - val_loss: 0.2886 - val_pos_accuracy: 0.7775\n",
      "Epoch 2260/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0451 - pos_accuracy: 0.9437 - val_loss: 0.2891 - val_pos_accuracy: 0.7775\n",
      "Epoch 2261/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0452 - pos_accuracy: 0.9456 - val_loss: 0.2881 - val_pos_accuracy: 0.7775\n",
      "Epoch 2262/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0452 - pos_accuracy: 0.9438 - val_loss: 0.2896 - val_pos_accuracy: 0.7775\n",
      "Epoch 2263/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0451 - pos_accuracy: 0.9440 - val_loss: 0.2888 - val_pos_accuracy: 0.7775\n",
      "Epoch 2264/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0451 - pos_accuracy: 0.9469 - val_loss: 0.2890 - val_pos_accuracy: 0.7775\n",
      "Epoch 2265/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0451 - pos_accuracy: 0.9447 - val_loss: 0.2883 - val_pos_accuracy: 0.7775\n",
      "Epoch 2266/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0450 - pos_accuracy: 0.9462 - val_loss: 0.2883 - val_pos_accuracy: 0.7775\n",
      "Epoch 2267/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0450 - pos_accuracy: 0.9460 - val_loss: 0.2895 - val_pos_accuracy: 0.7775\n",
      "Epoch 2268/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0450 - pos_accuracy: 0.9449 - val_loss: 0.2885 - val_pos_accuracy: 0.7775\n",
      "Epoch 2269/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0449 - pos_accuracy: 0.9466 - val_loss: 0.2881 - val_pos_accuracy: 0.7775\n",
      "Epoch 2270/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0449 - pos_accuracy: 0.9456 - val_loss: 0.2882 - val_pos_accuracy: 0.7775\n",
      "Epoch 2271/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0448 - pos_accuracy: 0.9462 - val_loss: 0.2886 - val_pos_accuracy: 0.7775\n",
      "Epoch 2272/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0448 - pos_accuracy: 0.9435 - val_loss: 0.2884 - val_pos_accuracy: 0.7775\n",
      "Epoch 2273/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0447 - pos_accuracy: 0.9457 - val_loss: 0.2885 - val_pos_accuracy: 0.7775\n",
      "Epoch 2274/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0448 - pos_accuracy: 0.9456 - val_loss: 0.2889 - val_pos_accuracy: 0.7775\n",
      "Epoch 2275/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0446 - pos_accuracy: 0.9450 - val_loss: 0.2882 - val_pos_accuracy: 0.7775\n",
      "Epoch 2276/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0446 - pos_accuracy: 0.9445 - val_loss: 0.2885 - val_pos_accuracy: 0.7775\n",
      "Epoch 2277/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0447 - pos_accuracy: 0.9417 - val_loss: 0.2879 - val_pos_accuracy: 0.7775\n",
      "Epoch 2278/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0446 - pos_accuracy: 0.9474 - val_loss: 0.2890 - val_pos_accuracy: 0.7775\n",
      "Epoch 2279/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0445 - pos_accuracy: 0.9455 - val_loss: 0.2875 - val_pos_accuracy: 0.7775\n",
      "Epoch 2280/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0446 - pos_accuracy: 0.9469 - val_loss: 0.2874 - val_pos_accuracy: 0.7775\n",
      "Epoch 2281/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0446 - pos_accuracy: 0.9478 - val_loss: 0.2872 - val_pos_accuracy: 0.7775\n",
      "Epoch 2282/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0445 - pos_accuracy: 0.9475 - val_loss: 0.2875 - val_pos_accuracy: 0.7775\n",
      "Epoch 2283/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0445 - pos_accuracy: 0.9457 - val_loss: 0.2874 - val_pos_accuracy: 0.7775\n",
      "Epoch 2284/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0445 - pos_accuracy: 0.9422 - val_loss: 0.2878 - val_pos_accuracy: 0.7775\n",
      "Epoch 2285/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0444 - pos_accuracy: 0.9445 - val_loss: 0.2883 - val_pos_accuracy: 0.7775\n",
      "Epoch 2286/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0443 - pos_accuracy: 0.9455 - val_loss: 0.2875 - val_pos_accuracy: 0.7775\n",
      "Epoch 2287/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0443 - pos_accuracy: 0.9489 - val_loss: 0.2872 - val_pos_accuracy: 0.7775\n",
      "Epoch 2288/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0444 - pos_accuracy: 0.9463 - val_loss: 0.2880 - val_pos_accuracy: 0.7775\n",
      "Epoch 2289/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0441 - pos_accuracy: 0.9470 - val_loss: 0.2880 - val_pos_accuracy: 0.7775\n",
      "Epoch 2290/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0442 - pos_accuracy: 0.9427 - val_loss: 0.2874 - val_pos_accuracy: 0.7775\n",
      "Epoch 2291/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0442 - pos_accuracy: 0.9485 - val_loss: 0.2876 - val_pos_accuracy: 0.7775\n",
      "Epoch 2292/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0442 - pos_accuracy: 0.9456 - val_loss: 0.2869 - val_pos_accuracy: 0.7775\n",
      "Epoch 2293/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0443 - pos_accuracy: 0.9455 - val_loss: 0.2870 - val_pos_accuracy: 0.7775\n",
      "Epoch 2294/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0441 - pos_accuracy: 0.9479 - val_loss: 0.2885 - val_pos_accuracy: 0.7775\n",
      "Epoch 2295/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0441 - pos_accuracy: 0.9482 - val_loss: 0.2877 - val_pos_accuracy: 0.7775\n",
      "Epoch 2296/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0440 - pos_accuracy: 0.9494 - val_loss: 0.2880 - val_pos_accuracy: 0.7775\n",
      "Epoch 2297/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0440 - pos_accuracy: 0.9438 - val_loss: 0.2883 - val_pos_accuracy: 0.7775\n",
      "Epoch 2298/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0440 - pos_accuracy: 0.9462 - val_loss: 0.2876 - val_pos_accuracy: 0.7775\n",
      "Epoch 2299/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0438 - pos_accuracy: 0.9481 - val_loss: 0.2872 - val_pos_accuracy: 0.7775\n",
      "Epoch 2300/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0440 - pos_accuracy: 0.9459 - val_loss: 0.2875 - val_pos_accuracy: 0.7775\n",
      "Epoch 2301/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0439 - pos_accuracy: 0.9456 - val_loss: 0.2875 - val_pos_accuracy: 0.7775\n",
      "Epoch 2302/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0438 - pos_accuracy: 0.9498 - val_loss: 0.2879 - val_pos_accuracy: 0.7775\n",
      "Epoch 2303/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0438 - pos_accuracy: 0.9460 - val_loss: 0.2875 - val_pos_accuracy: 0.7775\n",
      "Epoch 2304/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0437 - pos_accuracy: 0.9432 - val_loss: 0.2880 - val_pos_accuracy: 0.7775\n",
      "Epoch 2305/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0437 - pos_accuracy: 0.9479 - val_loss: 0.2872 - val_pos_accuracy: 0.7775\n",
      "Epoch 2306/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0436 - pos_accuracy: 0.9441 - val_loss: 0.2866 - val_pos_accuracy: 0.7775\n",
      "Epoch 2307/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0436 - pos_accuracy: 0.9482 - val_loss: 0.2874 - val_pos_accuracy: 0.7775\n",
      "Epoch 2308/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0436 - pos_accuracy: 0.9438 - val_loss: 0.2873 - val_pos_accuracy: 0.7775\n",
      "Epoch 2309/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0435 - pos_accuracy: 0.9494 - val_loss: 0.2870 - val_pos_accuracy: 0.7775\n",
      "Epoch 2310/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0435 - pos_accuracy: 0.9471 - val_loss: 0.2872 - val_pos_accuracy: 0.7775\n",
      "Epoch 2311/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0435 - pos_accuracy: 0.9467 - val_loss: 0.2875 - val_pos_accuracy: 0.7775\n",
      "Epoch 2312/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0435 - pos_accuracy: 0.9434 - val_loss: 0.2879 - val_pos_accuracy: 0.7775\n",
      "Epoch 2313/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0434 - pos_accuracy: 0.9455 - val_loss: 0.2869 - val_pos_accuracy: 0.7775\n",
      "Epoch 2314/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0434 - pos_accuracy: 0.9452 - val_loss: 0.2866 - val_pos_accuracy: 0.7775\n",
      "Epoch 2315/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0435 - pos_accuracy: 0.9472 - val_loss: 0.2874 - val_pos_accuracy: 0.7775\n",
      "Epoch 2316/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0435 - pos_accuracy: 0.9468 - val_loss: 0.2884 - val_pos_accuracy: 0.7775\n",
      "Epoch 2317/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0435 - pos_accuracy: 0.9472 - val_loss: 0.2880 - val_pos_accuracy: 0.7775\n",
      "Epoch 2318/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0434 - pos_accuracy: 0.9477 - val_loss: 0.2866 - val_pos_accuracy: 0.7800\n",
      "Epoch 2319/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0434 - pos_accuracy: 0.9480 - val_loss: 0.2868 - val_pos_accuracy: 0.7775\n",
      "Epoch 2320/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0434 - pos_accuracy: 0.9454 - val_loss: 0.2876 - val_pos_accuracy: 0.7775\n",
      "Epoch 2321/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0432 - pos_accuracy: 0.9496 - val_loss: 0.2864 - val_pos_accuracy: 0.7775\n",
      "Epoch 2322/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0431 - pos_accuracy: 0.9481 - val_loss: 0.2865 - val_pos_accuracy: 0.7775\n",
      "Epoch 2323/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0434 - pos_accuracy: 0.9456 - val_loss: 0.2865 - val_pos_accuracy: 0.7775\n",
      "Epoch 2324/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0431 - pos_accuracy: 0.9499 - val_loss: 0.2864 - val_pos_accuracy: 0.7775\n",
      "Epoch 2325/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0430 - pos_accuracy: 0.9487 - val_loss: 0.2865 - val_pos_accuracy: 0.7800\n",
      "Epoch 2326/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0430 - pos_accuracy: 0.9470 - val_loss: 0.2866 - val_pos_accuracy: 0.7775\n",
      "Epoch 2327/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0429 - pos_accuracy: 0.9461 - val_loss: 0.2865 - val_pos_accuracy: 0.7775\n",
      "Epoch 2328/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0429 - pos_accuracy: 0.9480 - val_loss: 0.2861 - val_pos_accuracy: 0.7775\n",
      "Epoch 2329/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0429 - pos_accuracy: 0.9515 - val_loss: 0.2859 - val_pos_accuracy: 0.7775\n",
      "Epoch 2330/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0429 - pos_accuracy: 0.9497 - val_loss: 0.2857 - val_pos_accuracy: 0.7775\n",
      "Epoch 2331/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0430 - pos_accuracy: 0.9437 - val_loss: 0.2862 - val_pos_accuracy: 0.7775\n",
      "Epoch 2332/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0428 - pos_accuracy: 0.9480 - val_loss: 0.2864 - val_pos_accuracy: 0.7775\n",
      "Epoch 2333/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0428 - pos_accuracy: 0.9476 - val_loss: 0.2857 - val_pos_accuracy: 0.7800\n",
      "Epoch 2334/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0428 - pos_accuracy: 0.9486 - val_loss: 0.2863 - val_pos_accuracy: 0.7775\n",
      "Epoch 2335/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0428 - pos_accuracy: 0.9476 - val_loss: 0.2855 - val_pos_accuracy: 0.7800\n",
      "Epoch 2336/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0428 - pos_accuracy: 0.9461 - val_loss: 0.2854 - val_pos_accuracy: 0.7800\n",
      "Epoch 2337/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0427 - pos_accuracy: 0.9475 - val_loss: 0.2865 - val_pos_accuracy: 0.7775\n",
      "Epoch 2338/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0427 - pos_accuracy: 0.9487 - val_loss: 0.2862 - val_pos_accuracy: 0.7775\n",
      "Epoch 2339/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0426 - pos_accuracy: 0.9484 - val_loss: 0.2860 - val_pos_accuracy: 0.7775\n",
      "Epoch 2340/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0426 - pos_accuracy: 0.9473 - val_loss: 0.2861 - val_pos_accuracy: 0.7775\n",
      "Epoch 2341/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0426 - pos_accuracy: 0.9491 - val_loss: 0.2862 - val_pos_accuracy: 0.7775\n",
      "Epoch 2342/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0426 - pos_accuracy: 0.9514 - val_loss: 0.2866 - val_pos_accuracy: 0.7775\n",
      "Epoch 2343/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0426 - pos_accuracy: 0.9481 - val_loss: 0.2859 - val_pos_accuracy: 0.7800\n",
      "Epoch 2344/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0424 - pos_accuracy: 0.9511 - val_loss: 0.2861 - val_pos_accuracy: 0.7775\n",
      "Epoch 2345/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0425 - pos_accuracy: 0.9443 - val_loss: 0.2855 - val_pos_accuracy: 0.7800\n",
      "Epoch 2346/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0425 - pos_accuracy: 0.9469 - val_loss: 0.2848 - val_pos_accuracy: 0.7775\n",
      "Epoch 2347/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0425 - pos_accuracy: 0.9489 - val_loss: 0.2856 - val_pos_accuracy: 0.7775\n",
      "Epoch 2348/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0424 - pos_accuracy: 0.9477 - val_loss: 0.2847 - val_pos_accuracy: 0.7775\n",
      "Epoch 2349/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0425 - pos_accuracy: 0.9490 - val_loss: 0.2861 - val_pos_accuracy: 0.7775\n",
      "Epoch 2350/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0423 - pos_accuracy: 0.9469 - val_loss: 0.2861 - val_pos_accuracy: 0.7800\n",
      "Epoch 2351/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0424 - pos_accuracy: 0.9506 - val_loss: 0.2864 - val_pos_accuracy: 0.7800\n",
      "Epoch 2352/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0423 - pos_accuracy: 0.9467 - val_loss: 0.2868 - val_pos_accuracy: 0.7775\n",
      "Epoch 2353/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0423 - pos_accuracy: 0.9467 - val_loss: 0.2842 - val_pos_accuracy: 0.7775\n",
      "Epoch 2354/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0425 - pos_accuracy: 0.9492 - val_loss: 0.2853 - val_pos_accuracy: 0.7775\n",
      "Epoch 2355/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0422 - pos_accuracy: 0.9517 - val_loss: 0.2864 - val_pos_accuracy: 0.7800\n",
      "Epoch 2356/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0424 - pos_accuracy: 0.9502 - val_loss: 0.2868 - val_pos_accuracy: 0.7775\n",
      "Epoch 2357/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0423 - pos_accuracy: 0.9498 - val_loss: 0.2856 - val_pos_accuracy: 0.7800\n",
      "Epoch 2358/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0420 - pos_accuracy: 0.9489 - val_loss: 0.2850 - val_pos_accuracy: 0.7800\n",
      "Epoch 2359/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0420 - pos_accuracy: 0.9478 - val_loss: 0.2858 - val_pos_accuracy: 0.7775\n",
      "Epoch 2360/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0420 - pos_accuracy: 0.9492 - val_loss: 0.2842 - val_pos_accuracy: 0.7800\n",
      "Epoch 2361/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0422 - pos_accuracy: 0.9492 - val_loss: 0.2849 - val_pos_accuracy: 0.7800\n",
      "Epoch 2362/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0419 - pos_accuracy: 0.9493 - val_loss: 0.2848 - val_pos_accuracy: 0.7800\n",
      "Epoch 2363/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0418 - pos_accuracy: 0.9505 - val_loss: 0.2858 - val_pos_accuracy: 0.7775\n",
      "Epoch 2364/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0419 - pos_accuracy: 0.9489 - val_loss: 0.2859 - val_pos_accuracy: 0.7775\n",
      "Epoch 2365/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0419 - pos_accuracy: 0.9481 - val_loss: 0.2856 - val_pos_accuracy: 0.7775\n",
      "Epoch 2366/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0419 - pos_accuracy: 0.9486 - val_loss: 0.2843 - val_pos_accuracy: 0.7800\n",
      "Epoch 2367/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0419 - pos_accuracy: 0.9494 - val_loss: 0.2849 - val_pos_accuracy: 0.7800\n",
      "Epoch 2368/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0418 - pos_accuracy: 0.9478 - val_loss: 0.2851 - val_pos_accuracy: 0.7800\n",
      "Epoch 2369/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0417 - pos_accuracy: 0.9493 - val_loss: 0.2845 - val_pos_accuracy: 0.7800\n",
      "Epoch 2370/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0417 - pos_accuracy: 0.9472 - val_loss: 0.2856 - val_pos_accuracy: 0.7800\n",
      "Epoch 2371/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0416 - pos_accuracy: 0.9487 - val_loss: 0.2854 - val_pos_accuracy: 0.7800\n",
      "Epoch 2372/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0416 - pos_accuracy: 0.9493 - val_loss: 0.2848 - val_pos_accuracy: 0.7800\n",
      "Epoch 2373/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0416 - pos_accuracy: 0.9501 - val_loss: 0.2853 - val_pos_accuracy: 0.7800\n",
      "Epoch 2374/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0416 - pos_accuracy: 0.9493 - val_loss: 0.2850 - val_pos_accuracy: 0.7800\n",
      "Epoch 2375/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0415 - pos_accuracy: 0.9489 - val_loss: 0.2855 - val_pos_accuracy: 0.7800\n",
      "Epoch 2376/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0415 - pos_accuracy: 0.9517 - val_loss: 0.2848 - val_pos_accuracy: 0.7800\n",
      "Epoch 2377/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0415 - pos_accuracy: 0.9487 - val_loss: 0.2850 - val_pos_accuracy: 0.7800\n",
      "Epoch 2378/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0415 - pos_accuracy: 0.9507 - val_loss: 0.2847 - val_pos_accuracy: 0.7800\n",
      "Epoch 2379/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0414 - pos_accuracy: 0.9487 - val_loss: 0.2847 - val_pos_accuracy: 0.7800\n",
      "Epoch 2380/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0414 - pos_accuracy: 0.9484 - val_loss: 0.2844 - val_pos_accuracy: 0.7800\n",
      "Epoch 2381/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0414 - pos_accuracy: 0.9495 - val_loss: 0.2840 - val_pos_accuracy: 0.7800\n",
      "Epoch 2382/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0413 - pos_accuracy: 0.9493 - val_loss: 0.2845 - val_pos_accuracy: 0.7800\n",
      "Epoch 2383/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0413 - pos_accuracy: 0.9497 - val_loss: 0.2852 - val_pos_accuracy: 0.7775\n",
      "Epoch 2384/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0413 - pos_accuracy: 0.9524 - val_loss: 0.2845 - val_pos_accuracy: 0.7800\n",
      "Epoch 2385/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0412 - pos_accuracy: 0.9512 - val_loss: 0.2852 - val_pos_accuracy: 0.7800\n",
      "Epoch 2386/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0414 - pos_accuracy: 0.9470 - val_loss: 0.2836 - val_pos_accuracy: 0.7775\n",
      "Epoch 2387/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0412 - pos_accuracy: 0.9491 - val_loss: 0.2839 - val_pos_accuracy: 0.7775\n",
      "Epoch 2388/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0412 - pos_accuracy: 0.9482 - val_loss: 0.2851 - val_pos_accuracy: 0.7775\n",
      "Epoch 2389/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0413 - pos_accuracy: 0.9513 - val_loss: 0.2849 - val_pos_accuracy: 0.7800\n",
      "Epoch 2390/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0412 - pos_accuracy: 0.9494 - val_loss: 0.2843 - val_pos_accuracy: 0.7800\n",
      "Epoch 2391/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0411 - pos_accuracy: 0.9483 - val_loss: 0.2835 - val_pos_accuracy: 0.7775\n",
      "Epoch 2392/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0412 - pos_accuracy: 0.9503 - val_loss: 0.2840 - val_pos_accuracy: 0.7800\n",
      "Epoch 2393/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0411 - pos_accuracy: 0.9488 - val_loss: 0.2843 - val_pos_accuracy: 0.7800\n",
      "Epoch 2394/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0410 - pos_accuracy: 0.9514 - val_loss: 0.2836 - val_pos_accuracy: 0.7800\n",
      "Epoch 2395/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0409 - pos_accuracy: 0.9479 - val_loss: 0.2846 - val_pos_accuracy: 0.7800\n",
      "Epoch 2396/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0409 - pos_accuracy: 0.9508 - val_loss: 0.2838 - val_pos_accuracy: 0.7800\n",
      "Epoch 2397/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0410 - pos_accuracy: 0.9526 - val_loss: 0.2837 - val_pos_accuracy: 0.7800\n",
      "Epoch 2398/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0408 - pos_accuracy: 0.9501 - val_loss: 0.2842 - val_pos_accuracy: 0.7800\n",
      "Epoch 2399/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0408 - pos_accuracy: 0.9526 - val_loss: 0.2837 - val_pos_accuracy: 0.7800\n",
      "Epoch 2400/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0409 - pos_accuracy: 0.9510 - val_loss: 0.2833 - val_pos_accuracy: 0.7800\n",
      "Epoch 2401/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0409 - pos_accuracy: 0.9478 - val_loss: 0.2836 - val_pos_accuracy: 0.7800\n",
      "Epoch 2402/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0407 - pos_accuracy: 0.9508 - val_loss: 0.2835 - val_pos_accuracy: 0.7800\n",
      "Epoch 2403/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0409 - pos_accuracy: 0.9498 - val_loss: 0.2834 - val_pos_accuracy: 0.7800\n",
      "Epoch 2404/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0407 - pos_accuracy: 0.9481 - val_loss: 0.2848 - val_pos_accuracy: 0.7800\n",
      "Epoch 2405/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0408 - pos_accuracy: 0.9495 - val_loss: 0.2846 - val_pos_accuracy: 0.7800\n",
      "Epoch 2406/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0409 - pos_accuracy: 0.9475 - val_loss: 0.2847 - val_pos_accuracy: 0.7800\n",
      "Epoch 2407/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0406 - pos_accuracy: 0.9527 - val_loss: 0.2833 - val_pos_accuracy: 0.7800\n",
      "Epoch 2408/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0409 - pos_accuracy: 0.9469 - val_loss: 0.2831 - val_pos_accuracy: 0.7800\n",
      "Epoch 2409/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0406 - pos_accuracy: 0.9480 - val_loss: 0.2835 - val_pos_accuracy: 0.7825\n",
      "Epoch 2410/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0405 - pos_accuracy: 0.9522 - val_loss: 0.2836 - val_pos_accuracy: 0.7825\n",
      "Epoch 2411/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0405 - pos_accuracy: 0.9513 - val_loss: 0.2828 - val_pos_accuracy: 0.7825\n",
      "Epoch 2412/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0405 - pos_accuracy: 0.9481 - val_loss: 0.2833 - val_pos_accuracy: 0.7825\n",
      "Epoch 2413/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0405 - pos_accuracy: 0.9491 - val_loss: 0.2843 - val_pos_accuracy: 0.7800\n",
      "Epoch 2414/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0405 - pos_accuracy: 0.9476 - val_loss: 0.2840 - val_pos_accuracy: 0.7800\n",
      "Epoch 2415/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0403 - pos_accuracy: 0.9507 - val_loss: 0.2833 - val_pos_accuracy: 0.7800\n",
      "Epoch 2416/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0404 - pos_accuracy: 0.9499 - val_loss: 0.2843 - val_pos_accuracy: 0.7800\n",
      "Epoch 2417/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0404 - pos_accuracy: 0.9518 - val_loss: 0.2835 - val_pos_accuracy: 0.7800\n",
      "Epoch 2418/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0403 - pos_accuracy: 0.9504 - val_loss: 0.2838 - val_pos_accuracy: 0.7800\n",
      "Epoch 2419/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0404 - pos_accuracy: 0.9499 - val_loss: 0.2839 - val_pos_accuracy: 0.7800\n",
      "Epoch 2420/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0403 - pos_accuracy: 0.9526 - val_loss: 0.2835 - val_pos_accuracy: 0.7800\n",
      "Epoch 2421/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0402 - pos_accuracy: 0.9511 - val_loss: 0.2821 - val_pos_accuracy: 0.7825\n",
      "Epoch 2422/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0404 - pos_accuracy: 0.9473 - val_loss: 0.2826 - val_pos_accuracy: 0.7800\n",
      "Epoch 2423/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0402 - pos_accuracy: 0.9479 - val_loss: 0.2839 - val_pos_accuracy: 0.7800\n",
      "Epoch 2424/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0401 - pos_accuracy: 0.9494 - val_loss: 0.2836 - val_pos_accuracy: 0.7800\n",
      "Epoch 2425/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0402 - pos_accuracy: 0.9518 - val_loss: 0.2837 - val_pos_accuracy: 0.7800\n",
      "Epoch 2426/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0401 - pos_accuracy: 0.9479 - val_loss: 0.2833 - val_pos_accuracy: 0.7800\n",
      "Epoch 2427/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0400 - pos_accuracy: 0.9476 - val_loss: 0.2826 - val_pos_accuracy: 0.7800\n",
      "Epoch 2428/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0400 - pos_accuracy: 0.9504 - val_loss: 0.2828 - val_pos_accuracy: 0.7800\n",
      "Epoch 2429/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0400 - pos_accuracy: 0.9484 - val_loss: 0.2828 - val_pos_accuracy: 0.7800\n",
      "Epoch 2430/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0400 - pos_accuracy: 0.9508 - val_loss: 0.2831 - val_pos_accuracy: 0.7800\n",
      "Epoch 2431/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0399 - pos_accuracy: 0.9531 - val_loss: 0.2827 - val_pos_accuracy: 0.7800\n",
      "Epoch 2432/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0398 - pos_accuracy: 0.9507 - val_loss: 0.2828 - val_pos_accuracy: 0.7800\n",
      "Epoch 2433/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0399 - pos_accuracy: 0.9511 - val_loss: 0.2828 - val_pos_accuracy: 0.7800\n",
      "Epoch 2434/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0398 - pos_accuracy: 0.9478 - val_loss: 0.2826 - val_pos_accuracy: 0.7800\n",
      "Epoch 2435/3000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0398 - pos_accuracy: 0.9538 - val_loss: 0.2828 - val_pos_accuracy: 0.7800\n",
      "Epoch 2436/3000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0398 - pos_accuracy: 0.9528 - val_loss: 0.2833 - val_pos_accuracy: 0.7800\n",
      "Epoch 2437/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0398 - pos_accuracy: 0.9527 - val_loss: 0.2828 - val_pos_accuracy: 0.7800\n",
      "Epoch 2438/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0397 - pos_accuracy: 0.9473 - val_loss: 0.2831 - val_pos_accuracy: 0.7800\n",
      "Epoch 2439/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0397 - pos_accuracy: 0.9504 - val_loss: 0.2839 - val_pos_accuracy: 0.7800\n",
      "Epoch 2440/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0398 - pos_accuracy: 0.9504 - val_loss: 0.2833 - val_pos_accuracy: 0.7800\n",
      "Epoch 2441/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0396 - pos_accuracy: 0.9508 - val_loss: 0.2828 - val_pos_accuracy: 0.7800\n",
      "Epoch 2442/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0397 - pos_accuracy: 0.9518 - val_loss: 0.2829 - val_pos_accuracy: 0.7800\n",
      "Epoch 2443/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0396 - pos_accuracy: 0.9497 - val_loss: 0.2830 - val_pos_accuracy: 0.7800\n",
      "Epoch 2444/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0397 - pos_accuracy: 0.9498 - val_loss: 0.2830 - val_pos_accuracy: 0.7800\n",
      "Epoch 2445/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0396 - pos_accuracy: 0.9509 - val_loss: 0.2820 - val_pos_accuracy: 0.7800\n",
      "Epoch 2446/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0396 - pos_accuracy: 0.9546 - val_loss: 0.2836 - val_pos_accuracy: 0.7800\n",
      "Epoch 2447/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0396 - pos_accuracy: 0.9540 - val_loss: 0.2818 - val_pos_accuracy: 0.7800\n",
      "Epoch 2448/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0395 - pos_accuracy: 0.9485 - val_loss: 0.2829 - val_pos_accuracy: 0.7800\n",
      "Epoch 2449/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0395 - pos_accuracy: 0.9503 - val_loss: 0.2827 - val_pos_accuracy: 0.7800\n",
      "Epoch 2450/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0394 - pos_accuracy: 0.9526 - val_loss: 0.2824 - val_pos_accuracy: 0.7825\n",
      "Epoch 2451/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0394 - pos_accuracy: 0.9481 - val_loss: 0.2833 - val_pos_accuracy: 0.7800\n",
      "Epoch 2452/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0394 - pos_accuracy: 0.9509 - val_loss: 0.2816 - val_pos_accuracy: 0.7850\n",
      "Epoch 2453/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0394 - pos_accuracy: 0.9520 - val_loss: 0.2816 - val_pos_accuracy: 0.7825\n",
      "Epoch 2454/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0393 - pos_accuracy: 0.9501 - val_loss: 0.2822 - val_pos_accuracy: 0.7800\n",
      "Epoch 2455/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0393 - pos_accuracy: 0.9533 - val_loss: 0.2823 - val_pos_accuracy: 0.7825\n",
      "Epoch 2456/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0393 - pos_accuracy: 0.9512 - val_loss: 0.2836 - val_pos_accuracy: 0.7800\n",
      "Epoch 2457/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0393 - pos_accuracy: 0.9525 - val_loss: 0.2822 - val_pos_accuracy: 0.7825\n",
      "Epoch 2458/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0392 - pos_accuracy: 0.9501 - val_loss: 0.2817 - val_pos_accuracy: 0.7825\n",
      "Epoch 2459/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0392 - pos_accuracy: 0.9498 - val_loss: 0.2831 - val_pos_accuracy: 0.7800\n",
      "Epoch 2460/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0392 - pos_accuracy: 0.9523 - val_loss: 0.2824 - val_pos_accuracy: 0.7825\n",
      "Epoch 2461/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0391 - pos_accuracy: 0.9524 - val_loss: 0.2829 - val_pos_accuracy: 0.7850\n",
      "Epoch 2462/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0392 - pos_accuracy: 0.9480 - val_loss: 0.2821 - val_pos_accuracy: 0.7800\n",
      "Epoch 2463/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0391 - pos_accuracy: 0.9513 - val_loss: 0.2805 - val_pos_accuracy: 0.7875\n",
      "Epoch 2464/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0391 - pos_accuracy: 0.9509 - val_loss: 0.2820 - val_pos_accuracy: 0.7825\n",
      "Epoch 2465/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0389 - pos_accuracy: 0.9524 - val_loss: 0.2814 - val_pos_accuracy: 0.7825\n",
      "Epoch 2466/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0392 - pos_accuracy: 0.9499 - val_loss: 0.2825 - val_pos_accuracy: 0.7800\n",
      "Epoch 2467/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0389 - pos_accuracy: 0.9516 - val_loss: 0.2823 - val_pos_accuracy: 0.7825\n",
      "Epoch 2468/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0390 - pos_accuracy: 0.9497 - val_loss: 0.2829 - val_pos_accuracy: 0.7800\n",
      "Epoch 2469/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0390 - pos_accuracy: 0.9519 - val_loss: 0.2823 - val_pos_accuracy: 0.7800\n",
      "Epoch 2470/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0388 - pos_accuracy: 0.9532 - val_loss: 0.2818 - val_pos_accuracy: 0.7825\n",
      "Epoch 2471/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0388 - pos_accuracy: 0.9494 - val_loss: 0.2815 - val_pos_accuracy: 0.7850\n",
      "Epoch 2472/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0387 - pos_accuracy: 0.9509 - val_loss: 0.2814 - val_pos_accuracy: 0.7850\n",
      "Epoch 2473/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0387 - pos_accuracy: 0.9514 - val_loss: 0.2818 - val_pos_accuracy: 0.7850\n",
      "Epoch 2474/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0387 - pos_accuracy: 0.9513 - val_loss: 0.2821 - val_pos_accuracy: 0.7850\n",
      "Epoch 2475/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0386 - pos_accuracy: 0.9521 - val_loss: 0.2819 - val_pos_accuracy: 0.7850\n",
      "Epoch 2476/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0386 - pos_accuracy: 0.9525 - val_loss: 0.2822 - val_pos_accuracy: 0.7850\n",
      "Epoch 2477/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0387 - pos_accuracy: 0.9513 - val_loss: 0.2824 - val_pos_accuracy: 0.7850\n",
      "Epoch 2478/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0386 - pos_accuracy: 0.9504 - val_loss: 0.2816 - val_pos_accuracy: 0.7850\n",
      "Epoch 2479/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0386 - pos_accuracy: 0.9492 - val_loss: 0.2816 - val_pos_accuracy: 0.7825\n",
      "Epoch 2480/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0385 - pos_accuracy: 0.9516 - val_loss: 0.2815 - val_pos_accuracy: 0.7875\n",
      "Epoch 2481/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0385 - pos_accuracy: 0.9536 - val_loss: 0.2815 - val_pos_accuracy: 0.7875\n",
      "Epoch 2482/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0384 - pos_accuracy: 0.9505 - val_loss: 0.2816 - val_pos_accuracy: 0.7875\n",
      "Epoch 2483/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0384 - pos_accuracy: 0.9519 - val_loss: 0.2813 - val_pos_accuracy: 0.7875\n",
      "Epoch 2484/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0384 - pos_accuracy: 0.9514 - val_loss: 0.2813 - val_pos_accuracy: 0.7875\n",
      "Epoch 2485/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0384 - pos_accuracy: 0.9517 - val_loss: 0.2819 - val_pos_accuracy: 0.7825\n",
      "Epoch 2486/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0385 - pos_accuracy: 0.9486 - val_loss: 0.2816 - val_pos_accuracy: 0.7850\n",
      "Epoch 2487/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0384 - pos_accuracy: 0.9515 - val_loss: 0.2816 - val_pos_accuracy: 0.7850\n",
      "Epoch 2488/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0383 - pos_accuracy: 0.9532 - val_loss: 0.2814 - val_pos_accuracy: 0.7875\n",
      "Epoch 2489/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0382 - pos_accuracy: 0.9523 - val_loss: 0.2810 - val_pos_accuracy: 0.7875\n",
      "Epoch 2490/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0383 - pos_accuracy: 0.9490 - val_loss: 0.2810 - val_pos_accuracy: 0.7875\n",
      "Epoch 2491/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0382 - pos_accuracy: 0.9525 - val_loss: 0.2810 - val_pos_accuracy: 0.7875\n",
      "Epoch 2492/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0382 - pos_accuracy: 0.9502 - val_loss: 0.2808 - val_pos_accuracy: 0.7875\n",
      "Epoch 2493/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0383 - pos_accuracy: 0.9540 - val_loss: 0.2809 - val_pos_accuracy: 0.7875\n",
      "Epoch 2494/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0382 - pos_accuracy: 0.9529 - val_loss: 0.2800 - val_pos_accuracy: 0.7875\n",
      "Epoch 2495/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0382 - pos_accuracy: 0.9529 - val_loss: 0.2811 - val_pos_accuracy: 0.7850\n",
      "Epoch 2496/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0382 - pos_accuracy: 0.9525 - val_loss: 0.2812 - val_pos_accuracy: 0.7850\n",
      "Epoch 2497/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0381 - pos_accuracy: 0.9542 - val_loss: 0.2799 - val_pos_accuracy: 0.7850\n",
      "Epoch 2498/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0382 - pos_accuracy: 0.9530 - val_loss: 0.2811 - val_pos_accuracy: 0.7850\n",
      "Epoch 2499/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0381 - pos_accuracy: 0.9555 - val_loss: 0.2815 - val_pos_accuracy: 0.7850\n",
      "Epoch 2500/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0381 - pos_accuracy: 0.9537 - val_loss: 0.2805 - val_pos_accuracy: 0.7875\n",
      "Epoch 2501/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0380 - pos_accuracy: 0.9511 - val_loss: 0.2799 - val_pos_accuracy: 0.7850\n",
      "Epoch 2502/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0380 - pos_accuracy: 0.9530 - val_loss: 0.2814 - val_pos_accuracy: 0.7850\n",
      "Epoch 2503/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0380 - pos_accuracy: 0.9496 - val_loss: 0.2802 - val_pos_accuracy: 0.7850\n",
      "Epoch 2504/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0379 - pos_accuracy: 0.9541 - val_loss: 0.2801 - val_pos_accuracy: 0.7875\n",
      "Epoch 2505/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0379 - pos_accuracy: 0.9534 - val_loss: 0.2796 - val_pos_accuracy: 0.7850\n",
      "Epoch 2506/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0379 - pos_accuracy: 0.9558 - val_loss: 0.2802 - val_pos_accuracy: 0.7875\n",
      "Epoch 2507/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0378 - pos_accuracy: 0.9553 - val_loss: 0.2800 - val_pos_accuracy: 0.7875\n",
      "Epoch 2508/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0380 - pos_accuracy: 0.9513 - val_loss: 0.2809 - val_pos_accuracy: 0.7875\n",
      "Epoch 2509/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0377 - pos_accuracy: 0.9515 - val_loss: 0.2798 - val_pos_accuracy: 0.7875\n",
      "Epoch 2510/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0377 - pos_accuracy: 0.9526 - val_loss: 0.2803 - val_pos_accuracy: 0.7875\n",
      "Epoch 2511/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0377 - pos_accuracy: 0.9518 - val_loss: 0.2797 - val_pos_accuracy: 0.7875\n",
      "Epoch 2512/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0377 - pos_accuracy: 0.9502 - val_loss: 0.2799 - val_pos_accuracy: 0.7875\n",
      "Epoch 2513/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0377 - pos_accuracy: 0.9525 - val_loss: 0.2794 - val_pos_accuracy: 0.7875\n",
      "Epoch 2514/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0378 - pos_accuracy: 0.9514 - val_loss: 0.2795 - val_pos_accuracy: 0.7875\n",
      "Epoch 2515/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0377 - pos_accuracy: 0.9549 - val_loss: 0.2795 - val_pos_accuracy: 0.7875\n",
      "Epoch 2516/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0377 - pos_accuracy: 0.9487 - val_loss: 0.2805 - val_pos_accuracy: 0.7850\n",
      "Epoch 2517/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0377 - pos_accuracy: 0.9496 - val_loss: 0.2807 - val_pos_accuracy: 0.7850\n",
      "Epoch 2518/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0376 - pos_accuracy: 0.9516 - val_loss: 0.2806 - val_pos_accuracy: 0.7875\n",
      "Epoch 2519/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0375 - pos_accuracy: 0.9544 - val_loss: 0.2800 - val_pos_accuracy: 0.7875\n",
      "Epoch 2520/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0374 - pos_accuracy: 0.9567 - val_loss: 0.2789 - val_pos_accuracy: 0.7875\n",
      "Epoch 2521/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0375 - pos_accuracy: 0.9564 - val_loss: 0.2799 - val_pos_accuracy: 0.7875\n",
      "Epoch 2522/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0374 - pos_accuracy: 0.9569 - val_loss: 0.2800 - val_pos_accuracy: 0.7875\n",
      "Epoch 2523/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0375 - pos_accuracy: 0.9543 - val_loss: 0.2807 - val_pos_accuracy: 0.7875\n",
      "Epoch 2524/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0374 - pos_accuracy: 0.9545 - val_loss: 0.2798 - val_pos_accuracy: 0.7875\n",
      "Epoch 2525/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0374 - pos_accuracy: 0.9549 - val_loss: 0.2800 - val_pos_accuracy: 0.7875\n",
      "Epoch 2526/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0373 - pos_accuracy: 0.9545 - val_loss: 0.2802 - val_pos_accuracy: 0.7875\n",
      "Epoch 2527/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0373 - pos_accuracy: 0.9558 - val_loss: 0.2797 - val_pos_accuracy: 0.7875\n",
      "Epoch 2528/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0375 - pos_accuracy: 0.9522 - val_loss: 0.2818 - val_pos_accuracy: 0.7850\n",
      "Epoch 2529/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0374 - pos_accuracy: 0.9516 - val_loss: 0.2792 - val_pos_accuracy: 0.7875\n",
      "Epoch 2530/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0374 - pos_accuracy: 0.9510 - val_loss: 0.2793 - val_pos_accuracy: 0.7875\n",
      "Epoch 2531/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0372 - pos_accuracy: 0.9566 - val_loss: 0.2806 - val_pos_accuracy: 0.7900\n",
      "Epoch 2532/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0372 - pos_accuracy: 0.9513 - val_loss: 0.2808 - val_pos_accuracy: 0.7900\n",
      "Epoch 2533/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0372 - pos_accuracy: 0.9584 - val_loss: 0.2799 - val_pos_accuracy: 0.7925\n",
      "Epoch 2534/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0371 - pos_accuracy: 0.9536 - val_loss: 0.2797 - val_pos_accuracy: 0.7925\n",
      "Epoch 2535/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0372 - pos_accuracy: 0.9529 - val_loss: 0.2803 - val_pos_accuracy: 0.7900\n",
      "Epoch 2536/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0372 - pos_accuracy: 0.9555 - val_loss: 0.2797 - val_pos_accuracy: 0.7925\n",
      "Epoch 2537/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0370 - pos_accuracy: 0.9529 - val_loss: 0.2788 - val_pos_accuracy: 0.7850\n",
      "Epoch 2538/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0370 - pos_accuracy: 0.9593 - val_loss: 0.2801 - val_pos_accuracy: 0.7925\n",
      "Epoch 2539/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0371 - pos_accuracy: 0.9522 - val_loss: 0.2787 - val_pos_accuracy: 0.7850\n",
      "Epoch 2540/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0371 - pos_accuracy: 0.9537 - val_loss: 0.2794 - val_pos_accuracy: 0.7875\n",
      "Epoch 2541/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0370 - pos_accuracy: 0.9538 - val_loss: 0.2794 - val_pos_accuracy: 0.7875\n",
      "Epoch 2542/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0369 - pos_accuracy: 0.9558 - val_loss: 0.2795 - val_pos_accuracy: 0.7875\n",
      "Epoch 2543/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0368 - pos_accuracy: 0.9501 - val_loss: 0.2789 - val_pos_accuracy: 0.7875\n",
      "Epoch 2544/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0369 - pos_accuracy: 0.9538 - val_loss: 0.2793 - val_pos_accuracy: 0.7875\n",
      "Epoch 2545/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0368 - pos_accuracy: 0.9562 - val_loss: 0.2796 - val_pos_accuracy: 0.7875\n",
      "Epoch 2546/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0368 - pos_accuracy: 0.9517 - val_loss: 0.2797 - val_pos_accuracy: 0.7900\n",
      "Epoch 2547/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0368 - pos_accuracy: 0.9539 - val_loss: 0.2794 - val_pos_accuracy: 0.7850\n",
      "Epoch 2548/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0368 - pos_accuracy: 0.9546 - val_loss: 0.2792 - val_pos_accuracy: 0.7875\n",
      "Epoch 2549/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0367 - pos_accuracy: 0.9571 - val_loss: 0.2790 - val_pos_accuracy: 0.7875\n",
      "Epoch 2550/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0367 - pos_accuracy: 0.9564 - val_loss: 0.2786 - val_pos_accuracy: 0.7875\n",
      "Epoch 2551/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0366 - pos_accuracy: 0.9556 - val_loss: 0.2794 - val_pos_accuracy: 0.7875\n",
      "Epoch 2552/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0367 - pos_accuracy: 0.9581 - val_loss: 0.2798 - val_pos_accuracy: 0.7900\n",
      "Epoch 2553/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0366 - pos_accuracy: 0.9576 - val_loss: 0.2795 - val_pos_accuracy: 0.7900\n",
      "Epoch 2554/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0366 - pos_accuracy: 0.9545 - val_loss: 0.2790 - val_pos_accuracy: 0.7875\n",
      "Epoch 2555/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0366 - pos_accuracy: 0.9527 - val_loss: 0.2797 - val_pos_accuracy: 0.7900\n",
      "Epoch 2556/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0366 - pos_accuracy: 0.9577 - val_loss: 0.2792 - val_pos_accuracy: 0.7900\n",
      "Epoch 2557/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0365 - pos_accuracy: 0.9596 - val_loss: 0.2787 - val_pos_accuracy: 0.7875\n",
      "Epoch 2558/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0365 - pos_accuracy: 0.9580 - val_loss: 0.2788 - val_pos_accuracy: 0.7900\n",
      "Epoch 2559/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0365 - pos_accuracy: 0.9535 - val_loss: 0.2792 - val_pos_accuracy: 0.7900\n",
      "Epoch 2560/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0364 - pos_accuracy: 0.9572 - val_loss: 0.2781 - val_pos_accuracy: 0.7875\n",
      "Epoch 2561/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0364 - pos_accuracy: 0.9575 - val_loss: 0.2790 - val_pos_accuracy: 0.7925\n",
      "Epoch 2562/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0364 - pos_accuracy: 0.9577 - val_loss: 0.2791 - val_pos_accuracy: 0.7900\n",
      "Epoch 2563/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0364 - pos_accuracy: 0.9596 - val_loss: 0.2783 - val_pos_accuracy: 0.7900\n",
      "Epoch 2564/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0363 - pos_accuracy: 0.9581 - val_loss: 0.2784 - val_pos_accuracy: 0.7900\n",
      "Epoch 2565/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0364 - pos_accuracy: 0.9577 - val_loss: 0.2786 - val_pos_accuracy: 0.7900\n",
      "Epoch 2566/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0364 - pos_accuracy: 0.9592 - val_loss: 0.2784 - val_pos_accuracy: 0.7875\n",
      "Epoch 2567/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0363 - pos_accuracy: 0.9549 - val_loss: 0.2791 - val_pos_accuracy: 0.7900\n",
      "Epoch 2568/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0362 - pos_accuracy: 0.9551 - val_loss: 0.2780 - val_pos_accuracy: 0.7875\n",
      "Epoch 2569/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0363 - pos_accuracy: 0.9580 - val_loss: 0.2795 - val_pos_accuracy: 0.7875\n",
      "Epoch 2570/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0363 - pos_accuracy: 0.9578 - val_loss: 0.2790 - val_pos_accuracy: 0.7900\n",
      "Epoch 2571/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0362 - pos_accuracy: 0.9566 - val_loss: 0.2780 - val_pos_accuracy: 0.7875\n",
      "Epoch 2572/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0361 - pos_accuracy: 0.9582 - val_loss: 0.2778 - val_pos_accuracy: 0.7875\n",
      "Epoch 2573/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0363 - pos_accuracy: 0.9576 - val_loss: 0.2773 - val_pos_accuracy: 0.7875\n",
      "Epoch 2574/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0363 - pos_accuracy: 0.9602 - val_loss: 0.2771 - val_pos_accuracy: 0.7850\n",
      "Epoch 2575/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0361 - pos_accuracy: 0.9569 - val_loss: 0.2775 - val_pos_accuracy: 0.7850\n",
      "Epoch 2576/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0361 - pos_accuracy: 0.9564 - val_loss: 0.2782 - val_pos_accuracy: 0.7925\n",
      "Epoch 2577/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0361 - pos_accuracy: 0.9567 - val_loss: 0.2775 - val_pos_accuracy: 0.7925\n",
      "Epoch 2578/3000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0361 - pos_accuracy: 0.9544 - val_loss: 0.2773 - val_pos_accuracy: 0.7850\n",
      "Epoch 2579/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0362 - pos_accuracy: 0.9583 - val_loss: 0.2771 - val_pos_accuracy: 0.7850\n",
      "Epoch 2580/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0360 - pos_accuracy: 0.9572 - val_loss: 0.2778 - val_pos_accuracy: 0.7925\n",
      "Epoch 2581/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0360 - pos_accuracy: 0.9586 - val_loss: 0.2778 - val_pos_accuracy: 0.7950\n",
      "Epoch 2582/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0359 - pos_accuracy: 0.9518 - val_loss: 0.2784 - val_pos_accuracy: 0.7900\n",
      "Epoch 2583/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0359 - pos_accuracy: 0.9562 - val_loss: 0.2775 - val_pos_accuracy: 0.7900\n",
      "Epoch 2584/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0358 - pos_accuracy: 0.9564 - val_loss: 0.2778 - val_pos_accuracy: 0.7900\n",
      "Epoch 2585/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0358 - pos_accuracy: 0.9550 - val_loss: 0.2779 - val_pos_accuracy: 0.7900\n",
      "Epoch 2586/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0357 - pos_accuracy: 0.9576 - val_loss: 0.2776 - val_pos_accuracy: 0.7950\n",
      "Epoch 2587/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0358 - pos_accuracy: 0.9533 - val_loss: 0.2782 - val_pos_accuracy: 0.7950\n",
      "Epoch 2588/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0358 - pos_accuracy: 0.9570 - val_loss: 0.2776 - val_pos_accuracy: 0.7950\n",
      "Epoch 2589/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0357 - pos_accuracy: 0.9583 - val_loss: 0.2774 - val_pos_accuracy: 0.7875\n",
      "Epoch 2590/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0357 - pos_accuracy: 0.9569 - val_loss: 0.2771 - val_pos_accuracy: 0.7900\n",
      "Epoch 2591/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0357 - pos_accuracy: 0.9573 - val_loss: 0.2776 - val_pos_accuracy: 0.7950\n",
      "Epoch 2592/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0356 - pos_accuracy: 0.9550 - val_loss: 0.2775 - val_pos_accuracy: 0.7950\n",
      "Epoch 2593/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0357 - pos_accuracy: 0.9577 - val_loss: 0.2777 - val_pos_accuracy: 0.7950\n",
      "Epoch 2594/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0356 - pos_accuracy: 0.9571 - val_loss: 0.2773 - val_pos_accuracy: 0.7900\n",
      "Epoch 2595/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0356 - pos_accuracy: 0.9569 - val_loss: 0.2776 - val_pos_accuracy: 0.7950\n",
      "Epoch 2596/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0355 - pos_accuracy: 0.9554 - val_loss: 0.2779 - val_pos_accuracy: 0.7950\n",
      "Epoch 2597/3000\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0355 - pos_accuracy: 0.9569 - val_loss: 0.2777 - val_pos_accuracy: 0.7900\n",
      "Epoch 2598/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0355 - pos_accuracy: 0.9582 - val_loss: 0.2778 - val_pos_accuracy: 0.7900\n",
      "Epoch 2599/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0354 - pos_accuracy: 0.9594 - val_loss: 0.2777 - val_pos_accuracy: 0.7950\n",
      "Epoch 2600/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0355 - pos_accuracy: 0.9589 - val_loss: 0.2775 - val_pos_accuracy: 0.7950\n",
      "Epoch 2601/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0354 - pos_accuracy: 0.9587 - val_loss: 0.2776 - val_pos_accuracy: 0.7950\n",
      "Epoch 2602/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0354 - pos_accuracy: 0.9601 - val_loss: 0.2777 - val_pos_accuracy: 0.7950\n",
      "Epoch 2603/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0354 - pos_accuracy: 0.9593 - val_loss: 0.2775 - val_pos_accuracy: 0.7950\n",
      "Epoch 2604/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0354 - pos_accuracy: 0.9597 - val_loss: 0.2770 - val_pos_accuracy: 0.7925\n",
      "Epoch 2605/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0353 - pos_accuracy: 0.9589 - val_loss: 0.2773 - val_pos_accuracy: 0.7925\n",
      "Epoch 2606/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0353 - pos_accuracy: 0.9589 - val_loss: 0.2784 - val_pos_accuracy: 0.7925\n",
      "Epoch 2607/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0354 - pos_accuracy: 0.9590 - val_loss: 0.2780 - val_pos_accuracy: 0.7950\n",
      "Epoch 2608/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0353 - pos_accuracy: 0.9573 - val_loss: 0.2782 - val_pos_accuracy: 0.7975\n",
      "Epoch 2609/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0353 - pos_accuracy: 0.9547 - val_loss: 0.2777 - val_pos_accuracy: 0.7900\n",
      "Epoch 2610/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0353 - pos_accuracy: 0.9553 - val_loss: 0.2772 - val_pos_accuracy: 0.7950\n",
      "Epoch 2611/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0353 - pos_accuracy: 0.9569 - val_loss: 0.2764 - val_pos_accuracy: 0.7875\n",
      "Epoch 2612/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0352 - pos_accuracy: 0.9535 - val_loss: 0.2774 - val_pos_accuracy: 0.7900\n",
      "Epoch 2613/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0352 - pos_accuracy: 0.9553 - val_loss: 0.2769 - val_pos_accuracy: 0.7925\n",
      "Epoch 2614/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0351 - pos_accuracy: 0.9574 - val_loss: 0.2774 - val_pos_accuracy: 0.7900\n",
      "Epoch 2615/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0351 - pos_accuracy: 0.9579 - val_loss: 0.2770 - val_pos_accuracy: 0.7925\n",
      "Epoch 2616/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0353 - pos_accuracy: 0.9562 - val_loss: 0.2764 - val_pos_accuracy: 0.7850\n",
      "Epoch 2617/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0353 - pos_accuracy: 0.9572 - val_loss: 0.2769 - val_pos_accuracy: 0.7875\n",
      "Epoch 2618/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0350 - pos_accuracy: 0.9559 - val_loss: 0.2775 - val_pos_accuracy: 0.7950\n",
      "Epoch 2619/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0350 - pos_accuracy: 0.9602 - val_loss: 0.2772 - val_pos_accuracy: 0.7900\n",
      "Epoch 2620/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0350 - pos_accuracy: 0.9575 - val_loss: 0.2774 - val_pos_accuracy: 0.7900\n",
      "Epoch 2621/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0349 - pos_accuracy: 0.9570 - val_loss: 0.2775 - val_pos_accuracy: 0.7950\n",
      "Epoch 2622/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0349 - pos_accuracy: 0.9594 - val_loss: 0.2771 - val_pos_accuracy: 0.7950\n",
      "Epoch 2623/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0348 - pos_accuracy: 0.9593 - val_loss: 0.2773 - val_pos_accuracy: 0.7950\n",
      "Epoch 2624/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0349 - pos_accuracy: 0.9588 - val_loss: 0.2773 - val_pos_accuracy: 0.7950\n",
      "Epoch 2625/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0349 - pos_accuracy: 0.9571 - val_loss: 0.2774 - val_pos_accuracy: 0.7900\n",
      "Epoch 2626/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0348 - pos_accuracy: 0.9594 - val_loss: 0.2770 - val_pos_accuracy: 0.7950\n",
      "Epoch 2627/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0348 - pos_accuracy: 0.9579 - val_loss: 0.2772 - val_pos_accuracy: 0.7950\n",
      "Epoch 2628/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0348 - pos_accuracy: 0.9563 - val_loss: 0.2778 - val_pos_accuracy: 0.7950\n",
      "Epoch 2629/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0348 - pos_accuracy: 0.9598 - val_loss: 0.2769 - val_pos_accuracy: 0.7950\n",
      "Epoch 2630/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0347 - pos_accuracy: 0.9555 - val_loss: 0.2765 - val_pos_accuracy: 0.7950\n",
      "Epoch 2631/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0346 - pos_accuracy: 0.9620 - val_loss: 0.2776 - val_pos_accuracy: 0.7950\n",
      "Epoch 2632/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0347 - pos_accuracy: 0.9564 - val_loss: 0.2762 - val_pos_accuracy: 0.7950\n",
      "Epoch 2633/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0347 - pos_accuracy: 0.9601 - val_loss: 0.2761 - val_pos_accuracy: 0.7925\n",
      "Epoch 2634/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0347 - pos_accuracy: 0.9571 - val_loss: 0.2758 - val_pos_accuracy: 0.7925\n",
      "Epoch 2635/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0347 - pos_accuracy: 0.9571 - val_loss: 0.2759 - val_pos_accuracy: 0.7925\n",
      "Epoch 2636/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0346 - pos_accuracy: 0.9579 - val_loss: 0.2758 - val_pos_accuracy: 0.7925\n",
      "Epoch 2637/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0347 - pos_accuracy: 0.9579 - val_loss: 0.2759 - val_pos_accuracy: 0.7925\n",
      "Epoch 2638/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0346 - pos_accuracy: 0.9586 - val_loss: 0.2760 - val_pos_accuracy: 0.7925\n",
      "Epoch 2639/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0346 - pos_accuracy: 0.9586 - val_loss: 0.2772 - val_pos_accuracy: 0.7950\n",
      "Epoch 2640/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0346 - pos_accuracy: 0.9567 - val_loss: 0.2773 - val_pos_accuracy: 0.7950\n",
      "Epoch 2641/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0345 - pos_accuracy: 0.9590 - val_loss: 0.2763 - val_pos_accuracy: 0.7950\n",
      "Epoch 2642/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0344 - pos_accuracy: 0.9568 - val_loss: 0.2768 - val_pos_accuracy: 0.7950\n",
      "Epoch 2643/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0344 - pos_accuracy: 0.9594 - val_loss: 0.2762 - val_pos_accuracy: 0.7950\n",
      "Epoch 2644/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0344 - pos_accuracy: 0.9617 - val_loss: 0.2762 - val_pos_accuracy: 0.7975\n",
      "Epoch 2645/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0343 - pos_accuracy: 0.9590 - val_loss: 0.2761 - val_pos_accuracy: 0.7950\n",
      "Epoch 2646/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0344 - pos_accuracy: 0.9606 - val_loss: 0.2766 - val_pos_accuracy: 0.7975\n",
      "Epoch 2647/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0345 - pos_accuracy: 0.9599 - val_loss: 0.2765 - val_pos_accuracy: 0.7925\n",
      "Epoch 2648/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0345 - pos_accuracy: 0.9598 - val_loss: 0.2766 - val_pos_accuracy: 0.7925\n",
      "Epoch 2649/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0345 - pos_accuracy: 0.9593 - val_loss: 0.2769 - val_pos_accuracy: 0.7925\n",
      "Epoch 2650/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0345 - pos_accuracy: 0.9590 - val_loss: 0.2770 - val_pos_accuracy: 0.7925\n",
      "Epoch 2651/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0343 - pos_accuracy: 0.9575 - val_loss: 0.2763 - val_pos_accuracy: 0.7900\n",
      "Epoch 2652/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0343 - pos_accuracy: 0.9590 - val_loss: 0.2762 - val_pos_accuracy: 0.7900\n",
      "Epoch 2653/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0343 - pos_accuracy: 0.9581 - val_loss: 0.2758 - val_pos_accuracy: 0.7900\n",
      "Epoch 2654/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0342 - pos_accuracy: 0.9558 - val_loss: 0.2761 - val_pos_accuracy: 0.7875\n",
      "Epoch 2655/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0342 - pos_accuracy: 0.9602 - val_loss: 0.2762 - val_pos_accuracy: 0.7950\n",
      "Epoch 2656/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0343 - pos_accuracy: 0.9590 - val_loss: 0.2764 - val_pos_accuracy: 0.7950\n",
      "Epoch 2657/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0341 - pos_accuracy: 0.9575 - val_loss: 0.2762 - val_pos_accuracy: 0.7950\n",
      "Epoch 2658/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0341 - pos_accuracy: 0.9598 - val_loss: 0.2758 - val_pos_accuracy: 0.7950\n",
      "Epoch 2659/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0340 - pos_accuracy: 0.9579 - val_loss: 0.2761 - val_pos_accuracy: 0.7950\n",
      "Epoch 2660/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0341 - pos_accuracy: 0.9564 - val_loss: 0.2765 - val_pos_accuracy: 0.7950\n",
      "Epoch 2661/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0340 - pos_accuracy: 0.9609 - val_loss: 0.2757 - val_pos_accuracy: 0.7950\n",
      "Epoch 2662/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0340 - pos_accuracy: 0.9587 - val_loss: 0.2759 - val_pos_accuracy: 0.7950\n",
      "Epoch 2663/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0340 - pos_accuracy: 0.9587 - val_loss: 0.2762 - val_pos_accuracy: 0.7950\n",
      "Epoch 2664/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0339 - pos_accuracy: 0.9621 - val_loss: 0.2758 - val_pos_accuracy: 0.7950\n",
      "Epoch 2665/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0341 - pos_accuracy: 0.9582 - val_loss: 0.2756 - val_pos_accuracy: 0.7950\n",
      "Epoch 2666/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0340 - pos_accuracy: 0.9578 - val_loss: 0.2760 - val_pos_accuracy: 0.7925\n",
      "Epoch 2667/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0339 - pos_accuracy: 0.9575 - val_loss: 0.2753 - val_pos_accuracy: 0.7950\n",
      "Epoch 2668/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0339 - pos_accuracy: 0.9590 - val_loss: 0.2756 - val_pos_accuracy: 0.7975\n",
      "Epoch 2669/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0339 - pos_accuracy: 0.9587 - val_loss: 0.2755 - val_pos_accuracy: 0.7975\n",
      "Epoch 2670/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0339 - pos_accuracy: 0.9602 - val_loss: 0.2754 - val_pos_accuracy: 0.7975\n",
      "Epoch 2671/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0337 - pos_accuracy: 0.9582 - val_loss: 0.2758 - val_pos_accuracy: 0.7950\n",
      "Epoch 2672/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0338 - pos_accuracy: 0.9569 - val_loss: 0.2765 - val_pos_accuracy: 0.7925\n",
      "Epoch 2673/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0338 - pos_accuracy: 0.9576 - val_loss: 0.2755 - val_pos_accuracy: 0.7900\n",
      "Epoch 2674/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0337 - pos_accuracy: 0.9562 - val_loss: 0.2757 - val_pos_accuracy: 0.7950\n",
      "Epoch 2675/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0337 - pos_accuracy: 0.9613 - val_loss: 0.2757 - val_pos_accuracy: 0.7950\n",
      "Epoch 2676/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0336 - pos_accuracy: 0.9602 - val_loss: 0.2756 - val_pos_accuracy: 0.7975\n",
      "Epoch 2677/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0337 - pos_accuracy: 0.9597 - val_loss: 0.2751 - val_pos_accuracy: 0.7975\n",
      "Epoch 2678/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0337 - pos_accuracy: 0.9593 - val_loss: 0.2755 - val_pos_accuracy: 0.7950\n",
      "Epoch 2679/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0336 - pos_accuracy: 0.9583 - val_loss: 0.2758 - val_pos_accuracy: 0.7950\n",
      "Epoch 2680/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0336 - pos_accuracy: 0.9579 - val_loss: 0.2756 - val_pos_accuracy: 0.7950\n",
      "Epoch 2681/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0335 - pos_accuracy: 0.9590 - val_loss: 0.2754 - val_pos_accuracy: 0.7950\n",
      "Epoch 2682/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0335 - pos_accuracy: 0.9579 - val_loss: 0.2751 - val_pos_accuracy: 0.7950\n",
      "Epoch 2683/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0337 - pos_accuracy: 0.9602 - val_loss: 0.2755 - val_pos_accuracy: 0.7950\n",
      "Epoch 2684/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0335 - pos_accuracy: 0.9587 - val_loss: 0.2754 - val_pos_accuracy: 0.7950\n",
      "Epoch 2685/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0335 - pos_accuracy: 0.9610 - val_loss: 0.2753 - val_pos_accuracy: 0.7925\n",
      "Epoch 2686/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0337 - pos_accuracy: 0.9582 - val_loss: 0.2780 - val_pos_accuracy: 0.7925\n",
      "Epoch 2687/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0339 - pos_accuracy: 0.9579 - val_loss: 0.2758 - val_pos_accuracy: 0.7925\n",
      "Epoch 2688/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0335 - pos_accuracy: 0.9575 - val_loss: 0.2745 - val_pos_accuracy: 0.7950\n",
      "Epoch 2689/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0334 - pos_accuracy: 0.9570 - val_loss: 0.2746 - val_pos_accuracy: 0.7950\n",
      "Epoch 2690/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0334 - pos_accuracy: 0.9568 - val_loss: 0.2765 - val_pos_accuracy: 0.7975\n",
      "Epoch 2691/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0334 - pos_accuracy: 0.9590 - val_loss: 0.2758 - val_pos_accuracy: 0.7925\n",
      "Epoch 2692/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0334 - pos_accuracy: 0.9589 - val_loss: 0.2763 - val_pos_accuracy: 0.7925\n",
      "Epoch 2693/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0335 - pos_accuracy: 0.9578 - val_loss: 0.2767 - val_pos_accuracy: 0.7975\n",
      "Epoch 2694/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0333 - pos_accuracy: 0.9602 - val_loss: 0.2746 - val_pos_accuracy: 0.7950\n",
      "Epoch 2695/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0332 - pos_accuracy: 0.9587 - val_loss: 0.2753 - val_pos_accuracy: 0.7950\n",
      "Epoch 2696/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0333 - pos_accuracy: 0.9587 - val_loss: 0.2755 - val_pos_accuracy: 0.7950\n",
      "Epoch 2697/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0332 - pos_accuracy: 0.9590 - val_loss: 0.2758 - val_pos_accuracy: 0.7925\n",
      "Epoch 2698/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0332 - pos_accuracy: 0.9575 - val_loss: 0.2742 - val_pos_accuracy: 0.7950\n",
      "Epoch 2699/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0332 - pos_accuracy: 0.9610 - val_loss: 0.2753 - val_pos_accuracy: 0.7950\n",
      "Epoch 2700/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0331 - pos_accuracy: 0.9583 - val_loss: 0.2752 - val_pos_accuracy: 0.7925\n",
      "Epoch 2701/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0332 - pos_accuracy: 0.9602 - val_loss: 0.2755 - val_pos_accuracy: 0.7950\n",
      "Epoch 2702/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0331 - pos_accuracy: 0.9568 - val_loss: 0.2750 - val_pos_accuracy: 0.7950\n",
      "Epoch 2703/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0331 - pos_accuracy: 0.9568 - val_loss: 0.2746 - val_pos_accuracy: 0.7950\n",
      "Epoch 2704/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0331 - pos_accuracy: 0.9584 - val_loss: 0.2751 - val_pos_accuracy: 0.7925\n",
      "Epoch 2705/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0330 - pos_accuracy: 0.9571 - val_loss: 0.2751 - val_pos_accuracy: 0.7925\n",
      "Epoch 2706/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0331 - pos_accuracy: 0.9598 - val_loss: 0.2749 - val_pos_accuracy: 0.7925\n",
      "Epoch 2707/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0330 - pos_accuracy: 0.9579 - val_loss: 0.2756 - val_pos_accuracy: 0.7975\n",
      "Epoch 2708/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0331 - pos_accuracy: 0.9568 - val_loss: 0.2751 - val_pos_accuracy: 0.8000\n",
      "Epoch 2709/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0330 - pos_accuracy: 0.9595 - val_loss: 0.2746 - val_pos_accuracy: 0.7925\n",
      "Epoch 2710/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0330 - pos_accuracy: 0.9575 - val_loss: 0.2738 - val_pos_accuracy: 0.7950\n",
      "Epoch 2711/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0329 - pos_accuracy: 0.9568 - val_loss: 0.2745 - val_pos_accuracy: 0.7950\n",
      "Epoch 2712/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0329 - pos_accuracy: 0.9582 - val_loss: 0.2756 - val_pos_accuracy: 0.7950\n",
      "Epoch 2713/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0328 - pos_accuracy: 0.9602 - val_loss: 0.2742 - val_pos_accuracy: 0.7950\n",
      "Epoch 2714/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0328 - pos_accuracy: 0.9594 - val_loss: 0.2740 - val_pos_accuracy: 0.7950\n",
      "Epoch 2715/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0328 - pos_accuracy: 0.9580 - val_loss: 0.2744 - val_pos_accuracy: 0.7950\n",
      "Epoch 2716/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0328 - pos_accuracy: 0.9590 - val_loss: 0.2750 - val_pos_accuracy: 0.7950\n",
      "Epoch 2717/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0328 - pos_accuracy: 0.9590 - val_loss: 0.2741 - val_pos_accuracy: 0.7950\n",
      "Epoch 2718/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0328 - pos_accuracy: 0.9555 - val_loss: 0.2740 - val_pos_accuracy: 0.7950\n",
      "Epoch 2719/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0329 - pos_accuracy: 0.9587 - val_loss: 0.2738 - val_pos_accuracy: 0.7950\n",
      "Epoch 2720/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0328 - pos_accuracy: 0.9594 - val_loss: 0.2738 - val_pos_accuracy: 0.7975\n",
      "Epoch 2721/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0328 - pos_accuracy: 0.9580 - val_loss: 0.2739 - val_pos_accuracy: 0.7975\n",
      "Epoch 2722/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0327 - pos_accuracy: 0.9579 - val_loss: 0.2739 - val_pos_accuracy: 0.7975\n",
      "Epoch 2723/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0327 - pos_accuracy: 0.9602 - val_loss: 0.2738 - val_pos_accuracy: 0.7975\n",
      "Epoch 2724/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0327 - pos_accuracy: 0.9614 - val_loss: 0.2745 - val_pos_accuracy: 0.7950\n",
      "Epoch 2725/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0327 - pos_accuracy: 0.9603 - val_loss: 0.2745 - val_pos_accuracy: 0.8000\n",
      "Epoch 2726/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0327 - pos_accuracy: 0.9560 - val_loss: 0.2744 - val_pos_accuracy: 0.8000\n",
      "Epoch 2727/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0326 - pos_accuracy: 0.9599 - val_loss: 0.2740 - val_pos_accuracy: 0.7975\n",
      "Epoch 2728/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0328 - pos_accuracy: 0.9574 - val_loss: 0.2734 - val_pos_accuracy: 0.8000\n",
      "Epoch 2729/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0327 - pos_accuracy: 0.9591 - val_loss: 0.2742 - val_pos_accuracy: 0.7950\n",
      "Epoch 2730/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0326 - pos_accuracy: 0.9590 - val_loss: 0.2740 - val_pos_accuracy: 0.8000\n",
      "Epoch 2731/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0325 - pos_accuracy: 0.9593 - val_loss: 0.2743 - val_pos_accuracy: 0.7925\n",
      "Epoch 2732/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0325 - pos_accuracy: 0.9599 - val_loss: 0.2743 - val_pos_accuracy: 0.7950\n",
      "Epoch 2733/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0325 - pos_accuracy: 0.9598 - val_loss: 0.2736 - val_pos_accuracy: 0.8000\n",
      "Epoch 2734/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0326 - pos_accuracy: 0.9588 - val_loss: 0.2739 - val_pos_accuracy: 0.8000\n",
      "Epoch 2735/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0324 - pos_accuracy: 0.9588 - val_loss: 0.2742 - val_pos_accuracy: 0.7925\n",
      "Epoch 2736/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0324 - pos_accuracy: 0.9629 - val_loss: 0.2741 - val_pos_accuracy: 0.7975\n",
      "Epoch 2737/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0324 - pos_accuracy: 0.9568 - val_loss: 0.2733 - val_pos_accuracy: 0.7975\n",
      "Epoch 2738/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0324 - pos_accuracy: 0.9579 - val_loss: 0.2731 - val_pos_accuracy: 0.7950\n",
      "Epoch 2739/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0324 - pos_accuracy: 0.9591 - val_loss: 0.2738 - val_pos_accuracy: 0.7950\n",
      "Epoch 2740/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0323 - pos_accuracy: 0.9614 - val_loss: 0.2737 - val_pos_accuracy: 0.7975\n",
      "Epoch 2741/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0323 - pos_accuracy: 0.9599 - val_loss: 0.2735 - val_pos_accuracy: 0.7975\n",
      "Epoch 2742/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0323 - pos_accuracy: 0.9622 - val_loss: 0.2737 - val_pos_accuracy: 0.7975\n",
      "Epoch 2743/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0322 - pos_accuracy: 0.9603 - val_loss: 0.2736 - val_pos_accuracy: 0.7975\n",
      "Epoch 2744/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0322 - pos_accuracy: 0.9603 - val_loss: 0.2737 - val_pos_accuracy: 0.7950\n",
      "Epoch 2745/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0322 - pos_accuracy: 0.9614 - val_loss: 0.2734 - val_pos_accuracy: 0.7975\n",
      "Epoch 2746/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0322 - pos_accuracy: 0.9614 - val_loss: 0.2735 - val_pos_accuracy: 0.7950\n",
      "Epoch 2747/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0322 - pos_accuracy: 0.9552 - val_loss: 0.2729 - val_pos_accuracy: 0.7975\n",
      "Epoch 2748/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0323 - pos_accuracy: 0.9588 - val_loss: 0.2728 - val_pos_accuracy: 0.7950\n",
      "Epoch 2749/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0321 - pos_accuracy: 0.9578 - val_loss: 0.2739 - val_pos_accuracy: 0.7975\n",
      "Epoch 2750/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0321 - pos_accuracy: 0.9614 - val_loss: 0.2731 - val_pos_accuracy: 0.7950\n",
      "Epoch 2751/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0322 - pos_accuracy: 0.9570 - val_loss: 0.2728 - val_pos_accuracy: 0.7950\n",
      "Epoch 2752/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0322 - pos_accuracy: 0.9583 - val_loss: 0.2726 - val_pos_accuracy: 0.8000\n",
      "Epoch 2753/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0321 - pos_accuracy: 0.9606 - val_loss: 0.2728 - val_pos_accuracy: 0.8000\n",
      "Epoch 2754/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0320 - pos_accuracy: 0.9590 - val_loss: 0.2730 - val_pos_accuracy: 0.7950\n",
      "Epoch 2755/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0320 - pos_accuracy: 0.9570 - val_loss: 0.2738 - val_pos_accuracy: 0.8000\n",
      "Epoch 2756/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0321 - pos_accuracy: 0.9556 - val_loss: 0.2745 - val_pos_accuracy: 0.8000\n",
      "Epoch 2757/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0320 - pos_accuracy: 0.9615 - val_loss: 0.2735 - val_pos_accuracy: 0.7975\n",
      "Epoch 2758/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0318 - pos_accuracy: 0.9622 - val_loss: 0.2731 - val_pos_accuracy: 0.7975\n",
      "Epoch 2759/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0318 - pos_accuracy: 0.9599 - val_loss: 0.2732 - val_pos_accuracy: 0.7975\n",
      "Epoch 2760/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0319 - pos_accuracy: 0.9607 - val_loss: 0.2737 - val_pos_accuracy: 0.7950\n",
      "Epoch 2761/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0318 - pos_accuracy: 0.9571 - val_loss: 0.2733 - val_pos_accuracy: 0.7950\n",
      "Epoch 2762/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0318 - pos_accuracy: 0.9575 - val_loss: 0.2730 - val_pos_accuracy: 0.8000\n",
      "Epoch 2763/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0318 - pos_accuracy: 0.9607 - val_loss: 0.2731 - val_pos_accuracy: 0.7975\n",
      "Epoch 2764/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0318 - pos_accuracy: 0.9603 - val_loss: 0.2730 - val_pos_accuracy: 0.7950\n",
      "Epoch 2765/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0317 - pos_accuracy: 0.9576 - val_loss: 0.2729 - val_pos_accuracy: 0.7975\n",
      "Epoch 2766/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0317 - pos_accuracy: 0.9572 - val_loss: 0.2741 - val_pos_accuracy: 0.8050\n",
      "Epoch 2767/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0318 - pos_accuracy: 0.9628 - val_loss: 0.2730 - val_pos_accuracy: 0.7975\n",
      "Epoch 2768/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0317 - pos_accuracy: 0.9607 - val_loss: 0.2743 - val_pos_accuracy: 0.8000\n",
      "Epoch 2769/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0318 - pos_accuracy: 0.9607 - val_loss: 0.2740 - val_pos_accuracy: 0.8025\n",
      "Epoch 2770/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0316 - pos_accuracy: 0.9594 - val_loss: 0.2727 - val_pos_accuracy: 0.7975\n",
      "Epoch 2771/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0316 - pos_accuracy: 0.9576 - val_loss: 0.2743 - val_pos_accuracy: 0.8000\n",
      "Epoch 2772/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0319 - pos_accuracy: 0.9588 - val_loss: 0.2742 - val_pos_accuracy: 0.8000\n",
      "Epoch 2773/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0317 - pos_accuracy: 0.9608 - val_loss: 0.2725 - val_pos_accuracy: 0.8000\n",
      "Epoch 2774/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0317 - pos_accuracy: 0.9608 - val_loss: 0.2734 - val_pos_accuracy: 0.8025\n",
      "Epoch 2775/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0316 - pos_accuracy: 0.9585 - val_loss: 0.2741 - val_pos_accuracy: 0.8000\n",
      "Epoch 2776/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0315 - pos_accuracy: 0.9612 - val_loss: 0.2721 - val_pos_accuracy: 0.8000\n",
      "Epoch 2777/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0317 - pos_accuracy: 0.9589 - val_loss: 0.2723 - val_pos_accuracy: 0.8000\n",
      "Epoch 2778/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0317 - pos_accuracy: 0.9608 - val_loss: 0.2723 - val_pos_accuracy: 0.7950\n",
      "Epoch 2779/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0316 - pos_accuracy: 0.9599 - val_loss: 0.2736 - val_pos_accuracy: 0.8025\n",
      "Epoch 2780/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0315 - pos_accuracy: 0.9589 - val_loss: 0.2722 - val_pos_accuracy: 0.8025\n",
      "Epoch 2781/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0314 - pos_accuracy: 0.9591 - val_loss: 0.2726 - val_pos_accuracy: 0.8025\n",
      "Epoch 2782/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0313 - pos_accuracy: 0.9610 - val_loss: 0.2721 - val_pos_accuracy: 0.8025\n",
      "Epoch 2783/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0314 - pos_accuracy: 0.9608 - val_loss: 0.2725 - val_pos_accuracy: 0.8000\n",
      "Epoch 2784/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0314 - pos_accuracy: 0.9604 - val_loss: 0.2724 - val_pos_accuracy: 0.8000\n",
      "Epoch 2785/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0313 - pos_accuracy: 0.9581 - val_loss: 0.2727 - val_pos_accuracy: 0.8025\n",
      "Epoch 2786/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0313 - pos_accuracy: 0.9615 - val_loss: 0.2727 - val_pos_accuracy: 0.8000\n",
      "Epoch 2787/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0313 - pos_accuracy: 0.9577 - val_loss: 0.2721 - val_pos_accuracy: 0.8000\n",
      "Epoch 2788/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0312 - pos_accuracy: 0.9608 - val_loss: 0.2721 - val_pos_accuracy: 0.8000\n",
      "Epoch 2789/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0312 - pos_accuracy: 0.9600 - val_loss: 0.2726 - val_pos_accuracy: 0.8000\n",
      "Epoch 2790/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0312 - pos_accuracy: 0.9626 - val_loss: 0.2724 - val_pos_accuracy: 0.8075\n",
      "Epoch 2791/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0312 - pos_accuracy: 0.9605 - val_loss: 0.2723 - val_pos_accuracy: 0.8025\n",
      "Epoch 2792/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0311 - pos_accuracy: 0.9608 - val_loss: 0.2725 - val_pos_accuracy: 0.8025\n",
      "Epoch 2793/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0311 - pos_accuracy: 0.9623 - val_loss: 0.2726 - val_pos_accuracy: 0.8025\n",
      "Epoch 2794/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0312 - pos_accuracy: 0.9597 - val_loss: 0.2725 - val_pos_accuracy: 0.8025\n",
      "Epoch 2795/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0312 - pos_accuracy: 0.9609 - val_loss: 0.2723 - val_pos_accuracy: 0.8000\n",
      "Epoch 2796/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0312 - pos_accuracy: 0.9637 - val_loss: 0.2718 - val_pos_accuracy: 0.8050\n",
      "Epoch 2797/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0311 - pos_accuracy: 0.9589 - val_loss: 0.2720 - val_pos_accuracy: 0.8050\n",
      "Epoch 2798/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0311 - pos_accuracy: 0.9597 - val_loss: 0.2719 - val_pos_accuracy: 0.8075\n",
      "Epoch 2799/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0310 - pos_accuracy: 0.9601 - val_loss: 0.2723 - val_pos_accuracy: 0.8000\n",
      "Epoch 2800/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0310 - pos_accuracy: 0.9619 - val_loss: 0.2722 - val_pos_accuracy: 0.8025\n",
      "Epoch 2801/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0310 - pos_accuracy: 0.9615 - val_loss: 0.2720 - val_pos_accuracy: 0.8000\n",
      "Epoch 2802/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0310 - pos_accuracy: 0.9586 - val_loss: 0.2725 - val_pos_accuracy: 0.8025\n",
      "Epoch 2803/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0310 - pos_accuracy: 0.9586 - val_loss: 0.2718 - val_pos_accuracy: 0.7975\n",
      "Epoch 2804/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0310 - pos_accuracy: 0.9600 - val_loss: 0.2722 - val_pos_accuracy: 0.8025\n",
      "Epoch 2805/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0310 - pos_accuracy: 0.9585 - val_loss: 0.2721 - val_pos_accuracy: 0.8050\n",
      "Epoch 2806/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0309 - pos_accuracy: 0.9613 - val_loss: 0.2720 - val_pos_accuracy: 0.8025\n",
      "Epoch 2807/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0309 - pos_accuracy: 0.9614 - val_loss: 0.2717 - val_pos_accuracy: 0.7975\n",
      "Epoch 2808/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0310 - pos_accuracy: 0.9605 - val_loss: 0.2716 - val_pos_accuracy: 0.8050\n",
      "Epoch 2809/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0308 - pos_accuracy: 0.9594 - val_loss: 0.2730 - val_pos_accuracy: 0.7975\n",
      "Epoch 2810/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0309 - pos_accuracy: 0.9599 - val_loss: 0.2733 - val_pos_accuracy: 0.8050\n",
      "Epoch 2811/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0309 - pos_accuracy: 0.9578 - val_loss: 0.2726 - val_pos_accuracy: 0.8025\n",
      "Epoch 2812/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0309 - pos_accuracy: 0.9605 - val_loss: 0.2726 - val_pos_accuracy: 0.8000\n",
      "Epoch 2813/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0308 - pos_accuracy: 0.9608 - val_loss: 0.2736 - val_pos_accuracy: 0.8025\n",
      "Epoch 2814/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0310 - pos_accuracy: 0.9599 - val_loss: 0.2727 - val_pos_accuracy: 0.8050\n",
      "Epoch 2815/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0308 - pos_accuracy: 0.9628 - val_loss: 0.2733 - val_pos_accuracy: 0.8025\n",
      "Epoch 2816/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0309 - pos_accuracy: 0.9574 - val_loss: 0.2727 - val_pos_accuracy: 0.7975\n",
      "Epoch 2817/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0307 - pos_accuracy: 0.9608 - val_loss: 0.2709 - val_pos_accuracy: 0.8000\n",
      "Epoch 2818/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0307 - pos_accuracy: 0.9613 - val_loss: 0.2720 - val_pos_accuracy: 0.8050\n",
      "Epoch 2819/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0307 - pos_accuracy: 0.9575 - val_loss: 0.2707 - val_pos_accuracy: 0.8050\n",
      "Epoch 2820/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0308 - pos_accuracy: 0.9609 - val_loss: 0.2720 - val_pos_accuracy: 0.7975\n",
      "Epoch 2821/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0306 - pos_accuracy: 0.9601 - val_loss: 0.2721 - val_pos_accuracy: 0.8025\n",
      "Epoch 2822/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0305 - pos_accuracy: 0.9622 - val_loss: 0.2721 - val_pos_accuracy: 0.8025\n",
      "Epoch 2823/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0305 - pos_accuracy: 0.9612 - val_loss: 0.2719 - val_pos_accuracy: 0.8025\n",
      "Epoch 2824/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0305 - pos_accuracy: 0.9577 - val_loss: 0.2714 - val_pos_accuracy: 0.8075\n",
      "Epoch 2825/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0305 - pos_accuracy: 0.9601 - val_loss: 0.2721 - val_pos_accuracy: 0.8000\n",
      "Epoch 2826/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0305 - pos_accuracy: 0.9609 - val_loss: 0.2717 - val_pos_accuracy: 0.8025\n",
      "Epoch 2827/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0306 - pos_accuracy: 0.9619 - val_loss: 0.2713 - val_pos_accuracy: 0.8100\n",
      "Epoch 2828/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0305 - pos_accuracy: 0.9618 - val_loss: 0.2722 - val_pos_accuracy: 0.8025\n",
      "Epoch 2829/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0306 - pos_accuracy: 0.9608 - val_loss: 0.2724 - val_pos_accuracy: 0.8025\n",
      "Epoch 2830/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0305 - pos_accuracy: 0.9621 - val_loss: 0.2713 - val_pos_accuracy: 0.8025\n",
      "Epoch 2831/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0304 - pos_accuracy: 0.9608 - val_loss: 0.2708 - val_pos_accuracy: 0.8100\n",
      "Epoch 2832/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0304 - pos_accuracy: 0.9625 - val_loss: 0.2707 - val_pos_accuracy: 0.8100\n",
      "Epoch 2833/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0304 - pos_accuracy: 0.9637 - val_loss: 0.2709 - val_pos_accuracy: 0.8075\n",
      "Epoch 2834/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0303 - pos_accuracy: 0.9602 - val_loss: 0.2707 - val_pos_accuracy: 0.8100\n",
      "Epoch 2835/3000\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0304 - pos_accuracy: 0.9589 - val_loss: 0.2712 - val_pos_accuracy: 0.8025\n",
      "Epoch 2836/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0303 - pos_accuracy: 0.9618 - val_loss: 0.2712 - val_pos_accuracy: 0.8075\n",
      "Epoch 2837/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0303 - pos_accuracy: 0.9619 - val_loss: 0.2709 - val_pos_accuracy: 0.8075\n",
      "Epoch 2838/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0302 - pos_accuracy: 0.9619 - val_loss: 0.2708 - val_pos_accuracy: 0.8075\n",
      "Epoch 2839/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0302 - pos_accuracy: 0.9622 - val_loss: 0.2710 - val_pos_accuracy: 0.8075\n",
      "Epoch 2840/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0303 - pos_accuracy: 0.9637 - val_loss: 0.2707 - val_pos_accuracy: 0.8075\n",
      "Epoch 2841/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0303 - pos_accuracy: 0.9622 - val_loss: 0.2706 - val_pos_accuracy: 0.8075\n",
      "Epoch 2842/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0303 - pos_accuracy: 0.9641 - val_loss: 0.2713 - val_pos_accuracy: 0.8075\n",
      "Epoch 2843/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0302 - pos_accuracy: 0.9610 - val_loss: 0.2708 - val_pos_accuracy: 0.8100\n",
      "Epoch 2844/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0302 - pos_accuracy: 0.9619 - val_loss: 0.2710 - val_pos_accuracy: 0.8075\n",
      "Epoch 2845/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0302 - pos_accuracy: 0.9607 - val_loss: 0.2718 - val_pos_accuracy: 0.8025\n",
      "Epoch 2846/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0301 - pos_accuracy: 0.9638 - val_loss: 0.2715 - val_pos_accuracy: 0.8025\n",
      "Epoch 2847/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0301 - pos_accuracy: 0.9622 - val_loss: 0.2710 - val_pos_accuracy: 0.8075\n",
      "Epoch 2848/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0301 - pos_accuracy: 0.9630 - val_loss: 0.2707 - val_pos_accuracy: 0.8075\n",
      "Epoch 2849/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0301 - pos_accuracy: 0.9619 - val_loss: 0.2710 - val_pos_accuracy: 0.8075\n",
      "Epoch 2850/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0300 - pos_accuracy: 0.9626 - val_loss: 0.2706 - val_pos_accuracy: 0.8075\n",
      "Epoch 2851/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0300 - pos_accuracy: 0.9611 - val_loss: 0.2713 - val_pos_accuracy: 0.8075\n",
      "Epoch 2852/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0301 - pos_accuracy: 0.9657 - val_loss: 0.2709 - val_pos_accuracy: 0.8000\n",
      "Epoch 2853/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0301 - pos_accuracy: 0.9614 - val_loss: 0.2708 - val_pos_accuracy: 0.7975\n",
      "Epoch 2854/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0300 - pos_accuracy: 0.9618 - val_loss: 0.2717 - val_pos_accuracy: 0.8050\n",
      "Epoch 2855/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0299 - pos_accuracy: 0.9642 - val_loss: 0.2709 - val_pos_accuracy: 0.8000\n",
      "Epoch 2856/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0299 - pos_accuracy: 0.9611 - val_loss: 0.2710 - val_pos_accuracy: 0.8000\n",
      "Epoch 2857/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0299 - pos_accuracy: 0.9619 - val_loss: 0.2709 - val_pos_accuracy: 0.8000\n",
      "Epoch 2858/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0300 - pos_accuracy: 0.9621 - val_loss: 0.2707 - val_pos_accuracy: 0.7975\n",
      "Epoch 2859/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0299 - pos_accuracy: 0.9658 - val_loss: 0.2707 - val_pos_accuracy: 0.8025\n",
      "Epoch 2860/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0298 - pos_accuracy: 0.9630 - val_loss: 0.2711 - val_pos_accuracy: 0.8050\n",
      "Epoch 2861/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0299 - pos_accuracy: 0.9603 - val_loss: 0.2713 - val_pos_accuracy: 0.8050\n",
      "Epoch 2862/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0299 - pos_accuracy: 0.9644 - val_loss: 0.2708 - val_pos_accuracy: 0.8075\n",
      "Epoch 2863/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0298 - pos_accuracy: 0.9607 - val_loss: 0.2709 - val_pos_accuracy: 0.8075\n",
      "Epoch 2864/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0298 - pos_accuracy: 0.9620 - val_loss: 0.2708 - val_pos_accuracy: 0.8100\n",
      "Epoch 2865/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0297 - pos_accuracy: 0.9654 - val_loss: 0.2706 - val_pos_accuracy: 0.8075\n",
      "Epoch 2866/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0297 - pos_accuracy: 0.9630 - val_loss: 0.2707 - val_pos_accuracy: 0.8100\n",
      "Epoch 2867/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0297 - pos_accuracy: 0.9615 - val_loss: 0.2707 - val_pos_accuracy: 0.8075\n",
      "Epoch 2868/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0296 - pos_accuracy: 0.9639 - val_loss: 0.2707 - val_pos_accuracy: 0.8100\n",
      "Epoch 2869/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0296 - pos_accuracy: 0.9622 - val_loss: 0.2707 - val_pos_accuracy: 0.8075\n",
      "Epoch 2870/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0297 - pos_accuracy: 0.9623 - val_loss: 0.2703 - val_pos_accuracy: 0.8075\n",
      "Epoch 2871/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0297 - pos_accuracy: 0.9621 - val_loss: 0.2699 - val_pos_accuracy: 0.8125\n",
      "Epoch 2872/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0298 - pos_accuracy: 0.9626 - val_loss: 0.2709 - val_pos_accuracy: 0.8075\n",
      "Epoch 2873/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0297 - pos_accuracy: 0.9608 - val_loss: 0.2709 - val_pos_accuracy: 0.8075\n",
      "Epoch 2874/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0297 - pos_accuracy: 0.9631 - val_loss: 0.2709 - val_pos_accuracy: 0.8075\n",
      "Epoch 2875/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0296 - pos_accuracy: 0.9644 - val_loss: 0.2688 - val_pos_accuracy: 0.8075\n",
      "Epoch 2876/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0296 - pos_accuracy: 0.9591 - val_loss: 0.2711 - val_pos_accuracy: 0.8075\n",
      "Epoch 2877/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0295 - pos_accuracy: 0.9631 - val_loss: 0.2698 - val_pos_accuracy: 0.8050\n",
      "Epoch 2878/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0295 - pos_accuracy: 0.9631 - val_loss: 0.2715 - val_pos_accuracy: 0.8075\n",
      "Epoch 2879/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0296 - pos_accuracy: 0.9632 - val_loss: 0.2715 - val_pos_accuracy: 0.8050\n",
      "Epoch 2880/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0295 - pos_accuracy: 0.9652 - val_loss: 0.2689 - val_pos_accuracy: 0.8100\n",
      "Epoch 2881/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0296 - pos_accuracy: 0.9633 - val_loss: 0.2689 - val_pos_accuracy: 0.8075\n",
      "Epoch 2882/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0295 - pos_accuracy: 0.9611 - val_loss: 0.2693 - val_pos_accuracy: 0.8100\n",
      "Epoch 2883/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0295 - pos_accuracy: 0.9628 - val_loss: 0.2695 - val_pos_accuracy: 0.8100\n",
      "Epoch 2884/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0295 - pos_accuracy: 0.9651 - val_loss: 0.2691 - val_pos_accuracy: 0.8075\n",
      "Epoch 2885/3000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0295 - pos_accuracy: 0.9609 - val_loss: 0.2708 - val_pos_accuracy: 0.8050\n",
      "Epoch 2886/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0294 - pos_accuracy: 0.9608 - val_loss: 0.2708 - val_pos_accuracy: 0.8075\n",
      "Epoch 2887/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0294 - pos_accuracy: 0.9623 - val_loss: 0.2706 - val_pos_accuracy: 0.8050\n",
      "Epoch 2888/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0294 - pos_accuracy: 0.9623 - val_loss: 0.2707 - val_pos_accuracy: 0.8050\n",
      "Epoch 2889/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0293 - pos_accuracy: 0.9647 - val_loss: 0.2702 - val_pos_accuracy: 0.8075\n",
      "Epoch 2890/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0293 - pos_accuracy: 0.9642 - val_loss: 0.2703 - val_pos_accuracy: 0.8050\n",
      "Epoch 2891/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0293 - pos_accuracy: 0.9646 - val_loss: 0.2697 - val_pos_accuracy: 0.8025\n",
      "Epoch 2892/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0293 - pos_accuracy: 0.9611 - val_loss: 0.2690 - val_pos_accuracy: 0.8050\n",
      "Epoch 2893/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0293 - pos_accuracy: 0.9607 - val_loss: 0.2702 - val_pos_accuracy: 0.8050\n",
      "Epoch 2894/3000\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0292 - pos_accuracy: 0.9616 - val_loss: 0.2706 - val_pos_accuracy: 0.8050\n",
      "Epoch 2895/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0291 - pos_accuracy: 0.9627 - val_loss: 0.2701 - val_pos_accuracy: 0.8050\n",
      "Epoch 2896/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0292 - pos_accuracy: 0.9617 - val_loss: 0.2699 - val_pos_accuracy: 0.8000\n",
      "Epoch 2897/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0291 - pos_accuracy: 0.9616 - val_loss: 0.2700 - val_pos_accuracy: 0.8050\n",
      "Epoch 2898/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0292 - pos_accuracy: 0.9606 - val_loss: 0.2698 - val_pos_accuracy: 0.8050\n",
      "Epoch 2899/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0292 - pos_accuracy: 0.9625 - val_loss: 0.2692 - val_pos_accuracy: 0.8050\n",
      "Epoch 2900/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0291 - pos_accuracy: 0.9615 - val_loss: 0.2694 - val_pos_accuracy: 0.8075\n",
      "Epoch 2901/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0292 - pos_accuracy: 0.9607 - val_loss: 0.2697 - val_pos_accuracy: 0.8050\n",
      "Epoch 2902/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0292 - pos_accuracy: 0.9612 - val_loss: 0.2693 - val_pos_accuracy: 0.8050\n",
      "Epoch 2903/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0291 - pos_accuracy: 0.9663 - val_loss: 0.2701 - val_pos_accuracy: 0.8050\n",
      "Epoch 2904/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0291 - pos_accuracy: 0.9617 - val_loss: 0.2699 - val_pos_accuracy: 0.8100\n",
      "Epoch 2905/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0291 - pos_accuracy: 0.9633 - val_loss: 0.2706 - val_pos_accuracy: 0.8075\n",
      "Epoch 2906/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0291 - pos_accuracy: 0.9640 - val_loss: 0.2703 - val_pos_accuracy: 0.8050\n",
      "Epoch 2907/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0290 - pos_accuracy: 0.9628 - val_loss: 0.2701 - val_pos_accuracy: 0.8100\n",
      "Epoch 2908/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0290 - pos_accuracy: 0.9648 - val_loss: 0.2694 - val_pos_accuracy: 0.8075\n",
      "Epoch 2909/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0290 - pos_accuracy: 0.9651 - val_loss: 0.2696 - val_pos_accuracy: 0.8100\n",
      "Epoch 2910/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0289 - pos_accuracy: 0.9637 - val_loss: 0.2696 - val_pos_accuracy: 0.8100\n",
      "Epoch 2911/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0289 - pos_accuracy: 0.9617 - val_loss: 0.2696 - val_pos_accuracy: 0.8100\n",
      "Epoch 2912/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0289 - pos_accuracy: 0.9617 - val_loss: 0.2695 - val_pos_accuracy: 0.8100\n",
      "Epoch 2913/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0289 - pos_accuracy: 0.9629 - val_loss: 0.2696 - val_pos_accuracy: 0.8075\n",
      "Epoch 2914/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0288 - pos_accuracy: 0.9618 - val_loss: 0.2690 - val_pos_accuracy: 0.8050\n",
      "Epoch 2915/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0288 - pos_accuracy: 0.9626 - val_loss: 0.2693 - val_pos_accuracy: 0.8100\n",
      "Epoch 2916/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0287 - pos_accuracy: 0.9617 - val_loss: 0.2696 - val_pos_accuracy: 0.8100\n",
      "Epoch 2917/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0288 - pos_accuracy: 0.9617 - val_loss: 0.2694 - val_pos_accuracy: 0.8100\n",
      "Epoch 2918/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0287 - pos_accuracy: 0.9625 - val_loss: 0.2687 - val_pos_accuracy: 0.8050\n",
      "Epoch 2919/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0287 - pos_accuracy: 0.9642 - val_loss: 0.2688 - val_pos_accuracy: 0.8075\n",
      "Epoch 2920/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0287 - pos_accuracy: 0.9621 - val_loss: 0.2691 - val_pos_accuracy: 0.8075\n",
      "Epoch 2921/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0286 - pos_accuracy: 0.9601 - val_loss: 0.2693 - val_pos_accuracy: 0.8100\n",
      "Epoch 2922/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0287 - pos_accuracy: 0.9648 - val_loss: 0.2692 - val_pos_accuracy: 0.8075\n",
      "Epoch 2923/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0287 - pos_accuracy: 0.9617 - val_loss: 0.2690 - val_pos_accuracy: 0.8075\n",
      "Epoch 2924/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0286 - pos_accuracy: 0.9627 - val_loss: 0.2692 - val_pos_accuracy: 0.8100\n",
      "Epoch 2925/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0287 - pos_accuracy: 0.9619 - val_loss: 0.2690 - val_pos_accuracy: 0.8100\n",
      "Epoch 2926/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0286 - pos_accuracy: 0.9644 - val_loss: 0.2695 - val_pos_accuracy: 0.8100\n",
      "Epoch 2927/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0286 - pos_accuracy: 0.9618 - val_loss: 0.2684 - val_pos_accuracy: 0.8075\n",
      "Epoch 2928/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0286 - pos_accuracy: 0.9646 - val_loss: 0.2684 - val_pos_accuracy: 0.8075\n",
      "Epoch 2929/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0286 - pos_accuracy: 0.9628 - val_loss: 0.2690 - val_pos_accuracy: 0.8075\n",
      "Epoch 2930/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0285 - pos_accuracy: 0.9610 - val_loss: 0.2690 - val_pos_accuracy: 0.8100\n",
      "Epoch 2931/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0285 - pos_accuracy: 0.9625 - val_loss: 0.2695 - val_pos_accuracy: 0.8100\n",
      "Epoch 2932/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0286 - pos_accuracy: 0.9646 - val_loss: 0.2694 - val_pos_accuracy: 0.8100\n",
      "Epoch 2933/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0285 - pos_accuracy: 0.9637 - val_loss: 0.2687 - val_pos_accuracy: 0.8100\n",
      "Epoch 2934/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0285 - pos_accuracy: 0.9652 - val_loss: 0.2693 - val_pos_accuracy: 0.8100\n",
      "Epoch 2935/3000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0286 - pos_accuracy: 0.9637 - val_loss: 0.2699 - val_pos_accuracy: 0.8075\n",
      "Epoch 2936/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0286 - pos_accuracy: 0.9622 - val_loss: 0.2684 - val_pos_accuracy: 0.8075\n",
      "Epoch 2937/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0284 - pos_accuracy: 0.9617 - val_loss: 0.2694 - val_pos_accuracy: 0.8100\n",
      "Epoch 2938/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0284 - pos_accuracy: 0.9656 - val_loss: 0.2708 - val_pos_accuracy: 0.8050\n",
      "Epoch 2939/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0285 - pos_accuracy: 0.9652 - val_loss: 0.2682 - val_pos_accuracy: 0.8050\n",
      "Epoch 2940/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0283 - pos_accuracy: 0.9647 - val_loss: 0.2681 - val_pos_accuracy: 0.8100\n",
      "Epoch 2941/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0286 - pos_accuracy: 0.9652 - val_loss: 0.2677 - val_pos_accuracy: 0.8075\n",
      "Epoch 2942/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0285 - pos_accuracy: 0.9626 - val_loss: 0.2679 - val_pos_accuracy: 0.8075\n",
      "Epoch 2943/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0283 - pos_accuracy: 0.9596 - val_loss: 0.2691 - val_pos_accuracy: 0.8050\n",
      "Epoch 2944/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0284 - pos_accuracy: 0.9631 - val_loss: 0.2697 - val_pos_accuracy: 0.8050\n",
      "Epoch 2945/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0283 - pos_accuracy: 0.9645 - val_loss: 0.2694 - val_pos_accuracy: 0.8050\n",
      "Epoch 2946/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0284 - pos_accuracy: 0.9652 - val_loss: 0.2698 - val_pos_accuracy: 0.8050\n",
      "Epoch 2947/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0284 - pos_accuracy: 0.9652 - val_loss: 0.2689 - val_pos_accuracy: 0.8050\n",
      "Epoch 2948/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0284 - pos_accuracy: 0.9636 - val_loss: 0.2688 - val_pos_accuracy: 0.8075\n",
      "Epoch 2949/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0284 - pos_accuracy: 0.9641 - val_loss: 0.2692 - val_pos_accuracy: 0.8050\n",
      "Epoch 2950/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0282 - pos_accuracy: 0.9632 - val_loss: 0.2680 - val_pos_accuracy: 0.8100\n",
      "Epoch 2951/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0284 - pos_accuracy: 0.9631 - val_loss: 0.2676 - val_pos_accuracy: 0.8075\n",
      "Epoch 2952/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0282 - pos_accuracy: 0.9648 - val_loss: 0.2694 - val_pos_accuracy: 0.8050\n",
      "Epoch 2953/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0281 - pos_accuracy: 0.9637 - val_loss: 0.2688 - val_pos_accuracy: 0.8100\n",
      "Epoch 2954/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0280 - pos_accuracy: 0.9606 - val_loss: 0.2687 - val_pos_accuracy: 0.8100\n",
      "Epoch 2955/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0281 - pos_accuracy: 0.9645 - val_loss: 0.2678 - val_pos_accuracy: 0.8100\n",
      "Epoch 2956/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0281 - pos_accuracy: 0.9636 - val_loss: 0.2690 - val_pos_accuracy: 0.8100\n",
      "Epoch 2957/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0280 - pos_accuracy: 0.9625 - val_loss: 0.2680 - val_pos_accuracy: 0.8100\n",
      "Epoch 2958/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0280 - pos_accuracy: 0.9625 - val_loss: 0.2677 - val_pos_accuracy: 0.8075\n",
      "Epoch 2959/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0280 - pos_accuracy: 0.9641 - val_loss: 0.2680 - val_pos_accuracy: 0.8100\n",
      "Epoch 2960/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0281 - pos_accuracy: 0.9641 - val_loss: 0.2678 - val_pos_accuracy: 0.8100\n",
      "Epoch 2961/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0280 - pos_accuracy: 0.9641 - val_loss: 0.2684 - val_pos_accuracy: 0.8075\n",
      "Epoch 2962/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0280 - pos_accuracy: 0.9632 - val_loss: 0.2685 - val_pos_accuracy: 0.8075\n",
      "Epoch 2963/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0279 - pos_accuracy: 0.9625 - val_loss: 0.2679 - val_pos_accuracy: 0.8100\n",
      "Epoch 2964/3000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0280 - pos_accuracy: 0.9656 - val_loss: 0.2683 - val_pos_accuracy: 0.8075\n",
      "Epoch 2965/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0279 - pos_accuracy: 0.9641 - val_loss: 0.2676 - val_pos_accuracy: 0.8100\n",
      "Epoch 2966/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0279 - pos_accuracy: 0.9637 - val_loss: 0.2680 - val_pos_accuracy: 0.8100\n",
      "Epoch 2967/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0279 - pos_accuracy: 0.9660 - val_loss: 0.2681 - val_pos_accuracy: 0.8075\n",
      "Epoch 2968/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0278 - pos_accuracy: 0.9652 - val_loss: 0.2677 - val_pos_accuracy: 0.8100\n",
      "Epoch 2969/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0278 - pos_accuracy: 0.9637 - val_loss: 0.2674 - val_pos_accuracy: 0.8100\n",
      "Epoch 2970/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0279 - pos_accuracy: 0.9629 - val_loss: 0.2682 - val_pos_accuracy: 0.8100\n",
      "Epoch 2971/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0277 - pos_accuracy: 0.9645 - val_loss: 0.2682 - val_pos_accuracy: 0.8100\n",
      "Epoch 2972/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0278 - pos_accuracy: 0.9652 - val_loss: 0.2683 - val_pos_accuracy: 0.8075\n",
      "Epoch 2973/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0277 - pos_accuracy: 0.9645 - val_loss: 0.2674 - val_pos_accuracy: 0.8100\n",
      "Epoch 2974/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0278 - pos_accuracy: 0.9629 - val_loss: 0.2682 - val_pos_accuracy: 0.8100\n",
      "Epoch 2975/3000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0278 - pos_accuracy: 0.9652 - val_loss: 0.2679 - val_pos_accuracy: 0.8100\n",
      "Epoch 2976/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0277 - pos_accuracy: 0.9626 - val_loss: 0.2675 - val_pos_accuracy: 0.8100\n",
      "Epoch 2977/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0277 - pos_accuracy: 0.9641 - val_loss: 0.2682 - val_pos_accuracy: 0.8100\n",
      "Epoch 2978/3000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0277 - pos_accuracy: 0.9648 - val_loss: 0.2674 - val_pos_accuracy: 0.8075\n",
      "Epoch 2979/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0277 - pos_accuracy: 0.9645 - val_loss: 0.2680 - val_pos_accuracy: 0.8075\n",
      "Epoch 2980/3000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0276 - pos_accuracy: 0.9641 - val_loss: 0.2683 - val_pos_accuracy: 0.8100\n",
      "Epoch 2981/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0276 - pos_accuracy: 0.9626 - val_loss: 0.2682 - val_pos_accuracy: 0.8075\n",
      "Epoch 2982/3000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0276 - pos_accuracy: 0.9629 - val_loss: 0.2684 - val_pos_accuracy: 0.8100\n",
      "Epoch 2983/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0276 - pos_accuracy: 0.9626 - val_loss: 0.2680 - val_pos_accuracy: 0.8075\n",
      "Epoch 2984/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0275 - pos_accuracy: 0.9637 - val_loss: 0.2678 - val_pos_accuracy: 0.8075\n",
      "Epoch 2985/3000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0276 - pos_accuracy: 0.9664 - val_loss: 0.2674 - val_pos_accuracy: 0.8075\n",
      "Epoch 2986/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0275 - pos_accuracy: 0.9656 - val_loss: 0.2673 - val_pos_accuracy: 0.8075\n",
      "Epoch 2987/3000\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0275 - pos_accuracy: 0.9652 - val_loss: 0.2686 - val_pos_accuracy: 0.8050\n",
      "Epoch 2988/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0276 - pos_accuracy: 0.9623 - val_loss: 0.2685 - val_pos_accuracy: 0.8100\n",
      "Epoch 2989/3000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0275 - pos_accuracy: 0.9612 - val_loss: 0.2672 - val_pos_accuracy: 0.8075\n",
      "Epoch 2990/3000\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0275 - pos_accuracy: 0.9627 - val_loss: 0.2674 - val_pos_accuracy: 0.8075\n",
      "Epoch 2991/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0275 - pos_accuracy: 0.9646 - val_loss: 0.2677 - val_pos_accuracy: 0.8100\n",
      "Epoch 2992/3000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0274 - pos_accuracy: 0.9637 - val_loss: 0.2676 - val_pos_accuracy: 0.8100\n",
      "Epoch 2993/3000\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0274 - pos_accuracy: 0.9650 - val_loss: 0.2676 - val_pos_accuracy: 0.8075\n",
      "Epoch 2994/3000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0274 - pos_accuracy: 0.9631 - val_loss: 0.2674 - val_pos_accuracy: 0.8075\n",
      "Epoch 2995/3000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0274 - pos_accuracy: 0.9629 - val_loss: 0.2674 - val_pos_accuracy: 0.8075\n",
      "Epoch 2996/3000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0273 - pos_accuracy: 0.9633 - val_loss: 0.2671 - val_pos_accuracy: 0.8050\n",
      "Epoch 2997/3000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0275 - pos_accuracy: 0.9638 - val_loss: 0.2691 - val_pos_accuracy: 0.8100\n",
      "Epoch 2998/3000\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0274 - pos_accuracy: 0.9658 - val_loss: 0.2671 - val_pos_accuracy: 0.8075\n",
      "Epoch 2999/3000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0273 - pos_accuracy: 0.9626 - val_loss: 0.2687 - val_pos_accuracy: 0.8075\n",
      "Epoch 3000/3000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0274 - pos_accuracy: 0.9646 - val_loss: 0.2671 - val_pos_accuracy: 0.8075\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:42:25.830455: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1687: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate :기존 축을 따라 배열 시퀀스를 결합합니다.\\naxis : 배열이 결합될 축입니다. axis가 None이면 사용하기 전에 배열이 병합됩니다.\\n 기본값은 0입니다.\\n\",\n",
      "        \"\\n세 개의 정수( nrows , ncols , index ).\\n서브플롯은 nrows 행과 ncols 열 이 있는 그리드 에서 인덱스 위치를 취합니다 .\\n인덱스 는 왼쪽 상단 모서리에서 1에서 시작하여 오른쪽으로 증가합니다.\\nplt.subplot(121)은 1행 2열 첫번째 인덱스\\nplt.subplot(122)은 1행 2열 두번째 인덱스 를 뜻합니다.\\n\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all은 요소별 연산에 대한 축소 tf.math.logical_and연산입니다.\\ninput_tensor에 주어진 치수를 따라 감소 합니다 axis. true keepdims가 아니면 텐서의 순위는 axis고유해야 하는 의 각 항목에 대해 1씩 줄어듭니다.\\ntrue 이면 keepdims축소된 치수가 길이 1로 유지됩니다.\\naxis가 None 이면 모든 차원이 축소되고 단일 요소가 있는 텐서가 반환됩니다.\\n\",\n",
      "        \"\\n축의 차원에 걸친 요소의 평균을 계산하여 축에 주어진 차원을 따라 input_tensor를 줄입니다.\\n keepdims가 참이 아닌 한, 텐서의 순위는 축의 각 항목에 대해 1씩 감소하며, 이는 고유해야 한다. keepdims가 참이면 축소된 치수는 길이 1로 유지됩니다.\\n축이 없음이면 모든 치수가 감소하고 단일 요소가 있는 텐서가 반환됩니다. \\n\",\n",
      "        \"\\nLinear함수 만으로 충분히 분류가 가능함\\n\",\n",
      "        \"\\n코드의 결과를 재현하기 위해 항상 시드를 설정한다.\\n\",\n",
      "        false,\n",
      "        \"\\n출력층 활성화 함수 None을 사용한 이유가 강의 및 강의자료 구글링해도 잘 안나와서 자유롭게 적었습니다.\\n정확한 정답이 뭔지 궁금합니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 54.848484848484844,\n",
      "    \"accuracy\": 0.805\n",
      "}\"report1/박준하_46046.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/박준하_46046.ipynb to python\n",
      "[NbConvertApp] Writing 35827 bytes to report1/박준하_46046.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:241: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]]\n",
      "[[5. 7. 9.]\n",
      " [6. 8. 0.]]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 301976.60it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:42:32.304419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:42:32.684449: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:42:32.684488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:42:32.919861: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:42:33.557684: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 104768.55it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 16.4276 - pos_accuracy: 0.1131 - val_loss: 1.1622 - val_pos_accuracy: 0.2075\n",
      "Epoch 2/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.6956 - pos_accuracy: 0.3019 - val_loss: 0.6519 - val_pos_accuracy: 0.3800\n",
      "Epoch 3/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.4418 - pos_accuracy: 0.4306 - val_loss: 0.4951 - val_pos_accuracy: 0.3925\n",
      "Epoch 4/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.3351 - pos_accuracy: 0.5287 - val_loss: 0.3875 - val_pos_accuracy: 0.5225\n",
      "Epoch 5/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.2632 - pos_accuracy: 0.6038 - val_loss: 0.3413 - val_pos_accuracy: 0.5475\n",
      "Epoch 6/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.2199 - pos_accuracy: 0.6594 - val_loss: 0.2897 - val_pos_accuracy: 0.6425\n",
      "Epoch 7/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.1861 - pos_accuracy: 0.7144 - val_loss: 0.2637 - val_pos_accuracy: 0.6300\n",
      "Epoch 8/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.1670 - pos_accuracy: 0.7275 - val_loss: 0.2565 - val_pos_accuracy: 0.6500\n",
      "Epoch 9/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.1509 - pos_accuracy: 0.7525 - val_loss: 0.2156 - val_pos_accuracy: 0.7000\n",
      "Epoch 10/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.1309 - pos_accuracy: 0.7913 - val_loss: 0.1965 - val_pos_accuracy: 0.7075\n",
      "Epoch 11/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.1168 - pos_accuracy: 0.8019 - val_loss: 0.1760 - val_pos_accuracy: 0.7325\n",
      "Epoch 12/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.1113 - pos_accuracy: 0.8138 - val_loss: 0.1718 - val_pos_accuracy: 0.7450\n",
      "Epoch 13/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0995 - pos_accuracy: 0.8363 - val_loss: 0.1701 - val_pos_accuracy: 0.7025\n",
      "Epoch 14/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0914 - pos_accuracy: 0.8544 - val_loss: 0.1499 - val_pos_accuracy: 0.7750\n",
      "Epoch 15/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0830 - pos_accuracy: 0.8688 - val_loss: 0.1420 - val_pos_accuracy: 0.7850\n",
      "Epoch 16/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0745 - pos_accuracy: 0.8844 - val_loss: 0.1348 - val_pos_accuracy: 0.7900\n",
      "Epoch 17/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0692 - pos_accuracy: 0.9031 - val_loss: 0.1282 - val_pos_accuracy: 0.7975\n",
      "Epoch 18/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0609 - pos_accuracy: 0.9125 - val_loss: 0.1264 - val_pos_accuracy: 0.8225\n",
      "Epoch 19/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0565 - pos_accuracy: 0.9194 - val_loss: 0.1133 - val_pos_accuracy: 0.8375\n",
      "Epoch 20/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0556 - pos_accuracy: 0.9294 - val_loss: 0.1051 - val_pos_accuracy: 0.8500\n",
      "Epoch 21/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0546 - pos_accuracy: 0.9169 - val_loss: 0.1073 - val_pos_accuracy: 0.8575\n",
      "Epoch 22/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0461 - pos_accuracy: 0.9362 - val_loss: 0.0993 - val_pos_accuracy: 0.8550\n",
      "Epoch 23/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0411 - pos_accuracy: 0.9425 - val_loss: 0.0906 - val_pos_accuracy: 0.8825\n",
      "Epoch 24/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0354 - pos_accuracy: 0.9544 - val_loss: 0.0980 - val_pos_accuracy: 0.8700\n",
      "Epoch 25/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0399 - pos_accuracy: 0.9387 - val_loss: 0.0868 - val_pos_accuracy: 0.8875\n",
      "Epoch 26/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0373 - pos_accuracy: 0.9513 - val_loss: 0.0845 - val_pos_accuracy: 0.8825\n",
      "Epoch 27/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0350 - pos_accuracy: 0.9575 - val_loss: 0.0866 - val_pos_accuracy: 0.8800\n",
      "Epoch 28/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0296 - pos_accuracy: 0.9631 - val_loss: 0.0821 - val_pos_accuracy: 0.8825\n",
      "Epoch 29/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0296 - pos_accuracy: 0.9675 - val_loss: 0.0781 - val_pos_accuracy: 0.8875\n",
      "Epoch 30/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0406 - pos_accuracy: 0.9506 - val_loss: 0.0862 - val_pos_accuracy: 0.8750\n",
      "Epoch 31/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0335 - pos_accuracy: 0.9531 - val_loss: 0.0735 - val_pos_accuracy: 0.8950\n",
      "Epoch 32/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0292 - pos_accuracy: 0.9644 - val_loss: 0.0740 - val_pos_accuracy: 0.8925\n",
      "Epoch 33/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0219 - pos_accuracy: 0.9750 - val_loss: 0.0663 - val_pos_accuracy: 0.9025\n",
      "Epoch 34/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0199 - pos_accuracy: 0.9800 - val_loss: 0.0633 - val_pos_accuracy: 0.9050\n",
      "Epoch 35/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0169 - pos_accuracy: 0.9837 - val_loss: 0.0634 - val_pos_accuracy: 0.9075\n",
      "Epoch 36/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0153 - pos_accuracy: 0.9856 - val_loss: 0.0613 - val_pos_accuracy: 0.9050\n",
      "Epoch 37/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0150 - pos_accuracy: 0.9850 - val_loss: 0.0591 - val_pos_accuracy: 0.9125\n",
      "Epoch 38/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0143 - pos_accuracy: 0.9862 - val_loss: 0.0570 - val_pos_accuracy: 0.9100\n",
      "Epoch 39/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0131 - pos_accuracy: 0.9844 - val_loss: 0.0580 - val_pos_accuracy: 0.9100\n",
      "Epoch 40/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0138 - pos_accuracy: 0.9862 - val_loss: 0.0558 - val_pos_accuracy: 0.9100\n",
      "Epoch 41/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0118 - pos_accuracy: 0.9881 - val_loss: 0.0561 - val_pos_accuracy: 0.9125\n",
      "Epoch 42/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0110 - pos_accuracy: 0.9881 - val_loss: 0.0543 - val_pos_accuracy: 0.9200\n",
      "Epoch 43/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0102 - pos_accuracy: 0.9887 - val_loss: 0.0543 - val_pos_accuracy: 0.9150\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0096 - pos_accuracy: 0.9900 - val_loss: 0.0524 - val_pos_accuracy: 0.9100\n",
      "Epoch 45/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0092 - pos_accuracy: 0.9888 - val_loss: 0.0504 - val_pos_accuracy: 0.9175\n",
      "Epoch 46/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0091 - pos_accuracy: 0.9881 - val_loss: 0.0518 - val_pos_accuracy: 0.9225\n",
      "Epoch 47/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0088 - pos_accuracy: 0.9906 - val_loss: 0.0503 - val_pos_accuracy: 0.9175\n",
      "Epoch 48/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0082 - pos_accuracy: 0.9906 - val_loss: 0.0500 - val_pos_accuracy: 0.9200\n",
      "Epoch 49/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0078 - pos_accuracy: 0.9906 - val_loss: 0.0495 - val_pos_accuracy: 0.9125\n",
      "Epoch 50/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0077 - pos_accuracy: 0.9906 - val_loss: 0.0481 - val_pos_accuracy: 0.9200\n",
      "Epoch 51/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0087 - pos_accuracy: 0.9906 - val_loss: 0.0480 - val_pos_accuracy: 0.9225\n",
      "Epoch 52/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0082 - pos_accuracy: 0.9906 - val_loss: 0.0470 - val_pos_accuracy: 0.9200\n",
      "Epoch 53/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0080 - pos_accuracy: 0.9900 - val_loss: 0.0466 - val_pos_accuracy: 0.9200\n",
      "Epoch 54/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0087 - pos_accuracy: 0.9887 - val_loss: 0.0470 - val_pos_accuracy: 0.9125\n",
      "Epoch 55/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0074 - pos_accuracy: 0.9906 - val_loss: 0.0476 - val_pos_accuracy: 0.9175\n",
      "Epoch 56/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0088 - pos_accuracy: 0.9894 - val_loss: 0.0461 - val_pos_accuracy: 0.9175\n",
      "Epoch 57/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0088 - pos_accuracy: 0.9913 - val_loss: 0.0460 - val_pos_accuracy: 0.9200\n",
      "Epoch 58/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0089 - pos_accuracy: 0.9887 - val_loss: 0.0486 - val_pos_accuracy: 0.9175\n",
      "Epoch 59/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0074 - pos_accuracy: 0.9925 - val_loss: 0.0514 - val_pos_accuracy: 0.9050\n",
      "Epoch 60/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0073 - pos_accuracy: 0.9919 - val_loss: 0.0443 - val_pos_accuracy: 0.9275\n",
      "Epoch 61/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0068 - pos_accuracy: 0.9919 - val_loss: 0.0445 - val_pos_accuracy: 0.9250\n",
      "Epoch 62/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0067 - pos_accuracy: 0.9931 - val_loss: 0.0451 - val_pos_accuracy: 0.9250\n",
      "Epoch 63/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0061 - pos_accuracy: 0.9931 - val_loss: 0.0445 - val_pos_accuracy: 0.9225\n",
      "Epoch 64/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0057 - pos_accuracy: 0.9931 - val_loss: 0.0428 - val_pos_accuracy: 0.9275\n",
      "Epoch 65/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0073 - pos_accuracy: 0.9937 - val_loss: 0.0445 - val_pos_accuracy: 0.9225\n",
      "Epoch 66/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0075 - pos_accuracy: 0.9931 - val_loss: 0.0430 - val_pos_accuracy: 0.9250\n",
      "Epoch 67/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0066 - pos_accuracy: 0.9944 - val_loss: 0.0433 - val_pos_accuracy: 0.9250\n",
      "Epoch 68/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0060 - pos_accuracy: 0.9950 - val_loss: 0.0419 - val_pos_accuracy: 0.9275\n",
      "Epoch 69/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0055 - pos_accuracy: 0.9944 - val_loss: 0.0425 - val_pos_accuracy: 0.9250\n",
      "Epoch 70/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0060 - pos_accuracy: 0.9950 - val_loss: 0.0433 - val_pos_accuracy: 0.9275\n",
      "Epoch 71/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0073 - pos_accuracy: 0.9937 - val_loss: 0.0431 - val_pos_accuracy: 0.9275\n",
      "Epoch 72/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0078 - pos_accuracy: 0.9944 - val_loss: 0.0434 - val_pos_accuracy: 0.9275\n",
      "Epoch 73/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0098 - pos_accuracy: 0.9913 - val_loss: 0.0436 - val_pos_accuracy: 0.9250\n",
      "Epoch 74/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0109 - pos_accuracy: 0.9887 - val_loss: 0.0464 - val_pos_accuracy: 0.9275\n",
      "Epoch 75/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0083 - pos_accuracy: 0.9950 - val_loss: 0.0416 - val_pos_accuracy: 0.9275\n",
      "Epoch 76/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0080 - pos_accuracy: 0.9944 - val_loss: 0.0436 - val_pos_accuracy: 0.9300\n",
      "Epoch 77/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0091 - pos_accuracy: 0.9913 - val_loss: 0.0422 - val_pos_accuracy: 0.9300\n",
      "Epoch 78/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0073 - pos_accuracy: 0.9963 - val_loss: 0.0439 - val_pos_accuracy: 0.9325\n",
      "Epoch 79/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0060 - pos_accuracy: 0.9969 - val_loss: 0.0423 - val_pos_accuracy: 0.9275\n",
      "Epoch 80/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0065 - pos_accuracy: 0.9975 - val_loss: 0.0410 - val_pos_accuracy: 0.9325\n",
      "Epoch 81/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0063 - pos_accuracy: 0.9981 - val_loss: 0.0435 - val_pos_accuracy: 0.9300\n",
      "Epoch 82/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0076 - pos_accuracy: 0.9950 - val_loss: 0.0401 - val_pos_accuracy: 0.9300\n",
      "Epoch 83/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0056 - pos_accuracy: 0.9988 - val_loss: 0.0396 - val_pos_accuracy: 0.9300\n",
      "Epoch 84/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0052 - pos_accuracy: 0.9987 - val_loss: 0.0396 - val_pos_accuracy: 0.9325\n",
      "Epoch 85/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0046 - pos_accuracy: 0.9987 - val_loss: 0.0390 - val_pos_accuracy: 0.9350\n",
      "Epoch 86/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0040 - pos_accuracy: 0.9975 - val_loss: 0.0383 - val_pos_accuracy: 0.9350\n",
      "Epoch 87/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0036 - pos_accuracy: 0.9994 - val_loss: 0.0380 - val_pos_accuracy: 0.9300\n",
      "Epoch 88/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0037 - pos_accuracy: 0.9994 - val_loss: 0.0382 - val_pos_accuracy: 0.9325\n",
      "Epoch 89/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0030 - pos_accuracy: 0.9994 - val_loss: 0.0378 - val_pos_accuracy: 0.9350\n",
      "Epoch 90/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0026 - pos_accuracy: 0.9994 - val_loss: 0.0371 - val_pos_accuracy: 0.9325\n",
      "Epoch 91/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0026 - pos_accuracy: 0.9994 - val_loss: 0.0372 - val_pos_accuracy: 0.9375\n",
      "Epoch 92/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0027 - pos_accuracy: 0.9994 - val_loss: 0.0365 - val_pos_accuracy: 0.9350\n",
      "Epoch 93/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0027 - pos_accuracy: 0.9994 - val_loss: 0.0368 - val_pos_accuracy: 0.9400\n",
      "Epoch 94/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0372 - val_pos_accuracy: 0.9300\n",
      "Epoch 95/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0023 - pos_accuracy: 0.9994 - val_loss: 0.0373 - val_pos_accuracy: 0.9325\n",
      "Epoch 96/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0365 - val_pos_accuracy: 0.9375\n",
      "Epoch 97/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.0359 - val_pos_accuracy: 0.9325\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.0364 - val_pos_accuracy: 0.9350\n",
      "Epoch 99/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.0361 - val_pos_accuracy: 0.9400\n",
      "Epoch 100/100\n",
      "320/320 [==============================] - 1s 3ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.0359 - val_pos_accuracy: 0.9350\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:44:22.121452: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "0.6013071895424837\n",
      "0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1681: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concatenate 메소드는 선택한 축(axis)의 방향으로 배열을 연결해주는 메소드 입니다. \\naxis=1 이란 배열들을 행방향으로 합치라는 의미입니다. \\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\n행수, 열수, 그리는 그래프의 위치를 의미합니다. 121은 1x2그리드에 첫 번째 subplot, 122는 1x2그리드에 두 번째 subplot을 의미합니다. \\n\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntf.reduce_all은 \\\"tf.math.logical_and 텐서 차원의 요소\\\"(And 연산)를 계산 합니다. \\naxis란 지정된 축을 축소하는 것인데, axis = 1을 적용하면 두 번째 axis를 기준으로 연산합니다.즉 data들이 각자 가지고 있는 attribute들의 연산(행 단위 연산)이 됩니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\ntf.reduce_mean(is_correct)처럼 두번째 인자를 적지 않은 경우 변수 is_correct가 가리키는 배열 전체 원소의 합을 원소 개수로 나누어 계산합니다. \\ntf.reduce_mean(x, 0)는 열 단위로 평균을 냅니다. \\ntf.reduce_mean(x, 1)는 행 단위로 평균을 냅니다. \\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n회귀 문제의 특성상 데이터 간의 연속적인 관계가 있어야하기 때문에 다른 활성화 함수가 필요 없습니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n난수를 예측가능하도록 만들어, 난수의 생성 패턴을 동일하게 관리할 수 있습니다.\\n\",\n",
      "        true,\n",
      "        \"\\n0.6013072\\n\"\n",
      "    ],\n",
      "    \"score\": 96.0,\n",
      "    \"accuracy\": 0.94\n",
      "}\"report1/김미현_46026.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김미현_46026.ipynb to python\n",
      "[NbConvertApp] Writing 36380 bytes to report1/김미현_46026.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[6.5751332e+07 1.4374166e+08]\n",
      "[0. 0.]\n",
      "[6.57513350e+07 1.43741667e+08]\n",
      "[0. 0.]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -16. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 303330.61it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:44:30.435841: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:44:30.811508: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:44:30.811547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:44:31.035101: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:44:31.666300: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 99494.83it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 101,792\n",
      "Trainable params: 101,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(2, 2)\n",
      "(2,)\n",
      "[ True False]\n",
      "[1. 0.]\n",
      "0.5\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 247.1101 - val_loss: 223.9925\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 240.6013 - val_loss: 217.9710\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 234.3235 - val_loss: 212.1665\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 4ms/step - loss: 228.2634 - val_loss: 206.5681\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 222.4134 - val_loss: 201.1700\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 216.7615 - val_loss: 195.9584\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 211.3000 - val_loss: 190.9266\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 206.0181 - val_loss: 186.0658\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 200.9076 - val_loss: 181.3671\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 195.9584 - val_loss: 176.8201\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 101,792\n",
      "Trainable params: 101,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 192.3409 - pos_accuracy: 6.2500e-04 - val_loss: 174.6013 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 189.9782 - pos_accuracy: 6.2500e-04 - val_loss: 172.4158 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 187.6506 - pos_accuracy: 0.0000e+00 - val_loss: 170.2657 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 185.3558 - pos_accuracy: 6.2500e-04 - val_loss: 168.1459 - val_pos_accuracy: 0.0025\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 183.0939 - pos_accuracy: 6.2500e-04 - val_loss: 166.0574 - val_pos_accuracy: 0.0025\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 180.8614 - pos_accuracy: 6.2500e-04 - val_loss: 163.9975 - val_pos_accuracy: 0.0025\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 178.6579 - pos_accuracy: 6.2500e-04 - val_loss: 161.9644 - val_pos_accuracy: 0.0025\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 176.4801 - pos_accuracy: 6.2500e-04 - val_loss: 159.9586 - val_pos_accuracy: 0.0025\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 174.3290 - pos_accuracy: 6.2500e-04 - val_loss: 157.9748 - val_pos_accuracy: 0.0025\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 172.1993 - pos_accuracy: 6.2500e-04 - val_loss: 156.0145 - val_pos_accuracy: 0.0025\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 170.0920 - pos_accuracy: 6.2500e-04 - val_loss: 154.0724 - val_pos_accuracy: 0.0025\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 168.0014 - pos_accuracy: 6.2500e-04 - val_loss: 152.1492 - val_pos_accuracy: 0.0025\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 165.9272 - pos_accuracy: 6.2500e-04 - val_loss: 150.2407 - val_pos_accuracy: 0.0025\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 163.8647 - pos_accuracy: 6.2500e-04 - val_loss: 148.3421 - val_pos_accuracy: 0.0025\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 161.8100 - pos_accuracy: 6.2500e-04 - val_loss: 146.4528 - val_pos_accuracy: 0.0025\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 159.7627 - pos_accuracy: 6.2500e-04 - val_loss: 144.5690 - val_pos_accuracy: 0.0025\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 157.7147 - pos_accuracy: 6.2500e-04 - val_loss: 142.6874 - val_pos_accuracy: 0.0025\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 155.6628 - pos_accuracy: 6.2500e-04 - val_loss: 140.8009 - val_pos_accuracy: 0.0025\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 153.6033 - pos_accuracy: 6.2500e-04 - val_loss: 138.9039 - val_pos_accuracy: 0.0025\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 151.5289 - pos_accuracy: 6.2500e-04 - val_loss: 136.9945 - val_pos_accuracy: 0.0025\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 149.4354 - pos_accuracy: 6.2500e-04 - val_loss: 135.0715 - val_pos_accuracy: 0.0025\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 147.3180 - pos_accuracy: 6.2500e-04 - val_loss: 133.1279 - val_pos_accuracy: 0.0025\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 145.1786 - pos_accuracy: 6.2500e-04 - val_loss: 131.1582 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 143.0112 - pos_accuracy: 0.0012 - val_loss: 129.1725 - val_pos_accuracy: 0.0075\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 140.8221 - pos_accuracy: 0.0025 - val_loss: 127.1657 - val_pos_accuracy: 0.0050\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 138.6172 - pos_accuracy: 0.0025 - val_loss: 125.1520 - val_pos_accuracy: 0.0050\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 136.4070 - pos_accuracy: 0.0025 - val_loss: 123.1345 - val_pos_accuracy: 0.0050\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 134.2035 - pos_accuracy: 0.0025 - val_loss: 121.1373 - val_pos_accuracy: 0.0050\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 132.0190 - pos_accuracy: 0.0025 - val_loss: 119.1648 - val_pos_accuracy: 0.0050\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 129.8672 - pos_accuracy: 0.0025 - val_loss: 117.2189 - val_pos_accuracy: 0.0050\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 127.7532 - pos_accuracy: 0.0019 - val_loss: 115.3155 - val_pos_accuracy: 0.0025\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 125.6887 - pos_accuracy: 0.0019 - val_loss: 113.4593 - val_pos_accuracy: 0.0025\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 123.6805 - pos_accuracy: 0.0025 - val_loss: 111.6686 - val_pos_accuracy: 0.0050\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 121.7316 - pos_accuracy: 0.0019 - val_loss: 109.9199 - val_pos_accuracy: 0.0050\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 119.8415 - pos_accuracy: 0.0019 - val_loss: 108.2414 - val_pos_accuracy: 0.0050\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 118.0124 - pos_accuracy: 0.0019 - val_loss: 106.6139 - val_pos_accuracy: 0.0050\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 116.2442 - pos_accuracy: 0.0019 - val_loss: 105.0413 - val_pos_accuracy: 0.0050\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 114.5363 - pos_accuracy: 0.0019 - val_loss: 103.5318 - val_pos_accuracy: 0.0050\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 112.8822 - pos_accuracy: 0.0025 - val_loss: 102.0654 - val_pos_accuracy: 0.0025\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 111.2811 - pos_accuracy: 0.0019 - val_loss: 100.6559 - val_pos_accuracy: 0.0025\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 109.7381 - pos_accuracy: 0.0031 - val_loss: 99.2905 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 108.2429 - pos_accuracy: 0.0012 - val_loss: 97.9720 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 106.7967 - pos_accuracy: 0.0012 - val_loss: 96.6930 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 7ms/step - loss: 105.3955 - pos_accuracy: 0.0012 - val_loss: 95.4660 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 104.0402 - pos_accuracy: 0.0012 - val_loss: 94.2793 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 102.7318 - pos_accuracy: 0.0012 - val_loss: 93.1362 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 101.4643 - pos_accuracy: 6.2500e-04 - val_loss: 92.0287 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 100.2299 - pos_accuracy: 0.0000e+00 - val_loss: 90.9509 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 99.0355 - pos_accuracy: 0.0000e+00 - val_loss: 89.9013 - val_pos_accuracy: 0.0100\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 97.8782 - pos_accuracy: 0.0019 - val_loss: 88.8985 - val_pos_accuracy: 0.0100\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 101,792\n",
      "Trainable params: 101,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:44:37.598019: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1709: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1730: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate는 넘파일 배열을 하나로 합치는 데 유용하게 쓰이는 함수이며,\\naxis=1은 y축을 기준으로 작동됨을 의미, 컬럼을 합치는 합산 작용.\\n\",\n",
      "        \"\\nsubplot(121) : 1*2 그리드의 첫번째를 뜻함\\nsubplot(122) : 1*2 그리드의 두번째를 뜻함\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\n차원을 줄여 나가며 and연산을 가로축 기준으로 계산함을 의미합니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n'label_true == label_pred'에 대한 and연산을 한 결과를 넘파일 배열로 만든후 \\n그것을 실수형으로 변환 후 차원을 줄이며 평균을 내는 것을 의미합니다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유:\\n정의되지 않은 상태로 어떤 배치크기에도 유연하게 대응할 수 있도록 \\n해주기 위함입니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 :\\n난수에 대한 기준을 주어 균일한 값으로 배열 생성하기 위함입니다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 76.36363636363636,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/이민기_46048.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이민기_46048.ipynb to python\n",
      "[NbConvertApp] Writing 36350 bytes to report1/이민기_46048.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "[[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 291311.57it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:44:45.003748: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:44:45.388626: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:44:45.388665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:44:45.615293: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:44:46.230938: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 107119.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 101,792\n",
      "Trainable params: 101,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 58.3587 - pos_accuracy: 0.0209 - val_loss: 7.1394 - val_pos_accuracy: 0.0190\n",
      "Epoch 2/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2.2577 - pos_accuracy: 0.1244 - val_loss: 1.2187 - val_pos_accuracy: 0.2107\n",
      "Epoch 3/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 1.0201 - pos_accuracy: 0.2642 - val_loss: 1.5526 - val_pos_accuracy: 0.1738\n",
      "Epoch 4/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.6403 - pos_accuracy: 0.3387 - val_loss: 0.5517 - val_pos_accuracy: 0.3619\n",
      "Epoch 5/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.4666 - pos_accuracy: 0.4458 - val_loss: 0.4465 - val_pos_accuracy: 0.4667\n",
      "Epoch 6/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.2633 - pos_accuracy: 0.5640 - val_loss: 0.6174 - val_pos_accuracy: 0.3583\n",
      "Epoch 7/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.4047 - pos_accuracy: 0.5776 - val_loss: 0.2846 - val_pos_accuracy: 0.6607\n",
      "Epoch 8/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1535 - pos_accuracy: 0.7340 - val_loss: 0.3285 - val_pos_accuracy: 0.4643\n",
      "Epoch 9/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1410 - pos_accuracy: 0.7555 - val_loss: 0.3717 - val_pos_accuracy: 0.6262\n",
      "Epoch 10/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.1007 - pos_accuracy: 0.8368 - val_loss: 0.1710 - val_pos_accuracy: 0.7595\n",
      "Epoch 11/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0575 - pos_accuracy: 0.9181 - val_loss: 0.1260 - val_pos_accuracy: 0.8607\n",
      "Epoch 12/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0489 - pos_accuracy: 0.9390 - val_loss: 0.1400 - val_pos_accuracy: 0.8143\n",
      "Epoch 13/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0381 - pos_accuracy: 0.9686 - val_loss: 0.0899 - val_pos_accuracy: 0.9262\n",
      "Epoch 14/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0348 - pos_accuracy: 0.9581 - val_loss: 0.0911 - val_pos_accuracy: 0.9190\n",
      "Epoch 15/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0229 - pos_accuracy: 0.9791 - val_loss: 0.2541 - val_pos_accuracy: 0.5690\n",
      "Epoch 16/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0373 - pos_accuracy: 0.9569 - val_loss: 0.1160 - val_pos_accuracy: 0.8464\n",
      "Epoch 17/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0366 - pos_accuracy: 0.9477 - val_loss: 0.0813 - val_pos_accuracy: 0.9238\n",
      "Epoch 18/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9895 - val_loss: 0.0715 - val_pos_accuracy: 0.9381\n",
      "Epoch 19/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0148 - pos_accuracy: 0.9914 - val_loss: 0.1365 - val_pos_accuracy: 0.7619\n",
      "Epoch 20/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0212 - pos_accuracy: 0.9828 - val_loss: 0.0679 - val_pos_accuracy: 0.9333\n",
      "Epoch 21/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0111 - pos_accuracy: 0.9932 - val_loss: 0.0661 - val_pos_accuracy: 0.9310\n",
      "Epoch 22/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0132 - pos_accuracy: 0.9865 - val_loss: 0.0662 - val_pos_accuracy: 0.9429\n",
      "Epoch 23/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0094 - pos_accuracy: 0.9926 - val_loss: 0.0717 - val_pos_accuracy: 0.9190\n",
      "Epoch 24/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0116 - pos_accuracy: 0.9951 - val_loss: 0.0623 - val_pos_accuracy: 0.9333\n",
      "Epoch 25/25\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 0.0080 - pos_accuracy: 0.9957 - val_loss: 0.0642 - val_pos_accuracy: 0.9381\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:44:54.617631: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1687: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\n - np.concatenate의 역할 : A,B 라는 배열이 있을 때 지정된 축(axis)의 방향으로 두 배열이 결합됩니다.\\n - axis=1의 의미 : 배열이 결합되는 축 또는 방향을 뜻하는데, axis=1로 지정하게 되면 행방향으로 합쳐지게 됩니다.\\n\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\n - plt.sub에서 121, 122의 의미 : 다수의 plot을 표현할 때 subplot을 사용하는데,121의 경우에는 1행, 2열, 1번째인덱스라는 의미이고, 122의 경우에는 1행, 2열, 2번째인덱스라는 의미입니다.\\n\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n - tf.reduce_all의 역할 : 텐서의 차원을 축소하면서 합치는 역할\\n - axis=1의 의미 : 텐서의 차원을 축소하면서 합칠때 가로 방향으로 합치도록 합니다. \\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n - tf.reduce_mean의 역할 : 텐서 차원을 축소하면서 평균값을 구하는 역할입니다.\\n                           reduce_all 함수를 통해 참값과 예측값의 일치 여부를 확인하며 텐서 배열을 합치고, cast를 통해 텐서를 True/False에 따라 1 또는 0으로 변환하게 됩니다.\\n                           그 후 reduce_mean 함수를 통해 텐서값의 전체적인 평균값을 확인하여 정확도를 확인하게 됩니다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n - 출력값은 0~28 사이의 위치값 두개가 나와야 하는데 활성함수를 사용하면 0~1사이의 값으로 출력범위가 제한되기 때문입니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n - 학습에 사용할 숫자를 랜덤하게 생성하는데(난수), 그 난수가 고정된 패턴으로 나타나게 하기 위해 사용합니다.\\n   고정된 패턴이 아니라면 코드를 실행할 때마다 결과물들이 다르게 나타납니다.\\n\",\n",
      "        true,\n",
      "        \"\\n안녕하십니까, 미래학부 202132184 이민기입니다.\\n저는 식품 전공을 하고 현재 농심에서 근무하고 있는데, 현업에서 딥러닝과 비전 시스템이 다양한 방식으로 도입되고 있습니다.\\n교수님 강의를 통해 딥러닝과 비전 시스템에 대해 배우게 되어 현업에 있는 시스템들에 대해 이해하는데 많은 도움이 되고 있습니다.\\n교수님의 수준 높은 강의에 비해 과제 제출 수준이 많이 떨어지지만, 더 많이 배우도록 하겠습니다.\\n감사합니다 ^^\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.9262\n",
      "}\"report1/강연실_46034.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/강연실_46034.ipynb to python\n",
      "[NbConvertApp] Writing 37763 bytes to report1/강연실_46034.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:248: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "52\n",
      "(2, 2)\n",
      "[[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]]\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "(2, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 298314.65it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:45:01.104500: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:45:01.485061: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:45:01.485103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:45:01.712362: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:45:02.322394: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 4ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 111036.80it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 104,674\n",
      "Trainable params: 104,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 60.8652 - pos_accuracy: 0.0181 - val_loss: 5.3778 - val_pos_accuracy: 0.1027\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8890 - pos_accuracy: 0.1419 - val_loss: 1.9306 - val_pos_accuracy: 0.2299\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7640 - pos_accuracy: 0.1919 - val_loss: 2.6696 - val_pos_accuracy: 0.0714\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.5146 - pos_accuracy: 0.2081 - val_loss: 1.0923 - val_pos_accuracy: 0.3125\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.8496 - pos_accuracy: 0.3081 - val_loss: 0.8788 - val_pos_accuracy: 0.3996\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6837 - pos_accuracy: 0.3669 - val_loss: 0.8403 - val_pos_accuracy: 0.3661\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6420 - pos_accuracy: 0.3606 - val_loss: 0.7824 - val_pos_accuracy: 0.3705\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4996 - pos_accuracy: 0.4613 - val_loss: 0.6402 - val_pos_accuracy: 0.4911\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.5655 - pos_accuracy: 0.3725 - val_loss: 0.5942 - val_pos_accuracy: 0.4955\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3831 - pos_accuracy: 0.5462 - val_loss: 0.6029 - val_pos_accuracy: 0.4152\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3752 - pos_accuracy: 0.4900 - val_loss: 0.5013 - val_pos_accuracy: 0.4866\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3075 - pos_accuracy: 0.6150 - val_loss: 0.4545 - val_pos_accuracy: 0.5402\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2696 - pos_accuracy: 0.6263 - val_loss: 0.4293 - val_pos_accuracy: 0.5714\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2452 - pos_accuracy: 0.6556 - val_loss: 0.4163 - val_pos_accuracy: 0.5759\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2239 - pos_accuracy: 0.6769 - val_loss: 0.4689 - val_pos_accuracy: 0.5536\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2602 - pos_accuracy: 0.6031 - val_loss: 0.4880 - val_pos_accuracy: 0.3996\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2410 - pos_accuracy: 0.6281 - val_loss: 0.5048 - val_pos_accuracy: 0.4085\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2039 - pos_accuracy: 0.6931 - val_loss: 0.3788 - val_pos_accuracy: 0.6004\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1719 - pos_accuracy: 0.7387 - val_loss: 0.3238 - val_pos_accuracy: 0.6741\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1780 - pos_accuracy: 0.7475 - val_loss: 0.3223 - val_pos_accuracy: 0.6964\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1787 - pos_accuracy: 0.7394 - val_loss: 0.3629 - val_pos_accuracy: 0.5871\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1565 - pos_accuracy: 0.7669 - val_loss: 0.2929 - val_pos_accuracy: 0.7344\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1220 - pos_accuracy: 0.8275 - val_loss: 0.2906 - val_pos_accuracy: 0.7076\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1165 - pos_accuracy: 0.8256 - val_loss: 0.2822 - val_pos_accuracy: 0.7254\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1260 - pos_accuracy: 0.8112 - val_loss: 0.3080 - val_pos_accuracy: 0.6964\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1242 - pos_accuracy: 0.8119 - val_loss: 0.3411 - val_pos_accuracy: 0.5759\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1493 - pos_accuracy: 0.7475 - val_loss: 0.2882 - val_pos_accuracy: 0.7009\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1157 - pos_accuracy: 0.8331 - val_loss: 0.2832 - val_pos_accuracy: 0.7210\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0986 - pos_accuracy: 0.8512 - val_loss: 0.2594 - val_pos_accuracy: 0.7701\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0822 - pos_accuracy: 0.8900 - val_loss: 0.2739 - val_pos_accuracy: 0.6964\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0839 - pos_accuracy: 0.8856 - val_loss: 0.2286 - val_pos_accuracy: 0.8080\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0719 - pos_accuracy: 0.9106 - val_loss: 0.2317 - val_pos_accuracy: 0.8147\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0650 - pos_accuracy: 0.9112 - val_loss: 0.2445 - val_pos_accuracy: 0.7790\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0641 - pos_accuracy: 0.9187 - val_loss: 0.2215 - val_pos_accuracy: 0.7835\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0604 - pos_accuracy: 0.9237 - val_loss: 0.2313 - val_pos_accuracy: 0.7879\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0542 - pos_accuracy: 0.9306 - val_loss: 0.2153 - val_pos_accuracy: 0.8326\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0580 - pos_accuracy: 0.9312 - val_loss: 0.2139 - val_pos_accuracy: 0.8281\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0519 - pos_accuracy: 0.9381 - val_loss: 0.2082 - val_pos_accuracy: 0.8482\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0507 - pos_accuracy: 0.9381 - val_loss: 0.2094 - val_pos_accuracy: 0.8482\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0641 - pos_accuracy: 0.9206 - val_loss: 0.2217 - val_pos_accuracy: 0.8036\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0525 - pos_accuracy: 0.9425 - val_loss: 0.2072 - val_pos_accuracy: 0.8281\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0486 - pos_accuracy: 0.9425 - val_loss: 0.2413 - val_pos_accuracy: 0.7656\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0508 - pos_accuracy: 0.9481 - val_loss: 0.1994 - val_pos_accuracy: 0.8594\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0383 - pos_accuracy: 0.9594 - val_loss: 0.1973 - val_pos_accuracy: 0.8326\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0366 - pos_accuracy: 0.9619 - val_loss: 0.1965 - val_pos_accuracy: 0.8393\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0365 - pos_accuracy: 0.9631 - val_loss: 0.1948 - val_pos_accuracy: 0.8527\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0310 - pos_accuracy: 0.9663 - val_loss: 0.1955 - val_pos_accuracy: 0.8438\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0343 - pos_accuracy: 0.9644 - val_loss: 0.1980 - val_pos_accuracy: 0.8326\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0399 - pos_accuracy: 0.9575 - val_loss: 0.1989 - val_pos_accuracy: 0.8571\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0316 - pos_accuracy: 0.9694 - val_loss: 0.1807 - val_pos_accuracy: 0.8571\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:45:10.198456: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.795207\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1800: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할 : 두 행렬을 더해서 하나의 행렬로 만든다.\\naxis=1의 의미 : axis=0은 행방향으로 합치는 것, axis=1은 열방향으로 합치는 것이다.\\n여기서는 A행렬(2x2)과 b행렬(2x1)을 열방향으로 더해 M행렬(2x3)을 만들었다.\\n\",\n",
      "        \"\\nsubplot(nrows, ncols, index)를 의미한다. subplot이 9개보다 적다면 쉼표없이 적어도 된다.\\nsubplot의 역할 : 주어진 플롯(여기서는 imshow와 title 플롯)을 위치시킨다.\\n여기서 subplot(121)은 1행, 2열, 왼쪽에서부터 첫번째를 의미하고,\\nsubplot(122)는 1행, 2열, 왼쪽에서부터 두번째를 의미한다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할 : tf.reduce는 연산 함수이다. tf.reduce_all은 원소들을 AND 연산한다.\\naxis=1의 의미 : axis=0은 열방향으로 연산하라는 것, axis=1은 행방향으로 연산하라는 것이다.\\n\",\n",
      "        \"\\ntf.reduce_mean의 역할 : 원소들의 평균을 연산한다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 : 최종 출력층의 활성화 함수는 딥러닝 모델의 목적에 따라 달라진다.\\n분류가 목적이었다면 Sigmoid나 ReLU를 사용했겠지만, 여기서는 목적이 없으므로 None(Linear)을 사용했다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 : 최초 생성된 난수가 유지되게 하기 위함이다.\\n최초 데이터 세트를 바탕으로 모델을 생성하여 정확도를 구한 상태를 유지하기 위함이다.\\n\",\n",
      "        true,\n",
      "        \"\\n항상 잘 준비된, 심도깊은 강의에 감사드립니다.\\n딥러닝 과목을 2번째 수강하고 있지만, 1주 수강분 학습하는데 강의시간의 2~3배가 걸립니다.\\n여기에 4주마다 과제가 4주분이 주어져서 주말동안 공부만 하고 있습니다.\\n과제를 매주 내주시고 제출만 4주차에 한꺼번에 내도록 해주시면 감사드리겠습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.8571\n",
      "}\"report1/조성백_46071.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/조성백_46071.ipynb to python\n",
      "[NbConvertApp] Writing 35489 bytes to report1/조성백_46071.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:212: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:246: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(3, 2) (2, 3) (2, 3)\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.1232343e-17 -1.0000000e+00  2.0000000e+02]\n",
      " [ 1.0000000e+00  6.1232343e-17 -1.4210855e-14]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 199.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 281223.24it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:45:17.567115: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:45:17.953232: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:45:17.953270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:45:18.179914: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:45:18.799181: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 116057.11it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/27\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 120.5007 - pos_accuracy: 0.0012 - val_loss: 79.9998 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 2/27\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 22.2140 - pos_accuracy: 0.0231 - val_loss: 20.7802 - val_pos_accuracy: 0.0067\n",
      "Epoch 3/27\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.4678 - pos_accuracy: 0.0581 - val_loss: 3.2679 - val_pos_accuracy: 0.0647\n",
      "Epoch 4/27\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4148 - pos_accuracy: 0.1831 - val_loss: 0.7909 - val_pos_accuracy: 0.3170\n",
      "Epoch 5/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2.0774 - pos_accuracy: 0.1419 - val_loss: 1.1655 - val_pos_accuracy: 0.1964\n",
      "Epoch 6/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6662 - pos_accuracy: 0.2637 - val_loss: 0.7570 - val_pos_accuracy: 0.2679\n",
      "Epoch 7/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.3736 - pos_accuracy: 0.2212 - val_loss: 0.4698 - val_pos_accuracy: 0.4040\n",
      "Epoch 8/27\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3461 - pos_accuracy: 0.4844 - val_loss: 0.4027 - val_pos_accuracy: 0.5179\n",
      "Epoch 9/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4592 - pos_accuracy: 0.3787 - val_loss: 0.3229 - val_pos_accuracy: 0.5893\n",
      "Epoch 10/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2330 - pos_accuracy: 0.5994 - val_loss: 0.3334 - val_pos_accuracy: 0.5469\n",
      "Epoch 11/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4374 - pos_accuracy: 0.4044 - val_loss: 0.5980 - val_pos_accuracy: 0.3571\n",
      "Epoch 12/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2753 - pos_accuracy: 0.5238 - val_loss: 0.2858 - val_pos_accuracy: 0.5960\n",
      "Epoch 13/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1489 - pos_accuracy: 0.7312 - val_loss: 0.2113 - val_pos_accuracy: 0.7299\n",
      "Epoch 14/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1343 - pos_accuracy: 0.7812 - val_loss: 0.2010 - val_pos_accuracy: 0.7522\n",
      "Epoch 15/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1590 - pos_accuracy: 0.7194 - val_loss: 0.2388 - val_pos_accuracy: 0.6540\n",
      "Epoch 16/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1900 - pos_accuracy: 0.6644 - val_loss: 0.3139 - val_pos_accuracy: 0.4732\n",
      "Epoch 17/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1303 - pos_accuracy: 0.7769 - val_loss: 0.2581 - val_pos_accuracy: 0.5737\n",
      "Epoch 18/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1296 - pos_accuracy: 0.7781 - val_loss: 0.3063 - val_pos_accuracy: 0.4688\n",
      "Epoch 19/27\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1633 - pos_accuracy: 0.6875 - val_loss: 0.2702 - val_pos_accuracy: 0.5737\n",
      "Epoch 20/27\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2531 - pos_accuracy: 0.5437 - val_loss: 0.2460 - val_pos_accuracy: 0.7121\n",
      "Epoch 21/27\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1971 - pos_accuracy: 0.6331 - val_loss: 0.1997 - val_pos_accuracy: 0.6696\n",
      "Epoch 22/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1396 - pos_accuracy: 0.7644 - val_loss: 0.1805 - val_pos_accuracy: 0.7433\n",
      "Epoch 23/27\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0597 - pos_accuracy: 0.9144 - val_loss: 0.1535 - val_pos_accuracy: 0.8237\n",
      "Epoch 24/27\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0652 - pos_accuracy: 0.9100 - val_loss: 0.1589 - val_pos_accuracy: 0.7857\n",
      "Epoch 25/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0975 - pos_accuracy: 0.8369 - val_loss: 0.1503 - val_pos_accuracy: 0.8259\n",
      "Epoch 26/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0478 - pos_accuracy: 0.9456 - val_loss: 0.1550 - val_pos_accuracy: 0.7969\n",
      "Epoch 27/27\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0439 - pos_accuracy: 0.9456 - val_loss: 0.1090 - val_pos_accuracy: 0.8951\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:45:24.039918: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1698: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\n제 생각엔 np.concatenate란 선택한 방향으로 배열을 연결해 주는 역할이라고 생각합니다.\\n(axis=1 은 배열의 열 방향을 의미합니다.)\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\n121에서 첫번째 1은 행의 수, 두번째 2는 열의 수, 3번째 1은 인덱스 입니다.\\n>> 첫번째 행 두번째 열의 첫 인덱스를 가르킵니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\n열 기준으로 element의 조건부를 대입하여 값을 찾음\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다.\\n\\nelement의 평균을 구해주는 함수\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n\\n0과 1로 구분하는 이진 분류 경우에는 최종 출력층의 activation 함수를 지정하지 않는다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n\\nrandom 성을 제어하기 위해, random seed를 고정하여 난수의 생생 패턴을 동일하게 관리할 수 있다.\\n\",\n",
      "        true,\n",
      "        \"\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.8906\n",
      "}\"report1/조민지_46076.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/조민지_46076.ipynb to python\n",
      "[NbConvertApp] Writing 35380 bytes to report1/조민지_46076.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:246: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)\n",
      "[ 0. nan]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:247: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(y - y)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(3, 1, 2)\n",
      "(2, 1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.12323400e-17 -7.07106781e-01  1.50000000e+02]\n",
      " [ 7.07106781e-01  6.12323400e-17  5.00000000e+01]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 281100.73it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:45:31.419374: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:45:31.806609: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:45:31.806648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:45:32.033844: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:45:32.644932: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 182131.40it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 1,576\n",
      "Trainable params: 1,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 96.1988 - accuracy: 0.5281 - val_loss: 33.1298 - val_accuracy: 0.6025\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 34.7380 - accuracy: 0.6044 - val_loss: 29.3114 - val_accuracy: 0.8725\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:45:34.968331: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1677: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        \"\\nnp.concatenate는 여러개의 배열을 하나로 합치는 역할을 합니다. \\naxis = 1로 지정하면 2번째의 축(1차원째)이 열이므로 가로로 결합된다.\\n\",\n",
      "        \"\\nsubplot에서 121은 1행쨰의 2열의 첫번째를 의미하고 122는 1행쨰의 2열의 두번쨰를 의미합니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        \"\\n입력되는 텐서와 axis에서 주어진 차원을 줄인다. axis = 1은 차원을 뜻한다. \\n\",\n",
      "        \"\\n평균을 구한다.\\n\",\n",
      "        \"\\n출력값을 제한하기 위해서입니다. \\n\",\n",
      "        \"\\n무작위로 다양하게 테스트 요소들을 생성하기 위해서입니다.  \\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 88.36363636363636,\n",
      "    \"accuracy\": 0.8725\n",
      "}\"report1/박사일_46055.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/박사일_46055.ipynb to python\n",
      "[NbConvertApp] Writing 34852 bytes to report1/박사일_46055.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:212: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:245: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 303517.19it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:45:41.480971: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:45:41.867200: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:45:41.867245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:45:42.094286: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:45:42.705033: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 93906.88it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 25,186\n",
      "Trainable params: 25,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 63.8065 - accuracy: 0.8600 - val_loss: 35.4986 - val_accuracy: 0.9000\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.9417 - accuracy: 0.9413 - val_loss: 7.9733 - val_accuracy: 0.9675\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.6982 - accuracy: 0.9606 - val_loss: 16.9245 - val_accuracy: 0.8975\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.3022 - accuracy: 0.9644 - val_loss: 0.7783 - val_accuracy: 0.9700\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6181 - accuracy: 0.9719 - val_loss: 0.5457 - val_accuracy: 0.9725\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0656 - accuracy: 0.9719 - val_loss: 0.6363 - val_accuracy: 0.9800\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1026 - accuracy: 0.9725 - val_loss: 0.4372 - val_accuracy: 0.9725\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2647 - accuracy: 0.9756 - val_loss: 0.3438 - val_accuracy: 0.9850\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3759 - accuracy: 0.9775 - val_loss: 0.3213 - val_accuracy: 0.9700\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1871 - accuracy: 0.9744 - val_loss: 0.3445 - val_accuracy: 0.9850\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1539 - accuracy: 0.9775 - val_loss: 0.3164 - val_accuracy: 0.9825\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.9762 - val_loss: 0.2458 - val_accuracy: 0.9800\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.9787 - val_loss: 0.2352 - val_accuracy: 0.9750\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1340 - accuracy: 0.9762 - val_loss: 0.2183 - val_accuracy: 0.9750\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1049 - accuracy: 0.9781 - val_loss: 0.2335 - val_accuracy: 0.9800\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1221 - accuracy: 0.9756 - val_loss: 0.2848 - val_accuracy: 0.9625\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0871 - accuracy: 0.9731 - val_loss: 0.2005 - val_accuracy: 0.9875\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.9769 - val_loss: 0.1710 - val_accuracy: 0.9750\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.9781 - val_loss: 0.1738 - val_accuracy: 0.9775\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0514 - accuracy: 0.9769 - val_loss: 0.1714 - val_accuracy: 0.9800\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0570 - accuracy: 0.9762 - val_loss: 0.1821 - val_accuracy: 0.9825\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9775 - val_loss: 0.1541 - val_accuracy: 0.9675\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.9781 - val_loss: 0.1558 - val_accuracy: 0.9700\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0415 - accuracy: 0.9737 - val_loss: 0.1459 - val_accuracy: 0.9850\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0460 - accuracy: 0.9775 - val_loss: 0.1559 - val_accuracy: 0.9925\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9775 - val_loss: 0.1484 - val_accuracy: 0.9900\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9794 - val_loss: 0.1445 - val_accuracy: 0.9750\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9800 - val_loss: 0.1454 - val_accuracy: 0.9625\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9800 - val_loss: 0.1359 - val_accuracy: 0.9850\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.9781 - val_loss: 0.1953 - val_accuracy: 0.9925\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0401 - accuracy: 0.9806 - val_loss: 0.1300 - val_accuracy: 0.9750\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9800 - val_loss: 0.1332 - val_accuracy: 0.9850\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9781 - val_loss: 0.1308 - val_accuracy: 0.9600\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.9787 - val_loss: 0.1309 - val_accuracy: 0.9875\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.9806 - val_loss: 0.1307 - val_accuracy: 0.9675\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9812 - val_loss: 0.1273 - val_accuracy: 0.9850\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.9794 - val_loss: 0.1206 - val_accuracy: 0.9825\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 0.9762 - val_loss: 0.1243 - val_accuracy: 0.9775\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.9831 - val_loss: 0.1238 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9800 - val_loss: 0.1210 - val_accuracy: 0.9825\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9806 - val_loss: 0.1188 - val_accuracy: 0.9775\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9781 - val_loss: 0.1156 - val_accuracy: 0.9625\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9769 - val_loss: 0.1190 - val_accuracy: 0.9600\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9762 - val_loss: 0.1181 - val_accuracy: 0.9700\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9800 - val_loss: 0.1168 - val_accuracy: 0.9600\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9800 - val_loss: 0.1161 - val_accuracy: 0.9800\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9756 - val_loss: 0.1156 - val_accuracy: 0.9875\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9781 - val_loss: 0.1152 - val_accuracy: 0.9800\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9750 - val_loss: 0.1147 - val_accuracy: 0.9925\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9781 - val_loss: 0.1128 - val_accuracy: 0.9550\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9812 - val_loss: 0.1101 - val_accuracy: 0.9625\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.9775 - val_loss: 0.1114 - val_accuracy: 0.9550\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9812 - val_loss: 0.1112 - val_accuracy: 0.9550\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 0.9756 - val_loss: 0.1122 - val_accuracy: 0.9825\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.9769 - val_loss: 0.1115 - val_accuracy: 0.9850\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9794 - val_loss: 0.1077 - val_accuracy: 0.9675\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.9825 - val_loss: 0.1068 - val_accuracy: 0.9600\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.9800 - val_loss: 0.1061 - val_accuracy: 0.9750\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 0.9781 - val_loss: 0.1081 - val_accuracy: 0.9775\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9806 - val_loss: 0.1078 - val_accuracy: 0.9725\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9794 - val_loss: 0.1050 - val_accuracy: 0.9825\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.9769 - val_loss: 0.1055 - val_accuracy: 0.9625\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.9800 - val_loss: 0.1055 - val_accuracy: 0.9775\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9762 - val_loss: 0.1098 - val_accuracy: 0.9925\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.9800 - val_loss: 0.1048 - val_accuracy: 0.9675\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.9800 - val_loss: 0.1050 - val_accuracy: 0.9725\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.9800 - val_loss: 0.1053 - val_accuracy: 0.9900\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.9787 - val_loss: 0.1036 - val_accuracy: 0.9600\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.9825 - val_loss: 0.1031 - val_accuracy: 0.9750\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 0.9794 - val_loss: 0.1032 - val_accuracy: 0.9725\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.9756 - val_loss: 0.1042 - val_accuracy: 0.9925\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9775 - val_loss: 0.1025 - val_accuracy: 0.9900\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9787 - val_loss: 0.1020 - val_accuracy: 0.9775\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9781 - val_loss: 0.1018 - val_accuracy: 0.9575\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.9762 - val_loss: 0.1012 - val_accuracy: 0.9750\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9787 - val_loss: 0.1016 - val_accuracy: 0.9775\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9756 - val_loss: 0.1018 - val_accuracy: 0.9800\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9781 - val_loss: 0.1011 - val_accuracy: 0.9775\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.9781 - val_loss: 0.1020 - val_accuracy: 0.9550\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9787 - val_loss: 0.1007 - val_accuracy: 0.9825\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.9781 - val_loss: 0.1008 - val_accuracy: 0.9650\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.9837 - val_loss: 0.1025 - val_accuracy: 0.9550\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.9775 - val_loss: 0.1008 - val_accuracy: 0.9600\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 0.9794 - val_loss: 0.1026 - val_accuracy: 0.9675\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 0.9744 - val_loss: 0.1006 - val_accuracy: 0.9675\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 0.9775 - val_loss: 0.1012 - val_accuracy: 0.9800\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9762 - val_loss: 0.1009 - val_accuracy: 0.9750\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 0.9762 - val_loss: 0.1003 - val_accuracy: 0.9675\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 0.9787 - val_loss: 0.1002 - val_accuracy: 0.9625\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 0.9756 - val_loss: 0.0995 - val_accuracy: 0.9850\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 0.9769 - val_loss: 0.0999 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 0.9775 - val_loss: 0.0994 - val_accuracy: 0.9775\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.8345e-04 - accuracy: 0.9800 - val_loss: 0.0991 - val_accuracy: 0.9650\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.7543e-04 - accuracy: 0.9794 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.8851e-04 - accuracy: 0.9750 - val_loss: 0.0994 - val_accuracy: 0.9550\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 0.9769 - val_loss: 0.0991 - val_accuracy: 0.9700\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.6931e-04 - accuracy: 0.9787 - val_loss: 0.0989 - val_accuracy: 0.9775\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.8992e-04 - accuracy: 0.9800 - val_loss: 0.0989 - val_accuracy: 0.9750\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 8.2476e-04 - accuracy: 0.9787 - val_loss: 0.0987 - val_accuracy: 0.9750\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 8.1901e-04 - accuracy: 0.9787 - val_loss: 0.0983 - val_accuracy: 0.9850\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:45:55.278930: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.7952071\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.7952071\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1663: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할 : 차원이 동일한 array 결합\\naxis=1의 의미 : 1축을 따라 옆으로 결합\\n\",\n",
      "        \"\\nsubplot에서 121 의미 : 1행 2열로 구성한 그림 중 첫번째\\n            122 의미 : 1행 2열로 구성한 그림 중 두번째\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다:\\ntf.reduce_all은 element의 AND 연산에 활용, axis=1은 두 번째 dimension에 있는 값들을 모두 더한 것\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지 :  \\n차원을 감소하여 평균을 구함\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 :\\ny = wx + b 구조의 네트워크 때문임\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 :\\n동일한 난수의 조합을 고정시키기 위함\\n\",\n",
      "        false,\n",
      "        \"\\n과제내용이 강의내용에 비해 많이 어렵습니다. 강의시간에 과제해답 및 설명 부탁드립니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 80.0,\n",
      "    \"accuracy\": 0.9825\n",
      "}\"report1/함현석_46003.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/함현석_46003.ipynb to python\n",
      "[NbConvertApp] Writing 35493 bytes to report1/함현석_46003.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[8.18746544e+307 1.78989511e+308]\n",
      "[0. 0.]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:258: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(2, 2)\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 298654.51it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:46:01.804596: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:46:02.187578: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:46:02.187618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:46:02.416844: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:46:03.048952: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 112390.58it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 44.7635 - pos_accuracy: 0.0431 - val_loss: 11.5289 - val_pos_accuracy: 0.1106\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 4.1431 - pos_accuracy: 0.1944 - val_loss: 1.2453 - val_pos_accuracy: 0.2933\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.3312 - pos_accuracy: 0.3425 - val_loss: 1.0659 - val_pos_accuracy: 0.3582\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5318 - pos_accuracy: 0.4806 - val_loss: 0.7969 - val_pos_accuracy: 0.4255\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3752 - pos_accuracy: 0.5663 - val_loss: 0.2805 - val_pos_accuracy: 0.7067\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2385 - pos_accuracy: 0.6775 - val_loss: 0.2591 - val_pos_accuracy: 0.7043\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2684 - pos_accuracy: 0.6844 - val_loss: 0.2094 - val_pos_accuracy: 0.8077\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1497 - pos_accuracy: 0.7763 - val_loss: 0.2677 - val_pos_accuracy: 0.7428\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0838 - pos_accuracy: 0.8644 - val_loss: 0.1710 - val_pos_accuracy: 0.8269\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0769 - pos_accuracy: 0.8712 - val_loss: 0.1498 - val_pos_accuracy: 0.8678\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0582 - pos_accuracy: 0.9225 - val_loss: 0.1792 - val_pos_accuracy: 0.7909\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0465 - pos_accuracy: 0.9444 - val_loss: 0.1248 - val_pos_accuracy: 0.8966\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0409 - pos_accuracy: 0.9538 - val_loss: 0.1307 - val_pos_accuracy: 0.8726\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0325 - pos_accuracy: 0.9681 - val_loss: 0.1223 - val_pos_accuracy: 0.8990\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0216 - pos_accuracy: 0.9831 - val_loss: 0.1234 - val_pos_accuracy: 0.8870\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0202 - pos_accuracy: 0.9881 - val_loss: 0.1245 - val_pos_accuracy: 0.8750\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0191 - pos_accuracy: 0.9869 - val_loss: 0.1110 - val_pos_accuracy: 0.9111\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0169 - pos_accuracy: 0.9887 - val_loss: 0.1140 - val_pos_accuracy: 0.9135\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0196 - pos_accuracy: 0.9881 - val_loss: 0.1066 - val_pos_accuracy: 0.9255\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0116 - pos_accuracy: 0.9937 - val_loss: 0.1018 - val_pos_accuracy: 0.9038\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9969 - val_loss: 0.0982 - val_pos_accuracy: 0.9183\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0086 - pos_accuracy: 0.9950 - val_loss: 0.0986 - val_pos_accuracy: 0.9062\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0088 - pos_accuracy: 0.9956 - val_loss: 0.1025 - val_pos_accuracy: 0.9111\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9969 - val_loss: 0.0934 - val_pos_accuracy: 0.9159\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9956 - val_loss: 0.0948 - val_pos_accuracy: 0.9062\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9975 - val_loss: 0.0936 - val_pos_accuracy: 0.9038\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9981 - val_loss: 0.1001 - val_pos_accuracy: 0.9062\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9987 - val_loss: 0.0923 - val_pos_accuracy: 0.9159\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9987 - val_loss: 0.0913 - val_pos_accuracy: 0.9207\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9994 - val_loss: 0.0923 - val_pos_accuracy: 0.9183\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9987 - val_loss: 0.0896 - val_pos_accuracy: 0.9231\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 1.0000 - val_loss: 0.0896 - val_pos_accuracy: 0.9111\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9994 - val_loss: 0.0895 - val_pos_accuracy: 0.9231\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.0883 - val_pos_accuracy: 0.9183\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 1.0000 - val_loss: 0.0883 - val_pos_accuracy: 0.9183\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.0897 - val_pos_accuracy: 0.9159\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 1.0000 - val_loss: 0.0871 - val_pos_accuracy: 0.9183\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 1.0000 - val_loss: 0.0869 - val_pos_accuracy: 0.9231\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 1.0000 - val_loss: 0.0873 - val_pos_accuracy: 0.9183\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 1.0000 - val_loss: 0.0864 - val_pos_accuracy: 0.9183\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 1.0000 - val_loss: 0.0859 - val_pos_accuracy: 0.9231\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 1.0000 - val_loss: 0.0862 - val_pos_accuracy: 0.9183\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.9574e-04 - pos_accuracy: 1.0000 - val_loss: 0.0850 - val_pos_accuracy: 0.9183\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.5859e-04 - pos_accuracy: 1.0000 - val_loss: 0.0858 - val_pos_accuracy: 0.9135\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.7040e-04 - pos_accuracy: 1.0000 - val_loss: 0.0857 - val_pos_accuracy: 0.9183\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.4534e-04 - pos_accuracy: 1.0000 - val_loss: 0.0849 - val_pos_accuracy: 0.9183\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.8744e-04 - pos_accuracy: 1.0000 - val_loss: 0.0851 - val_pos_accuracy: 0.9183\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.4705e-04 - pos_accuracy: 1.0000 - val_loss: 0.0853 - val_pos_accuracy: 0.9183\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.8590e-04 - pos_accuracy: 1.0000 - val_loss: 0.0845 - val_pos_accuracy: 0.9183\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4664e-04 - pos_accuracy: 1.0000 - val_loss: 0.0846 - val_pos_accuracy: 0.9135\n",
      "(2, 2)\n",
      "(2,)\n",
      "[ True False]\n",
      "[1. 0.]\n",
      "0.5\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0846 - pos_accuracy: 0.9135\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:46:14.550801: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "--2022-10-14 10:46:15--  http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "Resolving sipi.usc.edu (sipi.usc.edu)... 68.181.2.90\n",
      "접속 sipi.usc.edu (sipi.usc.edu)|68.181.2.90|:80... 접속됨.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://sipi.usc.edu/database/preview/misc/4.1.06.png [following]\n",
      "--2022-10-14 10:46:15--  https://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "접속 sipi.usc.edu (sipi.usc.edu)|68.181.2.90|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 35478 (35K) [image/png]\n",
      "Saving to: ‘image1.png’\n",
      "\n",
      "image1.png          100%[===================>]  34.65K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-10-14 10:46:16 (242 KB/s) - ‘image1.png’ saved [35478/35478]\n",
      "\n",
      "--2022-10-14 10:46:16--  http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "Resolving sipi.usc.edu (sipi.usc.edu)... 68.181.2.90\n",
      "접속 sipi.usc.edu (sipi.usc.edu)|68.181.2.90|:80... 접속됨.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://sipi.usc.edu/database/preview/misc/4.1.07.png [following]\n",
      "--2022-10-14 10:46:16--  https://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "접속 sipi.usc.edu (sipi.usc.edu)|68.181.2.90|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28517 (28K) [image/png]\n",
      "Saving to: ‘image2.png’\n",
      "\n",
      "image2.png          100%[===================>]  27.85K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-10-14 10:46:17 (199 KB/s) - ‘image2.png’ saved [28517/28517]\n",
      "\n",
      "<Figure size 1440x1440 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "0.6013071895424837\n",
      "0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1657: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        \"concatenate 메소드는 선택한 axis의 방향으로 배열을 연결해주는 메소드이며 axis=1은 열을 따라 위에서부터 아래로 배열이 연결됨을 의미합니다.\",\n",
      "        \"121, 122는 그리드 인자를 정수에 모아 표현한 것으로 subplot 121은 1*2그리드에 첫 번째 subplot, subplot 122는 1*2그리드에 두 번째 subplot을 의미합니다.\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"tf. reduce는 특정 차원을 제거하고 합계를 구하는 것을 의미하며 axis=1을 줌으로써 세로에 있는 값을 더하게 됩니다.\",\n",
      "        \"tf.reduce_mean은 tensor의 dimension에 걸쳐 평균을 계산하는 의미입니다.\",\n",
      "        \"출력층에서 sigmoid를 사용할 시 출력값이 0~1만 나올 수 있거나 activation 함수를 미분했을 때 값이 0에 수렴하여 학습이 잘 안되는 문제가 생길 수 있으므로 비슷한 결과값 출력을 유지하기 위해 activation을 none으로 유지할 수 있겠습니다.\",\n",
      "        \"알고리즘 모델이 지표가 좋아서인지 random성이 월등해서 좋아서인지 구별하기 위해 난수 생성 패턴을 동일하게 관리하고자 random seed를 사용할 수 있습니다.\",\n",
      "        true,\n",
      "        \"강의 준비해주시느라 노고 많으셨습니다. 최대한 코딩의 중간 과정을 이해하고 변경 시 이를 반영해보려 하였으나 충분히 소화하지는 못한 것 같습니다.\"\n",
      "    ],\n",
      "    \"score\": 96.0,\n",
      "    \"accuracy\": 0.9207\n",
      "}\"report1/장전인_46037.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/장전인_46037.ipynb to python\n",
      "[NbConvertApp] Writing 26362 bytes to report1/장전인_46037.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:193: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 279099.28it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:46:23.183115: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:46:23.586791: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:46:23.586828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:46:23.811327: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:46:24.408507: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:46:26.060147: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans01 error\n",
      "ans02 error\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1264: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  res[0] = (tmp == ans01.astype('float32')).all()\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1288: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1309: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 13.454545454545453,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/윤경일_46035.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/윤경일_46035.ipynb to python\n",
      "[NbConvertApp] Writing 36084 bytes to report1/윤경일_46035.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "ans01 --------------------------------------------------\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "v --------------------------------------------------\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "u --------------------------------------------------\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "ans02 --------------------------------------------------\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:218: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[5.6808489e+37 1.2419134e+38]\n",
      "[0. 0.]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "1번 아핀 변환 행렬 (A, b 부분을 수정합니다.) --------------------------------------------------------------\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "2번 아핀 변환 행렬 (A, b 부분을 수정합니다.) --------------------------------------------------------------\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "3번 아핀 변환 행렬 (A, b 부분을 수정합니다.) --------------------------------------------------------------\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 291180.12it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:46:32.423517: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:46:32.826159: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:46:32.826197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:46:33.047802: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:46:33.646712: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 109890.59it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 66.3050 - val_loss: 6.9794\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 2.8500 - val_loss: 0.8448\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.8412 - val_loss: 0.6865\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3634 - val_loss: 0.6259\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4469 - val_loss: 0.2744\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2334 - val_loss: 0.3186\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4709 - val_loss: 0.1946\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1669 - val_loss: 0.2464\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1004 - val_loss: 0.1480\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0858 - val_loss: 0.1352\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.1366\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.1006\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.1060\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.1099\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0305 - val_loss: 0.0983\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0293 - val_loss: 0.0844\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0285 - val_loss: 0.0797\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0241 - val_loss: 0.0730\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0233 - val_loss: 0.0815\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0212 - val_loss: 0.0811\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0698\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0177 - val_loss: 0.0681\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0138 - val_loss: 0.0623\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0591\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0113 - val_loss: 0.0607\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0104 - val_loss: 0.0590\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0095 - val_loss: 0.0659\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0084 - val_loss: 0.0590\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0081 - val_loss: 0.0566\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0644\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0081 - val_loss: 0.0543\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0067 - val_loss: 0.0555\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0539\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0522\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0523\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0049 - val_loss: 0.0512\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0512\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0506\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.0521\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0039 - val_loss: 0.0509\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0483\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.0492\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.0480\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0029 - val_loss: 0.0480\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - val_loss: 0.0478\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.0476\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.0489\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0024 - val_loss: 0.0470\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0022 - val_loss: 0.0480\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:46:44.416566: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1679: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \" concatenate 메소드는 선택한 축(axis)의 방향으로 배열을 연결해주는 메소드입니다. \\naxis는 축을 의미하고 0~2까지 가능하고 기본값은 0입니다.\\n\",\n",
      "        \" subplot에서 첫 번째 인수는 행을 두 번째 인수는 열로 구성됩니다. 세 번째 인수는 현재 플롯의 인덱스를 나타냅니다.\\n121은 1행에 2열에 첫번째 플롯을 의미합니다.\\n122은 1행에 2열에 두번째 플롯을 의미합니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all : tensor가 지정한 축 방향의 각 요소의 논리와 (and 연산)한다.\\naxis: 지정한 축은 지정하지 않으면 모든 원소의 평균값을 계산한다.\\n\",\n",
      "        \"\\ntf.reduce_mean : 특정 차원을 제거하고 평균을 구한다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 : 선형을 비활성화 처리하기 위해서 입니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 : 예측가능한 난수를 발생시키기 위함입니다. \\n\",\n",
      "        true,\n",
      "        \"\\n수학적 이해가 필요한 부분이 많아 걱정되었지만 강의를 통해 그리고 실습을 통해 조금씩 이해도가 높아지고 있습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 98.18181818181819,\n",
      "    \"accuracy\": 0.0017\n",
      "}\"report1/이형우_46056.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이형우_46056.ipynb to python\n",
      "[NbConvertApp] Writing 37180 bytes to report1/이형우_46056.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.1 0.2 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "422\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:251: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 293502.96it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:46:52.644625: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:46:53.055743: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:46:53.055783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:46:53.281170: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:46:53.927446: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 91749.97it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 13,138\n",
      "Trainable params: 13,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(2, 2)\n",
      "(2,)\n",
      "[ True False]\n",
      "[1. 0.]\n",
      "0.5\n",
      "Epoch 1/345\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 67.8615 - pos_accuracy: 0.0012 - val_loss: 43.0825 - val_pos_accuracy: 0.0025\n",
      "Epoch 2/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 36.3443 - pos_accuracy: 0.0119 - val_loss: 31.3034 - val_pos_accuracy: 0.0150\n",
      "Epoch 3/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 34.2648 - pos_accuracy: 0.0175 - val_loss: 30.9674 - val_pos_accuracy: 0.0100\n",
      "Epoch 4/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 34.3242 - pos_accuracy: 0.0294 - val_loss: 30.2006 - val_pos_accuracy: 0.0225\n",
      "Epoch 5/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.9779 - pos_accuracy: 0.0294 - val_loss: 30.3927 - val_pos_accuracy: 0.0400\n",
      "Epoch 6/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 34.0362 - pos_accuracy: 0.0244 - val_loss: 30.1085 - val_pos_accuracy: 0.0275\n",
      "Epoch 7/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.9066 - pos_accuracy: 0.0319 - val_loss: 30.1628 - val_pos_accuracy: 0.0550\n",
      "Epoch 8/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.9342 - pos_accuracy: 0.0294 - val_loss: 30.1298 - val_pos_accuracy: 0.0250\n",
      "Epoch 9/345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 4ms/step - loss: 33.8495 - pos_accuracy: 0.0250 - val_loss: 30.0968 - val_pos_accuracy: 0.0375\n",
      "Epoch 10/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.9994 - pos_accuracy: 0.0437 - val_loss: 30.0554 - val_pos_accuracy: 0.0300\n",
      "Epoch 11/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.8817 - pos_accuracy: 0.0312 - val_loss: 30.2809 - val_pos_accuracy: 0.0350\n",
      "Epoch 12/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.9069 - pos_accuracy: 0.0319 - val_loss: 30.0470 - val_pos_accuracy: 0.0550\n",
      "Epoch 13/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.8324 - pos_accuracy: 0.0319 - val_loss: 30.4203 - val_pos_accuracy: 0.0050\n",
      "Epoch 14/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.8175 - pos_accuracy: 0.0350 - val_loss: 30.3452 - val_pos_accuracy: 0.0500\n",
      "Epoch 15/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.7865 - pos_accuracy: 0.0388 - val_loss: 29.9897 - val_pos_accuracy: 0.0400\n",
      "Epoch 16/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 33.5502 - pos_accuracy: 0.0375 - val_loss: 29.8718 - val_pos_accuracy: 0.0250\n",
      "Epoch 17/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 26.7316 - pos_accuracy: 0.0481 - val_loss: 5.5559 - val_pos_accuracy: 0.0100\n",
      "Epoch 18/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 2.5224 - pos_accuracy: 0.1281 - val_loss: 1.4101 - val_pos_accuracy: 0.2050\n",
      "Epoch 19/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 1.1226 - pos_accuracy: 0.2062 - val_loss: 0.8544 - val_pos_accuracy: 0.2525\n",
      "Epoch 20/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.7774 - pos_accuracy: 0.2837 - val_loss: 0.5665 - val_pos_accuracy: 0.4275\n",
      "Epoch 21/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.5944 - pos_accuracy: 0.3162 - val_loss: 0.6159 - val_pos_accuracy: 0.3650\n",
      "Epoch 22/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.4680 - pos_accuracy: 0.3913 - val_loss: 0.4422 - val_pos_accuracy: 0.4400\n",
      "Epoch 23/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.4198 - pos_accuracy: 0.4387 - val_loss: 0.5057 - val_pos_accuracy: 0.3625\n",
      "Epoch 24/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.4469 - pos_accuracy: 0.4425 - val_loss: 0.4227 - val_pos_accuracy: 0.4550\n",
      "Epoch 25/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3468 - pos_accuracy: 0.4856 - val_loss: 0.3811 - val_pos_accuracy: 0.4975\n",
      "Epoch 26/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3355 - pos_accuracy: 0.4988 - val_loss: 0.3798 - val_pos_accuracy: 0.5100\n",
      "Epoch 27/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3084 - pos_accuracy: 0.5288 - val_loss: 0.3616 - val_pos_accuracy: 0.5325\n",
      "Epoch 28/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.2626 - pos_accuracy: 0.5813 - val_loss: 0.2918 - val_pos_accuracy: 0.6250\n",
      "Epoch 29/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.2622 - pos_accuracy: 0.5731 - val_loss: 0.3594 - val_pos_accuracy: 0.5500\n",
      "Epoch 30/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.2603 - pos_accuracy: 0.6137 - val_loss: 0.3198 - val_pos_accuracy: 0.5875\n",
      "Epoch 31/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.2115 - pos_accuracy: 0.6469 - val_loss: 0.3890 - val_pos_accuracy: 0.4800\n",
      "Epoch 32/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1998 - pos_accuracy: 0.6562 - val_loss: 0.2679 - val_pos_accuracy: 0.6400\n",
      "Epoch 33/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1900 - pos_accuracy: 0.6819 - val_loss: 0.2464 - val_pos_accuracy: 0.6900\n",
      "Epoch 34/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1742 - pos_accuracy: 0.7075 - val_loss: 0.2549 - val_pos_accuracy: 0.6550\n",
      "Epoch 35/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1773 - pos_accuracy: 0.7075 - val_loss: 0.2358 - val_pos_accuracy: 0.6800\n",
      "Epoch 36/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1616 - pos_accuracy: 0.7206 - val_loss: 0.2395 - val_pos_accuracy: 0.6700\n",
      "Epoch 37/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1569 - pos_accuracy: 0.7462 - val_loss: 0.2775 - val_pos_accuracy: 0.6450\n",
      "Epoch 38/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1603 - pos_accuracy: 0.7319 - val_loss: 0.2132 - val_pos_accuracy: 0.7150\n",
      "Epoch 39/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1380 - pos_accuracy: 0.7725 - val_loss: 0.2427 - val_pos_accuracy: 0.6900\n",
      "Epoch 40/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1387 - pos_accuracy: 0.7763 - val_loss: 0.4293 - val_pos_accuracy: 0.4375\n",
      "Epoch 41/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1413 - pos_accuracy: 0.7638 - val_loss: 0.2203 - val_pos_accuracy: 0.6925\n",
      "Epoch 42/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1236 - pos_accuracy: 0.7875 - val_loss: 0.2110 - val_pos_accuracy: 0.7450\n",
      "Epoch 43/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1158 - pos_accuracy: 0.8169 - val_loss: 0.2362 - val_pos_accuracy: 0.7200\n",
      "Epoch 44/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1093 - pos_accuracy: 0.8319 - val_loss: 0.1811 - val_pos_accuracy: 0.7800\n",
      "Epoch 45/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1080 - pos_accuracy: 0.8319 - val_loss: 0.1877 - val_pos_accuracy: 0.7850\n",
      "Epoch 46/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0995 - pos_accuracy: 0.8431 - val_loss: 0.1938 - val_pos_accuracy: 0.7600\n",
      "Epoch 47/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0925 - pos_accuracy: 0.8637 - val_loss: 0.1845 - val_pos_accuracy: 0.7725\n",
      "Epoch 48/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0979 - pos_accuracy: 0.8444 - val_loss: 0.1691 - val_pos_accuracy: 0.7950\n",
      "Epoch 49/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0915 - pos_accuracy: 0.8600 - val_loss: 0.1766 - val_pos_accuracy: 0.7875\n",
      "Epoch 50/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0924 - pos_accuracy: 0.8656 - val_loss: 0.1680 - val_pos_accuracy: 0.7900\n",
      "Epoch 51/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0830 - pos_accuracy: 0.8781 - val_loss: 0.1619 - val_pos_accuracy: 0.7950\n",
      "Epoch 52/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0831 - pos_accuracy: 0.8719 - val_loss: 0.1652 - val_pos_accuracy: 0.7950\n",
      "Epoch 53/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0790 - pos_accuracy: 0.8813 - val_loss: 0.1703 - val_pos_accuracy: 0.7750\n",
      "Epoch 54/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0770 - pos_accuracy: 0.8919 - val_loss: 0.1762 - val_pos_accuracy: 0.7900\n",
      "Epoch 55/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0737 - pos_accuracy: 0.8988 - val_loss: 0.1909 - val_pos_accuracy: 0.7650\n",
      "Epoch 56/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0684 - pos_accuracy: 0.9031 - val_loss: 0.1515 - val_pos_accuracy: 0.8100\n",
      "Epoch 57/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0695 - pos_accuracy: 0.9069 - val_loss: 0.1492 - val_pos_accuracy: 0.8125\n",
      "Epoch 58/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0664 - pos_accuracy: 0.9062 - val_loss: 0.1453 - val_pos_accuracy: 0.8225\n",
      "Epoch 59/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0655 - pos_accuracy: 0.9144 - val_loss: 0.1462 - val_pos_accuracy: 0.8175\n",
      "Epoch 60/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0629 - pos_accuracy: 0.9119 - val_loss: 0.2761 - val_pos_accuracy: 0.6275\n",
      "Epoch 61/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0586 - pos_accuracy: 0.9150 - val_loss: 0.1398 - val_pos_accuracy: 0.8175\n",
      "Epoch 62/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0585 - pos_accuracy: 0.9162 - val_loss: 0.1390 - val_pos_accuracy: 0.8350\n",
      "Epoch 63/345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0558 - pos_accuracy: 0.9237 - val_loss: 0.1553 - val_pos_accuracy: 0.8200\n",
      "Epoch 64/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0546 - pos_accuracy: 0.9250 - val_loss: 0.1360 - val_pos_accuracy: 0.8375\n",
      "Epoch 65/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0519 - pos_accuracy: 0.9306 - val_loss: 0.1381 - val_pos_accuracy: 0.8325\n",
      "Epoch 66/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0493 - pos_accuracy: 0.9350 - val_loss: 0.1354 - val_pos_accuracy: 0.8325\n",
      "Epoch 67/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0486 - pos_accuracy: 0.9325 - val_loss: 0.1357 - val_pos_accuracy: 0.8350\n",
      "Epoch 68/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0465 - pos_accuracy: 0.9356 - val_loss: 0.1407 - val_pos_accuracy: 0.8375\n",
      "Epoch 69/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0452 - pos_accuracy: 0.9394 - val_loss: 0.1434 - val_pos_accuracy: 0.8375\n",
      "Epoch 70/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0453 - pos_accuracy: 0.9381 - val_loss: 0.1306 - val_pos_accuracy: 0.8300\n",
      "Epoch 71/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0444 - pos_accuracy: 0.9381 - val_loss: 0.2369 - val_pos_accuracy: 0.6350\n",
      "Epoch 72/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0460 - pos_accuracy: 0.9344 - val_loss: 0.1409 - val_pos_accuracy: 0.8425\n",
      "Epoch 73/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0420 - pos_accuracy: 0.9419 - val_loss: 0.1244 - val_pos_accuracy: 0.8500\n",
      "Epoch 74/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0412 - pos_accuracy: 0.9456 - val_loss: 0.1307 - val_pos_accuracy: 0.8550\n",
      "Epoch 75/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0401 - pos_accuracy: 0.9444 - val_loss: 0.1253 - val_pos_accuracy: 0.8500\n",
      "Epoch 76/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0395 - pos_accuracy: 0.9463 - val_loss: 0.1239 - val_pos_accuracy: 0.8400\n",
      "Epoch 77/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0385 - pos_accuracy: 0.9463 - val_loss: 0.1299 - val_pos_accuracy: 0.8450\n",
      "Epoch 78/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0380 - pos_accuracy: 0.9475 - val_loss: 0.1315 - val_pos_accuracy: 0.8375\n",
      "Epoch 79/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0375 - pos_accuracy: 0.9481 - val_loss: 0.1264 - val_pos_accuracy: 0.8475\n",
      "Epoch 80/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0366 - pos_accuracy: 0.9506 - val_loss: 0.1233 - val_pos_accuracy: 0.8550\n",
      "Epoch 81/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0355 - pos_accuracy: 0.9506 - val_loss: 0.1233 - val_pos_accuracy: 0.8425\n",
      "Epoch 82/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0354 - pos_accuracy: 0.9469 - val_loss: 0.1191 - val_pos_accuracy: 0.8500\n",
      "Epoch 83/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0367 - pos_accuracy: 0.9444 - val_loss: 0.1200 - val_pos_accuracy: 0.8500\n",
      "Epoch 84/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0346 - pos_accuracy: 0.9506 - val_loss: 0.1257 - val_pos_accuracy: 0.8525\n",
      "Epoch 85/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0343 - pos_accuracy: 0.9494 - val_loss: 0.1259 - val_pos_accuracy: 0.8450\n",
      "Epoch 86/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0329 - pos_accuracy: 0.9500 - val_loss: 0.1255 - val_pos_accuracy: 0.8525\n",
      "Epoch 87/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0331 - pos_accuracy: 0.9481 - val_loss: 0.1206 - val_pos_accuracy: 0.8550\n",
      "Epoch 88/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0326 - pos_accuracy: 0.9544 - val_loss: 0.1279 - val_pos_accuracy: 0.8525\n",
      "Epoch 89/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0338 - pos_accuracy: 0.9506 - val_loss: 0.1221 - val_pos_accuracy: 0.8475\n",
      "Epoch 90/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0318 - pos_accuracy: 0.9525 - val_loss: 0.1170 - val_pos_accuracy: 0.8550\n",
      "Epoch 91/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0307 - pos_accuracy: 0.9531 - val_loss: 0.1231 - val_pos_accuracy: 0.8450\n",
      "Epoch 92/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0305 - pos_accuracy: 0.9525 - val_loss: 0.1190 - val_pos_accuracy: 0.8525\n",
      "Epoch 93/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0304 - pos_accuracy: 0.9556 - val_loss: 0.1175 - val_pos_accuracy: 0.8550\n",
      "Epoch 94/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0304 - pos_accuracy: 0.9500 - val_loss: 0.1264 - val_pos_accuracy: 0.8550\n",
      "Epoch 95/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0296 - pos_accuracy: 0.9550 - val_loss: 0.1170 - val_pos_accuracy: 0.8625\n",
      "Epoch 96/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0293 - pos_accuracy: 0.9519 - val_loss: 0.1213 - val_pos_accuracy: 0.8450\n",
      "Epoch 97/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0285 - pos_accuracy: 0.9550 - val_loss: 0.1157 - val_pos_accuracy: 0.8575\n",
      "Epoch 98/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0282 - pos_accuracy: 0.9569 - val_loss: 0.1179 - val_pos_accuracy: 0.8550\n",
      "Epoch 99/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0277 - pos_accuracy: 0.9550 - val_loss: 0.1189 - val_pos_accuracy: 0.8550\n",
      "Epoch 100/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0278 - pos_accuracy: 0.9556 - val_loss: 0.1227 - val_pos_accuracy: 0.8450\n",
      "Epoch 101/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0279 - pos_accuracy: 0.9563 - val_loss: 0.1210 - val_pos_accuracy: 0.8550\n",
      "Epoch 102/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0279 - pos_accuracy: 0.9550 - val_loss: 0.1146 - val_pos_accuracy: 0.8650\n",
      "Epoch 103/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0266 - pos_accuracy: 0.9588 - val_loss: 0.1167 - val_pos_accuracy: 0.8550\n",
      "Epoch 104/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0269 - pos_accuracy: 0.9575 - val_loss: 0.1165 - val_pos_accuracy: 0.8500\n",
      "Epoch 105/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0268 - pos_accuracy: 0.9556 - val_loss: 0.1244 - val_pos_accuracy: 0.8450\n",
      "Epoch 106/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0267 - pos_accuracy: 0.9588 - val_loss: 0.1194 - val_pos_accuracy: 0.8525\n",
      "Epoch 107/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0258 - pos_accuracy: 0.9550 - val_loss: 0.1167 - val_pos_accuracy: 0.8500\n",
      "Epoch 108/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0268 - pos_accuracy: 0.9569 - val_loss: 0.1154 - val_pos_accuracy: 0.8550\n",
      "Epoch 109/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0258 - pos_accuracy: 0.9575 - val_loss: 0.1151 - val_pos_accuracy: 0.8625\n",
      "Epoch 110/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0250 - pos_accuracy: 0.9594 - val_loss: 0.1166 - val_pos_accuracy: 0.8575\n",
      "Epoch 111/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0251 - pos_accuracy: 0.9575 - val_loss: 0.1771 - val_pos_accuracy: 0.8200\n",
      "Epoch 112/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0259 - pos_accuracy: 0.9588 - val_loss: 0.1143 - val_pos_accuracy: 0.8550\n",
      "Epoch 113/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0246 - pos_accuracy: 0.9575 - val_loss: 0.1193 - val_pos_accuracy: 0.8550\n",
      "Epoch 114/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0248 - pos_accuracy: 0.9581 - val_loss: 0.1158 - val_pos_accuracy: 0.8600\n",
      "Epoch 115/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0251 - pos_accuracy: 0.9588 - val_loss: 0.1136 - val_pos_accuracy: 0.8600\n",
      "Epoch 116/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0239 - pos_accuracy: 0.9606 - val_loss: 0.1111 - val_pos_accuracy: 0.8575\n",
      "Epoch 117/345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0241 - pos_accuracy: 0.9581 - val_loss: 0.1139 - val_pos_accuracy: 0.8500\n",
      "Epoch 118/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0236 - pos_accuracy: 0.9588 - val_loss: 0.1452 - val_pos_accuracy: 0.8425\n",
      "Epoch 119/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0246 - pos_accuracy: 0.9600 - val_loss: 0.1110 - val_pos_accuracy: 0.8550\n",
      "Epoch 120/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0232 - pos_accuracy: 0.9581 - val_loss: 0.1122 - val_pos_accuracy: 0.8575\n",
      "Epoch 121/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0240 - pos_accuracy: 0.9588 - val_loss: 0.1156 - val_pos_accuracy: 0.8600\n",
      "Epoch 122/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0232 - pos_accuracy: 0.9588 - val_loss: 0.1131 - val_pos_accuracy: 0.8550\n",
      "Epoch 123/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0231 - pos_accuracy: 0.9575 - val_loss: 0.1128 - val_pos_accuracy: 0.8625\n",
      "Epoch 124/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0227 - pos_accuracy: 0.9625 - val_loss: 0.1116 - val_pos_accuracy: 0.8575\n",
      "Epoch 125/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0222 - pos_accuracy: 0.9581 - val_loss: 0.1119 - val_pos_accuracy: 0.8575\n",
      "Epoch 126/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0237 - pos_accuracy: 0.9569 - val_loss: 0.1309 - val_pos_accuracy: 0.8525\n",
      "Epoch 127/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0232 - pos_accuracy: 0.9575 - val_loss: 0.1132 - val_pos_accuracy: 0.8550\n",
      "Epoch 128/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0222 - pos_accuracy: 0.9600 - val_loss: 0.1104 - val_pos_accuracy: 0.8600\n",
      "Epoch 129/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0221 - pos_accuracy: 0.9631 - val_loss: 0.1362 - val_pos_accuracy: 0.8475\n",
      "Epoch 130/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0218 - pos_accuracy: 0.9581 - val_loss: 0.1117 - val_pos_accuracy: 0.8500\n",
      "Epoch 131/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0219 - pos_accuracy: 0.9600 - val_loss: 0.1121 - val_pos_accuracy: 0.8600\n",
      "Epoch 132/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0217 - pos_accuracy: 0.9594 - val_loss: 0.1141 - val_pos_accuracy: 0.8475\n",
      "Epoch 133/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0213 - pos_accuracy: 0.9581 - val_loss: 0.1128 - val_pos_accuracy: 0.8600\n",
      "Epoch 134/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0215 - pos_accuracy: 0.9600 - val_loss: 0.1127 - val_pos_accuracy: 0.8500\n",
      "Epoch 135/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0217 - pos_accuracy: 0.9594 - val_loss: 0.1121 - val_pos_accuracy: 0.8600\n",
      "Epoch 136/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0215 - pos_accuracy: 0.9625 - val_loss: 0.1105 - val_pos_accuracy: 0.8600\n",
      "Epoch 137/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0211 - pos_accuracy: 0.9600 - val_loss: 0.1375 - val_pos_accuracy: 0.8375\n",
      "Epoch 138/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0210 - pos_accuracy: 0.9638 - val_loss: 0.1098 - val_pos_accuracy: 0.8575\n",
      "Epoch 139/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0204 - pos_accuracy: 0.9600 - val_loss: 0.1093 - val_pos_accuracy: 0.8625\n",
      "Epoch 140/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0208 - pos_accuracy: 0.9606 - val_loss: 0.1101 - val_pos_accuracy: 0.8575\n",
      "Epoch 141/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0202 - pos_accuracy: 0.9600 - val_loss: 0.1116 - val_pos_accuracy: 0.8550\n",
      "Epoch 142/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0205 - pos_accuracy: 0.9600 - val_loss: 0.1121 - val_pos_accuracy: 0.8650\n",
      "Epoch 143/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0203 - pos_accuracy: 0.9613 - val_loss: 0.1110 - val_pos_accuracy: 0.8575\n",
      "Epoch 144/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0196 - pos_accuracy: 0.9619 - val_loss: 0.1110 - val_pos_accuracy: 0.8500\n",
      "Epoch 145/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0197 - pos_accuracy: 0.9613 - val_loss: 0.1127 - val_pos_accuracy: 0.8675\n",
      "Epoch 146/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0200 - pos_accuracy: 0.9625 - val_loss: 0.1109 - val_pos_accuracy: 0.8500\n",
      "Epoch 147/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0194 - pos_accuracy: 0.9638 - val_loss: 0.1125 - val_pos_accuracy: 0.8575\n",
      "Epoch 148/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0193 - pos_accuracy: 0.9606 - val_loss: 0.1091 - val_pos_accuracy: 0.8650\n",
      "Epoch 149/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0195 - pos_accuracy: 0.9619 - val_loss: 0.1104 - val_pos_accuracy: 0.8575\n",
      "Epoch 150/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0190 - pos_accuracy: 0.9625 - val_loss: 0.1077 - val_pos_accuracy: 0.8650\n",
      "Epoch 151/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0192 - pos_accuracy: 0.9631 - val_loss: 0.1187 - val_pos_accuracy: 0.8525\n",
      "Epoch 152/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0196 - pos_accuracy: 0.9631 - val_loss: 0.1100 - val_pos_accuracy: 0.8500\n",
      "Epoch 153/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0192 - pos_accuracy: 0.9631 - val_loss: 0.1093 - val_pos_accuracy: 0.8575\n",
      "Epoch 154/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0188 - pos_accuracy: 0.9619 - val_loss: 0.1111 - val_pos_accuracy: 0.8475\n",
      "Epoch 155/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0188 - pos_accuracy: 0.9619 - val_loss: 0.1089 - val_pos_accuracy: 0.8575\n",
      "Epoch 156/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0188 - pos_accuracy: 0.9631 - val_loss: 0.1138 - val_pos_accuracy: 0.8550\n",
      "Epoch 157/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0186 - pos_accuracy: 0.9625 - val_loss: 0.1072 - val_pos_accuracy: 0.8600\n",
      "Epoch 158/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0186 - pos_accuracy: 0.9625 - val_loss: 0.1109 - val_pos_accuracy: 0.8600\n",
      "Epoch 159/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0183 - pos_accuracy: 0.9619 - val_loss: 0.1092 - val_pos_accuracy: 0.8700\n",
      "Epoch 160/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0181 - pos_accuracy: 0.9638 - val_loss: 0.1097 - val_pos_accuracy: 0.8575\n",
      "Epoch 161/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0183 - pos_accuracy: 0.9625 - val_loss: 0.1125 - val_pos_accuracy: 0.8575\n",
      "Epoch 162/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0181 - pos_accuracy: 0.9688 - val_loss: 0.1084 - val_pos_accuracy: 0.8650\n",
      "Epoch 163/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0180 - pos_accuracy: 0.9625 - val_loss: 0.1077 - val_pos_accuracy: 0.8625\n",
      "Epoch 164/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0178 - pos_accuracy: 0.9631 - val_loss: 0.1091 - val_pos_accuracy: 0.8600\n",
      "Epoch 165/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0182 - pos_accuracy: 0.9719 - val_loss: 0.1106 - val_pos_accuracy: 0.8650\n",
      "Epoch 166/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0179 - pos_accuracy: 0.9644 - val_loss: 0.1076 - val_pos_accuracy: 0.8625\n",
      "Epoch 167/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0177 - pos_accuracy: 0.9700 - val_loss: 0.1091 - val_pos_accuracy: 0.8625\n",
      "Epoch 168/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9625 - val_loss: 0.1072 - val_pos_accuracy: 0.8675\n",
      "Epoch 169/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9638 - val_loss: 0.1077 - val_pos_accuracy: 0.8625\n",
      "Epoch 170/345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0177 - pos_accuracy: 0.9650 - val_loss: 0.1087 - val_pos_accuracy: 0.8700\n",
      "Epoch 171/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0178 - pos_accuracy: 0.9656 - val_loss: 0.1078 - val_pos_accuracy: 0.8550\n",
      "Epoch 172/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0175 - pos_accuracy: 0.9638 - val_loss: 0.1073 - val_pos_accuracy: 0.8675\n",
      "Epoch 173/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0178 - pos_accuracy: 0.9644 - val_loss: 0.1066 - val_pos_accuracy: 0.8625\n",
      "Epoch 174/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9644 - val_loss: 0.1079 - val_pos_accuracy: 0.8625\n",
      "Epoch 175/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0174 - pos_accuracy: 0.9663 - val_loss: 0.1065 - val_pos_accuracy: 0.8725\n",
      "Epoch 176/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0177 - pos_accuracy: 0.9663 - val_loss: 0.1069 - val_pos_accuracy: 0.8700\n",
      "Epoch 177/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0175 - pos_accuracy: 0.9669 - val_loss: 0.1068 - val_pos_accuracy: 0.8625\n",
      "Epoch 178/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0172 - pos_accuracy: 0.9675 - val_loss: 0.1073 - val_pos_accuracy: 0.8675\n",
      "Epoch 179/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0172 - pos_accuracy: 0.9644 - val_loss: 0.1052 - val_pos_accuracy: 0.8825\n",
      "Epoch 180/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0172 - pos_accuracy: 0.9663 - val_loss: 0.1071 - val_pos_accuracy: 0.8675\n",
      "Epoch 181/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0170 - pos_accuracy: 0.9650 - val_loss: 0.1063 - val_pos_accuracy: 0.8650\n",
      "Epoch 182/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0170 - pos_accuracy: 0.9663 - val_loss: 0.1083 - val_pos_accuracy: 0.8600\n",
      "Epoch 183/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0172 - pos_accuracy: 0.9656 - val_loss: 0.1075 - val_pos_accuracy: 0.8650\n",
      "Epoch 184/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0171 - pos_accuracy: 0.9681 - val_loss: 0.1072 - val_pos_accuracy: 0.8650\n",
      "Epoch 185/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0169 - pos_accuracy: 0.9663 - val_loss: 0.1085 - val_pos_accuracy: 0.8550\n",
      "Epoch 186/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0169 - pos_accuracy: 0.9650 - val_loss: 0.1074 - val_pos_accuracy: 0.8675\n",
      "Epoch 187/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0169 - pos_accuracy: 0.9650 - val_loss: 0.1135 - val_pos_accuracy: 0.8650\n",
      "Epoch 188/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0167 - pos_accuracy: 0.9669 - val_loss: 0.1063 - val_pos_accuracy: 0.8700\n",
      "Epoch 189/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0167 - pos_accuracy: 0.9688 - val_loss: 0.1089 - val_pos_accuracy: 0.8625\n",
      "Epoch 190/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0168 - pos_accuracy: 0.9675 - val_loss: 0.1060 - val_pos_accuracy: 0.8725\n",
      "Epoch 191/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0167 - pos_accuracy: 0.9656 - val_loss: 0.1065 - val_pos_accuracy: 0.8875\n",
      "Epoch 192/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0168 - pos_accuracy: 0.9762 - val_loss: 0.1068 - val_pos_accuracy: 0.8725\n",
      "Epoch 193/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0167 - pos_accuracy: 0.9656 - val_loss: 0.1058 - val_pos_accuracy: 0.8700\n",
      "Epoch 194/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0164 - pos_accuracy: 0.9644 - val_loss: 0.1074 - val_pos_accuracy: 0.8650\n",
      "Epoch 195/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0164 - pos_accuracy: 0.9663 - val_loss: 0.1079 - val_pos_accuracy: 0.8725\n",
      "Epoch 196/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0165 - pos_accuracy: 0.9719 - val_loss: 0.1053 - val_pos_accuracy: 0.8625\n",
      "Epoch 197/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0165 - pos_accuracy: 0.9650 - val_loss: 0.1046 - val_pos_accuracy: 0.8650\n",
      "Epoch 198/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0161 - pos_accuracy: 0.9681 - val_loss: 0.1057 - val_pos_accuracy: 0.8675\n",
      "Epoch 199/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0161 - pos_accuracy: 0.9675 - val_loss: 0.1052 - val_pos_accuracy: 0.8725\n",
      "Epoch 200/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0162 - pos_accuracy: 0.9688 - val_loss: 0.1056 - val_pos_accuracy: 0.8675\n",
      "Epoch 201/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0161 - pos_accuracy: 0.9669 - val_loss: 0.1050 - val_pos_accuracy: 0.8625\n",
      "Epoch 202/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0162 - pos_accuracy: 0.9675 - val_loss: 0.1054 - val_pos_accuracy: 0.8725\n",
      "Epoch 203/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0160 - pos_accuracy: 0.9675 - val_loss: 0.1053 - val_pos_accuracy: 0.8725\n",
      "Epoch 204/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0159 - pos_accuracy: 0.9669 - val_loss: 0.1037 - val_pos_accuracy: 0.8825\n",
      "Epoch 205/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0159 - pos_accuracy: 0.9663 - val_loss: 0.1050 - val_pos_accuracy: 0.8725\n",
      "Epoch 206/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0159 - pos_accuracy: 0.9669 - val_loss: 0.1039 - val_pos_accuracy: 0.8750\n",
      "Epoch 207/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0159 - pos_accuracy: 0.9675 - val_loss: 0.1050 - val_pos_accuracy: 0.8750\n",
      "Epoch 208/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0161 - pos_accuracy: 0.9656 - val_loss: 0.1051 - val_pos_accuracy: 0.8775\n",
      "Epoch 209/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0159 - pos_accuracy: 0.9675 - val_loss: 0.1052 - val_pos_accuracy: 0.8675\n",
      "Epoch 210/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0158 - pos_accuracy: 0.9681 - val_loss: 0.1048 - val_pos_accuracy: 0.8650\n",
      "Epoch 211/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0156 - pos_accuracy: 0.9669 - val_loss: 0.1034 - val_pos_accuracy: 0.8625\n",
      "Epoch 212/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0154 - pos_accuracy: 0.9681 - val_loss: 0.1052 - val_pos_accuracy: 0.8700\n",
      "Epoch 213/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0155 - pos_accuracy: 0.9669 - val_loss: 0.1041 - val_pos_accuracy: 0.8850\n",
      "Epoch 214/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0155 - pos_accuracy: 0.9700 - val_loss: 0.1041 - val_pos_accuracy: 0.8625\n",
      "Epoch 215/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0155 - pos_accuracy: 0.9688 - val_loss: 0.1047 - val_pos_accuracy: 0.8750\n",
      "Epoch 216/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0153 - pos_accuracy: 0.9694 - val_loss: 0.1061 - val_pos_accuracy: 0.8600\n",
      "Epoch 217/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0155 - pos_accuracy: 0.9669 - val_loss: 0.1037 - val_pos_accuracy: 0.8700\n",
      "Epoch 218/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0152 - pos_accuracy: 0.9675 - val_loss: 0.1056 - val_pos_accuracy: 0.8725\n",
      "Epoch 219/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0154 - pos_accuracy: 0.9694 - val_loss: 0.1040 - val_pos_accuracy: 0.8750\n",
      "Epoch 220/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0154 - pos_accuracy: 0.9694 - val_loss: 0.1044 - val_pos_accuracy: 0.8775\n",
      "Epoch 221/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0150 - pos_accuracy: 0.9688 - val_loss: 0.1050 - val_pos_accuracy: 0.8625\n",
      "Epoch 222/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0150 - pos_accuracy: 0.9706 - val_loss: 0.1052 - val_pos_accuracy: 0.8775\n",
      "Epoch 223/345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0151 - pos_accuracy: 0.9681 - val_loss: 0.1038 - val_pos_accuracy: 0.8775\n",
      "Epoch 224/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0148 - pos_accuracy: 0.9675 - val_loss: 0.1044 - val_pos_accuracy: 0.8725\n",
      "Epoch 225/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0147 - pos_accuracy: 0.9706 - val_loss: 0.1049 - val_pos_accuracy: 0.8775\n",
      "Epoch 226/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0149 - pos_accuracy: 0.9669 - val_loss: 0.1033 - val_pos_accuracy: 0.8750\n",
      "Epoch 227/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0150 - pos_accuracy: 0.9737 - val_loss: 0.1061 - val_pos_accuracy: 0.8675\n",
      "Epoch 228/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0148 - pos_accuracy: 0.9694 - val_loss: 0.1047 - val_pos_accuracy: 0.8750\n",
      "Epoch 229/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0146 - pos_accuracy: 0.9669 - val_loss: 0.1041 - val_pos_accuracy: 0.8825\n",
      "Epoch 230/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0146 - pos_accuracy: 0.9700 - val_loss: 0.1035 - val_pos_accuracy: 0.8775\n",
      "Epoch 231/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0149 - pos_accuracy: 0.9681 - val_loss: 0.1045 - val_pos_accuracy: 0.8800\n",
      "Epoch 232/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9669 - val_loss: 0.1034 - val_pos_accuracy: 0.8725\n",
      "Epoch 233/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0145 - pos_accuracy: 0.9669 - val_loss: 0.1035 - val_pos_accuracy: 0.8775\n",
      "Epoch 234/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0145 - pos_accuracy: 0.9706 - val_loss: 0.1066 - val_pos_accuracy: 0.8650\n",
      "Epoch 235/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0149 - pos_accuracy: 0.9700 - val_loss: 0.1039 - val_pos_accuracy: 0.8775\n",
      "Epoch 236/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0147 - pos_accuracy: 0.9675 - val_loss: 0.1036 - val_pos_accuracy: 0.8700\n",
      "Epoch 237/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0142 - pos_accuracy: 0.9737 - val_loss: 0.1062 - val_pos_accuracy: 0.8750\n",
      "Epoch 238/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0143 - pos_accuracy: 0.9694 - val_loss: 0.1033 - val_pos_accuracy: 0.8750\n",
      "Epoch 239/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0144 - pos_accuracy: 0.9712 - val_loss: 0.1045 - val_pos_accuracy: 0.8900\n",
      "Epoch 240/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0134 - pos_accuracy: 0.9844 - val_loss: 0.1060 - val_pos_accuracy: 0.8850\n",
      "Epoch 241/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0105 - pos_accuracy: 0.9887 - val_loss: 0.1026 - val_pos_accuracy: 0.8900\n",
      "Epoch 242/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9925 - val_loss: 0.1003 - val_pos_accuracy: 0.8975\n",
      "Epoch 243/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0080 - pos_accuracy: 0.9919 - val_loss: 0.1002 - val_pos_accuracy: 0.9000\n",
      "Epoch 244/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0071 - pos_accuracy: 0.9950 - val_loss: 0.1000 - val_pos_accuracy: 0.8950\n",
      "Epoch 245/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0066 - pos_accuracy: 0.9950 - val_loss: 0.1019 - val_pos_accuracy: 0.9000\n",
      "Epoch 246/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0066 - pos_accuracy: 0.9956 - val_loss: 0.0994 - val_pos_accuracy: 0.9000\n",
      "Epoch 247/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0062 - pos_accuracy: 0.9950 - val_loss: 0.0984 - val_pos_accuracy: 0.9000\n",
      "Epoch 248/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9950 - val_loss: 0.0991 - val_pos_accuracy: 0.9000\n",
      "Epoch 249/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0059 - pos_accuracy: 0.9956 - val_loss: 0.0990 - val_pos_accuracy: 0.9000\n",
      "Epoch 250/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9956 - val_loss: 0.0991 - val_pos_accuracy: 0.9000\n",
      "Epoch 251/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9956 - val_loss: 0.0988 - val_pos_accuracy: 0.9000\n",
      "Epoch 252/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9950 - val_loss: 0.0986 - val_pos_accuracy: 0.8975\n",
      "Epoch 253/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9950 - val_loss: 0.0979 - val_pos_accuracy: 0.9075\n",
      "Epoch 254/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9956 - val_loss: 0.0991 - val_pos_accuracy: 0.9025\n",
      "Epoch 255/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9956 - val_loss: 0.0983 - val_pos_accuracy: 0.9000\n",
      "Epoch 256/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9950 - val_loss: 0.0989 - val_pos_accuracy: 0.9050\n",
      "Epoch 257/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9950 - val_loss: 0.0983 - val_pos_accuracy: 0.9000\n",
      "Epoch 258/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9950 - val_loss: 0.0977 - val_pos_accuracy: 0.9000\n",
      "Epoch 259/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9950 - val_loss: 0.0974 - val_pos_accuracy: 0.9025\n",
      "Epoch 260/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9950 - val_loss: 0.0978 - val_pos_accuracy: 0.9025\n",
      "Epoch 261/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9950 - val_loss: 0.0989 - val_pos_accuracy: 0.9025\n",
      "Epoch 262/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9950 - val_loss: 0.0973 - val_pos_accuracy: 0.9025\n",
      "Epoch 263/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9950 - val_loss: 0.0981 - val_pos_accuracy: 0.9000\n",
      "Epoch 264/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9950 - val_loss: 0.0976 - val_pos_accuracy: 0.9000\n",
      "Epoch 265/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9950 - val_loss: 0.0975 - val_pos_accuracy: 0.9025\n",
      "Epoch 266/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9950 - val_loss: 0.0980 - val_pos_accuracy: 0.9025\n",
      "Epoch 267/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9950 - val_loss: 0.0978 - val_pos_accuracy: 0.9050\n",
      "Epoch 268/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9950 - val_loss: 0.0983 - val_pos_accuracy: 0.9025\n",
      "Epoch 269/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0044 - pos_accuracy: 0.9950 - val_loss: 0.0975 - val_pos_accuracy: 0.9025\n",
      "Epoch 270/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9950 - val_loss: 0.0977 - val_pos_accuracy: 0.9000\n",
      "Epoch 271/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9950 - val_loss: 0.0971 - val_pos_accuracy: 0.9050\n",
      "Epoch 272/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9950 - val_loss: 0.0968 - val_pos_accuracy: 0.9025\n",
      "Epoch 273/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9950 - val_loss: 0.0967 - val_pos_accuracy: 0.9050\n",
      "Epoch 274/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9950 - val_loss: 0.0971 - val_pos_accuracy: 0.9025\n",
      "Epoch 275/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9950 - val_loss: 0.0970 - val_pos_accuracy: 0.9050\n",
      "Epoch 276/345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9950 - val_loss: 0.0964 - val_pos_accuracy: 0.9050\n",
      "Epoch 277/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0042 - pos_accuracy: 0.9950 - val_loss: 0.0963 - val_pos_accuracy: 0.9025\n",
      "Epoch 278/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9950 - val_loss: 0.0962 - val_pos_accuracy: 0.9025\n",
      "Epoch 279/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9950 - val_loss: 0.0968 - val_pos_accuracy: 0.9000\n",
      "Epoch 280/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9950 - val_loss: 0.0958 - val_pos_accuracy: 0.9050\n",
      "Epoch 281/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9950 - val_loss: 0.0973 - val_pos_accuracy: 0.9025\n",
      "Epoch 282/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9950 - val_loss: 0.0961 - val_pos_accuracy: 0.9050\n",
      "Epoch 283/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9950 - val_loss: 0.1015 - val_pos_accuracy: 0.9100\n",
      "Epoch 284/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0040 - pos_accuracy: 0.9950 - val_loss: 0.0961 - val_pos_accuracy: 0.9025\n",
      "Epoch 285/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9950 - val_loss: 0.0960 - val_pos_accuracy: 0.9050\n",
      "Epoch 286/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9950 - val_loss: 0.0964 - val_pos_accuracy: 0.9025\n",
      "Epoch 287/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9950 - val_loss: 0.0964 - val_pos_accuracy: 0.9025\n",
      "Epoch 288/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0039 - pos_accuracy: 0.9950 - val_loss: 0.0962 - val_pos_accuracy: 0.9025\n",
      "Epoch 289/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9950 - val_loss: 0.0971 - val_pos_accuracy: 0.9050\n",
      "Epoch 290/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9950 - val_loss: 0.0956 - val_pos_accuracy: 0.9050\n",
      "Epoch 291/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9950 - val_loss: 0.0959 - val_pos_accuracy: 0.9050\n",
      "Epoch 292/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9950 - val_loss: 0.0955 - val_pos_accuracy: 0.9025\n",
      "Epoch 293/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0967 - val_pos_accuracy: 0.9000\n",
      "Epoch 294/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0983 - val_pos_accuracy: 0.9050\n",
      "Epoch 295/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0954 - val_pos_accuracy: 0.9025\n",
      "Epoch 296/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0960 - val_pos_accuracy: 0.9050\n",
      "Epoch 297/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0960 - val_pos_accuracy: 0.9050\n",
      "Epoch 298/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0958 - val_pos_accuracy: 0.9000\n",
      "Epoch 299/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0957 - val_pos_accuracy: 0.9050\n",
      "Epoch 300/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9950 - val_loss: 0.0974 - val_pos_accuracy: 0.8975\n",
      "Epoch 301/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9950 - val_loss: 0.0956 - val_pos_accuracy: 0.9050\n",
      "Epoch 302/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9950 - val_loss: 0.0952 - val_pos_accuracy: 0.9025\n",
      "Epoch 303/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9950 - val_loss: 0.0957 - val_pos_accuracy: 0.9075\n",
      "Epoch 304/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9950 - val_loss: 0.0958 - val_pos_accuracy: 0.9025\n",
      "Epoch 305/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9950 - val_loss: 0.0959 - val_pos_accuracy: 0.9025\n",
      "Epoch 306/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9950 - val_loss: 0.0958 - val_pos_accuracy: 0.9025\n",
      "Epoch 307/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0035 - pos_accuracy: 0.9950 - val_loss: 0.0959 - val_pos_accuracy: 0.9025\n",
      "Epoch 308/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9950 - val_loss: 0.0957 - val_pos_accuracy: 0.9050\n",
      "Epoch 309/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9950 - val_loss: 0.0956 - val_pos_accuracy: 0.9025\n",
      "Epoch 310/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9950 - val_loss: 0.0958 - val_pos_accuracy: 0.9025\n",
      "Epoch 311/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0034 - pos_accuracy: 0.9950 - val_loss: 0.0957 - val_pos_accuracy: 0.9000\n",
      "Epoch 312/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9944 - val_loss: 0.0964 - val_pos_accuracy: 0.9025\n",
      "Epoch 313/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9950 - val_loss: 0.0955 - val_pos_accuracy: 0.9050\n",
      "Epoch 314/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9950 - val_loss: 0.0951 - val_pos_accuracy: 0.9050\n",
      "Epoch 315/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9944 - val_loss: 0.0957 - val_pos_accuracy: 0.9025\n",
      "Epoch 316/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9944 - val_loss: 0.0957 - val_pos_accuracy: 0.9000\n",
      "Epoch 317/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9944 - val_loss: 0.0955 - val_pos_accuracy: 0.9000\n",
      "Epoch 318/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9950 - val_loss: 0.0958 - val_pos_accuracy: 0.9050\n",
      "Epoch 319/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9950 - val_loss: 0.0956 - val_pos_accuracy: 0.9000\n",
      "Epoch 320/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9950 - val_loss: 0.0958 - val_pos_accuracy: 0.9025\n",
      "Epoch 321/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9944 - val_loss: 0.0955 - val_pos_accuracy: 0.9000\n",
      "Epoch 322/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9950 - val_loss: 0.0973 - val_pos_accuracy: 0.9050\n",
      "Epoch 323/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9950 - val_loss: 0.0957 - val_pos_accuracy: 0.9025\n",
      "Epoch 324/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9944 - val_loss: 0.0952 - val_pos_accuracy: 0.9000\n",
      "Epoch 325/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9950 - val_loss: 0.0954 - val_pos_accuracy: 0.9025\n",
      "Epoch 326/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0033 - pos_accuracy: 0.9944 - val_loss: 0.0952 - val_pos_accuracy: 0.9050\n",
      "Epoch 327/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9950 - val_loss: 0.0951 - val_pos_accuracy: 0.9025\n",
      "Epoch 328/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9944 - val_loss: 0.0953 - val_pos_accuracy: 0.9025\n",
      "Epoch 329/345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0032 - pos_accuracy: 0.9944 - val_loss: 0.0950 - val_pos_accuracy: 0.9025\n",
      "Epoch 330/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9944 - val_loss: 0.0954 - val_pos_accuracy: 0.9000\n",
      "Epoch 331/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9944 - val_loss: 0.0951 - val_pos_accuracy: 0.9025\n",
      "Epoch 332/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9944 - val_loss: 0.0952 - val_pos_accuracy: 0.9000\n",
      "Epoch 333/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0031 - pos_accuracy: 0.9944 - val_loss: 0.0954 - val_pos_accuracy: 0.9025\n",
      "Epoch 334/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9950 - val_loss: 0.0959 - val_pos_accuracy: 0.9100\n",
      "Epoch 335/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9950 - val_loss: 0.0953 - val_pos_accuracy: 0.9050\n",
      "Epoch 336/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9950 - val_loss: 0.0952 - val_pos_accuracy: 0.9075\n",
      "Epoch 337/345\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0032 - pos_accuracy: 0.9950 - val_loss: 0.0951 - val_pos_accuracy: 0.9050\n",
      "Epoch 338/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9950 - val_loss: 0.0948 - val_pos_accuracy: 0.9025\n",
      "Epoch 339/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9950 - val_loss: 0.0953 - val_pos_accuracy: 0.9050\n",
      "Epoch 340/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9950 - val_loss: 0.0951 - val_pos_accuracy: 0.9025\n",
      "Epoch 341/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9950 - val_loss: 0.0950 - val_pos_accuracy: 0.9050\n",
      "Epoch 342/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9950 - val_loss: 0.0951 - val_pos_accuracy: 0.9025\n",
      "Epoch 343/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9950 - val_loss: 0.0959 - val_pos_accuracy: 0.9050\n",
      "Epoch 344/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9950 - val_loss: 0.0950 - val_pos_accuracy: 0.9025\n",
      "Epoch 345/345\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9950 - val_loss: 0.0947 - val_pos_accuracy: 0.9100\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:55:11.226844: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs2.shape = (None, 80, 80, 3)\n",
      "outputs2.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average2 = 0.4139433551198257\n",
      "convolution result = 0.41394338\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1747: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할은 지정된 축에서 여러 배열을 연결하여 단일 배열로 만드는 것입니다.\\naxis=1 은 열이 증가하는 방식으로 연결이 되어 가로로 배열이 늘어나게 됩니다. .\\n\",\n",
      "        \"\\nsubplot에서 121는 1X2 그리드에서 첫번째, 122는 1X2 그리드에서 두번째 subplot을 의미합니다. \\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntf_reduce_all은 and 연산을 하기 위함이며, axis=1은 axis=0과는 직각 방향으로 열 기준으로 연산하기 위함입니다. \\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\ntf.reduce_mean은 평균값을 구합니다. \\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n최종 출력층의 경우 값을 출력 해야 하므로 activation=None으로 설정하여야 합니다. \\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\nrandom seed를 사용하는 이유는 같은 조건에서 테스트 할 경우 항상 같은 결과를 얻기 위해서 입니다. \\n\",\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n코딩은 완벽히 이해허고 실행해야 된다는 점에서 쉽지 않은 것 같습니다. \\n새로운 것을 한다는 재미도 있지만, 처음에는 벽이 너무 높게 느껴지는 것 같습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 66.0,\n",
      "    \"accuracy\": 0.91\n",
      "}\"report1/이민기_46093.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이민기_46093.ipynb to python\n",
      "[NbConvertApp] Writing 26929 bytes to report1/이민기_46093.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:208: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:240: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 286408.14it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:55:17.574615: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:55:17.957493: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:55:17.957547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:55:18.181981: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:55:18.786720: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:55:20.438262: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans01 error\n",
      "ans02 error\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1295: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  res[0] = (tmp == ans01.astype('float32')).all()\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1319: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1340: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 13.454545454545453,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/안다솜_46061.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/안다솜_46061.ipynb to python\n",
      "[NbConvertApp] Writing 37929 bytes to report1/안다솜_46061.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:213: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:246: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "(2, 3)\n",
      "[[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]]\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -25. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 302728.55it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:55:26.905061: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:55:27.292354: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:55:27.292393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:55:27.517919: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:55:28.150961: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 116025.01it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                396       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 25,542\n",
      "Trainable params: 25,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/41\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 34.6741 - pos_accuracy: 0.0306 - val_loss: 3.9847 - val_pos_accuracy: 0.0893\n",
      "Epoch 2/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2406 - pos_accuracy: 0.1444 - val_loss: 1.2973 - val_pos_accuracy: 0.2143\n",
      "Epoch 3/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1251 - pos_accuracy: 0.2031 - val_loss: 0.8902 - val_pos_accuracy: 0.3326\n",
      "Epoch 4/41\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.8570 - pos_accuracy: 0.2612 - val_loss: 1.1052 - val_pos_accuracy: 0.2433\n",
      "Epoch 5/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6565 - pos_accuracy: 0.3319 - val_loss: 0.5506 - val_pos_accuracy: 0.5000\n",
      "Epoch 6/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4361 - pos_accuracy: 0.4725 - val_loss: 0.4642 - val_pos_accuracy: 0.5312\n",
      "Epoch 7/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3886 - pos_accuracy: 0.4900 - val_loss: 0.3912 - val_pos_accuracy: 0.5826\n",
      "Epoch 8/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2827 - pos_accuracy: 0.5850 - val_loss: 0.3532 - val_pos_accuracy: 0.5871\n",
      "Epoch 9/41\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2626 - pos_accuracy: 0.6000 - val_loss: 0.3260 - val_pos_accuracy: 0.6049\n",
      "Epoch 10/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3614 - pos_accuracy: 0.4325 - val_loss: 0.4551 - val_pos_accuracy: 0.3259\n",
      "Epoch 11/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2605 - pos_accuracy: 0.5656 - val_loss: 0.2521 - val_pos_accuracy: 0.7165\n",
      "Epoch 12/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1551 - pos_accuracy: 0.7631 - val_loss: 0.2172 - val_pos_accuracy: 0.7589\n",
      "Epoch 13/41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1908 - pos_accuracy: 0.6650 - val_loss: 0.2045 - val_pos_accuracy: 0.7567\n",
      "Epoch 14/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1162 - pos_accuracy: 0.8288 - val_loss: 0.1829 - val_pos_accuracy: 0.7857\n",
      "Epoch 15/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1051 - pos_accuracy: 0.8375 - val_loss: 0.1995 - val_pos_accuracy: 0.7522\n",
      "Epoch 16/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0989 - pos_accuracy: 0.8413 - val_loss: 0.1822 - val_pos_accuracy: 0.7835\n",
      "Epoch 17/41\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0861 - pos_accuracy: 0.8650 - val_loss: 0.1466 - val_pos_accuracy: 0.8013\n",
      "Epoch 18/41\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0819 - pos_accuracy: 0.8681 - val_loss: 0.1483 - val_pos_accuracy: 0.8214\n",
      "Epoch 19/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0768 - pos_accuracy: 0.8875 - val_loss: 0.1326 - val_pos_accuracy: 0.8192\n",
      "Epoch 20/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0694 - pos_accuracy: 0.8931 - val_loss: 0.1295 - val_pos_accuracy: 0.8371\n",
      "Epoch 21/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0946 - pos_accuracy: 0.8388 - val_loss: 0.2569 - val_pos_accuracy: 0.5491\n",
      "Epoch 22/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0910 - pos_accuracy: 0.8550 - val_loss: 0.1196 - val_pos_accuracy: 0.8661\n",
      "Epoch 23/41\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0629 - pos_accuracy: 0.9225 - val_loss: 0.1165 - val_pos_accuracy: 0.8772\n",
      "Epoch 24/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0788 - pos_accuracy: 0.8994 - val_loss: 0.1043 - val_pos_accuracy: 0.8750\n",
      "Epoch 25/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0472 - pos_accuracy: 0.9394 - val_loss: 0.1003 - val_pos_accuracy: 0.8884\n",
      "Epoch 26/41\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0479 - pos_accuracy: 0.9406 - val_loss: 0.1135 - val_pos_accuracy: 0.8482\n",
      "Epoch 27/41\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0446 - pos_accuracy: 0.9438 - val_loss: 0.0920 - val_pos_accuracy: 0.8973\n",
      "Epoch 28/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0456 - pos_accuracy: 0.9419 - val_loss: 0.1267 - val_pos_accuracy: 0.8147\n",
      "Epoch 29/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0459 - pos_accuracy: 0.9438 - val_loss: 0.0877 - val_pos_accuracy: 0.8996\n",
      "Epoch 30/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0365 - pos_accuracy: 0.9500 - val_loss: 0.0895 - val_pos_accuracy: 0.9107\n",
      "Epoch 31/41\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0371 - pos_accuracy: 0.9544 - val_loss: 0.0816 - val_pos_accuracy: 0.9018\n",
      "Epoch 32/41\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0324 - pos_accuracy: 0.9544 - val_loss: 0.0790 - val_pos_accuracy: 0.9085\n",
      "Epoch 33/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0319 - pos_accuracy: 0.9569 - val_loss: 0.0737 - val_pos_accuracy: 0.9196\n",
      "Epoch 34/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0279 - pos_accuracy: 0.9600 - val_loss: 0.0731 - val_pos_accuracy: 0.9286\n",
      "Epoch 35/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0308 - pos_accuracy: 0.9581 - val_loss: 0.0893 - val_pos_accuracy: 0.9129\n",
      "Epoch 36/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0366 - pos_accuracy: 0.9525 - val_loss: 0.0794 - val_pos_accuracy: 0.9263\n",
      "Epoch 37/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1478 - pos_accuracy: 0.7569 - val_loss: 0.1131 - val_pos_accuracy: 0.8661\n",
      "Epoch 38/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0404 - pos_accuracy: 0.9388 - val_loss: 0.0704 - val_pos_accuracy: 0.9174\n",
      "Epoch 39/41\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0284 - pos_accuracy: 0.9644 - val_loss: 0.0707 - val_pos_accuracy: 0.9107\n",
      "Epoch 40/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0278 - pos_accuracy: 0.9638 - val_loss: 0.0668 - val_pos_accuracy: 0.9308\n",
      "Epoch 41/41\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0222 - pos_accuracy: 0.9719 - val_loss: 0.0637 - val_pos_accuracy: 0.9464\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:55:34.805447: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "extract1 = [[0.79607843 0.79215686 0.79215686]\n",
      " [0.8        0.79607843 0.78823529]\n",
      " [0.8        0.79607843 0.79607843]]\n",
      "extract2 = [[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "9 pixles average = 0.7952069716775599\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result 1 = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1765: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\nnp.concatenate 함수는 축(axis)에 따라 배열을 결합하하는 역할을 합니다.\\naxis는 축(차원)으로 axis=1 일 경우, 2번째 축(수평)으로 결합됩니다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\nsubplot 함수는 여러 개의 그래프를 하나의 그림으로 나타낼 수 있도록 하는 함수입니다.\\nsubplot에서 첫번째 정수는 표시될 그래프 배열의 행의 수, 두번째 정수는 열의 수, 세번째 정수는 인덱스를 나타냅니다.\\n즉 subplot에서 121의 의미는 1개의 행과 2개의 열, 즉 수평으로 두개의 그래프가 배열되고 그 중 첫 번째 위치에 해당 그래프가 표시된다는 의미입니다.\\n122는 배열에서 2번째로 그래프가 배열된다는 의미입니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all은 데이터가 조건에 맞으면 True, 그렇지 않으면 False를 표시하며 \\n이 코드에서는 예측값과 실제 값의 일치 여부를 판단하고 그 결과값을 True, False로 보여줍니다.\\naxis=1는 차원을 축소하지 않는 것을 의미하며 개별 값의 합집합에 대한 연산이 필요하기 때문에 별도 차원 축소를 진행하지 않습니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 변수가 의미하는 배열 전체 원소의 합을 원소 개수로 나누어 평균을 산출해주며, \\n이 코드에서는 정탐은 1, 오탐이면 0으로 변형되어 있는 상태라 이 값의 평균은 곧 모델의 정확도를 의미합니다.\\n\",\n",
      "        \"\\n최종 출력값이 x, y 의 선형함수로 출력되어야하기 때문에 별도의 활성화 함수가 사용되지 않았습니다.\\n\\n\",\n",
      "        \"\\n난수의 생성 패턴을 동일하게 관리하여 난수의 특성이 알고리즘의 성능에 미치는 영향을 제어하기 위함입니다.\\n\",\n",
      "        true,\n",
      "        \"\\n모델의 정확성을 높일 수 있는 방법이라던가 세부적인 함수의 의미에 대한 부연 설명 등 강의를 더 잘 이해할 수 있도록 하는 자료가 제공되면 좋을 것 같습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.942\n",
      "}\"report1/공선호_46039.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/공선호_46039.ipynb to python\n",
      "[NbConvertApp] Writing 36267 bytes to report1/공선호_46039.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 297405.09it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:55:42.247751: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:55:42.630572: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:55:42.630612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:55:42.852993: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:55:43.455067: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 164912.58it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 434,882\n",
      "Trainable params: 434,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(2, 2)\n",
      "(2,)\n",
      "[ True False]\n",
      "[1. 0.]\n",
      "0.5\n",
      "Epoch 1/19\n",
      "100/100 [==============================] - 1s 4ms/step - loss: 36.6668 - pos_accuracy: 0.0431 - val_loss: 1.5715 - val_pos_accuracy: 0.2075\n",
      "Epoch 2/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 2.0259 - pos_accuracy: 0.1975 - val_loss: 1.0317 - val_pos_accuracy: 0.2850\n",
      "Epoch 3/19\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 1.1081 - pos_accuracy: 0.3100 - val_loss: 0.4890 - val_pos_accuracy: 0.4625\n",
      "Epoch 4/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.3316 - pos_accuracy: 0.5981 - val_loss: 0.5961 - val_pos_accuracy: 0.4200\n",
      "Epoch 5/19\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3077 - pos_accuracy: 0.6231 - val_loss: 0.1770 - val_pos_accuracy: 0.7700\n",
      "Epoch 6/19\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1344 - pos_accuracy: 0.8031 - val_loss: 0.1494 - val_pos_accuracy: 0.8300\n",
      "Epoch 7/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0891 - pos_accuracy: 0.8656 - val_loss: 0.1212 - val_pos_accuracy: 0.8800\n",
      "Epoch 8/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0639 - pos_accuracy: 0.9200 - val_loss: 0.1038 - val_pos_accuracy: 0.8925\n",
      "Epoch 9/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0509 - pos_accuracy: 0.9400 - val_loss: 0.1319 - val_pos_accuracy: 0.8275\n",
      "Epoch 10/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0354 - pos_accuracy: 0.9588 - val_loss: 0.0823 - val_pos_accuracy: 0.9225\n",
      "Epoch 11/19\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0247 - pos_accuracy: 0.9775 - val_loss: 0.0787 - val_pos_accuracy: 0.9275\n",
      "Epoch 12/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0238 - pos_accuracy: 0.9812 - val_loss: 0.0696 - val_pos_accuracy: 0.9325\n",
      "Epoch 13/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0173 - pos_accuracy: 0.9894 - val_loss: 0.0640 - val_pos_accuracy: 0.9450\n",
      "Epoch 14/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0160 - pos_accuracy: 0.9912 - val_loss: 0.0589 - val_pos_accuracy: 0.9425\n",
      "Epoch 15/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0102 - pos_accuracy: 0.9925 - val_loss: 0.0657 - val_pos_accuracy: 0.9275\n",
      "Epoch 16/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9931 - val_loss: 0.0574 - val_pos_accuracy: 0.9400\n",
      "Epoch 17/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9937 - val_loss: 0.0506 - val_pos_accuracy: 0.9475\n",
      "Epoch 18/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0073 - pos_accuracy: 0.9937 - val_loss: 0.0505 - val_pos_accuracy: 0.9475\n",
      "Epoch 19/19\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9950 - val_loss: 0.0490 - val_pos_accuracy: 0.9500\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:55:52.375147: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1701: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nconcatenate 함수는 numpy 배열을 하나로 만드는 역할로\\naxis=1 일 경우 2차원 배열의 열방향으로 더해 하나의 배열로 만든다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n첫번째 숫자는 plot 의 행, 두번째 숫자는 plot의 열, 세번째 숫자는 plot 의 번호를 의미한다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\n지정한 축 방향 각 요소와 and 연산을 실시한다.\\naxis=1 이므로 배열의 같은 행 데이터를 모두 더하여 벡터로 변환한다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n\\n지정한 축 방향 모든 요소의 평균값을 계산하는 함수로 \\n여기서는 각 데이터의 is_corret 값의 평균을 계산하여 모델의 정확도를 평가할 수 있다.\\n\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n\\n회귀 모델의 경우 Linear 한 결과 값을 보여야하므로 일반적으로 활성함수를 사용하지 않는다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유\\n\\nrandom seed 를 사용하지 않을 경우 난수가 실행 시 마다 다르게 생성되므로 데이터에 변화가 생긴다.\\n따라서 모델의 정확도를 평가하기에 적합하지 않으므로 재현 가능한 결과를 도출할 수 있도록 random seed 를 사용한다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 90.0,\n",
      "    \"accuracy\": 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\"report1/김홍섭_46007.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김홍섭_46007.ipynb to python\n",
      "[NbConvertApp] Writing 34981 bytes to report1/김홍섭_46007.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:209: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 303429.36it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:55:59.714672: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:56:00.095605: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:56:00.095643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:56:00.320139: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:56:00.974960: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - pos_accuracy: 0.0000e+00 - val_loss: 212.3778 - val_pos_accuracy: 0.0045\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - pos_accuracy: 6.2500e-04 - val_loss: 195.9663 - val_pos_accuracy: 0.0000e+00\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 93010.40it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 66.8278 - pos_accuracy: 0.0144 - val_loss: 9.9763 - val_pos_accuracy: 0.0409\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 2.4878 - pos_accuracy: 0.1187 - val_loss: 0.9676 - val_pos_accuracy: 0.2548\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.7325 - pos_accuracy: 0.2594 - val_loss: 0.7776 - val_pos_accuracy: 0.3245\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3582 - pos_accuracy: 0.5094 - val_loss: 0.3093 - val_pos_accuracy: 0.6106\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2714 - pos_accuracy: 0.5888 - val_loss: 0.2684 - val_pos_accuracy: 0.6635\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2105 - pos_accuracy: 0.6737 - val_loss: 0.2585 - val_pos_accuracy: 0.6659\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2871 - pos_accuracy: 0.6631 - val_loss: 0.1939 - val_pos_accuracy: 0.7812\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1280 - pos_accuracy: 0.7875 - val_loss: 0.2542 - val_pos_accuracy: 0.5721\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0913 - pos_accuracy: 0.8475 - val_loss: 0.1282 - val_pos_accuracy: 0.8582\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0872 - pos_accuracy: 0.8462 - val_loss: 0.1261 - val_pos_accuracy: 0.8702\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0528 - pos_accuracy: 0.9369 - val_loss: 0.1229 - val_pos_accuracy: 0.8702\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0531 - pos_accuracy: 0.9388 - val_loss: 0.1009 - val_pos_accuracy: 0.8750\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0496 - pos_accuracy: 0.9312 - val_loss: 0.1097 - val_pos_accuracy: 0.8894\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0410 - pos_accuracy: 0.9544 - val_loss: 0.1121 - val_pos_accuracy: 0.8870\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0338 - pos_accuracy: 0.9675 - val_loss: 0.0938 - val_pos_accuracy: 0.9183\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0277 - pos_accuracy: 0.9756 - val_loss: 0.0912 - val_pos_accuracy: 0.8822\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0261 - pos_accuracy: 0.9794 - val_loss: 0.0790 - val_pos_accuracy: 0.9159\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0208 - pos_accuracy: 0.9794 - val_loss: 0.0783 - val_pos_accuracy: 0.9183\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0233 - pos_accuracy: 0.9800 - val_loss: 0.0814 - val_pos_accuracy: 0.9038\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0238 - pos_accuracy: 0.9806 - val_loss: 0.0900 - val_pos_accuracy: 0.8894\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0200 - pos_accuracy: 0.9812 - val_loss: 0.0785 - val_pos_accuracy: 0.9303\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0162 - pos_accuracy: 0.9887 - val_loss: 0.0701 - val_pos_accuracy: 0.9375\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0133 - pos_accuracy: 0.9900 - val_loss: 0.0645 - val_pos_accuracy: 0.9303\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0117 - pos_accuracy: 0.9912 - val_loss: 0.0598 - val_pos_accuracy: 0.9351\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0127 - pos_accuracy: 0.9900 - val_loss: 0.0640 - val_pos_accuracy: 0.9351\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0120 - pos_accuracy: 0.9931 - val_loss: 0.0656 - val_pos_accuracy: 0.9399\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0102 - pos_accuracy: 0.9931 - val_loss: 0.0699 - val_pos_accuracy: 0.9159\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0093 - pos_accuracy: 0.9931 - val_loss: 0.0604 - val_pos_accuracy: 0.9351\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0075 - pos_accuracy: 0.9944 - val_loss: 0.0582 - val_pos_accuracy: 0.9375\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0070 - pos_accuracy: 0.9937 - val_loss: 0.0625 - val_pos_accuracy: 0.9447\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9950 - val_loss: 0.0560 - val_pos_accuracy: 0.9399\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0066 - pos_accuracy: 0.9956 - val_loss: 0.0586 - val_pos_accuracy: 0.9447\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9950 - val_loss: 0.0567 - val_pos_accuracy: 0.9423\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0059 - pos_accuracy: 0.9950 - val_loss: 0.0548 - val_pos_accuracy: 0.9399\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9956 - val_loss: 0.0534 - val_pos_accuracy: 0.9423\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9962 - val_loss: 0.0533 - val_pos_accuracy: 0.9447\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9956 - val_loss: 0.0536 - val_pos_accuracy: 0.9399\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9962 - val_loss: 0.0517 - val_pos_accuracy: 0.9375\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9975 - val_loss: 0.0520 - val_pos_accuracy: 0.9351\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9975 - val_loss: 0.0524 - val_pos_accuracy: 0.9375\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9981 - val_loss: 0.0512 - val_pos_accuracy: 0.9351\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.0518 - val_pos_accuracy: 0.9447\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.0489 - val_pos_accuracy: 0.9471\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9975 - val_loss: 0.0493 - val_pos_accuracy: 0.9399\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.0503 - val_pos_accuracy: 0.9351\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9981 - val_loss: 0.0485 - val_pos_accuracy: 0.9399\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.0494 - val_pos_accuracy: 0.9423\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9987 - val_loss: 0.0504 - val_pos_accuracy: 0.9543\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.0492 - val_pos_accuracy: 0.9423\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9981 - val_loss: 0.0491 - val_pos_accuracy: 0.9495\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:56:12.663929: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1663: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nconcatenate함수는 배열들을 하나로 합치는데 사용한다.\\naxis=0은 합치는 기준을 배열의 가장 높은차원이고,\\naxis=1은 배열의 두번째 차원을 말하는것이다.\\n\",\n",
      "        \"\\n121 - 행,열,인덱스 순으로 되어있습니다. \\n121 - 그래프를 1행에 2열로 만들고 첫번째 이미지를 출력한다.\\n122 - 그래프를 1행에 2열로 만들고 두번째 이미지를 출력한다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\n예측값과 참값이 같고, y축 인것만 맞은것으로 인정. \\n\",\n",
      "        \"\\n예측값이 정답과 맞았을때의 평균값을 정확도로 정함\\n\",\n",
      "        \"\\n출력결과를 선형으로 표시하고자 함.\\n\",\n",
      "        \"\\n시드 재설정을 매번 사용할 때마다 동일한 숫자 세트가 나타나게 하기위함.\\n\",\n",
      "        true,\n",
      "        \"\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.9519\n",
      "}\"report1/한성우_46032.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/한성우_46032.ipynb to python\n",
      "[NbConvertApp] Writing 35977 bytes to report1/한성우_46032.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:213: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:250: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "float32의 경우 52번 반복할 시 무한대 행렬 값 형성.\n",
      "float64의 경우 422번 반복할 시 무한대 행렬 값 형성.\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "ans05 : \n",
      " [[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[   2.    0. -100.]\n",
      " [   0.    2. -100.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 310459.22it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:56:19.063489: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:56:19.443037: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:56:19.443095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:56:19.664800: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:56:20.268166: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 107965.66it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 25,682\n",
      "Trainable params: 25,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 19.7374 - pos_accuracy: 0.0644 - val_loss: 1.6563 - val_pos_accuracy: 0.1750\n",
      "Epoch 2/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.9248 - pos_accuracy: 0.2850 - val_loss: 0.6672 - val_pos_accuracy: 0.3875\n",
      "Epoch 3/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.3815 - pos_accuracy: 0.5306 - val_loss: 0.3751 - val_pos_accuracy: 0.5750\n",
      "Epoch 4/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2247 - pos_accuracy: 0.6837 - val_loss: 0.3012 - val_pos_accuracy: 0.6575\n",
      "Epoch 5/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1572 - pos_accuracy: 0.7487 - val_loss: 0.2419 - val_pos_accuracy: 0.7250\n",
      "Epoch 6/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1187 - pos_accuracy: 0.8125 - val_loss: 0.2168 - val_pos_accuracy: 0.7650\n",
      "Epoch 7/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0900 - pos_accuracy: 0.8619 - val_loss: 0.1796 - val_pos_accuracy: 0.8375\n",
      "Epoch 8/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0704 - pos_accuracy: 0.9044 - val_loss: 0.1682 - val_pos_accuracy: 0.8500\n",
      "Epoch 9/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0561 - pos_accuracy: 0.9381 - val_loss: 0.1455 - val_pos_accuracy: 0.8725\n",
      "Epoch 10/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0458 - pos_accuracy: 0.9544 - val_loss: 0.1395 - val_pos_accuracy: 0.8850\n",
      "Epoch 11/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0385 - pos_accuracy: 0.9650 - val_loss: 0.1364 - val_pos_accuracy: 0.8950\n",
      "Epoch 12/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0336 - pos_accuracy: 0.9725 - val_loss: 0.1243 - val_pos_accuracy: 0.9100\n",
      "Epoch 13/13\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.0291 - pos_accuracy: 0.9756 - val_loss: 0.1148 - val_pos_accuracy: 0.9250\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:57:27.937438: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1681: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\nnp.concatenate는 Numpy 배열(행렬)을 추가(합치는) 하는 역할이다.\\naxis = 0 은 여러 배열을 추가할 때 열의 형태를 파악하고 행의 형태로 합친다는 명령이다.\\n(원본 데이터, 합치려는 배열이 행 개수가 0일 때에는 합쳐진 배열의 형태가 list의 extend 명령으로 합쳐진 형태이다.)\\n(원본 데이터, 합치려는 배열이 행 개수가 1 이상이고 합치려는 배열의 열의 개수가 동일할 때.)\\naxis = 1 은 여러 배열을 추가할 때 행의 형태를 파악하고 열의 형태로 합친다는 명령이다.\\n(원본 데이터, 합치려는 배열이 행 개수가 0일 때에는 오류가 발생한다.)\\n(원본 데이터, 합치려는 배열이 행 개수가 1 이상이고 합치려는 배열의 행의 개수가 동일할 때.)\\n\",\n",
      "        \"\\nplt.subplot(121)은 1x2 그리드에 첫 번째 이미지를 의미한다.\\nplt.subplot(121)은 1x2 그리드에 두 번째 이미지를 의미한다.\\nplt.subplot(nrow(행 개수),ncolumn(열 개수),index(표시할 그림 위치)으로 3개의 정수로 입력받는다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 함수의 역할은 Numpy 배열(행렬)에서 AND 조건으로 BOOL 값을 추출하는 함수이다.\\naxis = 0은 열의 형태를 파악하고 BOOL 값을 추출하는 함수이다.\\naxis = 1은 행의 형태를 파악하고 BOOL 값을 추출하는 함수이다.\\n(행 개수가 0일 때에는 오류가 발생한다.)\\n\",\n",
      "        \"\\ntf.reduce_mean의 함수의 역할은 Numpy 배열(행렬)에서 평균 값을 추출하는 함수이다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유는 출력층에서는 Class(타깃)에 대한 정보가 담긴 데이터 또는 정확도가 출력되는데\\n그 데이터에 활성화 함수를 적용시키면 활성화 함수의 계산 식에 의하여 타깃 정답에 대한 데이터 또는 정확도를 흐려버리게 되기 때문에\\n최종 출력층에는 활성화 함수를 사용하지 않는다.\\n\",\n",
      "        \"\\nrandom seed 함수의 사용 이유는 \\n매번 같은 데이터로 훈련하거나 같은 가중치, 절편으로 뉴런에 적용시킨다면\\n결과가 같거나 학습률이 좋게 나오지 못하여서 다양한 데이터와 더 정확한 학습을 위하여 random seed 함수로 난수를 적용한다.\\n이와 같이 random seed 함수는 난수를 추출하여 더 다양한 데이터를 입력하거나 결괏값으로 받기 위해 사용된다.  \\n\",\n",
      "        true,\n",
      "        \"\\n교수님께서 이번에 출제하신 과제는 너무나도 예리하게 학생들에게 필요한 핵심과\\n무엇을 공부해야 하는지, 앞으로 가야 할 길에 무엇을 준비해야 하는지를 골라 문제를 잘 내주신 것 같습니다.\\n이런 것을 보아 학생들을 걱정하고 사랑하는 교수님의 마음이 보였습니다.\\n문제를 풀면서 조금만 더 일찍 2학기가 개강을 하여서 교수님 강의를 들을 수 있었다면\\n미리 학습한 지금의 제 자신까지 지름길로 도착하지 않았을까라는 가질 수 없는 작은 욕심과 아쉬움이 생겼었습니다.\\n좋은 강의를 가르쳐 주시는 이홍섭 교수님께 감사드리고 저희 고려 사이버 대학교 교수님들께도 감사드립니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 96.0,\n",
      "    \"accuracy\": 0.925\n",
      "}\"report1/소재훈_46082.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/소재훈_46082.ipynb to python\n",
      "[NbConvertApp] Writing 34931 bytes to report1/소재훈_46082.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[[5 7 9]\n",
      " [6 8 0]]\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -23. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 294719.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:57:35.274704: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:57:35.650638: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:57:35.650679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:57:35.881853: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:57:36.480814: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 91776.07it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:57:38.759489: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1665: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"np.concatenate의 의미는 사슬같이 잇다 이고 즉 역할은 함수 연결입니다. 각 axis 옵션에 따라 \\n무엇을 기준으로 연결할 것인지가 정해 집니다\\naxis=1은 행렬에서 열방향을 의미한다.\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"plt.subplot(121)와 plt.subplot(122)은 1행2열로 나눠서 1열의 위치에 사진출력을 의미 즉, 두개의 영상이 하나의 창에 출력된 것\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"3학년으로 편입을 하여서 그런지 모르겠지만 해당 수업은 학습하는데 있어\\n더더욱 어려움이 있습니다. 수업에서 사용하시는 언어들 조차 생소하고, 코드를\\n진행하며 같이 수업하는 실습시간에서 코드를 수행하여 나오는 결과를 과정을 순차적으로 해석하며\\n이루어지긴 하나, 이해를 돕는데 있어 부족하다는 생각이 많이 듭니다.\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 55.45454545454545,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/백찬영_46013.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/백찬영_46013.ipynb to python\n",
      "[NbConvertApp] Writing 34557 bytes to report1/백찬영_46013.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.1 0.3 0.4]\n",
      " [0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[0.2 0.6 0.8]\n",
      " [1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[1.6 1.8]\n",
      " [2.4 2.6]\n",
      " [3.2 3.4]\n",
      " [4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)\n",
      "[ 0. nan]\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 303319.64it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:57:45.267750: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:57:45.650379: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:57:45.650418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 10:57:45.871603: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:57:46.473699: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 165805.70it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:57:48.704668: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "[[0.79607843 0.79215686 0.79215686]\n",
      " [0.8        0.79607843 0.78823529]\n",
      " [0.8        0.79607843 0.79607843]]\n",
      "9 pixles average = 0.7952069716775599\n",
      "0.795207\n",
      "convolution result = 0.795207\n",
      "ans02 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1633: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  res[1] = (tmp == ans02.astype('float32')).all()\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1661: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 30.121212121212118,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/유하영_46019.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/유하영_46019.ipynb to python\n",
      "[NbConvertApp] Writing 31114 bytes to report1/유하영_46019.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[5. 6.]\n",
      "[0. 0.]\n",
      "[5. 6.]\n",
      "[0. 0.]\n",
      "[[[17 23  9]]\n",
      "\n",
      " [[39 53 27]]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 292367.49it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:57:56.047318: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:57:56.430733: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:57:56.430772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:57:56.657303: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:57:57.295003: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 113177.57it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 36)                28260     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 74        \n",
      "=================================================================\n",
      "Total params: 28,334\n",
      "Trainable params: 28,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 60.8464 - val_loss: 28.8130\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.6874 - val_loss: 9.0481\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2181 - val_loss: 3.6812\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.3807 - val_loss: 2.4646\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.6376 - val_loss: 1.9345\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2920 - val_loss: 1.6227\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.0984 - val_loss: 1.4217\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9526 - val_loss: 1.2842\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8527 - val_loss: 1.1634\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7752 - val_loss: 1.0730\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7144 - val_loss: 1.0005\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6637 - val_loss: 0.9401\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6217 - val_loss: 0.8963\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5853 - val_loss: 0.8463\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5537 - val_loss: 0.8116\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5260 - val_loss: 0.7701\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5004 - val_loss: 0.7400\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4792 - val_loss: 0.7128\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4571 - val_loss: 0.6852\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4413 - val_loss: 0.6641\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:58:00.779946: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans05 error\n",
      "ans17 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1515: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate는 '사슬 같이 연결하다' 말 그대로 두개의 행렬을 연결해 주는 함수 입니다.\\naxis=1의 의미는 선택[기준]이 되는 축의 방향을 의미 합니다. \\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\n여러개 그래프를 그릴때 그림의 위치를 정해주는 함수 입니다. \\nsubplot 행 열 인덱스 순서입니다.\\n121 를 풀어서 설명하면 행이 1개 이고 열이 2개인데 너는 첫번째 자리에 그림 그리면되. 의미 입니다. \\n122 를 풀어서 설명하면 행이 1개 이고 열이 2개인데 너는 두번째 자리에 그림 그리면되.\\n이러한 의미 입니다.\\n\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n차원수를 줄이는 항목이다. \\n\",\n",
      "        false,\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n활성화 감수 옵션이 들어갈 경우 실손이 많이 일어 나기 때문이다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n데이터를 고르고 균등하게 test / train 를 나누기 위함 \\n\",\n",
      "        false,\n",
      "        \"\\n제가 많이 부족해서 수업을 잘 따라가지 못하는 건지.. 교수님 수업이 조금 어렵습니다. \\n그나마 조금이라도 노력하여 수업 마무리 잘 해 보도록 하겠습니다.  \\n\"\n",
      "    ],\n",
      "    \"score\": 63.03030303030303,\n",
      "    \"accuracy\": 0\n",
      "}\"report1/남정표_46051.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/남정표_46051.ipynb to python\n",
      "[NbConvertApp] Writing 34783 bytes to report1/남정표_46051.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:212: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:245: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "x의 shape :  [[5 7 9]\n",
      " [6 8 0]]\n",
      "x.T의 shape :  [[5 6]\n",
      " [7 8]\n",
      " [9 0]]\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 289591.88it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:58:07.188918: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:58:07.565555: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:58:07.565593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:58:07.788810: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:58:08.418653: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 95628.28it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 10ms/step - loss: 113.2988 - pos_accuracy: 0.0018 - val_loss: 15.9693 - val_pos_accuracy: 0.0137\n",
      "Epoch 2/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.5598 - pos_accuracy: 0.0427 - val_loss: 3.0442 - val_pos_accuracy: 0.1270\n",
      "Epoch 3/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.7210 - pos_accuracy: 0.2254 - val_loss: 1.7398 - val_pos_accuracy: 0.1777\n",
      "Epoch 4/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.7095 - pos_accuracy: 0.0823 - val_loss: 8.9880 - val_pos_accuracy: 0.0918\n",
      "Epoch 5/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.0024 - pos_accuracy: 0.1274 - val_loss: 1.4587 - val_pos_accuracy: 0.1914\n",
      "Epoch 6/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.9085 - pos_accuracy: 0.1869 - val_loss: 1.4591 - val_pos_accuracy: 0.2871\n",
      "Epoch 7/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.0573 - pos_accuracy: 0.2055 - val_loss: 2.1123 - val_pos_accuracy: 0.2207\n",
      "Epoch 8/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.9022 - pos_accuracy: 0.3353 - val_loss: 0.8219 - val_pos_accuracy: 0.4414\n",
      "Epoch 9/118\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.3640 - pos_accuracy: 0.5487 - val_loss: 0.3531 - val_pos_accuracy: 0.5938\n",
      "Epoch 10/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2009 - pos_accuracy: 0.7025 - val_loss: 0.3135 - val_pos_accuracy: 0.6504\n",
      "Epoch 11/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1925 - pos_accuracy: 0.7163 - val_loss: 0.4367 - val_pos_accuracy: 0.5898\n",
      "Epoch 12/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2521 - pos_accuracy: 0.6484 - val_loss: 0.2577 - val_pos_accuracy: 0.7207\n",
      "Epoch 13/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1586 - pos_accuracy: 0.7524 - val_loss: 0.2329 - val_pos_accuracy: 0.7520\n",
      "Epoch 14/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1362 - pos_accuracy: 0.7921 - val_loss: 0.2237 - val_pos_accuracy: 0.7812\n",
      "Epoch 15/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0940 - pos_accuracy: 0.8732 - val_loss: 0.2017 - val_pos_accuracy: 0.7949\n",
      "Epoch 16/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1146 - pos_accuracy: 0.8095 - val_loss: 0.2116 - val_pos_accuracy: 0.7656\n",
      "Epoch 17/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1349 - pos_accuracy: 0.7909 - val_loss: 0.2752 - val_pos_accuracy: 0.6543\n",
      "Epoch 18/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2734 - pos_accuracy: 0.5709 - val_loss: 0.3477 - val_pos_accuracy: 0.6172\n",
      "Epoch 19/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2240 - pos_accuracy: 0.5980 - val_loss: 0.4388 - val_pos_accuracy: 0.5195\n",
      "Epoch 20/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3471 - pos_accuracy: 0.5246 - val_loss: 0.5319 - val_pos_accuracy: 0.4902\n",
      "Epoch 21/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.4275 - pos_accuracy: 0.5096 - val_loss: 0.2292 - val_pos_accuracy: 0.7266\n",
      "Epoch 22/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0925 - pos_accuracy: 0.8540 - val_loss: 0.1828 - val_pos_accuracy: 0.7910\n",
      "Epoch 23/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0724 - pos_accuracy: 0.8954 - val_loss: 0.1555 - val_pos_accuracy: 0.8438\n",
      "Epoch 24/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0458 - pos_accuracy: 0.9495 - val_loss: 0.1450 - val_pos_accuracy: 0.8496\n",
      "Epoch 25/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0427 - pos_accuracy: 0.9465 - val_loss: 0.1650 - val_pos_accuracy: 0.8574\n",
      "Epoch 26/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0710 - pos_accuracy: 0.8966 - val_loss: 0.1821 - val_pos_accuracy: 0.8105\n",
      "Epoch 27/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0733 - pos_accuracy: 0.8858 - val_loss: 0.1316 - val_pos_accuracy: 0.9004\n",
      "Epoch 28/118\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0307 - pos_accuracy: 0.9675 - val_loss: 0.1299 - val_pos_accuracy: 0.8984\n",
      "Epoch 29/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0369 - pos_accuracy: 0.9645 - val_loss: 0.1486 - val_pos_accuracy: 0.8750\n",
      "Epoch 30/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0400 - pos_accuracy: 0.9609 - val_loss: 0.1311 - val_pos_accuracy: 0.9004\n",
      "Epoch 31/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0252 - pos_accuracy: 0.9760 - val_loss: 0.1198 - val_pos_accuracy: 0.9062\n",
      "Epoch 32/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0318 - pos_accuracy: 0.9718 - val_loss: 0.1243 - val_pos_accuracy: 0.9004\n",
      "Epoch 33/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0267 - pos_accuracy: 0.9766 - val_loss: 0.1087 - val_pos_accuracy: 0.9199\n",
      "Epoch 34/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0223 - pos_accuracy: 0.9802 - val_loss: 0.1107 - val_pos_accuracy: 0.9121\n",
      "Epoch 35/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0315 - pos_accuracy: 0.9712 - val_loss: 0.1352 - val_pos_accuracy: 0.8652\n",
      "Epoch 36/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0376 - pos_accuracy: 0.9507 - val_loss: 0.1267 - val_pos_accuracy: 0.9023\n",
      "Epoch 37/118\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0317 - pos_accuracy: 0.9766 - val_loss: 0.1124 - val_pos_accuracy: 0.9160\n",
      "Epoch 38/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0258 - pos_accuracy: 0.9838 - val_loss: 0.1063 - val_pos_accuracy: 0.9180\n",
      "Epoch 39/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0188 - pos_accuracy: 0.9892 - val_loss: 0.1071 - val_pos_accuracy: 0.9141\n",
      "Epoch 40/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0157 - pos_accuracy: 0.9874 - val_loss: 0.1043 - val_pos_accuracy: 0.9180\n",
      "Epoch 41/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0149 - pos_accuracy: 0.9874 - val_loss: 0.0999 - val_pos_accuracy: 0.9141\n",
      "Epoch 42/118\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0135 - pos_accuracy: 0.9910 - val_loss: 0.0998 - val_pos_accuracy: 0.9199\n",
      "Epoch 43/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0126 - pos_accuracy: 0.9892 - val_loss: 0.0987 - val_pos_accuracy: 0.9160\n",
      "Epoch 44/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0116 - pos_accuracy: 0.9922 - val_loss: 0.0984 - val_pos_accuracy: 0.9160\n",
      "Epoch 45/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0128 - pos_accuracy: 0.9916 - val_loss: 0.1042 - val_pos_accuracy: 0.9141\n",
      "Epoch 46/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0136 - pos_accuracy: 0.9922 - val_loss: 0.0976 - val_pos_accuracy: 0.9180\n",
      "Epoch 47/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0100 - pos_accuracy: 0.9928 - val_loss: 0.0966 - val_pos_accuracy: 0.9180\n",
      "Epoch 48/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0097 - pos_accuracy: 0.9940 - val_loss: 0.0959 - val_pos_accuracy: 0.9199\n",
      "Epoch 49/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0094 - pos_accuracy: 0.9952 - val_loss: 0.0957 - val_pos_accuracy: 0.9180\n",
      "Epoch 50/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0086 - pos_accuracy: 0.9940 - val_loss: 0.0937 - val_pos_accuracy: 0.9199\n",
      "Epoch 51/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0079 - pos_accuracy: 0.9952 - val_loss: 0.0929 - val_pos_accuracy: 0.9219\n",
      "Epoch 52/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0077 - pos_accuracy: 0.9952 - val_loss: 0.0951 - val_pos_accuracy: 0.9180\n",
      "Epoch 53/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0094 - pos_accuracy: 0.9952 - val_loss: 0.0933 - val_pos_accuracy: 0.9199\n",
      "Epoch 54/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0090 - pos_accuracy: 0.9946 - val_loss: 0.0941 - val_pos_accuracy: 0.9219\n",
      "Epoch 55/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0106 - pos_accuracy: 0.9946 - val_loss: 0.0942 - val_pos_accuracy: 0.9199\n",
      "Epoch 56/118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0070 - pos_accuracy: 0.9958 - val_loss: 0.0910 - val_pos_accuracy: 0.9199\n",
      "Epoch 57/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0065 - pos_accuracy: 0.9958 - val_loss: 0.0897 - val_pos_accuracy: 0.9199\n",
      "Epoch 58/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0058 - pos_accuracy: 0.9970 - val_loss: 0.0892 - val_pos_accuracy: 0.9199\n",
      "Epoch 59/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0060 - pos_accuracy: 0.9970 - val_loss: 0.0897 - val_pos_accuracy: 0.9219\n",
      "Epoch 60/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0055 - pos_accuracy: 0.9976 - val_loss: 0.0901 - val_pos_accuracy: 0.9199\n",
      "Epoch 61/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0052 - pos_accuracy: 0.9970 - val_loss: 0.0879 - val_pos_accuracy: 0.9219\n",
      "Epoch 62/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0050 - pos_accuracy: 0.9976 - val_loss: 0.0884 - val_pos_accuracy: 0.9180\n",
      "Epoch 63/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0048 - pos_accuracy: 0.9976 - val_loss: 0.0895 - val_pos_accuracy: 0.9219\n",
      "Epoch 64/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0047 - pos_accuracy: 0.9970 - val_loss: 0.0868 - val_pos_accuracy: 0.9238\n",
      "Epoch 65/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0046 - pos_accuracy: 0.9976 - val_loss: 0.0912 - val_pos_accuracy: 0.9219\n",
      "Epoch 66/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0124 - pos_accuracy: 0.9976 - val_loss: 0.0981 - val_pos_accuracy: 0.9219\n",
      "Epoch 67/118\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0098 - pos_accuracy: 0.9976 - val_loss: 0.0940 - val_pos_accuracy: 0.9219\n",
      "Epoch 68/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0157 - pos_accuracy: 0.9970 - val_loss: 0.0879 - val_pos_accuracy: 0.9199\n",
      "Epoch 69/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0047 - pos_accuracy: 0.9976 - val_loss: 0.0863 - val_pos_accuracy: 0.9199\n",
      "Epoch 70/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0046 - pos_accuracy: 0.9976 - val_loss: 0.0883 - val_pos_accuracy: 0.9238\n",
      "Epoch 71/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0110 - pos_accuracy: 0.9976 - val_loss: 0.0904 - val_pos_accuracy: 0.9258\n",
      "Epoch 72/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0057 - pos_accuracy: 0.9970 - val_loss: 0.0852 - val_pos_accuracy: 0.9238\n",
      "Epoch 73/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0055 - pos_accuracy: 0.9982 - val_loss: 0.0855 - val_pos_accuracy: 0.9199\n",
      "Epoch 74/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0038 - pos_accuracy: 0.9976 - val_loss: 0.0850 - val_pos_accuracy: 0.9199\n",
      "Epoch 75/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0035 - pos_accuracy: 0.9970 - val_loss: 0.0843 - val_pos_accuracy: 0.9199\n",
      "Epoch 76/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0032 - pos_accuracy: 0.9976 - val_loss: 0.0841 - val_pos_accuracy: 0.9238\n",
      "Epoch 77/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0029 - pos_accuracy: 0.9982 - val_loss: 0.0842 - val_pos_accuracy: 0.9238\n",
      "Epoch 78/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0028 - pos_accuracy: 0.9982 - val_loss: 0.0832 - val_pos_accuracy: 0.9199\n",
      "Epoch 79/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0028 - pos_accuracy: 0.9976 - val_loss: 0.0836 - val_pos_accuracy: 0.9199\n",
      "Epoch 80/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0027 - pos_accuracy: 0.9982 - val_loss: 0.0827 - val_pos_accuracy: 0.9199\n",
      "Epoch 81/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0029 - pos_accuracy: 0.9982 - val_loss: 0.0829 - val_pos_accuracy: 0.9219\n",
      "Epoch 82/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0027 - pos_accuracy: 0.9982 - val_loss: 0.0838 - val_pos_accuracy: 0.9219\n",
      "Epoch 83/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0024 - pos_accuracy: 0.9982 - val_loss: 0.0833 - val_pos_accuracy: 0.9238\n",
      "Epoch 84/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0024 - pos_accuracy: 0.9976 - val_loss: 0.0836 - val_pos_accuracy: 0.9219\n",
      "Epoch 85/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0022 - pos_accuracy: 0.9982 - val_loss: 0.0824 - val_pos_accuracy: 0.9219\n",
      "Epoch 86/118\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0020 - pos_accuracy: 0.9982 - val_loss: 0.0828 - val_pos_accuracy: 0.9219\n",
      "Epoch 87/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0022 - pos_accuracy: 0.9982 - val_loss: 0.0824 - val_pos_accuracy: 0.9238\n",
      "Epoch 88/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 0.9982 - val_loss: 0.0822 - val_pos_accuracy: 0.9238\n",
      "Epoch 89/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0021 - pos_accuracy: 0.9976 - val_loss: 0.0817 - val_pos_accuracy: 0.9219\n",
      "Epoch 90/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0020 - pos_accuracy: 0.9982 - val_loss: 0.0818 - val_pos_accuracy: 0.9219\n",
      "Epoch 91/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 0.9976 - val_loss: 0.0810 - val_pos_accuracy: 0.9199\n",
      "Epoch 92/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0017 - pos_accuracy: 0.9976 - val_loss: 0.0816 - val_pos_accuracy: 0.9219\n",
      "Epoch 93/118\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0016 - pos_accuracy: 0.9988 - val_loss: 0.0807 - val_pos_accuracy: 0.9238\n",
      "Epoch 94/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0016 - pos_accuracy: 0.9982 - val_loss: 0.0813 - val_pos_accuracy: 0.9238\n",
      "Epoch 95/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0017 - pos_accuracy: 0.9982 - val_loss: 0.0806 - val_pos_accuracy: 0.9219\n",
      "Epoch 96/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0016 - pos_accuracy: 0.9988 - val_loss: 0.0802 - val_pos_accuracy: 0.9238\n",
      "Epoch 97/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.0805 - val_pos_accuracy: 0.9219\n",
      "Epoch 98/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 0.9988 - val_loss: 0.0820 - val_pos_accuracy: 0.9219\n",
      "Epoch 99/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0026 - pos_accuracy: 0.9994 - val_loss: 0.0828 - val_pos_accuracy: 0.9199\n",
      "Epoch 100/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0038 - pos_accuracy: 0.9994 - val_loss: 0.0811 - val_pos_accuracy: 0.9277\n",
      "Epoch 101/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0020 - pos_accuracy: 1.0000 - val_loss: 0.0812 - val_pos_accuracy: 0.9180\n",
      "Epoch 102/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 1.0000 - val_loss: 0.0799 - val_pos_accuracy: 0.9238\n",
      "Epoch 103/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0012 - pos_accuracy: 1.0000 - val_loss: 0.0801 - val_pos_accuracy: 0.9238\n",
      "Epoch 104/118\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0012 - pos_accuracy: 1.0000 - val_loss: 0.0798 - val_pos_accuracy: 0.9219\n",
      "Epoch 105/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.0794 - val_pos_accuracy: 0.9219\n",
      "Epoch 106/118\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0013 - pos_accuracy: 1.0000 - val_loss: 0.0794 - val_pos_accuracy: 0.9219\n",
      "Epoch 107/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0012 - pos_accuracy: 1.0000 - val_loss: 0.0795 - val_pos_accuracy: 0.9238\n",
      "Epoch 108/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 1.0000 - val_loss: 0.0792 - val_pos_accuracy: 0.9219\n",
      "Epoch 109/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.9428e-04 - pos_accuracy: 1.0000 - val_loss: 0.0791 - val_pos_accuracy: 0.9219\n",
      "Epoch 110/118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 9.5071e-04 - pos_accuracy: 1.0000 - val_loss: 0.0797 - val_pos_accuracy: 0.9219\n",
      "Epoch 111/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 1.0000 - val_loss: 0.0792 - val_pos_accuracy: 0.9219\n",
      "Epoch 112/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0831e-04 - pos_accuracy: 1.0000 - val_loss: 0.0791 - val_pos_accuracy: 0.9219\n",
      "Epoch 113/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0010 - pos_accuracy: 1.0000 - val_loss: 0.0795 - val_pos_accuracy: 0.9180\n",
      "Epoch 114/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0010 - pos_accuracy: 1.0000 - val_loss: 0.0789 - val_pos_accuracy: 0.9219\n",
      "Epoch 115/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.8914e-04 - pos_accuracy: 1.0000 - val_loss: 0.0784 - val_pos_accuracy: 0.9219\n",
      "Epoch 116/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.2259e-04 - pos_accuracy: 1.0000 - val_loss: 0.0786 - val_pos_accuracy: 0.9219\n",
      "Epoch 117/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.4192e-04 - pos_accuracy: 1.0000 - val_loss: 0.0787 - val_pos_accuracy: 0.9219\n",
      "Epoch 118/118\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.3174e-04 - pos_accuracy: 1.0000 - val_loss: 0.0790 - val_pos_accuracy: 0.9258\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:58:19.624909: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1660: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1671: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1681: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate:행연결\\naxis=1:연결할 행\\n\",\n",
      "        \"\\n위치 지정\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\n차원을 제거하고 평균을 구함 \\n\",\n",
      "        \"\\n활성화를 적용하지 않기위해\\n\",\n",
      "        \"\\n같은 결과값을 유지하기 위해\\n\",\n",
      "        false,\n",
      "        \"\\n매 강의 끝에 [놀면 뭐 할까?]를 해보고 싶습니다. \\n하지만 전혀 이해가 되지안아 무엇을 교수님께 물어봐야 할지 모르겠습니다.\\n참고가 될만한 힌트라도 공지사항에 공지해주시면 많은 도움이 될것같습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 63.51515151515151,\n",
      "    \"accuracy\": 0.9453\n",
      "}\"report1/황수연_46036.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/황수연_46036.ipynb to python\n",
      "[NbConvertApp] Writing 34682 bytes to report1/황수연_46036.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:209: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:240: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "(2, 3)\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.12323400e-17 -1.00000000e+00  2.00000000e+02]\n",
      " [ 1.00000000e+00  6.12323400e-17 -1.42108547e-14]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.12323400e-17 -1.00000000e+00  2.00000000e+02]\n",
      " [ 1.00000000e+00  6.12323400e-17 -1.42108547e-14]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 306175.93it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:58:26.123887: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:58:26.503457: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:58:26.503496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:58:26.731709: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:58:27.362958: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 122356.04it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 4s 3ms/step - loss: 8.9028 - pos_accuracy: 0.3531 - val_loss: 1.4396 - val_pos_accuracy: 0.4650\n",
      "Epoch 2/4\n",
      "1600/1600 [==============================] - 4s 3ms/step - loss: 1.0050 - pos_accuracy: 0.4681 - val_loss: 0.9612 - val_pos_accuracy: 0.4775\n",
      "Epoch 3/4\n",
      "1600/1600 [==============================] - 4s 3ms/step - loss: 0.7112 - pos_accuracy: 0.5044 - val_loss: 0.7621 - val_pos_accuracy: 0.5000\n",
      "Epoch 4/4\n",
      "1600/1600 [==============================] - 4s 3ms/step - loss: 0.5898 - pos_accuracy: 0.5069 - val_loss: 0.6646 - val_pos_accuracy: 0.5150\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 10:58:46.549219: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "[[0.79607843 0.79215686 0.79215686]\n",
      " [0.8        0.79607843 0.78823529]\n",
      " [0.8        0.79607843 0.79607843]]\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1656: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할을 특정 축(axis)의 방향으로 배열을 연결해줍니다.\\naxis=1은 각 차원의 열방향 또는 행방향을 뜻한다.\\n\",\n",
      "        \"\\n처음 두개 숫자는 전체 그리드 행렬의 모양을 지시하고, 세번째 숫자는 어느 위치인지 나타낸다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all은 AND 연산을 뜻하며, axis=1은 행을 기준으로 연산한다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 면적에 걸친 모든 요소의 평균을 구한다.\\n\",\n",
      "        false,\n",
      "        \"\\nrandom seed의 사용 이유는 동일한 세트의 난수를 보며 난수를 예측 가능하도록 만든다. \\n\",\n",
      "        false,\n",
      "        \"\\n안녕하세요 교수님, 직장에 다니면서도 공부에 뜻이 있어 학습에 성실히 임하고 있고 입학 이후 매학기 성적장학금을 받아왔었습니다.\\n그러나 이번 강좌는 코딩 입문자 레벨인 제가 잘못 수강하게 된 것 같습니다. \\n학교 강좌 외로도 부족함이 많아 인터넷 검색 학습을 굉장히 많이 함에도 불구하고 그 조차도\\n기본 수학 및 코딩 지식이 부족한 저로서는 전반적인 이해도가 많이 떨어집니다. 매 강의 최대한 많은 것을 알려주시려는 교수님의\\n열정에 항상 감사드립니다. 학기 끝까지 최선을 다해보겠습니다. 감사합니다.  \\n\"\n",
      "    ],\n",
      "    \"score\": 68.72727272727272,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/이지수_46068.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이지수_46068.ipynb to python\n",
      "[NbConvertApp] Writing 36563 bytes to report1/이지수_46068.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:212: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:246: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 3 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 293082.52it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 10:58:52.928446: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 10:58:53.308950: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 10:58:53.308991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 10:58:53.534621: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 10:58:54.210145: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - pos_accuracy: 0.0000e+00 - val_loss: 212.3778 - val_pos_accuracy: 0.0045\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - pos_accuracy: 6.2500e-04 - val_loss: 195.9663 - val_pos_accuracy: 0.0000e+00\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 161521.29it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 1,576\n",
      "Trainable params: 1,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 14.2894 - pos_accuracy: 0.1656 - val_loss: 1.3982 - val_pos_accuracy: 0.2575\n",
      "Epoch 2/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9985 - pos_accuracy: 0.3469 - val_loss: 0.9701 - val_pos_accuracy: 0.3300\n",
      "Epoch 3/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6919 - pos_accuracy: 0.3856 - val_loss: 0.7256 - val_pos_accuracy: 0.3750\n",
      "Epoch 4/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5682 - pos_accuracy: 0.4275 - val_loss: 0.6357 - val_pos_accuracy: 0.4625\n",
      "Epoch 5/135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4643 - pos_accuracy: 0.4694 - val_loss: 0.6476 - val_pos_accuracy: 0.3700\n",
      "Epoch 6/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4215 - pos_accuracy: 0.4956 - val_loss: 0.6579 - val_pos_accuracy: 0.3950\n",
      "Epoch 7/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3703 - pos_accuracy: 0.5081 - val_loss: 0.5989 - val_pos_accuracy: 0.4350\n",
      "Epoch 8/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3562 - pos_accuracy: 0.5056 - val_loss: 0.5691 - val_pos_accuracy: 0.4400\n",
      "Epoch 9/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3153 - pos_accuracy: 0.5381 - val_loss: 0.5720 - val_pos_accuracy: 0.3975\n",
      "Epoch 10/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2963 - pos_accuracy: 0.5550 - val_loss: 0.5427 - val_pos_accuracy: 0.4925\n",
      "Epoch 11/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2762 - pos_accuracy: 0.5612 - val_loss: 0.5863 - val_pos_accuracy: 0.4575\n",
      "Epoch 12/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2630 - pos_accuracy: 0.5863 - val_loss: 0.5406 - val_pos_accuracy: 0.4625\n",
      "Epoch 13/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2373 - pos_accuracy: 0.6144 - val_loss: 0.5356 - val_pos_accuracy: 0.4625\n",
      "Epoch 14/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2351 - pos_accuracy: 0.6306 - val_loss: 0.5640 - val_pos_accuracy: 0.4625\n",
      "Epoch 15/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2184 - pos_accuracy: 0.6169 - val_loss: 0.5984 - val_pos_accuracy: 0.3925\n",
      "Epoch 16/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2066 - pos_accuracy: 0.6494 - val_loss: 0.5892 - val_pos_accuracy: 0.4700\n",
      "Epoch 17/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2031 - pos_accuracy: 0.6475 - val_loss: 0.5192 - val_pos_accuracy: 0.5275\n",
      "Epoch 18/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1968 - pos_accuracy: 0.6669 - val_loss: 0.5262 - val_pos_accuracy: 0.5775\n",
      "Epoch 19/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1827 - pos_accuracy: 0.6800 - val_loss: 0.5407 - val_pos_accuracy: 0.4675\n",
      "Epoch 20/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1783 - pos_accuracy: 0.7000 - val_loss: 0.5566 - val_pos_accuracy: 0.5300\n",
      "Epoch 21/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1754 - pos_accuracy: 0.6944 - val_loss: 0.5339 - val_pos_accuracy: 0.5375\n",
      "Epoch 22/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1673 - pos_accuracy: 0.7056 - val_loss: 0.5215 - val_pos_accuracy: 0.5400\n",
      "Epoch 23/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1597 - pos_accuracy: 0.7237 - val_loss: 0.5373 - val_pos_accuracy: 0.5825\n",
      "Epoch 24/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1568 - pos_accuracy: 0.7231 - val_loss: 0.5262 - val_pos_accuracy: 0.5525\n",
      "Epoch 25/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1494 - pos_accuracy: 0.7387 - val_loss: 0.5103 - val_pos_accuracy: 0.5750\n",
      "Epoch 26/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1443 - pos_accuracy: 0.7425 - val_loss: 0.5817 - val_pos_accuracy: 0.5500\n",
      "Epoch 27/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1408 - pos_accuracy: 0.7431 - val_loss: 0.5445 - val_pos_accuracy: 0.5750\n",
      "Epoch 28/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1353 - pos_accuracy: 0.7613 - val_loss: 0.5388 - val_pos_accuracy: 0.6050\n",
      "Epoch 29/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1303 - pos_accuracy: 0.7769 - val_loss: 0.5483 - val_pos_accuracy: 0.6000\n",
      "Epoch 30/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1263 - pos_accuracy: 0.7738 - val_loss: 0.5631 - val_pos_accuracy: 0.5350\n",
      "Epoch 31/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1268 - pos_accuracy: 0.7775 - val_loss: 0.5325 - val_pos_accuracy: 0.6425\n",
      "Epoch 32/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1213 - pos_accuracy: 0.8000 - val_loss: 0.5640 - val_pos_accuracy: 0.6125\n",
      "Epoch 33/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1167 - pos_accuracy: 0.8006 - val_loss: 0.5439 - val_pos_accuracy: 0.6275\n",
      "Epoch 34/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1132 - pos_accuracy: 0.8175 - val_loss: 0.5511 - val_pos_accuracy: 0.6300\n",
      "Epoch 35/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1069 - pos_accuracy: 0.8138 - val_loss: 0.5249 - val_pos_accuracy: 0.6900\n",
      "Epoch 36/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1046 - pos_accuracy: 0.8288 - val_loss: 0.5356 - val_pos_accuracy: 0.6325\n",
      "Epoch 37/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1040 - pos_accuracy: 0.8238 - val_loss: 0.5363 - val_pos_accuracy: 0.6900\n",
      "Epoch 38/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1020 - pos_accuracy: 0.8288 - val_loss: 0.5269 - val_pos_accuracy: 0.6850\n",
      "Epoch 39/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0951 - pos_accuracy: 0.8419 - val_loss: 0.5584 - val_pos_accuracy: 0.6725\n",
      "Epoch 40/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0953 - pos_accuracy: 0.8487 - val_loss: 0.5497 - val_pos_accuracy: 0.6900\n",
      "Epoch 41/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0932 - pos_accuracy: 0.8406 - val_loss: 0.5614 - val_pos_accuracy: 0.7000\n",
      "Epoch 42/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0903 - pos_accuracy: 0.8469 - val_loss: 0.5545 - val_pos_accuracy: 0.6750\n",
      "Epoch 43/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0871 - pos_accuracy: 0.8631 - val_loss: 0.5634 - val_pos_accuracy: 0.7125\n",
      "Epoch 44/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0851 - pos_accuracy: 0.8594 - val_loss: 0.5712 - val_pos_accuracy: 0.6950\n",
      "Epoch 45/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0819 - pos_accuracy: 0.8763 - val_loss: 0.5639 - val_pos_accuracy: 0.6500\n",
      "Epoch 46/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0830 - pos_accuracy: 0.8637 - val_loss: 0.5611 - val_pos_accuracy: 0.6825\n",
      "Epoch 47/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0769 - pos_accuracy: 0.8737 - val_loss: 0.5657 - val_pos_accuracy: 0.7100\n",
      "Epoch 48/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0778 - pos_accuracy: 0.8794 - val_loss: 0.5642 - val_pos_accuracy: 0.7025\n",
      "Epoch 49/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0754 - pos_accuracy: 0.8856 - val_loss: 0.5685 - val_pos_accuracy: 0.6925\n",
      "Epoch 50/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0724 - pos_accuracy: 0.8913 - val_loss: 0.5747 - val_pos_accuracy: 0.7125\n",
      "Epoch 51/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0715 - pos_accuracy: 0.8944 - val_loss: 0.5748 - val_pos_accuracy: 0.7275\n",
      "Epoch 52/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0691 - pos_accuracy: 0.8963 - val_loss: 0.5736 - val_pos_accuracy: 0.7150\n",
      "Epoch 53/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0695 - pos_accuracy: 0.8925 - val_loss: 0.5740 - val_pos_accuracy: 0.7050\n",
      "Epoch 54/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0658 - pos_accuracy: 0.8975 - val_loss: 0.5776 - val_pos_accuracy: 0.7050\n",
      "Epoch 55/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0633 - pos_accuracy: 0.9062 - val_loss: 0.5656 - val_pos_accuracy: 0.7375\n",
      "Epoch 56/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0617 - pos_accuracy: 0.9100 - val_loss: 0.5726 - val_pos_accuracy: 0.7300\n",
      "Epoch 57/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0610 - pos_accuracy: 0.9112 - val_loss: 0.5811 - val_pos_accuracy: 0.7300\n",
      "Epoch 58/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0605 - pos_accuracy: 0.9175 - val_loss: 0.5750 - val_pos_accuracy: 0.7425\n",
      "Epoch 59/135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0582 - pos_accuracy: 0.9087 - val_loss: 0.5811 - val_pos_accuracy: 0.7325\n",
      "Epoch 60/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0572 - pos_accuracy: 0.9181 - val_loss: 0.5791 - val_pos_accuracy: 0.7425\n",
      "Epoch 61/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0556 - pos_accuracy: 0.9250 - val_loss: 0.5708 - val_pos_accuracy: 0.7600\n",
      "Epoch 62/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0542 - pos_accuracy: 0.9281 - val_loss: 0.5874 - val_pos_accuracy: 0.7525\n",
      "Epoch 63/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0526 - pos_accuracy: 0.9262 - val_loss: 0.5720 - val_pos_accuracy: 0.7400\n",
      "Epoch 64/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0511 - pos_accuracy: 0.9237 - val_loss: 0.6004 - val_pos_accuracy: 0.7450\n",
      "Epoch 65/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0509 - pos_accuracy: 0.9312 - val_loss: 0.5951 - val_pos_accuracy: 0.7475\n",
      "Epoch 66/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0497 - pos_accuracy: 0.9406 - val_loss: 0.5930 - val_pos_accuracy: 0.7500\n",
      "Epoch 67/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0491 - pos_accuracy: 0.9269 - val_loss: 0.5933 - val_pos_accuracy: 0.7500\n",
      "Epoch 68/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0462 - pos_accuracy: 0.9356 - val_loss: 0.5879 - val_pos_accuracy: 0.7675\n",
      "Epoch 69/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0468 - pos_accuracy: 0.9337 - val_loss: 0.5814 - val_pos_accuracy: 0.7600\n",
      "Epoch 70/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0448 - pos_accuracy: 0.9400 - val_loss: 0.5946 - val_pos_accuracy: 0.7575\n",
      "Epoch 71/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0435 - pos_accuracy: 0.9369 - val_loss: 0.6252 - val_pos_accuracy: 0.7050\n",
      "Epoch 72/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0430 - pos_accuracy: 0.9394 - val_loss: 0.5961 - val_pos_accuracy: 0.7725\n",
      "Epoch 73/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0428 - pos_accuracy: 0.9450 - val_loss: 0.5906 - val_pos_accuracy: 0.7850\n",
      "Epoch 74/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0414 - pos_accuracy: 0.9438 - val_loss: 0.5828 - val_pos_accuracy: 0.7825\n",
      "Epoch 75/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0414 - pos_accuracy: 0.9450 - val_loss: 0.5932 - val_pos_accuracy: 0.7825\n",
      "Epoch 76/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0396 - pos_accuracy: 0.9494 - val_loss: 0.5998 - val_pos_accuracy: 0.7750\n",
      "Epoch 77/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0384 - pos_accuracy: 0.9519 - val_loss: 0.5948 - val_pos_accuracy: 0.7750\n",
      "Epoch 78/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0378 - pos_accuracy: 0.9506 - val_loss: 0.5924 - val_pos_accuracy: 0.7825\n",
      "Epoch 79/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0375 - pos_accuracy: 0.9506 - val_loss: 0.6164 - val_pos_accuracy: 0.7725\n",
      "Epoch 80/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0362 - pos_accuracy: 0.9550 - val_loss: 0.6000 - val_pos_accuracy: 0.7875\n",
      "Epoch 81/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0357 - pos_accuracy: 0.9556 - val_loss: 0.5964 - val_pos_accuracy: 0.7825\n",
      "Epoch 82/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0355 - pos_accuracy: 0.9556 - val_loss: 0.6040 - val_pos_accuracy: 0.7925\n",
      "Epoch 83/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0338 - pos_accuracy: 0.9575 - val_loss: 0.6088 - val_pos_accuracy: 0.7875\n",
      "Epoch 84/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0331 - pos_accuracy: 0.9644 - val_loss: 0.6171 - val_pos_accuracy: 0.7975\n",
      "Epoch 85/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0323 - pos_accuracy: 0.9606 - val_loss: 0.6070 - val_pos_accuracy: 0.7950\n",
      "Epoch 86/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0316 - pos_accuracy: 0.9656 - val_loss: 0.6101 - val_pos_accuracy: 0.7825\n",
      "Epoch 87/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0313 - pos_accuracy: 0.9694 - val_loss: 0.6039 - val_pos_accuracy: 0.8050\n",
      "Epoch 88/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0305 - pos_accuracy: 0.9700 - val_loss: 0.6136 - val_pos_accuracy: 0.7950\n",
      "Epoch 89/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0303 - pos_accuracy: 0.9669 - val_loss: 0.5972 - val_pos_accuracy: 0.8050\n",
      "Epoch 90/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0300 - pos_accuracy: 0.9638 - val_loss: 0.6038 - val_pos_accuracy: 0.8125\n",
      "Epoch 91/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0287 - pos_accuracy: 0.9744 - val_loss: 0.6173 - val_pos_accuracy: 0.8050\n",
      "Epoch 92/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0285 - pos_accuracy: 0.9712 - val_loss: 0.6134 - val_pos_accuracy: 0.8025\n",
      "Epoch 93/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0275 - pos_accuracy: 0.9688 - val_loss: 0.6170 - val_pos_accuracy: 0.8150\n",
      "Epoch 94/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0269 - pos_accuracy: 0.9694 - val_loss: 0.6184 - val_pos_accuracy: 0.7975\n",
      "Epoch 95/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0273 - pos_accuracy: 0.9719 - val_loss: 0.6244 - val_pos_accuracy: 0.8175\n",
      "Epoch 96/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0261 - pos_accuracy: 0.9756 - val_loss: 0.6108 - val_pos_accuracy: 0.8125\n",
      "Epoch 97/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0258 - pos_accuracy: 0.9762 - val_loss: 0.6168 - val_pos_accuracy: 0.8175\n",
      "Epoch 98/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0249 - pos_accuracy: 0.9775 - val_loss: 0.6058 - val_pos_accuracy: 0.8150\n",
      "Epoch 99/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0245 - pos_accuracy: 0.9750 - val_loss: 0.6211 - val_pos_accuracy: 0.8100\n",
      "Epoch 100/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0239 - pos_accuracy: 0.9800 - val_loss: 0.6160 - val_pos_accuracy: 0.8250\n",
      "Epoch 101/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0238 - pos_accuracy: 0.9787 - val_loss: 0.6197 - val_pos_accuracy: 0.8125\n",
      "Epoch 102/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0233 - pos_accuracy: 0.9800 - val_loss: 0.6170 - val_pos_accuracy: 0.8150\n",
      "Epoch 103/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0227 - pos_accuracy: 0.9794 - val_loss: 0.6144 - val_pos_accuracy: 0.8100\n",
      "Epoch 104/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0224 - pos_accuracy: 0.9831 - val_loss: 0.6175 - val_pos_accuracy: 0.8200\n",
      "Epoch 105/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0217 - pos_accuracy: 0.9787 - val_loss: 0.6177 - val_pos_accuracy: 0.8200\n",
      "Epoch 106/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0217 - pos_accuracy: 0.9837 - val_loss: 0.6189 - val_pos_accuracy: 0.8100\n",
      "Epoch 107/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0210 - pos_accuracy: 0.9819 - val_loss: 0.6293 - val_pos_accuracy: 0.8150\n",
      "Epoch 108/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0211 - pos_accuracy: 0.9862 - val_loss: 0.6275 - val_pos_accuracy: 0.8225\n",
      "Epoch 109/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0203 - pos_accuracy: 0.9819 - val_loss: 0.6228 - val_pos_accuracy: 0.8250\n",
      "Epoch 110/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0204 - pos_accuracy: 0.9881 - val_loss: 0.6244 - val_pos_accuracy: 0.8250\n",
      "Epoch 111/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0190 - pos_accuracy: 0.9887 - val_loss: 0.6193 - val_pos_accuracy: 0.8300\n",
      "Epoch 112/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0196 - pos_accuracy: 0.9881 - val_loss: 0.6227 - val_pos_accuracy: 0.8350\n",
      "Epoch 113/135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0184 - pos_accuracy: 0.9906 - val_loss: 0.6256 - val_pos_accuracy: 0.8275\n",
      "Epoch 114/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0184 - pos_accuracy: 0.9869 - val_loss: 0.6352 - val_pos_accuracy: 0.8175\n",
      "Epoch 115/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0184 - pos_accuracy: 0.9887 - val_loss: 0.6302 - val_pos_accuracy: 0.8150\n",
      "Epoch 116/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0175 - pos_accuracy: 0.9862 - val_loss: 0.6334 - val_pos_accuracy: 0.8375\n",
      "Epoch 117/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0180 - pos_accuracy: 0.9894 - val_loss: 0.6260 - val_pos_accuracy: 0.8350\n",
      "Epoch 118/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0173 - pos_accuracy: 0.9912 - val_loss: 0.6427 - val_pos_accuracy: 0.8200\n",
      "Epoch 119/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0171 - pos_accuracy: 0.9925 - val_loss: 0.6332 - val_pos_accuracy: 0.8375\n",
      "Epoch 120/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0165 - pos_accuracy: 0.9894 - val_loss: 0.6350 - val_pos_accuracy: 0.8325\n",
      "Epoch 121/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0165 - pos_accuracy: 0.9925 - val_loss: 0.6321 - val_pos_accuracy: 0.8325\n",
      "Epoch 122/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0163 - pos_accuracy: 0.9919 - val_loss: 0.6264 - val_pos_accuracy: 0.8350\n",
      "Epoch 123/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0161 - pos_accuracy: 0.9912 - val_loss: 0.6345 - val_pos_accuracy: 0.8300\n",
      "Epoch 124/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0159 - pos_accuracy: 0.9919 - val_loss: 0.6349 - val_pos_accuracy: 0.8300\n",
      "Epoch 125/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0151 - pos_accuracy: 0.9912 - val_loss: 0.6351 - val_pos_accuracy: 0.8325\n",
      "Epoch 126/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0149 - pos_accuracy: 0.9944 - val_loss: 0.6405 - val_pos_accuracy: 0.8300\n",
      "Epoch 127/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0148 - pos_accuracy: 0.9919 - val_loss: 0.6412 - val_pos_accuracy: 0.8300\n",
      "Epoch 128/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0149 - pos_accuracy: 0.9925 - val_loss: 0.6357 - val_pos_accuracy: 0.8325\n",
      "Epoch 129/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0148 - pos_accuracy: 0.9931 - val_loss: 0.6380 - val_pos_accuracy: 0.8325\n",
      "Epoch 130/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0144 - pos_accuracy: 0.9925 - val_loss: 0.6460 - val_pos_accuracy: 0.8350\n",
      "Epoch 131/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0138 - pos_accuracy: 0.9944 - val_loss: 0.6443 - val_pos_accuracy: 0.8350\n",
      "Epoch 132/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0140 - pos_accuracy: 0.9919 - val_loss: 0.6400 - val_pos_accuracy: 0.8325\n",
      "Epoch 133/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0138 - pos_accuracy: 0.9937 - val_loss: 0.6385 - val_pos_accuracy: 0.8350\n",
      "Epoch 134/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0131 - pos_accuracy: 0.9937 - val_loss: 0.6386 - val_pos_accuracy: 0.8350\n",
      "Epoch 135/135\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0131 - pos_accuracy: 0.9931 - val_loss: 0.6420 - val_pos_accuracy: 0.8400\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:00:20.265446: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "[[0.79607843 0.79215686 0.79215686]\n",
      " [0.8        0.79607843 0.78823529]\n",
      " [0.8        0.79607843 0.79607843]]\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1705: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concatenate : Numpy 배열들을 하나로 합치는데 이용하는 함수로 기존 축을 따라 배열 시퀀스를 결합합니다.\\n\\naxis : axis = 1로 지정하면 2번째의 축(1차원째)이 열이므로 가로로 결합됩니다.\\n(axis 는 배열이 결합될 축으로 axis가 None이면 사용하기 전에 배열이 병합됩니다. 기본값은 0입니다.)\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n121 : 1행 2열의 첫 번째\\n122 : 1행 2열의 두 번째\\n를 의미하여 subplot(1, 2, 1) , subplot(1, 2, 2) 로 작성하여도 동일한 결과를 확인할 수 있다.\\n위의 템플릿 마지막 affine 변환에 변환 하나를 추가한 뒤 subplot(131), subplot(132), subplot(133) 으로 결과를 확인하니 3가지의 그림을 확인할 수 있었다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n-> reduce_all 은 tensor가 지정한 축 방향의 각 요소의 논리와 (and 연산) 계산하는 함수이다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n-> reduce_mean 은 특정 차원을 제거하고 평균을 구하는 함수이다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유\\n활성화 함수가 없는 경우 신경망 학습은 선형회귀와 동일하기 때문이다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유\\n컴퓨터 내 내장된 랜덤 테이블 중 랜덤 테이블 어떤 것을 불러올 것인지 결정 (seed값이 같으면 똑같은 랜덤 값 출력)\\n→ 일정한 결과값을 얻기 위해서는 numpy seed값과 tensorflow seed값 모두 설정해야 함\\n\",\n",
      "        false,\n",
      "        \"\\n딥러닝을 쉽게 알려주셔서 감사하게 수강하고 있습니다.\\n그럼에도 불구하고 내용 자체가 쉽지 않다보니 구글링을 통해 찾아가면서 학습을 하고 있는데요!\\n재미있어서 열심히 과제를 하게 되었지만 답안이나 해설이 따로 없어서 제가 맞게 했는지 잘 모르겠습니다.\\n\\n과제 풀이 및 해석 or 실습에서 조금 더 자세하게 설명해주시면 감사할 것 같습니다.\\n\\n감사합니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 80.0,\n",
      "    \"accuracy\": 0.84\n",
      "}\"report1/이루오_46050.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이루오_46050.ipynb to python\n",
      "[NbConvertApp] Writing 35855 bytes to report1/이루오_46050.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:208: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "float32 => 52\n",
      "float64 => 422\n",
      "(2, 2) (3, 2) (2, 3) (2, 3)\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -13. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 295134.50it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:00:26.776125: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:00:27.157980: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:00:27.158037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:00:27.380601: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:00:28.001566: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 100355.41it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 27,362\n",
      "Trainable params: 27,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/46\n",
      "100/100 [==============================] - 1s 4ms/step - loss: 40.9373 - pos_accuracy: 0.0119 - val_loss: 3.8739 - val_pos_accuracy: 0.0700\n",
      "Epoch 2/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 3.1683 - pos_accuracy: 0.0781 - val_loss: 2.7925 - val_pos_accuracy: 0.0875\n",
      "Epoch 3/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 2.1163 - pos_accuracy: 0.1013 - val_loss: 0.9260 - val_pos_accuracy: 0.1900\n",
      "Epoch 4/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.8462 - pos_accuracy: 0.2175 - val_loss: 2.1478 - val_pos_accuracy: 0.1575\n",
      "Epoch 5/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.6390 - pos_accuracy: 0.2950 - val_loss: 0.3711 - val_pos_accuracy: 0.4275\n",
      "Epoch 6/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.6315 - pos_accuracy: 0.3331 - val_loss: 0.3374 - val_pos_accuracy: 0.4975\n",
      "Epoch 7/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.7277 - pos_accuracy: 0.3381 - val_loss: 0.4340 - val_pos_accuracy: 0.3500\n",
      "Epoch 8/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5850 - pos_accuracy: 0.4313 - val_loss: 0.4072 - val_pos_accuracy: 0.3575\n",
      "Epoch 9/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.2769 - pos_accuracy: 0.5494 - val_loss: 0.4123 - val_pos_accuracy: 0.5050\n",
      "Epoch 10/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1986 - pos_accuracy: 0.6438 - val_loss: 0.2153 - val_pos_accuracy: 0.6825\n",
      "Epoch 11/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1712 - pos_accuracy: 0.6687 - val_loss: 0.2026 - val_pos_accuracy: 0.7000\n",
      "Epoch 12/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.2174 - pos_accuracy: 0.6488 - val_loss: 0.2146 - val_pos_accuracy: 0.6500\n",
      "Epoch 13/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1503 - pos_accuracy: 0.7519 - val_loss: 0.1596 - val_pos_accuracy: 0.7900\n",
      "Epoch 14/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1799 - pos_accuracy: 0.7175 - val_loss: 0.1696 - val_pos_accuracy: 0.7700\n",
      "Epoch 15/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1265 - pos_accuracy: 0.7581 - val_loss: 0.2730 - val_pos_accuracy: 0.6025\n",
      "Epoch 16/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.1021 - pos_accuracy: 0.8219 - val_loss: 0.1278 - val_pos_accuracy: 0.8525\n",
      "Epoch 17/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0915 - pos_accuracy: 0.8487 - val_loss: 0.1638 - val_pos_accuracy: 0.7550\n",
      "Epoch 18/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.1243 - pos_accuracy: 0.8037 - val_loss: 0.1833 - val_pos_accuracy: 0.6925\n",
      "Epoch 19/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0575 - pos_accuracy: 0.9125 - val_loss: 0.2014 - val_pos_accuracy: 0.6875\n",
      "Epoch 20/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0535 - pos_accuracy: 0.9281 - val_loss: 0.1061 - val_pos_accuracy: 0.8800\n",
      "Epoch 21/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0430 - pos_accuracy: 0.9588 - val_loss: 0.1199 - val_pos_accuracy: 0.8675\n",
      "Epoch 22/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0478 - pos_accuracy: 0.9394 - val_loss: 0.1161 - val_pos_accuracy: 0.8600\n",
      "Epoch 23/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0521 - pos_accuracy: 0.9200 - val_loss: 0.1594 - val_pos_accuracy: 0.7800\n",
      "Epoch 24/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0964 - pos_accuracy: 0.8438 - val_loss: 0.0980 - val_pos_accuracy: 0.8800\n",
      "Epoch 25/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0573 - pos_accuracy: 0.9181 - val_loss: 0.1042 - val_pos_accuracy: 0.8650\n",
      "Epoch 26/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0407 - pos_accuracy: 0.9481 - val_loss: 0.1005 - val_pos_accuracy: 0.8700\n",
      "Epoch 27/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0351 - pos_accuracy: 0.9625 - val_loss: 0.1266 - val_pos_accuracy: 0.8950\n",
      "Epoch 28/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0360 - pos_accuracy: 0.9588 - val_loss: 0.0985 - val_pos_accuracy: 0.8725\n",
      "Epoch 29/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0349 - pos_accuracy: 0.9625 - val_loss: 0.0978 - val_pos_accuracy: 0.8825\n",
      "Epoch 30/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0413 - pos_accuracy: 0.9475 - val_loss: 0.0985 - val_pos_accuracy: 0.8950\n",
      "Epoch 31/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0282 - pos_accuracy: 0.9744 - val_loss: 0.0921 - val_pos_accuracy: 0.8925\n",
      "Epoch 32/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0177 - pos_accuracy: 0.9900 - val_loss: 0.0799 - val_pos_accuracy: 0.9100\n",
      "Epoch 33/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0179 - pos_accuracy: 0.9875 - val_loss: 0.0798 - val_pos_accuracy: 0.9000\n",
      "Epoch 34/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0212 - pos_accuracy: 0.9862 - val_loss: 0.0851 - val_pos_accuracy: 0.9075\n",
      "Epoch 35/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0156 - pos_accuracy: 0.9900 - val_loss: 0.0947 - val_pos_accuracy: 0.8675\n",
      "Epoch 36/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0159 - pos_accuracy: 0.9900 - val_loss: 0.0806 - val_pos_accuracy: 0.8950\n",
      "Epoch 37/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0187 - pos_accuracy: 0.9844 - val_loss: 0.0790 - val_pos_accuracy: 0.9075\n",
      "Epoch 38/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0173 - pos_accuracy: 0.9862 - val_loss: 0.0816 - val_pos_accuracy: 0.9000\n",
      "Epoch 39/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0122 - pos_accuracy: 0.9925 - val_loss: 0.0790 - val_pos_accuracy: 0.9125\n",
      "Epoch 40/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0111 - pos_accuracy: 0.9950 - val_loss: 0.0805 - val_pos_accuracy: 0.9125\n",
      "Epoch 41/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0103 - pos_accuracy: 0.9944 - val_loss: 0.0748 - val_pos_accuracy: 0.9075\n",
      "Epoch 42/46\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0188 - pos_accuracy: 0.9794 - val_loss: 0.1060 - val_pos_accuracy: 0.8925\n",
      "Epoch 43/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0099 - pos_accuracy: 0.9937 - val_loss: 0.0778 - val_pos_accuracy: 0.8925\n",
      "Epoch 44/46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0091 - pos_accuracy: 0.9931 - val_loss: 0.0769 - val_pos_accuracy: 0.9000\n",
      "Epoch 45/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0132 - pos_accuracy: 0.9869 - val_loss: 0.0737 - val_pos_accuracy: 0.9075\n",
      "Epoch 46/46\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0228 - pos_accuracy: 0.9762 - val_loss: 0.0747 - val_pos_accuracy: 0.9100\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:00:46.294803: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1682: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n(답변)\\n=> 두개의 행렬, 테이블, 배열을 join하는 역할\\n=> axis=0 인 경우에는 행방향으로 concat이 되고, axis=1 인 경우에는 열방향으로 concat이 됩니다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n(답변)\\n=> 121은 (1,2), 즉 1개의 행과 2개의 열이 있는 그래프 공간을 만들고 그 중 좌측기준 1번째 공간에 서브플롯을 그린다는 의미입니다.\\n=> 122는 상기와 동일한 그래프 공간에서 좌측 기준 2번째 공간에 서브플롯을 그래겠다는 의미입니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n(답변)\\n=> elements간 boolean값에 대한 AND연산을 의미하고, axis는 어느 축으로 연산을 할 것인지 결정한다. 0은 x축(행) 기준, 1은 y축(열) 기준\\n=> 그러므로 모델 평가시 y햇과 ground truth와의 값이 서로 같으면 1(True), 다르면 0(False)를 출력해주는 '채점자' 역할을 한다고 보면 됩니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n(답변)\\n=> elements간 값들의 평균을 구하는 코드로서, y햇과 ground truth간 채점 결과(예를 들어, [1,0,1,1,0,...])를 평균내는 역할을 합니다.\\n=> 즉, 모든 observations 혹은 predictions에 대한 평균을 구합니다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n(답변)\\n=> 해당 문제는 분류가 아닌 회귀문제이며, 회귀에서는 최종 출력층에서 마지막까지 계산된 값을 변환하지 않고 그대로 사용해야 하므로 항등 함수(identity function)를 사용하기 때문입니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n(답변)\\n=> 컴퓨터에서 랜덤값을 부여할 때, 매번 같은 seed value(초기 랜덤값)를 부여하여 하이퍼파라미터 조정시, 모델 평가에 대하여 experimentation의 객관성과 공정성을 부여하기 위함입니다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.9275\n",
      "}\"report1/박현준_46018.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/박현준_46018.ipynb to python\n",
      "[NbConvertApp] Writing 34852 bytes to report1/박현준_46018.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:246: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -13. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 296228.83it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:00:53.613010: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:00:53.996187: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:00:53.996231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:00:54.221018: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:00:54.849860: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 95647.90it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:00:57.122097: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7986928104575165\n",
      "convolution result = 0.8017429\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1680: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 67.27272727272728,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/김선호_46079.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김선호_46079.ipynb to python\n",
      "[NbConvertApp] Writing 34563 bytes to report1/김선호_46079.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.4]\n",
      " [4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:248: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  1. ]\n",
      " [0.  1.2 1. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 290524.62it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:01:03.544677: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:01:03.924938: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:01:03.924976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:01:04.149017: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:01:04.751736: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 91525.73it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:01:06.697383: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans02 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1628: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\nconcatenate 함수응 Numpy 배열들을 하나로 합치는데 이용이 되며 axis=1은 수평 방향을 의미해서 hstack과 동일한 결과를 나타냄\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n121의 의미는 1행 2열 1번째로 배치시기겠다이며, 122는 당연히 1행 2열 2번째로 배치시기겠다는 의미\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntf.reduce_all: AND 연산 \\\"axis=1은 열을 기준으로 동작하는 것\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\ntf.reduce_mean :  배치(batch)의 모든 예시에 대한 평균을 계산.\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n기본적으로 tf.keras.layers.Dense는 활성화 함수를 사용하지 않으며 신경망의 출력 (최종 출력층)은 실제로 이전 레이어 입력의 선형 조합일 뿐이며 회귀 문제에 적합해야 한다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n적절하게 random성을 제어해야 할 때가 있으며 np.random.seed()를 고정한 상태로 알고리즘을 실행하여, 난수의 생성 패턴을 동일하게 관리\",\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 50.72727272727273,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/유승진_46021.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/유승진_46021.ipynb to python\n",
      "[NbConvertApp] Writing 36456 bytes to report1/유승진_46021.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:245: RuntimeWarning: invalid value encountered in subtract\n",
      "  if((x - x)[0] != 0 or (x - x)[1] != 0):\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:269: RuntimeWarning: invalid value encountered in subtract\n",
      "  if((x - x)[0] != 0 or (x - x)[1] != 0):\n",
      "51\n",
      "421\n",
      "(2, 2)\n",
      "(3, 2)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 305195.66it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:01:13.204395: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:01:13.584487: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:01:13.584524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:01:13.806372: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:01:14.436401: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 7ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 113796.30it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                5160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 82        \n",
      "=================================================================\n",
      "Total params: 105,722\n",
      "Trainable params: 105,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 60.1120 - pos_accuracy: 0.0119 - val_loss: 21.9364 - val_pos_accuracy: 0.0325\n",
      "Epoch 2/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.6874 - pos_accuracy: 0.1187 - val_loss: 2.2929 - val_pos_accuracy: 0.2075\n",
      "Epoch 3/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4620 - pos_accuracy: 0.2319 - val_loss: 1.3167 - val_pos_accuracy: 0.2375\n",
      "Epoch 4/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9316 - pos_accuracy: 0.2987 - val_loss: 0.9469 - val_pos_accuracy: 0.3850\n",
      "Epoch 5/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7008 - pos_accuracy: 0.3931 - val_loss: 0.8815 - val_pos_accuracy: 0.2325\n",
      "Epoch 6/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6692 - pos_accuracy: 0.2706 - val_loss: 0.8248 - val_pos_accuracy: 0.2225\n",
      "Epoch 7/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5750 - pos_accuracy: 0.3525 - val_loss: 0.9918 - val_pos_accuracy: 0.2350\n",
      "Epoch 8/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6985 - pos_accuracy: 0.2906 - val_loss: 0.5625 - val_pos_accuracy: 0.4325\n",
      "Epoch 9/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3943 - pos_accuracy: 0.5306 - val_loss: 0.5063 - val_pos_accuracy: 0.4825\n",
      "Epoch 10/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3820 - pos_accuracy: 0.5050 - val_loss: 0.4620 - val_pos_accuracy: 0.5250\n",
      "Epoch 11/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3285 - pos_accuracy: 0.5819 - val_loss: 0.4453 - val_pos_accuracy: 0.5025\n",
      "Epoch 12/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3183 - pos_accuracy: 0.5612 - val_loss: 0.4794 - val_pos_accuracy: 0.4200\n",
      "Epoch 13/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3441 - pos_accuracy: 0.4775 - val_loss: 0.5006 - val_pos_accuracy: 0.3425\n",
      "Epoch 14/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3976 - pos_accuracy: 0.3725 - val_loss: 0.4524 - val_pos_accuracy: 0.4375\n",
      "Epoch 15/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3513 - pos_accuracy: 0.4263 - val_loss: 0.3732 - val_pos_accuracy: 0.5950\n",
      "Epoch 16/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2455 - pos_accuracy: 0.6494 - val_loss: 0.3711 - val_pos_accuracy: 0.5500\n",
      "Epoch 17/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2278 - pos_accuracy: 0.6681 - val_loss: 0.3502 - val_pos_accuracy: 0.5825\n",
      "Epoch 18/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2959 - pos_accuracy: 0.5306 - val_loss: 0.4490 - val_pos_accuracy: 0.4050\n",
      "Epoch 19/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2673 - pos_accuracy: 0.5719 - val_loss: 0.3083 - val_pos_accuracy: 0.6350\n",
      "Epoch 20/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1913 - pos_accuracy: 0.7094 - val_loss: 0.2993 - val_pos_accuracy: 0.6425\n",
      "Epoch 21/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1968 - pos_accuracy: 0.6944 - val_loss: 0.3063 - val_pos_accuracy: 0.6150\n",
      "Epoch 22/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2049 - pos_accuracy: 0.6631 - val_loss: 0.3136 - val_pos_accuracy: 0.5425\n",
      "Epoch 23/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1793 - pos_accuracy: 0.6988 - val_loss: 0.2795 - val_pos_accuracy: 0.6475\n",
      "Epoch 24/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1704 - pos_accuracy: 0.7250 - val_loss: 0.2663 - val_pos_accuracy: 0.6550\n",
      "Epoch 25/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3028 - pos_accuracy: 0.5381 - val_loss: 0.8068 - val_pos_accuracy: 0.1675\n",
      "Epoch 26/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3189 - pos_accuracy: 0.5394 - val_loss: 0.2855 - val_pos_accuracy: 0.5850\n",
      "Epoch 27/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1680 - pos_accuracy: 0.7169 - val_loss: 0.2519 - val_pos_accuracy: 0.6475\n",
      "Epoch 28/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1744 - pos_accuracy: 0.7063 - val_loss: 0.2452 - val_pos_accuracy: 0.6650\n",
      "Epoch 29/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1388 - pos_accuracy: 0.7644 - val_loss: 0.2341 - val_pos_accuracy: 0.6675\n",
      "Epoch 30/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1494 - pos_accuracy: 0.7487 - val_loss: 0.3497 - val_pos_accuracy: 0.3900\n",
      "Epoch 31/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1715 - pos_accuracy: 0.6838 - val_loss: 0.2185 - val_pos_accuracy: 0.6950\n",
      "Epoch 32/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1312 - pos_accuracy: 0.7837 - val_loss: 0.2566 - val_pos_accuracy: 0.6500\n",
      "Epoch 33/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1250 - pos_accuracy: 0.7994 - val_loss: 0.2056 - val_pos_accuracy: 0.7000\n",
      "Epoch 34/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1428 - pos_accuracy: 0.7588 - val_loss: 0.2296 - val_pos_accuracy: 0.6775\n",
      "Epoch 35/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2078 - pos_accuracy: 0.5794 - val_loss: 0.2774 - val_pos_accuracy: 0.5100\n",
      "Epoch 36/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1288 - pos_accuracy: 0.7719 - val_loss: 0.2079 - val_pos_accuracy: 0.7000\n",
      "Epoch 37/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1441 - pos_accuracy: 0.7406 - val_loss: 0.2089 - val_pos_accuracy: 0.6750\n",
      "Epoch 38/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1406 - pos_accuracy: 0.7850 - val_loss: 0.2450 - val_pos_accuracy: 0.6125\n",
      "Epoch 39/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1155 - pos_accuracy: 0.8094 - val_loss: 0.1892 - val_pos_accuracy: 0.7200\n",
      "Epoch 40/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1149 - pos_accuracy: 0.8256 - val_loss: 0.1828 - val_pos_accuracy: 0.7200\n",
      "Epoch 41/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0944 - pos_accuracy: 0.8438 - val_loss: 0.1755 - val_pos_accuracy: 0.7250\n",
      "Epoch 42/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1047 - pos_accuracy: 0.8413 - val_loss: 0.1915 - val_pos_accuracy: 0.7100\n",
      "Epoch 43/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0926 - pos_accuracy: 0.8531 - val_loss: 0.1665 - val_pos_accuracy: 0.7300\n",
      "Epoch 44/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0849 - pos_accuracy: 0.8644 - val_loss: 0.1728 - val_pos_accuracy: 0.7550\n",
      "Epoch 45/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0871 - pos_accuracy: 0.8725 - val_loss: 0.1839 - val_pos_accuracy: 0.7300\n",
      "Epoch 46/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1359 - pos_accuracy: 0.7825 - val_loss: 0.2478 - val_pos_accuracy: 0.5625\n",
      "Epoch 47/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1054 - pos_accuracy: 0.8456 - val_loss: 0.1863 - val_pos_accuracy: 0.7425\n",
      "Epoch 48/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0970 - pos_accuracy: 0.8631 - val_loss: 0.1633 - val_pos_accuracy: 0.7725\n",
      "Epoch 49/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0775 - pos_accuracy: 0.8856 - val_loss: 0.1560 - val_pos_accuracy: 0.7675\n",
      "Epoch 50/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0772 - pos_accuracy: 0.8875 - val_loss: 0.1517 - val_pos_accuracy: 0.7700\n",
      "Epoch 51/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0942 - pos_accuracy: 0.8519 - val_loss: 0.1923 - val_pos_accuracy: 0.7500\n",
      "Epoch 52/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0806 - pos_accuracy: 0.8875 - val_loss: 0.1606 - val_pos_accuracy: 0.7825\n",
      "Epoch 53/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0844 - pos_accuracy: 0.8831 - val_loss: 0.1526 - val_pos_accuracy: 0.7850\n",
      "Epoch 54/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1324 - pos_accuracy: 0.7919 - val_loss: 0.2973 - val_pos_accuracy: 0.4725\n",
      "Epoch 55/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2683 - pos_accuracy: 0.5481 - val_loss: 0.1533 - val_pos_accuracy: 0.7875\n",
      "Epoch 56/400\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0691 - pos_accuracy: 0.8975 - val_loss: 0.1393 - val_pos_accuracy: 0.7925\n",
      "Epoch 57/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0621 - pos_accuracy: 0.9025 - val_loss: 0.1346 - val_pos_accuracy: 0.7925\n",
      "Epoch 58/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0643 - pos_accuracy: 0.9025 - val_loss: 0.1413 - val_pos_accuracy: 0.7975\n",
      "Epoch 59/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0675 - pos_accuracy: 0.9031 - val_loss: 0.1461 - val_pos_accuracy: 0.7875\n",
      "Epoch 60/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1034 - pos_accuracy: 0.8400 - val_loss: 0.1850 - val_pos_accuracy: 0.6650\n",
      "Epoch 61/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1724 - pos_accuracy: 0.6256 - val_loss: 0.2913 - val_pos_accuracy: 0.4050\n",
      "Epoch 62/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2005 - pos_accuracy: 0.5756 - val_loss: 0.1765 - val_pos_accuracy: 0.6825\n",
      "Epoch 63/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0629 - pos_accuracy: 0.9019 - val_loss: 0.1276 - val_pos_accuracy: 0.8125\n",
      "Epoch 64/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0622 - pos_accuracy: 0.9100 - val_loss: 0.1274 - val_pos_accuracy: 0.8300\n",
      "Epoch 65/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0565 - pos_accuracy: 0.9175 - val_loss: 0.1239 - val_pos_accuracy: 0.8200\n",
      "Epoch 66/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0577 - pos_accuracy: 0.9256 - val_loss: 0.1192 - val_pos_accuracy: 0.8275\n",
      "Epoch 67/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0509 - pos_accuracy: 0.9269 - val_loss: 0.1260 - val_pos_accuracy: 0.8350\n",
      "Epoch 68/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0489 - pos_accuracy: 0.9287 - val_loss: 0.1126 - val_pos_accuracy: 0.8325\n",
      "Epoch 69/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0466 - pos_accuracy: 0.9300 - val_loss: 0.1123 - val_pos_accuracy: 0.8400\n",
      "Epoch 70/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0450 - pos_accuracy: 0.9337 - val_loss: 0.1109 - val_pos_accuracy: 0.8400\n",
      "Epoch 71/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0440 - pos_accuracy: 0.9369 - val_loss: 0.1072 - val_pos_accuracy: 0.8425\n",
      "Epoch 72/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0417 - pos_accuracy: 0.9375 - val_loss: 0.1087 - val_pos_accuracy: 0.8450\n",
      "Epoch 73/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0561 - pos_accuracy: 0.9275 - val_loss: 0.1130 - val_pos_accuracy: 0.8475\n",
      "Epoch 74/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0521 - pos_accuracy: 0.9362 - val_loss: 0.1077 - val_pos_accuracy: 0.8350\n",
      "Epoch 75/400\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0501 - pos_accuracy: 0.9400 - val_loss: 0.1155 - val_pos_accuracy: 0.8300\n",
      "Epoch 76/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0413 - pos_accuracy: 0.9456 - val_loss: 0.1052 - val_pos_accuracy: 0.8450\n",
      "Epoch 77/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0383 - pos_accuracy: 0.9456 - val_loss: 0.1007 - val_pos_accuracy: 0.8575\n",
      "Epoch 78/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0365 - pos_accuracy: 0.9456 - val_loss: 0.1000 - val_pos_accuracy: 0.8575\n",
      "Epoch 79/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0439 - pos_accuracy: 0.9487 - val_loss: 0.1143 - val_pos_accuracy: 0.8550\n",
      "Epoch 80/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0593 - pos_accuracy: 0.9300 - val_loss: 0.1591 - val_pos_accuracy: 0.6900\n",
      "Epoch 81/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1135 - pos_accuracy: 0.7788 - val_loss: 0.1122 - val_pos_accuracy: 0.8475\n",
      "Epoch 82/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0521 - pos_accuracy: 0.9438 - val_loss: 0.0983 - val_pos_accuracy: 0.8475\n",
      "Epoch 83/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0407 - pos_accuracy: 0.9531 - val_loss: 0.1008 - val_pos_accuracy: 0.8500\n",
      "Epoch 84/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0431 - pos_accuracy: 0.9537 - val_loss: 0.1016 - val_pos_accuracy: 0.8600\n",
      "Epoch 85/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0413 - pos_accuracy: 0.9512 - val_loss: 0.0947 - val_pos_accuracy: 0.8575\n",
      "Epoch 86/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0312 - pos_accuracy: 0.9556 - val_loss: 0.0937 - val_pos_accuracy: 0.8675\n",
      "Epoch 87/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0312 - pos_accuracy: 0.9556 - val_loss: 0.0920 - val_pos_accuracy: 0.8675\n",
      "Epoch 88/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0306 - pos_accuracy: 0.9581 - val_loss: 0.0908 - val_pos_accuracy: 0.8700\n",
      "Epoch 89/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0302 - pos_accuracy: 0.9581 - val_loss: 0.0914 - val_pos_accuracy: 0.8675\n",
      "Epoch 90/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0302 - pos_accuracy: 0.9569 - val_loss: 0.0886 - val_pos_accuracy: 0.8700\n",
      "Epoch 91/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0401 - pos_accuracy: 0.9550 - val_loss: 0.1023 - val_pos_accuracy: 0.8525\n",
      "Epoch 92/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0423 - pos_accuracy: 0.9556 - val_loss: 0.0902 - val_pos_accuracy: 0.8700\n",
      "Epoch 93/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0417 - pos_accuracy: 0.9512 - val_loss: 0.1016 - val_pos_accuracy: 0.8700\n",
      "Epoch 94/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0331 - pos_accuracy: 0.9594 - val_loss: 0.0922 - val_pos_accuracy: 0.8675\n",
      "Epoch 95/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0381 - pos_accuracy: 0.9613 - val_loss: 0.1073 - val_pos_accuracy: 0.8575\n",
      "Epoch 96/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0663 - pos_accuracy: 0.9419 - val_loss: 0.1137 - val_pos_accuracy: 0.8525\n",
      "Epoch 97/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0594 - pos_accuracy: 0.9188 - val_loss: 0.1422 - val_pos_accuracy: 0.7675\n",
      "Epoch 98/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0632 - pos_accuracy: 0.9125 - val_loss: 0.1185 - val_pos_accuracy: 0.8150\n",
      "Epoch 99/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0469 - pos_accuracy: 0.9462 - val_loss: 0.1066 - val_pos_accuracy: 0.8450\n",
      "Epoch 100/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0587 - pos_accuracy: 0.9262 - val_loss: 0.1174 - val_pos_accuracy: 0.8225\n",
      "Epoch 101/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0374 - pos_accuracy: 0.9594 - val_loss: 0.0998 - val_pos_accuracy: 0.8650\n",
      "Epoch 102/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0307 - pos_accuracy: 0.9694 - val_loss: 0.0825 - val_pos_accuracy: 0.8800\n",
      "Epoch 103/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0239 - pos_accuracy: 0.9681 - val_loss: 0.0843 - val_pos_accuracy: 0.8800\n",
      "Epoch 104/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0255 - pos_accuracy: 0.9719 - val_loss: 0.0803 - val_pos_accuracy: 0.8825\n",
      "Epoch 105/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0242 - pos_accuracy: 0.9700 - val_loss: 0.0915 - val_pos_accuracy: 0.8850\n",
      "Epoch 106/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0324 - pos_accuracy: 0.9737 - val_loss: 0.0958 - val_pos_accuracy: 0.8725\n",
      "Epoch 107/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0400 - pos_accuracy: 0.9619 - val_loss: 0.0849 - val_pos_accuracy: 0.8875\n",
      "Epoch 108/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0234 - pos_accuracy: 0.9744 - val_loss: 0.0780 - val_pos_accuracy: 0.8725\n",
      "Epoch 109/400\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0213 - pos_accuracy: 0.9781 - val_loss: 0.0787 - val_pos_accuracy: 0.8825\n",
      "Epoch 110/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0235 - pos_accuracy: 0.9769 - val_loss: 0.0801 - val_pos_accuracy: 0.8850\n",
      "Epoch 111/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0214 - pos_accuracy: 0.9806 - val_loss: 0.0832 - val_pos_accuracy: 0.8850\n",
      "Epoch 112/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0336 - pos_accuracy: 0.9706 - val_loss: 0.1026 - val_pos_accuracy: 0.8375\n",
      "Epoch 113/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0910 - pos_accuracy: 0.8000 - val_loss: 0.1245 - val_pos_accuracy: 0.7575\n",
      "Epoch 114/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1079 - pos_accuracy: 0.7519 - val_loss: 0.1987 - val_pos_accuracy: 0.5725\n",
      "Epoch 115/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0490 - pos_accuracy: 0.9200 - val_loss: 0.0769 - val_pos_accuracy: 0.8775\n",
      "Epoch 116/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0199 - pos_accuracy: 0.9812 - val_loss: 0.0746 - val_pos_accuracy: 0.8900\n",
      "Epoch 117/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0181 - pos_accuracy: 0.9794 - val_loss: 0.0733 - val_pos_accuracy: 0.8875\n",
      "Epoch 118/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0171 - pos_accuracy: 0.9825 - val_loss: 0.0723 - val_pos_accuracy: 0.8925\n",
      "Epoch 119/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0173 - pos_accuracy: 0.9831 - val_loss: 0.0730 - val_pos_accuracy: 0.8925\n",
      "Epoch 120/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0182 - pos_accuracy: 0.9825 - val_loss: 0.0729 - val_pos_accuracy: 0.8875\n",
      "Epoch 121/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0218 - pos_accuracy: 0.9831 - val_loss: 0.0878 - val_pos_accuracy: 0.9100\n",
      "Epoch 122/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0359 - pos_accuracy: 0.9713 - val_loss: 0.0723 - val_pos_accuracy: 0.8975\n",
      "Epoch 123/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0166 - pos_accuracy: 0.9825 - val_loss: 0.0761 - val_pos_accuracy: 0.8975\n",
      "Epoch 124/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0262 - pos_accuracy: 0.9775 - val_loss: 0.1025 - val_pos_accuracy: 0.8675\n",
      "Epoch 125/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0528 - pos_accuracy: 0.9375 - val_loss: 0.1027 - val_pos_accuracy: 0.8625\n",
      "Epoch 126/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0563 - pos_accuracy: 0.9294 - val_loss: 0.0864 - val_pos_accuracy: 0.8925\n",
      "Epoch 127/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0329 - pos_accuracy: 0.9787 - val_loss: 0.0836 - val_pos_accuracy: 0.8950\n",
      "Epoch 128/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0192 - pos_accuracy: 0.9850 - val_loss: 0.0702 - val_pos_accuracy: 0.8925\n",
      "Epoch 129/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0147 - pos_accuracy: 0.9856 - val_loss: 0.0696 - val_pos_accuracy: 0.8900\n",
      "Epoch 130/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0140 - pos_accuracy: 0.9844 - val_loss: 0.0678 - val_pos_accuracy: 0.9000\n",
      "Epoch 131/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0137 - pos_accuracy: 0.9875 - val_loss: 0.0676 - val_pos_accuracy: 0.9000\n",
      "Epoch 132/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0141 - pos_accuracy: 0.9850 - val_loss: 0.0715 - val_pos_accuracy: 0.9025\n",
      "Epoch 133/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0263 - pos_accuracy: 0.9856 - val_loss: 0.0765 - val_pos_accuracy: 0.8900\n",
      "Epoch 134/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0242 - pos_accuracy: 0.9862 - val_loss: 0.0760 - val_pos_accuracy: 0.9000\n",
      "Epoch 135/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0211 - pos_accuracy: 0.9869 - val_loss: 0.0751 - val_pos_accuracy: 0.9050\n",
      "Epoch 136/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0271 - pos_accuracy: 0.9856 - val_loss: 0.0750 - val_pos_accuracy: 0.9050\n",
      "Epoch 137/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0337 - pos_accuracy: 0.9719 - val_loss: 0.0852 - val_pos_accuracy: 0.9000\n",
      "Epoch 138/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0250 - pos_accuracy: 0.9856 - val_loss: 0.0905 - val_pos_accuracy: 0.9000\n",
      "Epoch 139/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0570 - pos_accuracy: 0.9475 - val_loss: 0.1108 - val_pos_accuracy: 0.8350\n",
      "Epoch 140/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0519 - pos_accuracy: 0.9450 - val_loss: 0.0900 - val_pos_accuracy: 0.8925\n",
      "Epoch 141/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0667 - pos_accuracy: 0.9175 - val_loss: 0.1040 - val_pos_accuracy: 0.8600\n",
      "Epoch 142/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0613 - pos_accuracy: 0.9162 - val_loss: 0.1139 - val_pos_accuracy: 0.8025\n",
      "Epoch 143/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0210 - pos_accuracy: 0.9844 - val_loss: 0.0662 - val_pos_accuracy: 0.8975\n",
      "Epoch 144/400\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0202 - pos_accuracy: 0.9875 - val_loss: 0.0704 - val_pos_accuracy: 0.9025\n",
      "Epoch 145/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0149 - pos_accuracy: 0.9869 - val_loss: 0.0671 - val_pos_accuracy: 0.9025\n",
      "Epoch 146/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0123 - pos_accuracy: 0.9875 - val_loss: 0.0653 - val_pos_accuracy: 0.9125\n",
      "Epoch 147/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0159 - pos_accuracy: 0.9881 - val_loss: 0.0698 - val_pos_accuracy: 0.9025\n",
      "Epoch 148/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0158 - pos_accuracy: 0.9875 - val_loss: 0.0669 - val_pos_accuracy: 0.9125\n",
      "Epoch 149/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0201 - pos_accuracy: 0.9869 - val_loss: 0.0651 - val_pos_accuracy: 0.9125\n",
      "Epoch 150/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0118 - pos_accuracy: 0.9881 - val_loss: 0.0666 - val_pos_accuracy: 0.9100\n",
      "Epoch 151/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0162 - pos_accuracy: 0.9881 - val_loss: 0.0681 - val_pos_accuracy: 0.9050\n",
      "Epoch 152/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0172 - pos_accuracy: 0.9881 - val_loss: 0.0665 - val_pos_accuracy: 0.9075\n",
      "Epoch 153/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0132 - pos_accuracy: 0.9881 - val_loss: 0.0659 - val_pos_accuracy: 0.9125\n",
      "Epoch 154/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0202 - pos_accuracy: 0.9881 - val_loss: 0.0922 - val_pos_accuracy: 0.8650\n",
      "Epoch 155/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0418 - pos_accuracy: 0.9488 - val_loss: 0.0722 - val_pos_accuracy: 0.9050\n",
      "Epoch 156/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0173 - pos_accuracy: 0.9875 - val_loss: 0.0744 - val_pos_accuracy: 0.9125\n",
      "Epoch 157/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0151 - pos_accuracy: 0.9875 - val_loss: 0.0644 - val_pos_accuracy: 0.9150\n",
      "Epoch 158/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0143 - pos_accuracy: 0.9881 - val_loss: 0.0674 - val_pos_accuracy: 0.9050\n",
      "Epoch 159/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0206 - pos_accuracy: 0.9869 - val_loss: 0.0703 - val_pos_accuracy: 0.9100\n",
      "Epoch 160/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0249 - pos_accuracy: 0.9806 - val_loss: 0.0883 - val_pos_accuracy: 0.8625\n",
      "Epoch 161/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0137 - pos_accuracy: 0.9887 - val_loss: 0.0629 - val_pos_accuracy: 0.9100\n",
      "Epoch 162/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0113 - pos_accuracy: 0.9888 - val_loss: 0.0616 - val_pos_accuracy: 0.9150\n",
      "Epoch 163/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0181 - pos_accuracy: 0.9881 - val_loss: 0.0771 - val_pos_accuracy: 0.9000\n",
      "Epoch 164/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0276 - pos_accuracy: 0.9863 - val_loss: 0.0695 - val_pos_accuracy: 0.9125\n",
      "Epoch 165/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0144 - pos_accuracy: 0.9887 - val_loss: 0.0593 - val_pos_accuracy: 0.9075\n",
      "Epoch 166/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0087 - pos_accuracy: 0.9894 - val_loss: 0.0592 - val_pos_accuracy: 0.9050\n",
      "Epoch 167/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0090 - pos_accuracy: 0.9894 - val_loss: 0.0603 - val_pos_accuracy: 0.9025\n",
      "Epoch 168/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0101 - pos_accuracy: 0.9894 - val_loss: 0.0628 - val_pos_accuracy: 0.9000\n",
      "Epoch 169/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0127 - pos_accuracy: 0.9887 - val_loss: 0.0642 - val_pos_accuracy: 0.8950\n",
      "Epoch 170/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0139 - pos_accuracy: 0.9894 - val_loss: 0.0658 - val_pos_accuracy: 0.8925\n",
      "Epoch 171/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0234 - pos_accuracy: 0.9881 - val_loss: 0.0852 - val_pos_accuracy: 0.8825\n",
      "Epoch 172/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0467 - pos_accuracy: 0.9531 - val_loss: 0.0774 - val_pos_accuracy: 0.9000\n",
      "Epoch 173/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0234 - pos_accuracy: 0.9844 - val_loss: 0.0827 - val_pos_accuracy: 0.8850\n",
      "Epoch 174/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0350 - pos_accuracy: 0.9688 - val_loss: 0.0813 - val_pos_accuracy: 0.8900\n",
      "Epoch 175/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0235 - pos_accuracy: 0.9831 - val_loss: 0.0811 - val_pos_accuracy: 0.9075\n",
      "Epoch 176/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0243 - pos_accuracy: 0.9894 - val_loss: 0.0718 - val_pos_accuracy: 0.9100\n",
      "Epoch 177/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0179 - pos_accuracy: 0.9875 - val_loss: 0.0775 - val_pos_accuracy: 0.9000\n",
      "Epoch 178/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0586 - pos_accuracy: 0.8888 - val_loss: 0.1118 - val_pos_accuracy: 0.8050\n",
      "Epoch 179/400\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0533 - pos_accuracy: 0.9144 - val_loss: 0.1309 - val_pos_accuracy: 0.7750\n",
      "Epoch 180/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0686 - pos_accuracy: 0.8681 - val_loss: 0.0851 - val_pos_accuracy: 0.8750\n",
      "Epoch 181/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0439 - pos_accuracy: 0.9469 - val_loss: 0.0882 - val_pos_accuracy: 0.8625\n",
      "Epoch 182/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0224 - pos_accuracy: 0.9806 - val_loss: 0.0608 - val_pos_accuracy: 0.9125\n",
      "Epoch 183/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0115 - pos_accuracy: 0.9894 - val_loss: 0.0659 - val_pos_accuracy: 0.9075\n",
      "Epoch 184/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0125 - pos_accuracy: 0.9894 - val_loss: 0.0624 - val_pos_accuracy: 0.9125\n",
      "Epoch 185/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0125 - pos_accuracy: 0.9894 - val_loss: 0.0571 - val_pos_accuracy: 0.9125\n",
      "Epoch 186/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0079 - pos_accuracy: 0.9894 - val_loss: 0.0562 - val_pos_accuracy: 0.9050\n",
      "Epoch 187/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0077 - pos_accuracy: 0.9894 - val_loss: 0.0566 - val_pos_accuracy: 0.9075\n",
      "Epoch 188/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0073 - pos_accuracy: 0.9894 - val_loss: 0.0557 - val_pos_accuracy: 0.9100\n",
      "Epoch 189/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0066 - pos_accuracy: 0.9894 - val_loss: 0.0555 - val_pos_accuracy: 0.9075\n",
      "Epoch 190/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0071 - pos_accuracy: 0.9894 - val_loss: 0.0557 - val_pos_accuracy: 0.9075\n",
      "Epoch 191/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0070 - pos_accuracy: 0.9894 - val_loss: 0.0549 - val_pos_accuracy: 0.9100\n",
      "Epoch 192/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0068 - pos_accuracy: 0.9894 - val_loss: 0.0559 - val_pos_accuracy: 0.9050\n",
      "Epoch 193/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0070 - pos_accuracy: 0.9894 - val_loss: 0.0561 - val_pos_accuracy: 0.9075\n",
      "Epoch 194/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0075 - pos_accuracy: 0.9894 - val_loss: 0.0550 - val_pos_accuracy: 0.9100\n",
      "Epoch 195/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0064 - pos_accuracy: 0.9900 - val_loss: 0.0555 - val_pos_accuracy: 0.9100\n",
      "Epoch 196/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0087 - pos_accuracy: 0.9900 - val_loss: 0.0551 - val_pos_accuracy: 0.9100\n",
      "Epoch 197/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0062 - pos_accuracy: 0.9900 - val_loss: 0.0546 - val_pos_accuracy: 0.9100\n",
      "Epoch 198/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0059 - pos_accuracy: 0.9900 - val_loss: 0.0544 - val_pos_accuracy: 0.9100\n",
      "Epoch 199/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9900 - val_loss: 0.0547 - val_pos_accuracy: 0.9075\n",
      "Epoch 200/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9900 - val_loss: 0.0549 - val_pos_accuracy: 0.9050\n",
      "Epoch 201/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0083 - pos_accuracy: 0.9894 - val_loss: 0.0570 - val_pos_accuracy: 0.9050\n",
      "Epoch 202/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0082 - pos_accuracy: 0.9900 - val_loss: 0.0555 - val_pos_accuracy: 0.9100\n",
      "Epoch 203/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0067 - pos_accuracy: 0.9900 - val_loss: 0.0570 - val_pos_accuracy: 0.9075\n",
      "Epoch 204/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0068 - pos_accuracy: 0.9900 - val_loss: 0.0536 - val_pos_accuracy: 0.9075\n",
      "Epoch 205/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0060 - pos_accuracy: 0.9894 - val_loss: 0.0545 - val_pos_accuracy: 0.9075\n",
      "Epoch 206/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0058 - pos_accuracy: 0.9906 - val_loss: 0.0539 - val_pos_accuracy: 0.9100\n",
      "Epoch 207/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0056 - pos_accuracy: 0.9900 - val_loss: 0.0535 - val_pos_accuracy: 0.9150\n",
      "Epoch 208/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0057 - pos_accuracy: 0.9906 - val_loss: 0.0535 - val_pos_accuracy: 0.9125\n",
      "Epoch 209/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0053 - pos_accuracy: 0.9919 - val_loss: 0.0533 - val_pos_accuracy: 0.9150\n",
      "Epoch 210/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0052 - pos_accuracy: 0.9912 - val_loss: 0.0534 - val_pos_accuracy: 0.9150\n",
      "Epoch 211/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0060 - pos_accuracy: 0.9906 - val_loss: 0.0538 - val_pos_accuracy: 0.9225\n",
      "Epoch 212/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0053 - pos_accuracy: 0.9925 - val_loss: 0.0532 - val_pos_accuracy: 0.9150\n",
      "Epoch 213/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0056 - pos_accuracy: 0.9912 - val_loss: 0.0566 - val_pos_accuracy: 0.9200\n",
      "Epoch 214/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0112 - pos_accuracy: 0.9912 - val_loss: 0.0664 - val_pos_accuracy: 0.9050\n",
      "Epoch 215/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0262 - pos_accuracy: 0.9894 - val_loss: 0.0632 - val_pos_accuracy: 0.9075\n",
      "Epoch 216/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0110 - pos_accuracy: 0.9906 - val_loss: 0.0551 - val_pos_accuracy: 0.9275\n",
      "Epoch 217/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0063 - pos_accuracy: 0.9931 - val_loss: 0.0552 - val_pos_accuracy: 0.9200\n",
      "Epoch 218/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0075 - pos_accuracy: 0.9931 - val_loss: 0.0557 - val_pos_accuracy: 0.9200\n",
      "Epoch 219/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0071 - pos_accuracy: 0.9919 - val_loss: 0.0541 - val_pos_accuracy: 0.9275\n",
      "Epoch 220/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0067 - pos_accuracy: 0.9931 - val_loss: 0.0539 - val_pos_accuracy: 0.9175\n",
      "Epoch 221/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0057 - pos_accuracy: 0.9931 - val_loss: 0.0540 - val_pos_accuracy: 0.9275\n",
      "Epoch 222/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0062 - pos_accuracy: 0.9931 - val_loss: 0.0534 - val_pos_accuracy: 0.9250\n",
      "Epoch 223/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0062 - pos_accuracy: 0.9931 - val_loss: 0.0554 - val_pos_accuracy: 0.9250\n",
      "Epoch 224/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0091 - pos_accuracy: 0.9912 - val_loss: 0.0593 - val_pos_accuracy: 0.9150\n",
      "Epoch 225/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0154 - pos_accuracy: 0.9900 - val_loss: 0.0613 - val_pos_accuracy: 0.9100\n",
      "Epoch 226/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0190 - pos_accuracy: 0.9912 - val_loss: 0.0582 - val_pos_accuracy: 0.9225\n",
      "Epoch 227/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0313 - pos_accuracy: 0.9656 - val_loss: 0.0873 - val_pos_accuracy: 0.8800\n",
      "Epoch 228/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0678 - pos_accuracy: 0.8794 - val_loss: 0.1704 - val_pos_accuracy: 0.6350\n",
      "Epoch 229/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0735 - pos_accuracy: 0.8419 - val_loss: 0.0979 - val_pos_accuracy: 0.8075\n",
      "Epoch 230/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0464 - pos_accuracy: 0.9162 - val_loss: 0.0733 - val_pos_accuracy: 0.9125\n",
      "Epoch 231/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0196 - pos_accuracy: 0.9831 - val_loss: 0.0599 - val_pos_accuracy: 0.9125\n",
      "Epoch 232/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0091 - pos_accuracy: 0.9919 - val_loss: 0.0515 - val_pos_accuracy: 0.9125\n",
      "Epoch 233/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0054 - pos_accuracy: 0.9919 - val_loss: 0.0517 - val_pos_accuracy: 0.9175\n",
      "Epoch 234/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0048 - pos_accuracy: 0.9931 - val_loss: 0.0509 - val_pos_accuracy: 0.9200\n",
      "Epoch 235/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0052 - pos_accuracy: 0.9937 - val_loss: 0.0514 - val_pos_accuracy: 0.9175\n",
      "Epoch 236/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0048 - pos_accuracy: 0.9937 - val_loss: 0.0507 - val_pos_accuracy: 0.9200\n",
      "Epoch 237/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9937 - val_loss: 0.0512 - val_pos_accuracy: 0.9175\n",
      "Epoch 238/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0056 - pos_accuracy: 0.9937 - val_loss: 0.0512 - val_pos_accuracy: 0.9175\n",
      "Epoch 239/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0047 - pos_accuracy: 0.9937 - val_loss: 0.0512 - val_pos_accuracy: 0.9175\n",
      "Epoch 240/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0043 - pos_accuracy: 0.9937 - val_loss: 0.0507 - val_pos_accuracy: 0.9175\n",
      "Epoch 241/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0045 - pos_accuracy: 0.9937 - val_loss: 0.0504 - val_pos_accuracy: 0.9175\n",
      "Epoch 242/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0053 - pos_accuracy: 0.9937 - val_loss: 0.0511 - val_pos_accuracy: 0.9100\n",
      "Epoch 243/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0046 - pos_accuracy: 0.9937 - val_loss: 0.0505 - val_pos_accuracy: 0.9175\n",
      "Epoch 244/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0046 - pos_accuracy: 0.9937 - val_loss: 0.0509 - val_pos_accuracy: 0.9150\n",
      "Epoch 245/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0060 - pos_accuracy: 0.9937 - val_loss: 0.0520 - val_pos_accuracy: 0.9150\n",
      "Epoch 246/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0051 - pos_accuracy: 0.9937 - val_loss: 0.0506 - val_pos_accuracy: 0.9150\n",
      "Epoch 247/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0039 - pos_accuracy: 0.9944 - val_loss: 0.0502 - val_pos_accuracy: 0.9175\n",
      "Epoch 248/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0046 - pos_accuracy: 0.9944 - val_loss: 0.0508 - val_pos_accuracy: 0.9175\n",
      "Epoch 249/400\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0046 - pos_accuracy: 0.9937 - val_loss: 0.0503 - val_pos_accuracy: 0.9175\n",
      "Epoch 250/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0038 - pos_accuracy: 0.9944 - val_loss: 0.0500 - val_pos_accuracy: 0.9175\n",
      "Epoch 251/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0038 - pos_accuracy: 0.9937 - val_loss: 0.0498 - val_pos_accuracy: 0.9175\n",
      "Epoch 252/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9944 - val_loss: 0.0497 - val_pos_accuracy: 0.9175\n",
      "Epoch 253/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9944 - val_loss: 0.0496 - val_pos_accuracy: 0.9175\n",
      "Epoch 254/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0036 - pos_accuracy: 0.9944 - val_loss: 0.0498 - val_pos_accuracy: 0.9200\n",
      "Epoch 255/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9944 - val_loss: 0.0502 - val_pos_accuracy: 0.9275\n",
      "Epoch 256/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0044 - pos_accuracy: 0.9944 - val_loss: 0.0502 - val_pos_accuracy: 0.9275\n",
      "Epoch 257/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9950 - val_loss: 0.0520 - val_pos_accuracy: 0.9250\n",
      "Epoch 258/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9944 - val_loss: 0.0521 - val_pos_accuracy: 0.9250\n",
      "Epoch 259/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0095 - pos_accuracy: 0.9944 - val_loss: 0.0595 - val_pos_accuracy: 0.9200\n",
      "Epoch 260/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0050 - pos_accuracy: 0.9944 - val_loss: 0.0496 - val_pos_accuracy: 0.9275\n",
      "Epoch 261/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0495 - val_pos_accuracy: 0.9175\n",
      "Epoch 262/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9944 - val_loss: 0.0491 - val_pos_accuracy: 0.9175\n",
      "Epoch 263/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0034 - pos_accuracy: 0.9944 - val_loss: 0.0491 - val_pos_accuracy: 0.9175\n",
      "Epoch 264/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0494 - val_pos_accuracy: 0.9150\n",
      "Epoch 265/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0038 - pos_accuracy: 0.9944 - val_loss: 0.0491 - val_pos_accuracy: 0.9175\n",
      "Epoch 266/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0050 - pos_accuracy: 0.9950 - val_loss: 0.0495 - val_pos_accuracy: 0.9175\n",
      "Epoch 267/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9944 - val_loss: 0.0491 - val_pos_accuracy: 0.9175\n",
      "Epoch 268/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9950 - val_loss: 0.0491 - val_pos_accuracy: 0.9175\n",
      "Epoch 269/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0034 - pos_accuracy: 0.9950 - val_loss: 0.0487 - val_pos_accuracy: 0.9175\n",
      "Epoch 270/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0033 - pos_accuracy: 0.9950 - val_loss: 0.0488 - val_pos_accuracy: 0.9175\n",
      "Epoch 271/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0033 - pos_accuracy: 0.9950 - val_loss: 0.0488 - val_pos_accuracy: 0.9175\n",
      "Epoch 272/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0041 - pos_accuracy: 0.9950 - val_loss: 0.0515 - val_pos_accuracy: 0.9150\n",
      "Epoch 273/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0063 - pos_accuracy: 0.9944 - val_loss: 0.0537 - val_pos_accuracy: 0.9100\n",
      "Epoch 274/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0081 - pos_accuracy: 0.9956 - val_loss: 0.0511 - val_pos_accuracy: 0.9100\n",
      "Epoch 275/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0084 - pos_accuracy: 0.9950 - val_loss: 0.0569 - val_pos_accuracy: 0.9075\n",
      "Epoch 276/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0131 - pos_accuracy: 0.9950 - val_loss: 0.0555 - val_pos_accuracy: 0.9150\n",
      "Epoch 277/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0084 - pos_accuracy: 0.9944 - val_loss: 0.0505 - val_pos_accuracy: 0.9200\n",
      "Epoch 278/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0055 - pos_accuracy: 0.9950 - val_loss: 0.0488 - val_pos_accuracy: 0.9200\n",
      "Epoch 279/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0041 - pos_accuracy: 0.9956 - val_loss: 0.0496 - val_pos_accuracy: 0.9275\n",
      "Epoch 280/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0055 - pos_accuracy: 0.9950 - val_loss: 0.0521 - val_pos_accuracy: 0.9225\n",
      "Epoch 281/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0052 - pos_accuracy: 0.9950 - val_loss: 0.0504 - val_pos_accuracy: 0.9175\n",
      "Epoch 282/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0063 - pos_accuracy: 0.9950 - val_loss: 0.0491 - val_pos_accuracy: 0.9150\n",
      "Epoch 283/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0040 - pos_accuracy: 0.9950 - val_loss: 0.0502 - val_pos_accuracy: 0.9175\n",
      "Epoch 284/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0046 - pos_accuracy: 0.9956 - val_loss: 0.0494 - val_pos_accuracy: 0.9150\n",
      "Epoch 285/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0054 - pos_accuracy: 0.9950 - val_loss: 0.0501 - val_pos_accuracy: 0.9125\n",
      "Epoch 286/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0044 - pos_accuracy: 0.9956 - val_loss: 0.0485 - val_pos_accuracy: 0.9150\n",
      "Epoch 287/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0038 - pos_accuracy: 0.9956 - val_loss: 0.0488 - val_pos_accuracy: 0.9175\n",
      "Epoch 288/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9956 - val_loss: 0.0479 - val_pos_accuracy: 0.9175\n",
      "Epoch 289/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0032 - pos_accuracy: 0.9956 - val_loss: 0.0482 - val_pos_accuracy: 0.9200\n",
      "Epoch 290/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9956 - val_loss: 0.0503 - val_pos_accuracy: 0.9150\n",
      "Epoch 291/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0046 - pos_accuracy: 0.9956 - val_loss: 0.0484 - val_pos_accuracy: 0.9175\n",
      "Epoch 292/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0030 - pos_accuracy: 0.9956 - val_loss: 0.0476 - val_pos_accuracy: 0.9175\n",
      "Epoch 293/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0029 - pos_accuracy: 0.9956 - val_loss: 0.0479 - val_pos_accuracy: 0.9250\n",
      "Epoch 294/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0031 - pos_accuracy: 0.9956 - val_loss: 0.0481 - val_pos_accuracy: 0.9175\n",
      "Epoch 295/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0032 - pos_accuracy: 0.9956 - val_loss: 0.0487 - val_pos_accuracy: 0.9250\n",
      "Epoch 296/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0029 - pos_accuracy: 0.9956 - val_loss: 0.0477 - val_pos_accuracy: 0.9200\n",
      "Epoch 297/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0029 - pos_accuracy: 0.9956 - val_loss: 0.0475 - val_pos_accuracy: 0.9175\n",
      "Epoch 298/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0027 - pos_accuracy: 0.9956 - val_loss: 0.0474 - val_pos_accuracy: 0.9175\n",
      "Epoch 299/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9956 - val_loss: 0.0474 - val_pos_accuracy: 0.9175\n",
      "Epoch 300/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0027 - pos_accuracy: 0.9956 - val_loss: 0.0473 - val_pos_accuracy: 0.9150\n",
      "Epoch 301/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9956 - val_loss: 0.0472 - val_pos_accuracy: 0.9175\n",
      "Epoch 302/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9956 - val_loss: 0.0472 - val_pos_accuracy: 0.9175\n",
      "Epoch 303/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9956 - val_loss: 0.0470 - val_pos_accuracy: 0.9175\n",
      "Epoch 304/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9956 - val_loss: 0.0471 - val_pos_accuracy: 0.9175\n",
      "Epoch 305/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0028 - pos_accuracy: 0.9956 - val_loss: 0.0474 - val_pos_accuracy: 0.9175\n",
      "Epoch 306/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0027 - pos_accuracy: 0.9956 - val_loss: 0.0470 - val_pos_accuracy: 0.9175\n",
      "Epoch 307/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9956 - val_loss: 0.0473 - val_pos_accuracy: 0.9175\n",
      "Epoch 308/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0033 - pos_accuracy: 0.9956 - val_loss: 0.0486 - val_pos_accuracy: 0.9175\n",
      "Epoch 309/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0032 - pos_accuracy: 0.9956 - val_loss: 0.0469 - val_pos_accuracy: 0.9175\n",
      "Epoch 310/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0025 - pos_accuracy: 0.9956 - val_loss: 0.0468 - val_pos_accuracy: 0.9200\n",
      "Epoch 311/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9956 - val_loss: 0.0473 - val_pos_accuracy: 0.9200\n",
      "Epoch 312/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0027 - pos_accuracy: 0.9956 - val_loss: 0.0473 - val_pos_accuracy: 0.9175\n",
      "Epoch 313/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0027 - pos_accuracy: 0.9956 - val_loss: 0.0468 - val_pos_accuracy: 0.9175\n",
      "Epoch 314/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0025 - pos_accuracy: 0.9956 - val_loss: 0.0467 - val_pos_accuracy: 0.9175\n",
      "Epoch 315/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0024 - pos_accuracy: 0.9956 - val_loss: 0.0470 - val_pos_accuracy: 0.9200\n",
      "Epoch 316/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0029 - pos_accuracy: 0.9956 - val_loss: 0.0480 - val_pos_accuracy: 0.9275\n",
      "Epoch 317/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9956 - val_loss: 0.0500 - val_pos_accuracy: 0.9175\n",
      "Epoch 318/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0049 - pos_accuracy: 0.9956 - val_loss: 0.0477 - val_pos_accuracy: 0.9275\n",
      "Epoch 319/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0066 - pos_accuracy: 0.9956 - val_loss: 0.0500 - val_pos_accuracy: 0.9250\n",
      "Epoch 320/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0060 - pos_accuracy: 0.9956 - val_loss: 0.0517 - val_pos_accuracy: 0.9250\n",
      "Epoch 321/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0083 - pos_accuracy: 0.9956 - val_loss: 0.0530 - val_pos_accuracy: 0.9250\n",
      "Epoch 322/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9962 - val_loss: 0.0490 - val_pos_accuracy: 0.9275\n",
      "Epoch 323/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9956 - val_loss: 0.0486 - val_pos_accuracy: 0.9250\n",
      "Epoch 324/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9956 - val_loss: 0.0552 - val_pos_accuracy: 0.9200\n",
      "Epoch 325/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9969 - val_loss: 0.0526 - val_pos_accuracy: 0.9200\n",
      "Epoch 326/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0246 - pos_accuracy: 0.9762 - val_loss: 0.0635 - val_pos_accuracy: 0.9175\n",
      "Epoch 327/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0312 - pos_accuracy: 0.9781 - val_loss: 0.0731 - val_pos_accuracy: 0.9075\n",
      "Epoch 328/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0429 - pos_accuracy: 0.9362 - val_loss: 0.0955 - val_pos_accuracy: 0.8275\n",
      "Epoch 329/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0300 - pos_accuracy: 0.9637 - val_loss: 0.0656 - val_pos_accuracy: 0.9075\n",
      "Epoch 330/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0104 - pos_accuracy: 0.9956 - val_loss: 0.0521 - val_pos_accuracy: 0.9275\n",
      "Epoch 331/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.0477 - val_pos_accuracy: 0.9300\n",
      "Epoch 332/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9963 - val_loss: 0.0468 - val_pos_accuracy: 0.9225\n",
      "Epoch 333/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0030 - pos_accuracy: 0.9962 - val_loss: 0.0476 - val_pos_accuracy: 0.9300\n",
      "Epoch 334/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9969 - val_loss: 0.0456 - val_pos_accuracy: 0.9225\n",
      "Epoch 335/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0030 - pos_accuracy: 0.9963 - val_loss: 0.0476 - val_pos_accuracy: 0.9275\n",
      "Epoch 336/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0033 - pos_accuracy: 0.9956 - val_loss: 0.0469 - val_pos_accuracy: 0.9300\n",
      "Epoch 337/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9969 - val_loss: 0.0464 - val_pos_accuracy: 0.9300\n",
      "Epoch 338/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9969 - val_loss: 0.0458 - val_pos_accuracy: 0.9275\n",
      "Epoch 339/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0028 - pos_accuracy: 0.9962 - val_loss: 0.0467 - val_pos_accuracy: 0.9250\n",
      "Epoch 340/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0032 - pos_accuracy: 0.9969 - val_loss: 0.0455 - val_pos_accuracy: 0.9300\n",
      "Epoch 341/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0021 - pos_accuracy: 0.9969 - val_loss: 0.0452 - val_pos_accuracy: 0.9225\n",
      "Epoch 342/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0021 - pos_accuracy: 0.9969 - val_loss: 0.0455 - val_pos_accuracy: 0.9200\n",
      "Epoch 343/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0023 - pos_accuracy: 0.9969 - val_loss: 0.0453 - val_pos_accuracy: 0.9225\n",
      "Epoch 344/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0025 - pos_accuracy: 0.9969 - val_loss: 0.0452 - val_pos_accuracy: 0.9225\n",
      "Epoch 345/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9969 - val_loss: 0.0460 - val_pos_accuracy: 0.9250\n",
      "Epoch 346/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0028 - pos_accuracy: 0.9969 - val_loss: 0.0456 - val_pos_accuracy: 0.9225\n",
      "Epoch 347/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9969 - val_loss: 0.0453 - val_pos_accuracy: 0.9225\n",
      "Epoch 348/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0022 - pos_accuracy: 0.9969 - val_loss: 0.0452 - val_pos_accuracy: 0.9225\n",
      "Epoch 349/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0020 - pos_accuracy: 0.9969 - val_loss: 0.0450 - val_pos_accuracy: 0.9225\n",
      "Epoch 350/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0021 - pos_accuracy: 0.9969 - val_loss: 0.0454 - val_pos_accuracy: 0.9200\n",
      "Epoch 351/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9969 - val_loss: 0.0449 - val_pos_accuracy: 0.9200\n",
      "Epoch 352/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9969 - val_loss: 0.0457 - val_pos_accuracy: 0.9225\n",
      "Epoch 353/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9969 - val_loss: 0.0469 - val_pos_accuracy: 0.9200\n",
      "Epoch 354/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9969 - val_loss: 0.0452 - val_pos_accuracy: 0.9175\n",
      "Epoch 355/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0057 - pos_accuracy: 0.9969 - val_loss: 0.0505 - val_pos_accuracy: 0.9150\n",
      "Epoch 356/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0121 - pos_accuracy: 0.9956 - val_loss: 0.0603 - val_pos_accuracy: 0.9100\n",
      "Epoch 357/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0274 - pos_accuracy: 0.9725 - val_loss: 0.0944 - val_pos_accuracy: 0.8225\n",
      "Epoch 358/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0349 - pos_accuracy: 0.9481 - val_loss: 0.0581 - val_pos_accuracy: 0.9100\n",
      "Epoch 359/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0151 - pos_accuracy: 0.9938 - val_loss: 0.0587 - val_pos_accuracy: 0.9150\n",
      "Epoch 360/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0090 - pos_accuracy: 0.9969 - val_loss: 0.0538 - val_pos_accuracy: 0.9175\n",
      "Epoch 361/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0388 - pos_accuracy: 0.9450 - val_loss: 0.0773 - val_pos_accuracy: 0.8625\n",
      "Epoch 362/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0278 - pos_accuracy: 0.9744 - val_loss: 0.0503 - val_pos_accuracy: 0.9225\n",
      "Epoch 363/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0077 - pos_accuracy: 0.9969 - val_loss: 0.0450 - val_pos_accuracy: 0.9200\n",
      "Epoch 364/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0024 - pos_accuracy: 0.9969 - val_loss: 0.0445 - val_pos_accuracy: 0.9225\n",
      "Epoch 365/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0021 - pos_accuracy: 0.9969 - val_loss: 0.0446 - val_pos_accuracy: 0.9200\n",
      "Epoch 366/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0025 - pos_accuracy: 0.9969 - val_loss: 0.0450 - val_pos_accuracy: 0.9200\n",
      "Epoch 367/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0030 - pos_accuracy: 0.9969 - val_loss: 0.0449 - val_pos_accuracy: 0.9200\n",
      "Epoch 368/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0024 - pos_accuracy: 0.9969 - val_loss: 0.0453 - val_pos_accuracy: 0.9200\n",
      "Epoch 369/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0055 - pos_accuracy: 0.9969 - val_loss: 0.0461 - val_pos_accuracy: 0.9225\n",
      "Epoch 370/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0044 - pos_accuracy: 0.9969 - val_loss: 0.0452 - val_pos_accuracy: 0.9200\n",
      "Epoch 371/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0024 - pos_accuracy: 0.9969 - val_loss: 0.0451 - val_pos_accuracy: 0.9175\n",
      "Epoch 372/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9969 - val_loss: 0.0445 - val_pos_accuracy: 0.9200\n",
      "Epoch 373/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0029 - pos_accuracy: 0.9969 - val_loss: 0.0452 - val_pos_accuracy: 0.9200\n",
      "Epoch 374/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0031 - pos_accuracy: 0.9969 - val_loss: 0.0443 - val_pos_accuracy: 0.9200\n",
      "Epoch 375/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0030 - pos_accuracy: 0.9969 - val_loss: 0.0446 - val_pos_accuracy: 0.9175\n",
      "Epoch 376/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0025 - pos_accuracy: 0.9969 - val_loss: 0.0442 - val_pos_accuracy: 0.9175\n",
      "Epoch 377/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0023 - pos_accuracy: 0.9969 - val_loss: 0.0436 - val_pos_accuracy: 0.9225\n",
      "Epoch 378/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0021 - pos_accuracy: 0.9969 - val_loss: 0.0438 - val_pos_accuracy: 0.9225\n",
      "Epoch 379/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0017 - pos_accuracy: 0.9969 - val_loss: 0.0437 - val_pos_accuracy: 0.9225\n",
      "Epoch 380/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0018 - pos_accuracy: 0.9969 - val_loss: 0.0437 - val_pos_accuracy: 0.9225\n",
      "Epoch 381/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0021 - pos_accuracy: 0.9969 - val_loss: 0.0438 - val_pos_accuracy: 0.9200\n",
      "Epoch 382/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0021 - pos_accuracy: 0.9969 - val_loss: 0.0439 - val_pos_accuracy: 0.9225\n",
      "Epoch 383/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0026 - pos_accuracy: 0.9969 - val_loss: 0.0438 - val_pos_accuracy: 0.9225\n",
      "Epoch 384/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0028 - pos_accuracy: 0.9969 - val_loss: 0.0438 - val_pos_accuracy: 0.9200\n",
      "Epoch 385/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0017 - pos_accuracy: 0.9969 - val_loss: 0.0435 - val_pos_accuracy: 0.9225\n",
      "Epoch 386/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0017 - pos_accuracy: 0.9969 - val_loss: 0.0433 - val_pos_accuracy: 0.9225\n",
      "Epoch 387/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0016 - pos_accuracy: 0.9969 - val_loss: 0.0435 - val_pos_accuracy: 0.9225\n",
      "Epoch 388/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0018 - pos_accuracy: 0.9969 - val_loss: 0.0439 - val_pos_accuracy: 0.9300\n",
      "Epoch 389/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0018 - pos_accuracy: 0.9969 - val_loss: 0.0436 - val_pos_accuracy: 0.9225\n",
      "Epoch 390/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0017 - pos_accuracy: 0.9969 - val_loss: 0.0434 - val_pos_accuracy: 0.9225\n",
      "Epoch 391/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0016 - pos_accuracy: 0.9969 - val_loss: 0.0433 - val_pos_accuracy: 0.9225\n",
      "Epoch 392/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0016 - pos_accuracy: 0.9969 - val_loss: 0.0433 - val_pos_accuracy: 0.9225\n",
      "Epoch 393/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0019 - pos_accuracy: 0.9969 - val_loss: 0.0436 - val_pos_accuracy: 0.9225\n",
      "Epoch 394/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9975 - val_loss: 0.0450 - val_pos_accuracy: 0.9200\n",
      "Epoch 395/400\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9975 - val_loss: 0.0441 - val_pos_accuracy: 0.9200\n",
      "Epoch 396/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0028 - pos_accuracy: 0.9981 - val_loss: 0.0430 - val_pos_accuracy: 0.9250\n",
      "Epoch 397/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.0433 - val_pos_accuracy: 0.9250\n",
      "Epoch 398/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0018 - pos_accuracy: 0.9987 - val_loss: 0.0437 - val_pos_accuracy: 0.9225\n",
      "Epoch 399/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9987 - val_loss: 0.0440 - val_pos_accuracy: 0.9200\n",
      "Epoch 400/400\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0024 - pos_accuracy: 0.9981 - val_loss: 0.0440 - val_pos_accuracy: 0.9275\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:01:48.492724: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1739: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\n서로 다른 numpy 객체를 결합할때 사용한다\\naxis는 축을 의미하는데 1은 y 가로방향의 결합을 의미한다.\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n1행 2열의 첫 번째 그래프로 표시\\n1행 2열의 두 번째 그래프로 표시\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\nand 연산을 한다. axis 1은 y축 방향의 각 요소와 논리 연산을 진행한다는 걸 의미\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n전체 평균을 구하고 하나의 값을 리턴\\n\",\n",
      "        false,\n",
      "        \"\\nrandom seed의 사용 이유\\n고정된 input값이 필요해서 \\n\",\n",
      "        true,\n",
      "        \"\\n\\n\"\n",
      "    ],\n",
      "    \"score\": 98.18181818181819,\n",
      "    \"accuracy\": 0.9225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\"report1/황창현_46085.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/황창현_46085.ipynb to python\n",
      "[NbConvertApp] Writing 35065 bytes to report1/황창현_46085.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "[[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]]\n",
      "[[5. 7. 9.]\n",
      " [6. 8. 0.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.12323400e-17 -1.00000000e+00  2.00000000e+02]\n",
      " [ 1.00000000e+00  6.12323400e-17 -1.42108547e-14]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 303012.86it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:01:55.804309: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:01:56.179358: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:01:56.179395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:01:56.420726: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:01:57.021039: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 102881.00it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 111.7641 - val_loss: 24.0673\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:01:59.573990: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1676: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과 배열을 선택한 축 방향으로 연결해준다. \\naxis=1의 의미 열방향을 의미한다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\nplt.subplot(121) = 1x2 그리드의 첫 번째 subplot\\nplt.subplot(122) = 1x2 그리드의 두 번째 subplot\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntf.reduce_all : and 연산\\naxis=1 : 축소할 치수가 1입니다. \\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다.\\ntf.reduce_mean : 정확도를 계산하기 위한 함수입니다.  \\n\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n사용할 활성화 함수입니다. 아무 것도 지정하지 않으면 활성화가 적용되지 않습니다\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\nrandom seed : 수를 지정해주지않으면 매번 다른 수를 사용한다.\\n\",\n",
      "        false,\n",
      "        \"\\n너무 어렵네요 공부 더하겠습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 74.54545454545455,\n",
      "    \"accuracy\": 111.7723\n",
      "}\"report1/김태권_46033.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김태권_46033.ipynb to python\n",
      "[NbConvertApp] Writing 36051 bytes to report1/김태권_46033.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:212: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "float32에서 inf가 출력된 최초의 n_iter값은 : 51 입니다.\n",
      "float64에서 inf가 출력된 최초의 n_iter값은 : 421 입니다.\n",
      "[[5. 7. 9.]\n",
      " [6. 8. 0.]]\n",
      "(2, 2)\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "200\n",
      "200\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 296166.08it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:02:06.011576: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:02:06.391134: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:02:06.391173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:02:06.617442: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:02:07.255099: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 4ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "[[12 15]\n",
      " [21  0]\n",
      " [ 3 27]\n",
      " ...\n",
      " [ 1 12]\n",
      " [16 18]\n",
      " [ 7  8]]\n",
      "[[[ 4.66908506e-310  6.94132178e-310  4.66908506e-310 ...\n",
      "    6.36601844e-314  0.00000000e+000  0.00000000e+000]\n",
      "  [ 8.17224103e-319  3.10791995e-319  0.00000000e+000 ...\n",
      "    2.42843146e-319  8.48798316e-314  1.97626258e-323]\n",
      "  [ 0.00000000e+000  2.12199589e-314  9.88131292e-324 ...\n",
      "    2.12199601e-314  9.88131292e-324  0.00000000e+000]\n",
      "  ...\n",
      "  [ 1.04713459e-317  2.42843146e-319  1.80369642e-312 ...\n",
      "    1.61895431e-319  1.86735630e-312  1.97626258e-323]\n",
      "  [ 0.00000000e+000  2.12199971e-314  9.88131292e-324 ...\n",
      "    2.12199985e-314  9.88131292e-324  0.00000000e+000]\n",
      "  [ 1.12763962e-317  2.68771711e-321  1.93101617e-312 ...\n",
      "    2.68771711e-321  1.99467604e-312  1.97626258e-323]]\n",
      "\n",
      " [[ 0.00000000e+000  2.12200002e-314  5.18068343e-318 ...\n",
      "    2.12200016e-314  5.18068343e-318  0.00000000e+000]\n",
      "  [ 1.19416852e-317  8.28449275e-320 -1.84151113e+106 ...\n",
      "    7.14616550e-320 -2.80992566e+101  6.32404027e-322]\n",
      "  [ 0.00000000e+000  2.12200034e-314  1.48219694e-323 ...\n",
      "    2.12200046e-314  1.48219694e-323  0.00000000e+000]\n",
      "  ...\n",
      "  [ 2.01686292e-317  8.15801194e-320 -3.93354855e+202 ...\n",
      "    8.28449275e-320 -3.93355228e+202  6.32404027e-322]\n",
      "  [ 0.00000000e+000  2.12200417e-314  1.48219694e-323 ...\n",
      "    2.12200430e-314  1.48219694e-323  0.00000000e+000]\n",
      "  [ 2.09104391e-317  2.42843146e-319  4.03179200e-312 ...\n",
      "    2.42843146e-319  4.09545188e-312  1.97626258e-323]]\n",
      "\n",
      " [[ 0.00000000e+000  2.12200447e-314  9.88131292e-324 ...\n",
      "    2.12200460e-314  9.88131292e-324  0.00000000e+000]\n",
      "  [ 2.17338292e-317  2.60866661e-321  4.15911175e-312 ...\n",
      "    2.60866661e-321  4.22277162e-312  1.97626258e-323]\n",
      "  [ 0.00000000e+000  2.12200475e-314  5.18068343e-318 ...\n",
      "    2.12200489e-314  5.18068343e-318  0.00000000e+000]\n",
      "  ...\n",
      "  [ 3.04110448e-317  2.56914136e-321  5.94158821e-312 ...\n",
      "    2.76676762e-321  6.00524809e-312  1.97626258e-323]\n",
      "  [ 0.00000000e+000  2.12200802e-314  5.18068343e-318 ...\n",
      "    2.12200813e-314  5.18068343e-318  0.00000000e+000]\n",
      "  [ 3.10156231e-317  6.38728067e-320  8.40228812e+298 ...\n",
      "    6.89320389e-320 -2.18818386e-106  6.32404027e-322]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
      "  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
      "  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
      "  ...\n",
      "  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
      "  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
      "  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]]\n",
      "\n",
      " [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
      "  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
      "  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
      "  ...\n",
      "  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
      "  [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...\n",
      "    6.57309436e-310  2.21203632e-308  1.95806170e-317]\n",
      "  [ 2.10556306e-308  1.86092444e-317  2.10556306e-308 ...\n",
      "    2.17294857e-311  2.20986343e-308  4.73318924e-314]]\n",
      "\n",
      " [[ 1.39192764e-250  2.61129059e-314  2.20986346e-308 ...\n",
      "                nan  2.20877699e-308  1.90998646e-313]\n",
      "  [ 2.20986339e-308  1.62976926e-311  2.20986346e-308 ...\n",
      "    6.51877125e-310  2.21964155e-308  1.82157755e-317]\n",
      "  [ 2.21203632e-308  5.14957430e-315  1.96649594e-308 ...\n",
      "    5.97554031e-310  2.20986339e-308  6.02986340e-310]\n",
      "  ...\n",
      "  [ 1.39192762e-250  1.59664332e-312  7.54565472e-270 ...\n",
      "    2.78134682e-309  2.10447659e-308  1.63932737e-312]\n",
      "  [ 7.54380621e-270  3.12901461e-309  2.10447659e-308 ...\n",
      "    1.66054830e-312  1.39158662e-250  2.55095408e-312]\n",
      "  [ 2.20877693e-308  1.68176794e-312  1.39192762e-250 ...\n",
      "    1.08629614e-312  2.20877694e-308  1.72437946e-312]]]\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 136646.76it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 35.5710 - accuracy: 0.9025 - val_loss: 1.8673 - val_accuracy: 0.9700\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.4053 - accuracy: 0.9756 - val_loss: 1.6824 - val_accuracy: 0.9700\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.1270 - accuracy: 0.9675 - val_loss: 1.5810 - val_accuracy: 0.9525\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.1751 - accuracy: 0.9781 - val_loss: 0.7188 - val_accuracy: 0.9625\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9723 - accuracy: 0.9781 - val_loss: 1.0720 - val_accuracy: 0.9575\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7059 - accuracy: 0.9769 - val_loss: 0.6226 - val_accuracy: 0.9850\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7298 - accuracy: 0.9800 - val_loss: 1.0252 - val_accuracy: 0.9675\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5147 - accuracy: 0.9744 - val_loss: 0.5812 - val_accuracy: 0.9575\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3495 - accuracy: 0.9775 - val_loss: 0.3883 - val_accuracy: 0.9550\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2569 - accuracy: 0.9794 - val_loss: 0.6087 - val_accuracy: 0.9600\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2624 - accuracy: 0.9787 - val_loss: 1.2540 - val_accuracy: 0.9625\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3434 - accuracy: 0.9762 - val_loss: 0.3129 - val_accuracy: 0.9575\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1564 - accuracy: 0.9769 - val_loss: 0.2862 - val_accuracy: 0.9950\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1979 - accuracy: 0.9762 - val_loss: 0.3384 - val_accuracy: 0.9675\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3162 - accuracy: 0.9800 - val_loss: 0.5941 - val_accuracy: 0.9925\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3612 - accuracy: 0.9744 - val_loss: 0.5752 - val_accuracy: 0.9550\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1232 - accuracy: 0.9744 - val_loss: 0.2549 - val_accuracy: 0.9950\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1050 - accuracy: 0.9781 - val_loss: 0.2660 - val_accuracy: 0.9825\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1995 - accuracy: 0.9787 - val_loss: 0.2582 - val_accuracy: 0.9775\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1057 - accuracy: 0.9769 - val_loss: 0.2655 - val_accuracy: 0.9575\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1273 - accuracy: 0.9794 - val_loss: 0.2209 - val_accuracy: 0.9875\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9794 - val_loss: 0.1895 - val_accuracy: 0.9550\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0533 - accuracy: 0.9769 - val_loss: 0.1794 - val_accuracy: 0.9650\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0474 - accuracy: 0.9762 - val_loss: 0.1769 - val_accuracy: 0.9725\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0453 - accuracy: 0.9775 - val_loss: 0.1891 - val_accuracy: 0.9725\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0529 - accuracy: 0.9794 - val_loss: 0.2043 - val_accuracy: 0.9900\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1485 - accuracy: 0.9794 - val_loss: 0.1622 - val_accuracy: 0.9725\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0573 - accuracy: 0.9800 - val_loss: 0.1618 - val_accuracy: 0.9575\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 0.9794 - val_loss: 0.1597 - val_accuracy: 0.9575\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0368 - accuracy: 0.9787 - val_loss: 0.2747 - val_accuracy: 0.9925\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1828 - accuracy: 0.9806 - val_loss: 0.1951 - val_accuracy: 0.9675\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.9756 - val_loss: 0.1753 - val_accuracy: 0.9825\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9775 - val_loss: 0.2380 - val_accuracy: 0.9550\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1205 - accuracy: 0.9750 - val_loss: 0.1455 - val_accuracy: 0.9825\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.9794 - val_loss: 0.1651 - val_accuracy: 0.9575\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9769 - val_loss: 0.2011 - val_accuracy: 0.9675\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0740 - accuracy: 0.9806 - val_loss: 0.1396 - val_accuracy: 0.9675\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9806 - val_loss: 0.1428 - val_accuracy: 0.9675\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.9781 - val_loss: 0.1389 - val_accuracy: 0.9825\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9825 - val_loss: 0.1353 - val_accuracy: 0.9725\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.9781 - val_loss: 0.1485 - val_accuracy: 0.9750\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9831 - val_loss: 0.2441 - val_accuracy: 0.9775\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0424 - accuracy: 0.9775 - val_loss: 0.1302 - val_accuracy: 0.9550\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9775 - val_loss: 0.1330 - val_accuracy: 0.9775\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 0.9800 - val_loss: 0.1337 - val_accuracy: 0.9550\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9794 - val_loss: 0.1369 - val_accuracy: 0.9725\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.9750 - val_loss: 0.1272 - val_accuracy: 0.9900\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9806 - val_loss: 0.1351 - val_accuracy: 0.9900\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9775 - val_loss: 0.1295 - val_accuracy: 0.9850\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9775 - val_loss: 0.1247 - val_accuracy: 0.9650\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9781 - val_loss: 0.1225 - val_accuracy: 0.9550\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9812 - val_loss: 0.1342 - val_accuracy: 0.9550\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0396 - accuracy: 0.9812 - val_loss: 0.1347 - val_accuracy: 0.9550\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9769 - val_loss: 0.1203 - val_accuracy: 0.9625\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9781 - val_loss: 0.1247 - val_accuracy: 0.9575\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9781 - val_loss: 0.1252 - val_accuracy: 0.9625\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.9825 - val_loss: 0.1212 - val_accuracy: 0.9575\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9769 - val_loss: 0.1188 - val_accuracy: 0.9850\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.9762 - val_loss: 0.1205 - val_accuracy: 0.9875\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9812 - val_loss: 0.1198 - val_accuracy: 0.9900\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9775 - val_loss: 0.1170 - val_accuracy: 0.9675\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.9737 - val_loss: 0.1161 - val_accuracy: 0.9550\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9775 - val_loss: 0.1168 - val_accuracy: 0.9925\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.9794 - val_loss: 0.1176 - val_accuracy: 0.9900\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 0.9831 - val_loss: 0.1175 - val_accuracy: 0.9550\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.9781 - val_loss: 0.1162 - val_accuracy: 0.9675\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 0.9769 - val_loss: 0.1161 - val_accuracy: 0.9900\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.9762 - val_loss: 0.1140 - val_accuracy: 0.9550\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.9762 - val_loss: 0.1163 - val_accuracy: 0.9725\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9819 - val_loss: 0.1202 - val_accuracy: 0.9550\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0241 - accuracy: 0.9750 - val_loss: 0.1143 - val_accuracy: 0.9850\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0059 - accuracy: 0.9806 - val_loss: 0.1227 - val_accuracy: 0.9825\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.9800 - val_loss: 0.1177 - val_accuracy: 0.9725\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9825 - val_loss: 0.1174 - val_accuracy: 0.9550\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.9794 - val_loss: 0.1148 - val_accuracy: 0.9700\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0066 - accuracy: 0.9744 - val_loss: 0.1129 - val_accuracy: 0.9700\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.9825 - val_loss: 0.1125 - val_accuracy: 0.9875\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.9819 - val_loss: 0.1116 - val_accuracy: 0.9575\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.9812 - val_loss: 0.1158 - val_accuracy: 0.9550\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.9794 - val_loss: 0.1120 - val_accuracy: 0.9700\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.9775 - val_loss: 0.1106 - val_accuracy: 0.9575\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.9794 - val_loss: 0.1111 - val_accuracy: 0.9600\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.9775 - val_loss: 0.1102 - val_accuracy: 0.9550\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 0.9781 - val_loss: 0.1180 - val_accuracy: 0.9750\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.9775 - val_loss: 0.1102 - val_accuracy: 0.9725\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9794 - val_loss: 0.1123 - val_accuracy: 0.9925\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.9819 - val_loss: 0.1111 - val_accuracy: 0.9900\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 0.9787 - val_loss: 0.1098 - val_accuracy: 0.9900\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9831 - val_loss: 0.1118 - val_accuracy: 0.9875\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.9781 - val_loss: 0.1088 - val_accuracy: 0.9825\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.9756 - val_loss: 0.1103 - val_accuracy: 0.9850\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.9781 - val_loss: 0.1097 - val_accuracy: 0.9800\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.9819 - val_loss: 0.1096 - val_accuracy: 0.9750\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.9756 - val_loss: 0.1181 - val_accuracy: 0.9700\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9769 - val_loss: 0.1090 - val_accuracy: 0.9550\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 0.9806 - val_loss: 0.1093 - val_accuracy: 0.9600\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 0.9756 - val_loss: 0.1099 - val_accuracy: 0.9850\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9787 - val_loss: 0.1083 - val_accuracy: 0.9650\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.9819 - val_loss: 0.1119 - val_accuracy: 0.9550\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.9769 - val_loss: 0.1115 - val_accuracy: 0.9850\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:02:20.004440: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "first_9 pixles average = 0.7952069716775599\n",
      "second_9 pixles average = 0.6013071895424837\n",
      "first_convolution result = 0.795207\n",
      "second_convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1712: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\n\\naxis 0 => 열(세로 기준) AND 연산 진행\\naxis 1 => 행(가로 기준) AND 연산 진행\\n\\n\",\n",
      "        \"\\nnumpy 배열의 평균값을 의미한다\\n\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n\\n결과가 그래프처럼 나타나는게 아닌 포인트로 나타나서\\n활성함수를 켰을때 오류 발생\\n\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n\\nrandom seed는 무작위 처럼 보이지만\\n특정알고리즘에 의해 돌아가므로 데이터 학습에 적합하여서\\n\\n\",\n",
      "        true,\n",
      "        \"\\n\\n제가 공부가 미흡해서 강의 내용을 따라오는것만해도 너무 벅찬거같습니다\\n기왕 시작한거 하나라도 건져갈수있도록 노력하겠습니다.\\n\\n\\n\"\n",
      "    ],\n",
      "    \"score\": 90.0,\n",
      "    \"accuracy\": 0.985\n",
      "}\"report1/김선형_46009.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김선형_46009.ipynb to python\n",
      "[NbConvertApp] Writing 29658 bytes to report1/김선형_46009.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:212: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)\n",
      "[nan nan]\n",
      "[[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]]\n",
      "[[5. 7. 9.]\n",
      " [6. 8. 0.]]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -15. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 2.]\n",
      " [0. 1. 2.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 300172.05it/s]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:02:27.335613: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:02:27.714189: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:02:27.714227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:02:27.938254: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:02:28.544857: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:02:30.210911: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1448: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntf.reduce_의 경우, AND연산과 동일하다고 할 수 있습니다. axis=1을 사용해서 가로방향으로의 연산을 구합니다. \\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\ntf.reduce_mean의 경우 평균 값을 구합니다. \\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유\\nactivation은 활성함수로 입력신호를 출력신호를 변환해주는데 (출력이 linear하게 나오도록 하기 위해서) 리니어한 결과치를 기대하지 않는 경우, none으로 표기(디폴트값)\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n램던성을 제어하기 위해서 사용한다. 난수의 생성패턴을 동일하게 관리하기 위한 것입니다.\\n\",\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 54.72727272727273,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/박찬우_46075.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/박찬우_46075.ipynb to python\n",
      "[NbConvertApp] Writing 34450 bytes to report1/박찬우_46075.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.1]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.1]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.2]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:209: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:240: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 294709.39it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:02:36.535866: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:02:36.912531: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:02:36.912589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 117,250\n",
      "Trainable params: 117,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:02:37.143360: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:02:37.798648: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 6ms/step - loss: 233.0740 - val_loss: 190.6969\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 155.1243 - val_loss: 74.3945\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 127787.46it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 3925      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 8         \n",
      "=================================================================\n",
      "Total params: 3,951\n",
      "Trainable params: 3,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 244.5254 - val_loss: 216.7405\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 228.1180 - val_loss: 201.9340\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:02:40.168867: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1643: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n상당히 어렸습니다.\\n강의와 노트만으로 문제를 풀기에는 역부족이었습니다.\\n난이도 조절 또는 강의노트에 문제풀이 예제가 필요합니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 49.09090909090909,\n",
      "    \"accuracy\": 0.91\n",
      "}\"report1/김동현_46027.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김동현_46027.ipynb to python\n",
      "[NbConvertApp] Writing 36468 bytes to report1/김동현_46027.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:243: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(3, 2) (2, 3) (2, 3)\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.1232343e-17 -1.0000000e+00  2.0000000e+02]\n",
      " [ 1.0000000e+00  6.1232343e-17 -1.4210855e-14]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 199.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 294958.09it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:02:47.560492: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:02:47.940381: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:02:47.940419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:02:48.162190: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:02:48.790045: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 131782.39it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 36.3811 - pos_accuracy: 0.0538 - val_loss: 2.3778 - val_pos_accuracy: 0.1707\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.6415 - pos_accuracy: 0.1625 - val_loss: 1.2366 - val_pos_accuracy: 0.2788\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.4005 - pos_accuracy: 0.2125 - val_loss: 2.2521 - val_pos_accuracy: 0.1178\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.8798 - pos_accuracy: 0.3044 - val_loss: 0.9561 - val_pos_accuracy: 0.2957\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.7480 - pos_accuracy: 0.3400 - val_loss: 0.9755 - val_pos_accuracy: 0.2692\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5764 - pos_accuracy: 0.4250 - val_loss: 0.6174 - val_pos_accuracy: 0.4231\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4400 - pos_accuracy: 0.4656 - val_loss: 0.5035 - val_pos_accuracy: 0.5120\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3797 - pos_accuracy: 0.4956 - val_loss: 0.5183 - val_pos_accuracy: 0.5312\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3255 - pos_accuracy: 0.5756 - val_loss: 0.4384 - val_pos_accuracy: 0.5745\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2704 - pos_accuracy: 0.6187 - val_loss: 0.4327 - val_pos_accuracy: 0.4952\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2142 - pos_accuracy: 0.6869 - val_loss: 0.3792 - val_pos_accuracy: 0.5938\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2056 - pos_accuracy: 0.6750 - val_loss: 0.3033 - val_pos_accuracy: 0.6851\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1810 - pos_accuracy: 0.7181 - val_loss: 0.3242 - val_pos_accuracy: 0.6514\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1701 - pos_accuracy: 0.7181 - val_loss: 0.2832 - val_pos_accuracy: 0.7163\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1233 - pos_accuracy: 0.8281 - val_loss: 0.3196 - val_pos_accuracy: 0.6755\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1179 - pos_accuracy: 0.8194 - val_loss: 0.4098 - val_pos_accuracy: 0.5889\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1384 - pos_accuracy: 0.7975 - val_loss: 0.2801 - val_pos_accuracy: 0.6971\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0960 - pos_accuracy: 0.8775 - val_loss: 0.2499 - val_pos_accuracy: 0.7909\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0864 - pos_accuracy: 0.8813 - val_loss: 0.2505 - val_pos_accuracy: 0.7692\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0803 - pos_accuracy: 0.8919 - val_loss: 0.2455 - val_pos_accuracy: 0.7885\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0770 - pos_accuracy: 0.8994 - val_loss: 0.2185 - val_pos_accuracy: 0.8197\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0782 - pos_accuracy: 0.8856 - val_loss: 0.2178 - val_pos_accuracy: 0.8077\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0624 - pos_accuracy: 0.9244 - val_loss: 0.2601 - val_pos_accuracy: 0.7067\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0550 - pos_accuracy: 0.9381 - val_loss: 0.2115 - val_pos_accuracy: 0.8389\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0492 - pos_accuracy: 0.9438 - val_loss: 0.2335 - val_pos_accuracy: 0.7837\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0509 - pos_accuracy: 0.9444 - val_loss: 0.2073 - val_pos_accuracy: 0.8510\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0390 - pos_accuracy: 0.9613 - val_loss: 0.2008 - val_pos_accuracy: 0.8438\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0390 - pos_accuracy: 0.9588 - val_loss: 0.1919 - val_pos_accuracy: 0.8510\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0367 - pos_accuracy: 0.9619 - val_loss: 0.1889 - val_pos_accuracy: 0.8534\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0339 - pos_accuracy: 0.9638 - val_loss: 0.2346 - val_pos_accuracy: 0.7620\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0391 - pos_accuracy: 0.9500 - val_loss: 0.1832 - val_pos_accuracy: 0.8582\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0318 - pos_accuracy: 0.9650 - val_loss: 0.1916 - val_pos_accuracy: 0.8582\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0312 - pos_accuracy: 0.9663 - val_loss: 0.1799 - val_pos_accuracy: 0.8606\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0253 - pos_accuracy: 0.9756 - val_loss: 0.1751 - val_pos_accuracy: 0.8582\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0248 - pos_accuracy: 0.9719 - val_loss: 0.1775 - val_pos_accuracy: 0.8606\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0242 - pos_accuracy: 0.9750 - val_loss: 0.1787 - val_pos_accuracy: 0.8630\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0227 - pos_accuracy: 0.9775 - val_loss: 0.1735 - val_pos_accuracy: 0.8654\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0254 - pos_accuracy: 0.9794 - val_loss: 0.1740 - val_pos_accuracy: 0.8726\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0203 - pos_accuracy: 0.9781 - val_loss: 0.1796 - val_pos_accuracy: 0.8654\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0172 - pos_accuracy: 0.9812 - val_loss: 0.1719 - val_pos_accuracy: 0.8750\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0161 - pos_accuracy: 0.9800 - val_loss: 0.1685 - val_pos_accuracy: 0.8702\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0154 - pos_accuracy: 0.9869 - val_loss: 0.1699 - val_pos_accuracy: 0.8726\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9856 - val_loss: 0.1660 - val_pos_accuracy: 0.8726\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0134 - pos_accuracy: 0.9881 - val_loss: 0.1668 - val_pos_accuracy: 0.8774\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0143 - pos_accuracy: 0.9894 - val_loss: 0.1643 - val_pos_accuracy: 0.8822\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0135 - pos_accuracy: 0.9894 - val_loss: 0.1696 - val_pos_accuracy: 0.8822\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0116 - pos_accuracy: 0.9919 - val_loss: 0.1675 - val_pos_accuracy: 0.8822\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0128 - pos_accuracy: 0.9912 - val_loss: 0.1636 - val_pos_accuracy: 0.8822\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9912 - val_loss: 0.1627 - val_pos_accuracy: 0.8846\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0103 - pos_accuracy: 0.9919 - val_loss: 0.1613 - val_pos_accuracy: 0.8870\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:03:00.409594: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "[[0.79607843 0.79215686 0.79215686]\n",
      " [0.8        0.79607843 0.78823529]\n",
      " [0.8        0.79607843 0.79607843]]\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1705: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concatenate 함수의 역활은 numpy 배열들을 하나로 연결해주는 역활입니다.\\naxis=1은 축을 의미하며, 0은 행 1은 열입니다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\n첫번째 정수는 subplot의 행의 수를 나타냅니다.\\n두번째 정수는 subplot의 열의 수를 나타냅니다.\\n세번째 정수는 index를 나타냅니다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\nreduce_all은 주어진 치수(axis)에 따라 차원을 감소시키는 역활입니다.\\naxis=1은 1차원으로 감소시킴을 의미합니다.\\n본 코드에서는 테스트 라벨과 예측 라벨이 같은지에 대한 bool 값을 1차원으로 계산합니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n\\nreduce_all로 계산된 1차원 bool 배열에서 평균값을 구하기 위한 역활을 합니다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n\\n신경망의 출력 값을 비선형적으로 변형시키는 역활을 하지만\\nsoftmax 0, 1 사이로가 변형되면 좌표를 정확하게 표현하지 못하기 때문입니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유\\n\\n실행될 알고리즘을 개발할 때 random성을 제어하기 위해 고정하여 난수를 생성할 패턴을 동일하게 관리하기 위함입니다.\\n\",\n",
      "        false,\n",
      "        \"\"\n",
      "    ],\n",
      "    \"score\": 80.0,\n",
      "    \"accuracy\": 0.8822\n",
      "}\"report1/김현빈_45995.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김현빈_45995.ipynb to python\n",
      "[NbConvertApp] Writing 28883 bytes to report1/김현빈_45995.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:241: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 294079.16it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:03:06.879554: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:03:07.253590: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:03:07.253644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:03:07.482313: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:03:08.084103: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:03:09.742952: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1414: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1435: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 46.78787878787878,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/정다훈_46072.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/정다훈_46072.ipynb to python\n",
      "[NbConvertApp] Writing 31888 bytes to report1/정다훈_46072.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:209: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "52\n",
      "421\n",
      "(2, 3)\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -25. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 0.  1.  0.]\n",
      " [ 1.  0. 10.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 302619.34it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:03:16.130941: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:03:16.520467: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:03:16.520507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:03:16.744582: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:03:17.351560: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 91268.81it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 241.5331 - val_loss: 212.8644\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.7366 - val_loss: 196.3783\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:03:19.658103: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1567: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할 : 2개이상의  배열들을 합쳐준다.\\naxis=1의 의미 : 1이라면 가로축으로 0이라면 세로축으로 합친다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미: 앞자리부터 각각 행, 열, 위치(index)\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할: 사용되지 않는 인수 제거\\naxis=1의 의미: 축소할 치수\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할: 정확도 계산 \\n\",\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 65.0909090909091,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/천동암_46080.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/천동암_46080.ipynb to python\n",
      "[NbConvertApp] Writing 52 bytes to report1/천동암_46080.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007{\n",
      "    \"res\": [\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false\n",
      "    ],\n",
      "    \"score\": 0.0,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/홍준의_46087.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/홍준의_46087.ipynb to python\n",
      "[NbConvertApp] Writing 35545 bytes to report1/홍준의_46087.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "rm: '/home/kotech/.keras/datasets/*.png'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "51 [3.0519118e+38           inf]\n",
      "421 [inf inf]\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "[[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 309371.49it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:03:27.878998: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:03:28.252339: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:03:28.252379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:03:28.477225: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:03:29.103862: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 93140.52it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 27,362\n",
      "Trainable params: 27,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "100/100 [==============================] - 1s 4ms/step - loss: 14.9686 - pos_accuracy: 0.0794 - val_loss: 1.5674 - val_pos_accuracy: 0.1200\n",
      "Epoch 2/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 2.0857 - pos_accuracy: 0.1456 - val_loss: 1.5905 - val_pos_accuracy: 0.1550\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.3202 - pos_accuracy: 0.2062 - val_loss: 0.8041 - val_pos_accuracy: 0.2800\n",
      "Epoch 4/25\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.9957 - pos_accuracy: 0.2313 - val_loss: 3.1859 - val_pos_accuracy: 0.0750\n",
      "Epoch 5/25\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.8833 - pos_accuracy: 0.2763 - val_loss: 0.8858 - val_pos_accuracy: 0.2650\n",
      "Epoch 6/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.6325 - pos_accuracy: 0.3225 - val_loss: 0.7045 - val_pos_accuracy: 0.3350\n",
      "Epoch 7/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.8110 - pos_accuracy: 0.3088 - val_loss: 0.8673 - val_pos_accuracy: 0.2900\n",
      "Epoch 8/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5262 - pos_accuracy: 0.3775 - val_loss: 0.9439 - val_pos_accuracy: 0.2225\n",
      "Epoch 9/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5665 - pos_accuracy: 0.3725 - val_loss: 0.7752 - val_pos_accuracy: 0.2575\n",
      "Epoch 10/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4154 - pos_accuracy: 0.4206 - val_loss: 0.5496 - val_pos_accuracy: 0.3825\n",
      "Epoch 11/25\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.3848 - pos_accuracy: 0.4450 - val_loss: 0.6878 - val_pos_accuracy: 0.3475\n",
      "Epoch 12/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4321 - pos_accuracy: 0.4062 - val_loss: 0.5558 - val_pos_accuracy: 0.4375\n",
      "Epoch 13/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3747 - pos_accuracy: 0.4563 - val_loss: 0.5812 - val_pos_accuracy: 0.4125\n",
      "Epoch 14/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3799 - pos_accuracy: 0.4419 - val_loss: 0.6179 - val_pos_accuracy: 0.3200\n",
      "Epoch 15/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3671 - pos_accuracy: 0.4663 - val_loss: 0.6736 - val_pos_accuracy: 0.3300\n",
      "Epoch 16/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3179 - pos_accuracy: 0.4725 - val_loss: 0.6277 - val_pos_accuracy: 0.3700\n",
      "Epoch 17/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2693 - pos_accuracy: 0.5425 - val_loss: 0.6915 - val_pos_accuracy: 0.3675\n",
      "Epoch 18/25\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.2907 - pos_accuracy: 0.5288 - val_loss: 0.5798 - val_pos_accuracy: 0.4425\n",
      "Epoch 19/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2464 - pos_accuracy: 0.5587 - val_loss: 0.5516 - val_pos_accuracy: 0.4075\n",
      "Epoch 20/25\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.2542 - pos_accuracy: 0.5337 - val_loss: 0.6410 - val_pos_accuracy: 0.4650\n",
      "Epoch 21/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2422 - pos_accuracy: 0.5763 - val_loss: 0.5496 - val_pos_accuracy: 0.4925\n",
      "Epoch 22/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2386 - pos_accuracy: 0.5775 - val_loss: 0.5570 - val_pos_accuracy: 0.4625\n",
      "Epoch 23/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2341 - pos_accuracy: 0.6019 - val_loss: 0.9016 - val_pos_accuracy: 0.2325\n",
      "Epoch 24/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2625 - pos_accuracy: 0.5331 - val_loss: 0.5573 - val_pos_accuracy: 0.4225\n",
      "Epoch 25/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1893 - pos_accuracy: 0.6425 - val_loss: 0.4959 - val_pos_accuracy: 0.5475\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:03:39.951724: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1696: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate는 2개의 배열을 1개로 합치는 역할을 합니다.\\naxis를 1로 설정하면 열방향(좌->우)로 연결하며, 0으로 설정하면 행방향(위->아래)로 연결합니다.\\n\",\n",
      "        \"\\n121은 1행, 2열 그리드 중 1번째 subplot이라는 뜻,\\n122는 1행, 2열 그리드 중 2번째 subplot이라는 뜻 입니다.\\n만약 513 이면 5행, 1열 그리드의 3번 째 subplot라는 의미가 됩니다\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\n특정 차원을 제거하고 AND 연산을 함.\\n2번째 인덱스를 제거하고 1번째 인덱스에 AND 연산을 합니다.\\n\",\n",
      "        \"\\n특정 차원을 제거하고 평균을 구함.\\n1, 0으로 정답, 오답이 적혀있는 iscorrect의 평균을 구해 점수를 측정함.\\n\",\n",
      "        \"\\n비선형으로 사용하기 위함.\\n\",\n",
      "        \"\\nseed 값을 사용하지 않으면 사용할 때마다 계속 다른 값이 나오기 때문에 제대로 학습시켰는지 확인하기 여려움.\\n\",\n",
      "        true,\n",
      "        \"\\n4번 문제가 너무 어렵네요.\\nDense 레이어를 어떻게 세팅해야 좋은지 잘 모르겠습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.5525\n",
      "}\"report1/이범윤_46045.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이범윤_46045.ipynb to python\n",
      "[NbConvertApp] Writing 29304 bytes to report1/이범윤_46045.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]]\n",
      "[[5. 7. 9.]\n",
      " [6. 8. 0.]]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 302074.47it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:03:46.471568: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:03:46.910589: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:03:46.910648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:03:47.135385: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:03:47.774669: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:03:49.396306: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1442: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\nnp.concatenate의 역할은 행렬을 좌우나 위아래로 갖다 붙여주는 기능이며, axis=1은 수평방향을 의미함\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n121은 1x2행렬에 첫번째칸에 그린다는 의미이며, 1x2행렬에 두번째칸에 그린다는 의미이다.\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 40.12121212121212,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/석병득_46022.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/석병득_46022.ipynb to python\n",
      "[NbConvertApp] Writing 29035 bytes to report1/석병득_46022.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:216: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:247: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "1000\n",
      "10000\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 290897.39it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:03:55.727479: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:03:56.111217: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:03:56.111257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:03:56.333686: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:03:56.934984: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:03:58.600282: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "ans05 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1425: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1446: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할\\n  np 행렬 a(2,3) 과 b(2,3) np.concatenate([a,b]) 시\\n  (4,3) 결합됨.\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n  가로로 결합이 이루어짐. (2,6)\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 23.454545454545453,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/고동일_46028.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/고동일_46028.ipynb to python\n",
      "[NbConvertApp] Writing 35800 bytes to report1/고동일_46028.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 290907.48it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:04:04.931896: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:04:05.318323: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:04:05.318362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:04:05.539618: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:04:06.171771: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 106358.59it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 1,592\n",
      "Trainable params: 1,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 166.2093 - pos_accuracy: 0.0019 - val_loss: 98.3565 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 2/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 83.6603 - pos_accuracy: 0.0031 - val_loss: 62.6963 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 3/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 59.6716 - pos_accuracy: 0.0012 - val_loss: 51.4488 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 4/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 49.7311 - pos_accuracy: 0.0069 - val_loss: 42.7528 - val_pos_accuracy: 0.0144\n",
      "Epoch 5/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 41.8312 - pos_accuracy: 0.0106 - val_loss: 36.9365 - val_pos_accuracy: 0.0096\n",
      "Epoch 6/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 36.9746 - pos_accuracy: 0.0137 - val_loss: 33.1520 - val_pos_accuracy: 0.0024\n",
      "Epoch 7/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 33.1425 - pos_accuracy: 0.0250 - val_loss: 30.0939 - val_pos_accuracy: 0.0144\n",
      "Epoch 8/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 29.8685 - pos_accuracy: 0.0369 - val_loss: 27.6278 - val_pos_accuracy: 0.0240\n",
      "Epoch 9/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 27.2448 - pos_accuracy: 0.0406 - val_loss: 24.7264 - val_pos_accuracy: 0.0312\n",
      "Epoch 10/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 25.0021 - pos_accuracy: 0.0431 - val_loss: 22.8529 - val_pos_accuracy: 0.0409\n",
      "Epoch 11/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 23.2219 - pos_accuracy: 0.0487 - val_loss: 21.5056 - val_pos_accuracy: 0.0192\n",
      "Epoch 12/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 21.5475 - pos_accuracy: 0.0544 - val_loss: 19.5416 - val_pos_accuracy: 0.0529\n",
      "Epoch 13/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 20.3413 - pos_accuracy: 0.0550 - val_loss: 18.6722 - val_pos_accuracy: 0.0288\n",
      "Epoch 14/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 19.1346 - pos_accuracy: 0.0494 - val_loss: 17.8505 - val_pos_accuracy: 0.0144\n",
      "Epoch 15/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 18.2188 - pos_accuracy: 0.0437 - val_loss: 16.7344 - val_pos_accuracy: 0.0697\n",
      "Epoch 16/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 17.2339 - pos_accuracy: 0.0487 - val_loss: 15.7365 - val_pos_accuracy: 0.0337\n",
      "Epoch 17/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 16.4239 - pos_accuracy: 0.0362 - val_loss: 15.2077 - val_pos_accuracy: 0.0385\n",
      "Epoch 18/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 15.7995 - pos_accuracy: 0.0450 - val_loss: 14.4024 - val_pos_accuracy: 0.0240\n",
      "Epoch 19/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 15.6683 - pos_accuracy: 0.0300 - val_loss: 14.2244 - val_pos_accuracy: 0.0096\n",
      "Epoch 20/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 15.2651 - pos_accuracy: 0.0406 - val_loss: 13.2181 - val_pos_accuracy: 0.0312\n",
      "Epoch 21/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.8089 - pos_accuracy: 0.0419 - val_loss: 17.5098 - val_pos_accuracy: 0.0072\n",
      "Epoch 22/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.7877 - pos_accuracy: 0.0350 - val_loss: 14.0381 - val_pos_accuracy: 0.0048\n",
      "Epoch 23/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.3538 - pos_accuracy: 0.0400 - val_loss: 13.4439 - val_pos_accuracy: 0.0240\n",
      "Epoch 24/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.5937 - pos_accuracy: 0.0275 - val_loss: 12.3533 - val_pos_accuracy: 0.0361\n",
      "Epoch 25/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.6000 - pos_accuracy: 0.0338 - val_loss: 12.4617 - val_pos_accuracy: 0.0505\n",
      "Epoch 26/44\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 14.4891 - pos_accuracy: 0.0206 - val_loss: 12.2905 - val_pos_accuracy: 0.0361\n",
      "Epoch 27/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 13.8656 - pos_accuracy: 0.0344 - val_loss: 11.5500 - val_pos_accuracy: 0.0312\n",
      "Epoch 28/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 13.7556 - pos_accuracy: 0.0275 - val_loss: 11.3147 - val_pos_accuracy: 0.0913\n",
      "Epoch 29/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 13.9496 - pos_accuracy: 0.0200 - val_loss: 11.5282 - val_pos_accuracy: 0.0312\n",
      "Epoch 30/44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 13.4561 - pos_accuracy: 0.0375 - val_loss: 10.7326 - val_pos_accuracy: 0.0793\n",
      "Epoch 31/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.2979 - pos_accuracy: 0.0306 - val_loss: 11.6252 - val_pos_accuracy: 0.0457\n",
      "Epoch 32/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.2477 - pos_accuracy: 0.0312 - val_loss: 13.4564 - val_pos_accuracy: 0.0361\n",
      "Epoch 33/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.2063 - pos_accuracy: 0.0288 - val_loss: 12.7225 - val_pos_accuracy: 0.0312\n",
      "Epoch 34/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.1636 - pos_accuracy: 0.0175 - val_loss: 14.0895 - val_pos_accuracy: 0.0240\n",
      "Epoch 35/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 13.9798 - pos_accuracy: 0.0244 - val_loss: 13.3902 - val_pos_accuracy: 0.0144\n",
      "Epoch 36/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.2448 - pos_accuracy: 0.0231 - val_loss: 11.7404 - val_pos_accuracy: 0.0361\n",
      "Epoch 37/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.2564 - pos_accuracy: 0.0225 - val_loss: 12.4381 - val_pos_accuracy: 0.0264\n",
      "Epoch 38/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.5459 - pos_accuracy: 0.0169 - val_loss: 15.7324 - val_pos_accuracy: 0.0144\n",
      "Epoch 39/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 13.7973 - pos_accuracy: 0.0219 - val_loss: 14.2294 - val_pos_accuracy: 0.0144\n",
      "Epoch 40/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.4560 - pos_accuracy: 0.0181 - val_loss: 10.8408 - val_pos_accuracy: 0.0721\n",
      "Epoch 41/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 13.8297 - pos_accuracy: 0.0206 - val_loss: 10.7318 - val_pos_accuracy: 0.0505\n",
      "Epoch 42/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.4008 - pos_accuracy: 0.0194 - val_loss: 11.5769 - val_pos_accuracy: 0.0264\n",
      "Epoch 43/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 13.6601 - pos_accuracy: 0.0237 - val_loss: 10.2740 - val_pos_accuracy: 0.0577\n",
      "Epoch 44/44\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 13.2236 - pos_accuracy: 0.0288 - val_loss: 13.9256 - val_pos_accuracy: 0.0337\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:04:16.916286: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1664: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\nnp.concatenate는 두 행렬을 좌우나 위아래로 붙여주는 기능을 한다.(행렬에 행 또는 열을 추가하기 위한 함수)\\naxis=1은 가로로 붙이는 역활을 한다(열기준으로 붙임)\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n121 = subplot의 1행 2열로 만든 도화지에 1번째에 plot을 오리지널 이미지를 표시한다.\\n122 = subplot의 1행 2열로 만든 도화지에 2번째에 plot을 어파인된 이미지를 표시한다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntf.reduce_all의 역할은 and연산 역할을 한다. 따라서 성공한 라벨의 전체 평균을 구한다.\\naxis=1은 accuracy를 열방향으로 출력하기 위함을 의미한다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n차원을 감소하면서 평균을 구하는것이다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\ny=wx+b라는 네트웍의 형태를 만들기 위해 또한 활성함수를 넣을 경우 정확도가 너무 떨어진다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유\\nrandom seed를 사용하지 않을 경우 위 알고리즘이 좋아서 개선된건지, \\nrandom성이 뛰어나서 개선된건지 알수 없다. 또한 아래의 loss, accuracy의 결과가 매번 달라진다.\\n따라서 난수생성 패턴을 동일하게 관리 하도록 사용한다.\\n\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요.\\n교수님 안녕하세요.\\n직장생활을 하며 학업을 병행하고 있는 학생입니다. 9월 30일경 교수님께서 보내주신 문자 메시지를 읽고\\n부랴부랴 과제를 시작하게 되었습니다. 미리 문자 주신 점 깊이 감사드립니다. 문자 내용만 봤을 때는 간단하게 끝냈을 것 같았던 과제라 생각했는데,\\n생각보다 할애하는 시간이 많아 황금연휴를 통으로 사용하게 되었네요. 과제를 하다 보면서 느낀 점은 처음에는 단순히 과제 생각보다\\n어렵다, 귀찮다, 걱정이라는 생각과 달리 과제를 수행하면서 교수님께서 왜 이렇게 과제를 진행하게 했는지 이해가 되더라고요,\\n간단한 수정작업이지만, 내가 직업 수정해야 함에 따라 오류도 떠보고 다시 고쳐보고 하면서 아.. 이게 이렇게 되는구나 깨닫게 되는 게\\n참 많았습니다. 그러면서 이거 만만하게 볼 게 아닌 과목이구나 좀 더 학업에 열심히 매진해서 이해되지 않으면 질문도 잘하고 또 계속 반\\n복적으로 학습이 필요하다 느끼게 된 과제였습니다. 비록 처음엔 채찍 같은 과제였으나, 결국은 딥러닝과 컴퓨터 비전이라는 과목을\\n이해하는데 많은 도움이 된 당근 같은 과제였다고 생각합니다. 항상 좋은 강의 감사합니다. - 미래학부 고동일 드림\\nps. 4번과제가 정말 어려웠습니다 ㅠ..\\n\"\n",
      "    ],\n",
      "    \"score\": 96.36363636363636,\n",
      "    \"accuracy\": 0.0649\n",
      "}\"report1/윤찬혁_46088.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/윤찬혁_46088.ipynb to python\n",
      "[NbConvertApp] Writing 34830 bytes to report1/윤찬혁_46088.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:217: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:250: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 290705.85it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:04:25.295083: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:04:25.670589: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:04:25.670637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:04:25.893506: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:04:26.494339: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 114843.22it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:04:28.741704: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1674: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\nnp.concatenate는 다차원의 배열을 합치는데 사용하는 함수이고\\naxis=1는 그 축이 하나 라는 뜻을 의미합니다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n같은 자료를 다른 방식으로 표현하기 위함.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 61.45454545454545,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/강길모_46096.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/강길모_46096.ipynb to python\n",
      "[NbConvertApp] Writing 35936 bytes to report1/강길모_46096.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:209: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:240: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 305284.52it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:04:35.104312: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:04:35.506821: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:04:35.506866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:04:35.733443: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:04:36.395585: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 7ms/step - loss: 175.8720 - pos_accuracy: 0.0012 - val_loss: 105.9368 - val_pos_accuracy: 0.0022\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 84.0050 - pos_accuracy: 0.0025 - val_loss: 54.7166 - val_pos_accuracy: 0.0067\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 93908.99it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 32.8955 - pos_accuracy: 0.0213 - val_loss: 10.2731 - val_pos_accuracy: 0.0457\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/step - loss: 3.0347 - pos_accuracy: 0.1663 - val_loss: 1.2565 - val_pos_accuracy: 0.3606\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.2409 - pos_accuracy: 0.2569 - val_loss: 1.2991 - val_pos_accuracy: 0.3582\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6791 - pos_accuracy: 0.4375 - val_loss: 0.3676 - val_pos_accuracy: 0.5577\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5371 - pos_accuracy: 0.5019 - val_loss: 0.4161 - val_pos_accuracy: 0.5264\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2242 - pos_accuracy: 0.6812 - val_loss: 0.2532 - val_pos_accuracy: 0.7284\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3161 - pos_accuracy: 0.6562 - val_loss: 0.2334 - val_pos_accuracy: 0.7212\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1861 - pos_accuracy: 0.7125 - val_loss: 0.1751 - val_pos_accuracy: 0.8221\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1139 - pos_accuracy: 0.8219 - val_loss: 0.1533 - val_pos_accuracy: 0.8413\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0766 - pos_accuracy: 0.8687 - val_loss: 0.1406 - val_pos_accuracy: 0.8846\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0511 - pos_accuracy: 0.9400 - val_loss: 0.1420 - val_pos_accuracy: 0.8702\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0366 - pos_accuracy: 0.9613 - val_loss: 0.1204 - val_pos_accuracy: 0.8942\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0359 - pos_accuracy: 0.9550 - val_loss: 0.1222 - val_pos_accuracy: 0.8774\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0324 - pos_accuracy: 0.9650 - val_loss: 0.1075 - val_pos_accuracy: 0.9038\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0244 - pos_accuracy: 0.9744 - val_loss: 0.1124 - val_pos_accuracy: 0.9014\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0184 - pos_accuracy: 0.9875 - val_loss: 0.1172 - val_pos_accuracy: 0.8678\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0492 - pos_accuracy: 0.9337 - val_loss: 0.0985 - val_pos_accuracy: 0.9062\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0191 - pos_accuracy: 0.9850 - val_loss: 0.1040 - val_pos_accuracy: 0.9014\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0692 - pos_accuracy: 0.9137 - val_loss: 0.1137 - val_pos_accuracy: 0.9014\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0170 - pos_accuracy: 0.9912 - val_loss: 0.0939 - val_pos_accuracy: 0.9159\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:04:42.304732: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "0.6013071895424837\n",
      "0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1691: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concatenate는 파이썬 배열을 Axis의 기중으로 연결 하나의 배열로 만들어주는 함수\\naxis=0은 행을 기준으로 연결(위에서 아래로 연결) \\naxis=1은 열을 기준으로 연결(좌에서 우로 연결) \\nEX)\\na=np.array([[1,2],[10,20]])\\nb=np.array([3,4],[30,40])\\nnp.concatenate((a,b),axis=0)\\n>>>[[1,2],[10,20],[3,4],[30,40]]\\nprint(np.concatenate((a,b),axis=1))\\n>>>[[1,2,3,4],[10,20,30,40]]\\n\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\nsubplot(121)과 subplot(122)에서 첫번째 숫자 1은 1행을 두번째 숫자 2는 2열을 의미한다.\\n하나의 plot에 두개의 subplot을 배치할 수 있다.\\n세번째 숫자는 sub plot의 위치를 의미한다. \\n1이면 첫번째, 2이면 두번째에 위치한다.\\n[subplot121 subplog 125]\\n\\nEX [subplot(221) subplot(222)\\n    subplot(223) subplot(224)]\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\ntf.reduce_all은label_true 참값과 label_pred 추정값이 동일할때 TRUE 아니면 False데이터로 axis=1(열) 기준으로 AND 계산 \\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n전체 행렬의 평균값으로 평균 정확도 를 보여줌\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\nlinear 함수로 나타내어야 한다. \\n\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 : 난수 생성시 seed를 설정하고 생성한 난수는, \\n다른 난수생성시 동일 시드르 가질때 동일한 난수를 생성할 수 있어 재현 가능한 난수를 생성할 수 있다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.9159\n",
      "}\"report1/허인회_46067.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/허인회_46067.ipynb to python\n",
      "[NbConvertApp] Writing 33803 bytes to report1/허인회_46067.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[2 3]\n",
      " [5 6]]\n",
      "[[ 4  6]\n",
      " [10 12]]\n",
      "[[ 7]\n",
      " [13]]\n",
      "[5. 6.]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:243: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[inf inf]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -18. ]\n",
      " [  0.    1.2 -19. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 311890.54it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:04:58.699668: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:04:59.080230: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:04:59.080283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:04:59.304152: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:04:59.924023: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 95565.09it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 100,738\n",
      "Trainable params: 100,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 48.6895 - pos_accuracy: 0.0475 - val_loss: 3.2691 - val_pos_accuracy: 0.1611\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 3.6345 - pos_accuracy: 0.2269 - val_loss: 1.0534 - val_pos_accuracy: 0.4207\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.9034 - pos_accuracy: 0.3750 - val_loss: 0.8029 - val_pos_accuracy: 0.4399\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4676 - pos_accuracy: 0.5244 - val_loss: 0.3574 - val_pos_accuracy: 0.6370\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5402 - pos_accuracy: 0.5425 - val_loss: 0.2467 - val_pos_accuracy: 0.6923\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2432 - pos_accuracy: 0.6875 - val_loss: 0.2250 - val_pos_accuracy: 0.7404\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1847 - pos_accuracy: 0.7594 - val_loss: 0.1673 - val_pos_accuracy: 0.8221\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1428 - pos_accuracy: 0.7856 - val_loss: 0.1715 - val_pos_accuracy: 0.8053\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0997 - pos_accuracy: 0.8525 - val_loss: 0.1370 - val_pos_accuracy: 0.8269\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0993 - pos_accuracy: 0.8519 - val_loss: 0.1246 - val_pos_accuracy: 0.8678\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0463 - pos_accuracy: 0.9494 - val_loss: 0.1337 - val_pos_accuracy: 0.8606\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0444 - pos_accuracy: 0.9369 - val_loss: 0.1063 - val_pos_accuracy: 0.8798\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0276 - pos_accuracy: 0.9781 - val_loss: 0.1160 - val_pos_accuracy: 0.8990\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0241 - pos_accuracy: 0.9831 - val_loss: 0.1008 - val_pos_accuracy: 0.9014\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0171 - pos_accuracy: 0.9875 - val_loss: 0.1101 - val_pos_accuracy: 0.9014\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0193 - pos_accuracy: 0.9819 - val_loss: 0.1008 - val_pos_accuracy: 0.8894\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9862 - val_loss: 0.0932 - val_pos_accuracy: 0.9014\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0165 - pos_accuracy: 0.9831 - val_loss: 0.1215 - val_pos_accuracy: 0.8558\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0209 - pos_accuracy: 0.9781 - val_loss: 0.0925 - val_pos_accuracy: 0.8942\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0101 - pos_accuracy: 0.9937 - val_loss: 0.0982 - val_pos_accuracy: 0.9038\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0082 - pos_accuracy: 0.9962 - val_loss: 0.0837 - val_pos_accuracy: 0.9159\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0063 - pos_accuracy: 0.9969 - val_loss: 0.0841 - val_pos_accuracy: 0.9087\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9969 - val_loss: 0.0847 - val_pos_accuracy: 0.9135\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9956 - val_loss: 0.0802 - val_pos_accuracy: 0.9135\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9969 - val_loss: 0.0809 - val_pos_accuracy: 0.9111\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9975 - val_loss: 0.0794 - val_pos_accuracy: 0.9159\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9975 - val_loss: 0.0838 - val_pos_accuracy: 0.9159\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9981 - val_loss: 0.0802 - val_pos_accuracy: 0.9111\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.0794 - val_pos_accuracy: 0.9159\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0021 - pos_accuracy: 0.9987 - val_loss: 0.0802 - val_pos_accuracy: 0.9159\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:05:07.556829: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1594: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"선택한 축을 따라 배열 시퀸스를 결합하는 함수가 np.concatenate이고 axis=1의 의미는 연결 배열의 방향성(1의 경우,열방향)을 의미합니다.\",\n",
      "        \"121은 1*2로 subplot을 잡은 것 중 첫번째, 122는 1*2로 subplot을 잡은것 중 두번째\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"축 방향에 따른 텐서를 줄여준다. 위 함수에경우 실제와 예측이 같은 텐서를 줄이고, 방향은 axis=1로 행방향이다.  \",\n",
      "        \"지정한 텐서를 줄여주고 평균을 구하는 함수이다. 위의 경우 스칼라 값의 결과가 나온다.\",\n",
      "        \"활성화함수는 정규화되어 있기 때문에, 데이터에 따라 None의 경우가 더 안정적일 수 있기 때문이다.\",\n",
      "        \"Random성을 입맛에 맞게 제어할 수 있는데 쓰이는 함수이다.\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 90.0,\n",
      "    \"accuracy\": 0.9399\n",
      "}\"report1/권대천_46052.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/권대천_46052.ipynb to python\n",
      "[NbConvertApp] Writing 36570 bytes to report1/권대천_46052.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 309371.49it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:05:14.883610: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:05:15.265888: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:05:15.265929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:05:15.498002: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:05:16.100882: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 100509.32it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 117,250\n",
      "Trainable params: 117,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 27.6923 - pos_accuracy: 0.0400 - val_loss: 1.9931 - val_pos_accuracy: 0.1825\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 1.3590 - pos_accuracy: 0.2275 - val_loss: 0.5062 - val_pos_accuracy: 0.4150\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.5443 - pos_accuracy: 0.4306 - val_loss: 0.3042 - val_pos_accuracy: 0.5575\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.3135 - pos_accuracy: 0.6206 - val_loss: 0.3283 - val_pos_accuracy: 0.6875\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.1628 - pos_accuracy: 0.7394 - val_loss: 0.1272 - val_pos_accuracy: 0.8100\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0924 - pos_accuracy: 0.8450 - val_loss: 0.2084 - val_pos_accuracy: 0.7000\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0686 - pos_accuracy: 0.9087 - val_loss: 0.0901 - val_pos_accuracy: 0.8975\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0460 - pos_accuracy: 0.9375 - val_loss: 0.1046 - val_pos_accuracy: 0.8650\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0320 - pos_accuracy: 0.9669 - val_loss: 0.0746 - val_pos_accuracy: 0.8975\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0238 - pos_accuracy: 0.9800 - val_loss: 0.0932 - val_pos_accuracy: 0.8825\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0201 - pos_accuracy: 0.9875 - val_loss: 0.0642 - val_pos_accuracy: 0.9250\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0128 - pos_accuracy: 0.9969 - val_loss: 0.0575 - val_pos_accuracy: 0.9300\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0113 - pos_accuracy: 0.9937 - val_loss: 0.0585 - val_pos_accuracy: 0.9350\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0093 - pos_accuracy: 0.9975 - val_loss: 0.0538 - val_pos_accuracy: 0.9350\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0056 - pos_accuracy: 0.9987 - val_loss: 0.0567 - val_pos_accuracy: 0.9275\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0055 - pos_accuracy: 0.9994 - val_loss: 0.0515 - val_pos_accuracy: 0.9375\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0046 - pos_accuracy: 0.9987 - val_loss: 0.0471 - val_pos_accuracy: 0.9375\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0032 - pos_accuracy: 1.0000 - val_loss: 0.0468 - val_pos_accuracy: 0.9375\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0029 - pos_accuracy: 1.0000 - val_loss: 0.0454 - val_pos_accuracy: 0.9475\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0021 - pos_accuracy: 1.0000 - val_loss: 0.0462 - val_pos_accuracy: 0.9400\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0019 - pos_accuracy: 1.0000 - val_loss: 0.0449 - val_pos_accuracy: 0.9400\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0014 - pos_accuracy: 1.0000 - val_loss: 0.0452 - val_pos_accuracy: 0.9375\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0012 - pos_accuracy: 1.0000 - val_loss: 0.0472 - val_pos_accuracy: 0.9400\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.0011 - pos_accuracy: 1.0000 - val_loss: 0.0445 - val_pos_accuracy: 0.9375\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 8.7698e-04 - pos_accuracy: 1.0000 - val_loss: 0.0452 - val_pos_accuracy: 0.9400\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 7.9213e-04 - pos_accuracy: 1.0000 - val_loss: 0.0452 - val_pos_accuracy: 0.9400\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 7.1764e-04 - pos_accuracy: 1.0000 - val_loss: 0.0464 - val_pos_accuracy: 0.9400\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 6.8106e-04 - pos_accuracy: 1.0000 - val_loss: 0.0453 - val_pos_accuracy: 0.9375\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 3s 3ms/step - loss: 4.8131e-04 - pos_accuracy: 1.0000 - val_loss: 0.0446 - val_pos_accuracy: 0.9400\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 4.2509e-04 - pos_accuracy: 1.0000 - val_loss: 0.0447 - val_pos_accuracy: 0.9375\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 4.2666e-04 - pos_accuracy: 1.0000 - val_loss: 0.0451 - val_pos_accuracy: 0.9375\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 3.9942e-04 - pos_accuracy: 1.0000 - val_loss: 0.0439 - val_pos_accuracy: 0.9400\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 2.7860e-04 - pos_accuracy: 1.0000 - val_loss: 0.0438 - val_pos_accuracy: 0.9400\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 2.4779e-04 - pos_accuracy: 1.0000 - val_loss: 0.0446 - val_pos_accuracy: 0.9400\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 2.2885e-04 - pos_accuracy: 1.0000 - val_loss: 0.0444 - val_pos_accuracy: 0.9400\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 1.9596e-04 - pos_accuracy: 1.0000 - val_loss: 0.0445 - val_pos_accuracy: 0.9400\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 1.8746e-04 - pos_accuracy: 1.0000 - val_loss: 0.0440 - val_pos_accuracy: 0.9400\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 1.5521e-04 - pos_accuracy: 1.0000 - val_loss: 0.0441 - val_pos_accuracy: 0.9400\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 1.3672e-04 - pos_accuracy: 1.0000 - val_loss: 0.0439 - val_pos_accuracy: 0.9400\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 1.3158e-04 - pos_accuracy: 1.0000 - val_loss: 0.0440 - val_pos_accuracy: 0.9400\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 1.2920e-04 - pos_accuracy: 1.0000 - val_loss: 0.0439 - val_pos_accuracy: 0.9400\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 1.0025e-04 - pos_accuracy: 1.0000 - val_loss: 0.0439 - val_pos_accuracy: 0.9400\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 8.6803e-05 - pos_accuracy: 1.0000 - val_loss: 0.0437 - val_pos_accuracy: 0.9400\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 8.3146e-05 - pos_accuracy: 1.0000 - val_loss: 0.0437 - val_pos_accuracy: 0.9375\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 6.6374e-05 - pos_accuracy: 1.0000 - val_loss: 0.0438 - val_pos_accuracy: 0.9400\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 6.9321e-05 - pos_accuracy: 1.0000 - val_loss: 0.0437 - val_pos_accuracy: 0.9400\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 5.8405e-05 - pos_accuracy: 1.0000 - val_loss: 0.0439 - val_pos_accuracy: 0.9400\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 5.4426e-05 - pos_accuracy: 1.0000 - val_loss: 0.0441 - val_pos_accuracy: 0.9400\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 5.1242e-05 - pos_accuracy: 1.0000 - val_loss: 0.0436 - val_pos_accuracy: 0.9400\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 4.4148e-05 - pos_accuracy: 1.0000 - val_loss: 0.0437 - val_pos_accuracy: 0.9400\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:07:27.498180: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "[[0.58823529 0.58823529 0.58823529]\n",
      " [0.58823529 0.58823529 0.61568627]\n",
      " [0.59607843 0.61568627 0.64313725]]\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1740: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concatenate는 행렬 혹은 배열을 합치는 역할을 한다.\\n합치는 방향을 정하는 것이 axis 이다.\\naxis = 0 일 때, 행의 진행 방향으로 합쳐지고,\\naxis = 1 일 때, 열의 진행 방향으로 합쳐진다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\nplt.subplot 은 하나의 큰 그림 속에 여러 개의 작은 그림이 표현되도록 한다.\\n121과 122에서 앞의 12는 작은 2개의 그림을 1행 2열의 형태로 나타나게 한다.\\n그리고 121의 뒤의 1과 122의 뒤의 2는 작은 그림의 위치를 지정하는데,\\n1일 때 왼쪽, 2일 때 오른쪽에 지정된다. \\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\ntf.reduce_all 은 원소를 비교하여 True 혹은 False 를 반환한다. \\nAnd 연산의 성질을 갖고 있기 때문에 비교되는 원소가 모두 일치하면, True 이다.\\n\\naxis는 비교하는 방향이다. axis = 1 이면 열의 진행 방향으로 비교가 이루어진다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n\\ntf.reduce_mean 은 텐서 내의 원소를 모두 더한 후에 평균값을 반환한다.\\n해당 문제에서 점의 위치가 정확하게 일치하면 1이 생성되고, 불일치이면 0이 생성된다.\\n근사하게 일치하면, 0과 1 사이의 값이 생성된다. \\n1에 가까울수록 일치 정도가 높고 0에 가까울수록 불일치의 정도가 높다.\\n\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n\\n해당 문제는 분류를 요하지 않고 있다.\\n점의 위치가 일치하는지를 확인하는 문제이다.\\n그렇기 때문에 activation = None로 설정하고 있다. \\n\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n\\nrandom 은 실행할 때마다 새로운 난수를 생성한다.\\n이렇게 되면 결과의 일관성이 사라진다.\\n그래서 seed 를 사용한다. \\nseed를 사용하면 기존에 생성되었던 난수를 그대로 이어서 활용할 수 있다.\\n\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\\n다음은 수업에 대한 감상입니다. \\n\\n마치 현장 실무에 아무 것도 모른채 투입되어 사수로부터 맞아가면서 익히는 느낌입니다.\\n교수자가 그러한 의도로 강의를 진행한다고 보지는 않습니다. \\n도리어 교수자와 같은 사수를 만나면, 행운일 것이라고 생각합니다.\\n\\n현재 학교의 커리큘럼을 들여다 보고 있습니다.\\n인공지능이라는 학문이 난삽한 면이 있기는 하지만 그걸 감안할지라도\\n학교 커리큘럼 자체에 문제가 많다고 생각이 듭니다. \\n\\n커리큘럼 문제와는 별도로 교수자에게 요청하고 싶은 부분이 있습니다.\\n제가 볼 때, 교수자의 강점은 이론보다 실전에 있는 것으로 보입니다. \\n그래서인지 설명 속에 도약하는 부분들이 자주 발견됩니다.\\n\\n도약된 부분을 이해하는 데에 상당한 어려움을 겪고 있습니다.\\n어떤 함수 혹은 어떤 옵션을 적용할 때, 왜 그것이 적용되는지에 대한 설명이 부족합니다.\\n\\n차라리 수업에서 이론 강의를 줄이고, 실습을 늘리는 것이 더 나을 것으로 사료됩니다.\\n실습 시간에 교수자가 적용한 함수나 옵션에 대한 발상을 충분히 이야기하면 좋을 것이라고 생각합니다. \\n\\n일일히 자료를 찾으면서 공부를 하지만, 효율이 매우 떨어집니다.\\n자기 스스로 찾으면서 공부를 하는 것이 원칙이지만, \\n어느 정도의 효율은 필요하다고 보기 때문입니다.\\n\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.945\n",
      "}\"report1/박시현_46031.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/박시현_46031.ipynb to python\n",
      "[NbConvertApp] Writing 35860 bytes to report1/박시현_46031.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "ans01: w를 copy \n",
      " [[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "v: w의 노란색 부분\n",
      " [[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "u: v*2\n",
      " [[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "ans02: u의 두번째 3번째 행에 1을 더한 a를 copy\n",
      " [[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:212: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "0 [17. 39.]\n",
      "1 [ 95. 207.]\n",
      "2 [ 509. 1113.]\n",
      "3 [2735. 5979.]\n",
      "4 [14693. 32121.]\n",
      "5 [ 78935. 172563.]\n",
      "6 [424061. 927057.]\n",
      "7 [2278175. 4980411.]\n",
      "8 [12238997. 26756169.]\n",
      "9 [6.57513350e+07 1.43741667e+08]\n",
      "10 [3.53234669e+08 7.72220673e+08]\n",
      "11 [1.89767602e+09 4.14858670e+09]\n",
      "12 [1.01948494e+10 2.22873748e+10]\n",
      "13 [5.47695991e+10 1.19734048e+11]\n",
      "14 [2.94237694e+11 6.43244988e+11]\n",
      "15 [1.58072767e+12 3.45569303e+12]\n",
      "16 [8.49211374e+12 1.85649551e+13]\n",
      "17 [4.56220240e+13 9.97361618e+13]\n",
      "18 [2.45094348e+14 5.35810719e+14]\n",
      "19 [1.31671579e+15 2.87852592e+15]\n",
      "20 [7.07376763e+15 1.54642510e+16]\n",
      "21 [3.80022697e+16 8.30783070e+16]\n",
      "22 [2.04158884e+17 4.46320037e+17]\n",
      "23 [1.09679896e+18 2.39775680e+18]\n",
      "24 [5.89231256e+18 1.28814241e+19]\n",
      "25 [3.16551607e+19 6.92026340e+19]\n",
      "26 [1.70060429e+20 3.71776018e+20]\n",
      "27 [9.13612465e+20 1.99728536e+21]\n",
      "28 [4.90818318e+21 1.07299788e+22]\n",
      "29 [2.63681408e+22 5.76444648e+22]\n",
      "30 [1.41657071e+23 3.09682282e+23]\n",
      "31 [7.61021634e+23 1.66370034e+24]\n",
      "32 [4.08842231e+24 8.93786626e+24]\n",
      "33 [2.19641548e+25 4.80167320e+25]\n",
      "34 [1.17997619e+26 2.57959392e+26]\n",
      "35 [6.33916404e+26 1.38583043e+27]\n",
      "36 [3.40557726e+27 7.44507091e+27]\n",
      "37 [1.82957191e+28 3.99970154e+28]\n",
      "38 [9.82897499e+28 2.14875219e+29]\n",
      "39 [5.28040188e+29 1.15437013e+30]\n",
      "40 [2.83678044e+30 6.20160107e+30]\n",
      "41 [1.52399826e+31 3.33167456e+31]\n",
      "42 [8.18734737e+31 1.78986930e+32]\n",
      "43 [4.39847334e+32 9.61568141e+32]\n",
      "44 [2.36298362e+33 5.16581457e+33]\n",
      "45 [1.26946127e+34 2.77522091e+34]\n",
      "46 [6.81990310e+34 1.49092675e+35]\n",
      "47 [3.66384380e+35 8.00967792e+35]\n",
      "48 [1.96831996e+36 4.30302431e+36]\n",
      "49 [1.05743686e+37 2.31170571e+37]\n",
      "50 [5.68084828e+37 1.24191334e+38]\n",
      "51 [3.05191151e+38 6.67190785e+38]\n",
      "52 [1.63957272e+39 3.58433660e+39]\n",
      "53 [8.80824591e+39 1.92560646e+40]\n",
      "54 [4.73203750e+40 1.03448996e+41]\n",
      "55 [2.54218367e+41 5.55757109e+41]\n",
      "56 [1.36573258e+42 2.98568354e+42]\n",
      "57 [7.33709966e+42 1.60399319e+43]\n",
      "58 [3.94169635e+43 8.61710266e+43]\n",
      "59 [2.11759017e+44 4.62934997e+44]\n",
      "60 [1.13762901e+45 2.48701704e+45]\n",
      "61 [6.11166308e+45 1.33609552e+46]\n",
      "62 [3.28335734e+46 7.17788099e+46]\n",
      "63 [1.76391193e+47 3.85615960e+47]\n",
      "64 [9.47623113e+47 2.07163742e+48]\n",
      "65 [5.09089795e+48 1.11294190e+49]\n",
      "66 [2.73497360e+49 5.97903699e+49]\n",
      "67 [1.46930476e+50 3.21210688e+50]\n",
      "68 [7.89351851e+50 1.72563418e+51]\n",
      "69 [4.24062021e+51 9.27059227e+51]\n",
      "70 [2.27818047e+52 4.98042297e+52]\n",
      "71 [1.22390264e+53 2.67562333e+53]\n",
      "72 [6.57514930e+53 1.43742012e+54]\n",
      "73 [3.53235518e+54 7.72222529e+54]\n",
      "74 [1.89768058e+55 4.14859667e+55]\n",
      "75 [1.01948739e+56 2.22874284e+56]\n",
      "76 [5.47697307e+56 1.19734335e+57]\n",
      "77 [2.94238401e+57 6.43246534e+57]\n",
      "78 [1.58073147e+58 3.45570134e+58]\n",
      "79 [8.49213415e+58 1.85649998e+59]\n",
      "80 [4.56221337e+59 9.97364015e+59]\n",
      "81 [2.45094937e+60 5.35812007e+60]\n",
      "82 [1.31671895e+61 2.87853284e+61]\n",
      "83 [7.07378463e+61 1.54642882e+62]\n",
      "84 [3.80023610e+62 8.30785067e+62]\n",
      "85 [2.04159374e+63 4.46321110e+63]\n",
      "86 [1.09680159e+64 2.39776256e+64]\n",
      "87 [5.89232672e+64 1.28814550e+65]\n",
      "88 [3.16552368e+65 6.92028003e+65]\n",
      "89 [1.70060837e+66 3.71776912e+66]\n",
      "90 [9.13614660e+66 1.99729016e+67]\n",
      "91 [4.90819498e+67 1.07300046e+68]\n",
      "92 [2.63682042e+68 5.76446034e+68]\n",
      "93 [1.41657411e+69 3.09683026e+69]\n",
      "94 [7.61023463e+69 1.66370434e+70]\n",
      "95 [4.08843214e+70 8.93788774e+70]\n",
      "96 [2.19642076e+71 4.80168474e+71]\n",
      "97 [1.17997902e+72 2.57960012e+72]\n",
      "98 [6.33917927e+72 1.38583376e+73]\n",
      "99 [3.40558544e+73 7.44508881e+73]\n",
      "100 [1.82957631e+74 3.99971116e+74]\n",
      "101 [9.82899862e+74 2.14875735e+75]\n",
      "102 [5.28041457e+75 1.15437290e+76]\n",
      "103 [2.83678726e+76 6.20161597e+76]\n",
      "104 [1.52400192e+77 3.33168257e+77]\n",
      "105 [8.18736705e+77 1.78987360e+78]\n",
      "106 [4.39848391e+78 9.61570452e+78]\n",
      "107 [2.36298930e+79 5.16582698e+79]\n",
      "108 [1.26946433e+80 2.77522758e+80]\n",
      "109 [6.81991949e+80 1.49093033e+81]\n",
      "110 [3.66385261e+81 8.00969717e+81]\n",
      "111 [1.96832469e+82 4.30303465e+82]\n",
      "112 [1.05743940e+83 2.31171127e+83]\n",
      "113 [5.68086194e+83 1.24191633e+84]\n",
      "114 [3.05191885e+84 6.67192389e+84]\n",
      "115 [1.63957666e+85 3.58434521e+85]\n",
      "116 [8.80826708e+85 1.92561108e+86]\n",
      "117 [4.73204887e+86 1.03449245e+87]\n",
      "118 [2.54218978e+87 5.55758445e+87]\n",
      "119 [1.36573587e+88 2.98569071e+88]\n",
      "120 [7.33711729e+88 1.60399704e+89]\n",
      "121 [3.94170582e+89 8.61712337e+89]\n",
      "122 [2.11759526e+90 4.62936109e+90]\n",
      "123 [1.13763174e+91 2.48702301e+91]\n",
      "124 [6.11167777e+91 1.33609873e+92]\n",
      "125 [3.28336523e+92 7.17789825e+92]\n",
      "126 [1.76391617e+93 3.85616887e+93]\n",
      "127 [9.47625391e+93 2.07164240e+94]\n",
      "128 [5.09091019e+94 1.11294458e+95]\n",
      "129 [2.73498017e+95 5.97905136e+95]\n",
      "130 [1.46930829e+96 3.21211460e+96]\n",
      "131 [7.89353748e+96 1.72563833e+97]\n",
      "132 [4.24063040e+97 9.27061455e+97]\n",
      "133 [2.27818595e+98 4.98043494e+98]\n",
      "134 [1.22390558e+99 2.67562976e+99]\n",
      "135 [6.57516510e+099 1.43742358e+100]\n",
      "136 [3.53236367e+100 7.72224385e+100]\n",
      "137 [1.89768514e+101 4.14860664e+101]\n",
      "138 [1.01948984e+102 2.22874820e+102]\n",
      "139 [5.47698624e+102 1.19734623e+103]\n",
      "140 [2.94239109e+103 6.43248080e+103]\n",
      "141 [1.58073527e+104 3.45570964e+104]\n",
      "142 [8.49215456e+104 1.85650444e+105]\n",
      "143 [4.56222433e+105 9.97366412e+105]\n",
      "144 [2.45095526e+106 5.35813295e+106]\n",
      "145 [1.31672212e+107 2.87853976e+107]\n",
      "146 [7.07380163e+107 1.54643254e+108]\n",
      "147 [3.80024524e+108 8.30787064e+108]\n",
      "148 [2.04159865e+109 4.46322183e+109]\n",
      "149 [1.09680423e+110 2.39776833e+110]\n",
      "150 [5.89234088e+110 1.28814860e+111]\n",
      "151 [3.16553129e+111 6.92029666e+111]\n",
      "152 [1.70061246e+112 3.71777805e+112]\n",
      "153 [9.13616856e+112 1.99729496e+113]\n",
      "154 [4.90820677e+113 1.07300304e+114]\n",
      "155 [2.63682676e+114 5.76447419e+114]\n",
      "156 [1.41657751e+115 3.09683770e+115]\n",
      "157 [7.61025292e+115 1.66370834e+116]\n",
      "158 [4.08844196e+116 8.93790922e+116]\n",
      "159 [2.19642604e+117 4.80169628e+117]\n",
      "160 [1.17998186e+118 2.57960632e+118]\n",
      "161 [6.33919451e+118 1.38583709e+119]\n",
      "162 [3.40559363e+119 7.44510670e+119]\n",
      "163 [1.82958070e+120 3.99972077e+120]\n",
      "164 [9.82902224e+120 2.14876252e+121]\n",
      "165 [5.28042726e+121 1.15437567e+122]\n",
      "166 [2.83679407e+122 6.20163088e+122]\n",
      "167 [1.52400558e+123 3.33169057e+123]\n",
      "168 [8.18738673e+123 1.78987790e+124]\n",
      "169 [4.39849448e+124 9.61572763e+124]\n",
      "170 [2.36299497e+125 5.16583940e+125]\n",
      "171 [1.26946738e+126 2.77523425e+126]\n",
      "172 [6.81993588e+126 1.49093391e+127]\n",
      "173 [3.66386142e+127 8.00971642e+127]\n",
      "174 [1.96832943e+128 4.30304499e+128]\n",
      "175 [1.05744194e+129 2.31171682e+129]\n",
      "176 [5.68087559e+129 1.24191931e+130]\n",
      "177 [3.05192618e+130 6.67193992e+130]\n",
      "178 [1.63958060e+131 3.58435382e+131]\n",
      "179 [8.80828825e+131 1.92561571e+132]\n",
      "180 [4.73206025e+132 1.03449493e+133]\n",
      "181 [2.54219589e+133 5.55759780e+133]\n",
      "182 [1.36573915e+134 2.98569789e+134]\n",
      "183 [7.33713492e+134 1.60400090e+135]\n",
      "184 [3.94171529e+135 8.61714408e+135]\n",
      "185 [2.11760034e+136 4.62937222e+136]\n",
      "186 [1.13763448e+137 2.48702899e+137]\n",
      "187 [6.11169246e+137 1.33610194e+138]\n",
      "188 [3.28337313e+138 7.17791550e+138]\n",
      "189 [1.76392041e+139 3.85617814e+139]\n",
      "190 [9.47627668e+139 2.07164738e+140]\n",
      "191 [5.09092242e+140 1.11294725e+141]\n",
      "192 [2.73498675e+141 5.97906573e+141]\n",
      "193 [1.46931182e+142 3.21212232e+142]\n",
      "194 [7.89355646e+142 1.72564247e+143]\n",
      "195 [4.24064059e+143 9.27063683e+143]\n",
      "196 [2.27819143e+144 4.98044691e+144]\n",
      "197 [1.22390852e+145 2.67563619e+145]\n",
      "198 [6.57518091e+145 1.43742703e+146]\n",
      "199 [3.53237216e+146 7.72226241e+146]\n",
      "200 [1.89768970e+147 4.14861661e+147]\n",
      "201 [1.01949229e+148 2.22875355e+148]\n",
      "202 [5.47699940e+148 1.19734911e+149]\n",
      "203 [2.94239816e+149 6.43249626e+149]\n",
      "204 [1.58073907e+150 3.45571795e+150]\n",
      "205 [8.49217497e+150 1.85650890e+151]\n",
      "206 [4.56223530e+151 9.97368809e+151]\n",
      "207 [2.45096115e+152 5.35814582e+152]\n",
      "208 [1.31672528e+153 2.87854667e+153]\n",
      "209 [7.07381863e+153 1.54643625e+154]\n",
      "210 [3.80025437e+154 8.30789060e+154]\n",
      "211 [2.04160356e+155 4.46323255e+155]\n",
      "212 [1.09680687e+156 2.39777409e+156]\n",
      "213 [5.89235504e+156 1.28815170e+157]\n",
      "214 [3.16553889e+157 6.92031329e+157]\n",
      "215 [1.70061655e+158 3.71778699e+158]\n",
      "216 [9.13619052e+158 1.99729976e+159]\n",
      "217 [4.90821857e+159 1.07300562e+160]\n",
      "218 [2.63683309e+160 5.76448805e+160]\n",
      "219 [1.41658092e+161 3.09684515e+161]\n",
      "220 [7.61027121e+161 1.66371233e+162]\n",
      "221 [4.08845179e+162 8.93793070e+162]\n",
      "222 [2.19643132e+163 4.80170782e+163]\n",
      "223 [1.17998470e+164 2.57961252e+164]\n",
      "224 [6.33920974e+164 1.38584042e+165]\n",
      "225 [3.40560181e+165 7.44512459e+165]\n",
      "226 [1.82958510e+166 3.99973038e+166]\n",
      "227 [9.82904586e+166 2.14876768e+167]\n",
      "228 [5.28043995e+167 1.15437845e+168]\n",
      "229 [2.83680089e+168 6.20164578e+168]\n",
      "230 [1.52400925e+169 3.33169858e+169]\n",
      "231 [8.18740640e+169 1.78988221e+170]\n",
      "232 [4.39850505e+170 9.61575074e+170]\n",
      "233 [2.36300065e+171 5.16585181e+171]\n",
      "234 [1.26947043e+172 2.77524092e+172]\n",
      "235 [6.81995227e+172 1.49093750e+173]\n",
      "236 [3.66387022e+173 8.00973567e+173]\n",
      "237 [1.96833416e+174 4.30305533e+174]\n",
      "238 [1.05744448e+175 2.31172238e+175]\n",
      "239 [5.68088924e+175 1.24192230e+176]\n",
      "240 [3.05193352e+176 6.67195596e+176]\n",
      "241 [1.63958454e+177 3.58436244e+177]\n",
      "242 [8.80830942e+177 1.92562034e+178]\n",
      "243 [4.73207162e+178 1.03449742e+179]\n",
      "244 [2.54220200e+179 5.55761116e+179]\n",
      "245 [1.36574243e+180 2.98570506e+180]\n",
      "246 [7.33715256e+180 1.60400475e+181]\n",
      "247 [3.94172477e+181 8.61716479e+181]\n",
      "248 [2.11760543e+182 4.62938334e+182]\n",
      "249 [1.13763721e+183 2.48703497e+183]\n",
      "250 [6.11170715e+183 1.33610515e+184]\n",
      "251 [3.28338102e+184 7.17793275e+184]\n",
      "252 [1.76392465e+185 3.85618740e+185]\n",
      "253 [9.47629946e+185 2.07165236e+186]\n",
      "254 [5.09093466e+186 1.11294993e+187]\n",
      "255 [2.73499332e+187 5.97908010e+187]\n",
      "256 [1.46931535e+188 3.21213004e+188]\n",
      "257 [7.89357543e+188 1.72564662e+189]\n",
      "258 [4.24065078e+189 9.27065911e+189]\n",
      "259 [2.27819690e+190 4.98045888e+190]\n",
      "260 [1.22391147e+191 2.67564262e+191]\n",
      "261 [6.57519671e+191 1.43743049e+192]\n",
      "262 [3.53238065e+192 7.72228097e+192]\n",
      "263 [1.89769426e+193 4.14862658e+193]\n",
      "264 [1.01949474e+194 2.22875891e+194]\n",
      "265 [5.47701256e+194 1.19735199e+195]\n",
      "266 [2.94240523e+195 6.43251172e+195]\n",
      "267 [1.58074287e+196 3.45572625e+196]\n",
      "268 [8.49219538e+196 1.85651336e+197]\n",
      "269 [4.56224626e+197 9.97371206e+197]\n",
      "270 [2.45096704e+198 5.35815870e+198]\n",
      "271 [1.31672844e+199 2.87855359e+199]\n",
      "272 [7.07383563e+199 1.54643997e+200]\n",
      "273 [3.80026350e+200 8.30791057e+200]\n",
      "274 [2.04160846e+201 4.46324328e+201]\n",
      "275 [1.09680950e+202 2.39777985e+202]\n",
      "276 [5.89236920e+202 1.28815479e+203]\n",
      "277 [3.16554650e+203 6.92032993e+203]\n",
      "278 [1.70062064e+204 3.71779592e+204]\n",
      "279 [9.13621248e+204 1.99730456e+205]\n",
      "280 [4.90823037e+205 1.07300820e+206]\n",
      "281 [2.63683943e+206 5.76450190e+206]\n",
      "282 [1.41658432e+207 3.09685259e+207]\n",
      "283 [7.61028950e+207 1.66371633e+208]\n",
      "284 [4.08846162e+208 8.93795218e+208]\n",
      "285 [2.19643660e+209 4.80171936e+209]\n",
      "286 [1.17998753e+210 2.57961872e+210]\n",
      "287 [6.33922498e+210 1.38584375e+211]\n",
      "288 [3.40560999e+211 7.44514249e+211]\n",
      "289 [1.82958950e+212 3.99973999e+212]\n",
      "290 [9.82906948e+212 2.14877285e+213]\n",
      "291 [5.28045264e+213 1.15438122e+214]\n",
      "292 [2.83680771e+214 6.20166068e+214]\n",
      "293 [1.52401291e+215 3.33170659e+215]\n",
      "294 [8.18742608e+215 1.78988651e+216]\n",
      "295 [4.39851562e+216 9.61577385e+216]\n",
      "296 [2.36300633e+217 5.16586423e+217]\n",
      "297 [1.26947348e+218 2.77524759e+218]\n",
      "298 [6.81996866e+218 1.49094108e+219]\n",
      "299 [3.66387903e+219 8.00975492e+219]\n",
      "300 [1.96833889e+220 4.30306568e+220]\n",
      "301 [1.05744702e+221 2.31172794e+221]\n",
      "302 [5.68090290e+221 1.24192528e+222]\n",
      "303 [3.05194085e+222 6.67197200e+222]\n",
      "304 [1.63958848e+223 3.58437105e+223]\n",
      "305 [8.80833059e+223 1.92562497e+224]\n",
      "306 [4.73208299e+224 1.03449990e+225]\n",
      "307 [2.54220811e+225 5.55762452e+225]\n",
      "308 [1.36574571e+226 2.98571224e+226]\n",
      "309 [7.33717019e+226 1.60400861e+227]\n",
      "310 [3.94173424e+227 8.61718550e+227]\n",
      "311 [2.11761052e+228 4.62939447e+228]\n",
      "312 [1.13763995e+229 2.48704094e+229]\n",
      "313 [6.11172184e+229 1.33610836e+230]\n",
      "314 [3.28338891e+230 7.17795000e+230]\n",
      "315 [1.76392889e+231 3.85619667e+231]\n",
      "316 [9.47632223e+231 2.07165734e+232]\n",
      "317 [5.09094689e+232 1.11295260e+233]\n",
      "318 [2.73499989e+233 5.97909447e+233]\n",
      "319 [1.46931888e+234 3.21213776e+234]\n",
      "320 [7.89359440e+234 1.72565077e+235]\n",
      "321 [4.24066098e+235 9.27068139e+235]\n",
      "322 [2.27820238e+236 4.98047085e+236]\n",
      "323 [1.22391441e+237 2.67564905e+237]\n",
      "324 [6.57521251e+237 1.43743394e+238]\n",
      "325 [3.53238914e+238 7.72229953e+238]\n",
      "326 [1.89769882e+239 4.14863655e+239]\n",
      "327 [1.01949719e+240 2.22876427e+240]\n",
      "328 [5.47702573e+240 1.19735486e+241]\n",
      "329 [2.94241230e+241 6.43252717e+241]\n",
      "330 [1.58074667e+242 3.45573456e+242]\n",
      "331 [8.49221579e+242 1.85651782e+243]\n",
      "332 [4.56225723e+243 9.97373603e+243]\n",
      "333 [2.45097293e+244 5.35817158e+244]\n",
      "334 [1.31673161e+245 2.87856051e+245]\n",
      "335 [7.07385263e+245 1.54644369e+246]\n",
      "336 [3.80027264e+246 8.30793054e+246]\n",
      "337 [2.04161337e+247 4.46325401e+247]\n",
      "338 [1.09681214e+248 2.39778561e+248]\n",
      "339 [5.89238337e+248 1.28815789e+249]\n",
      "340 [3.16555411e+249 6.92034656e+249]\n",
      "341 [1.70062472e+250 3.71780486e+250]\n",
      "342 [9.13623443e+250 1.99730936e+251]\n",
      "343 [4.90824216e+251 1.07301078e+252]\n",
      "344 [2.63684577e+252 5.76451576e+252]\n",
      "345 [1.41658773e+253 3.09686003e+253]\n",
      "346 [7.61030779e+253 1.66372033e+254]\n",
      "347 [4.08847144e+254 8.93797366e+254]\n",
      "348 [2.19644188e+255 4.80173090e+255]\n",
      "349 [1.17999037e+256 2.57962492e+256]\n",
      "350 [6.33924021e+256 1.38584708e+257]\n",
      "351 [3.40561818e+257 7.44516038e+257]\n",
      "352 [1.82959389e+258 3.99974961e+258]\n",
      "353 [9.82909311e+258 2.14877801e+259]\n",
      "354 [5.28046533e+259 1.15438400e+260]\n",
      "355 [2.83681453e+260 6.20167559e+260]\n",
      "356 [1.52401657e+261 3.33171459e+261]\n",
      "357 [8.18744576e+261 1.78989081e+262]\n",
      "358 [4.39852619e+262 9.61579696e+262]\n",
      "359 [2.36301201e+263 5.16587664e+263]\n",
      "360 [1.26947653e+264 2.77525426e+264]\n",
      "361 [6.81998505e+264 1.49094466e+265]\n",
      "362 [3.66388783e+265 8.00977417e+265]\n",
      "363 [1.96834362e+266 4.30307602e+266]\n",
      "364 [1.05744957e+267 2.31173349e+267]\n",
      "365 [5.68091655e+267 1.24192827e+268]\n",
      "366 [3.05194819e+268 6.67198803e+268]\n",
      "367 [1.63959242e+269 3.58437967e+269]\n",
      "368 [8.80835176e+269 1.92562959e+270]\n",
      "369 [4.73209437e+270 1.03450239e+271]\n",
      "370 [2.54221422e+271 5.55763787e+271]\n",
      "371 [1.36574900e+272 2.98571941e+272]\n",
      "372 [7.33718783e+272 1.60401246e+273]\n",
      "373 [3.94174371e+273 8.61720621e+273]\n",
      "374 [2.11761561e+274 4.62940560e+274]\n",
      "375 [1.13764268e+275 2.48704692e+275]\n",
      "376 [6.11173652e+275 1.33611157e+276]\n",
      "377 [3.28339680e+276 7.17796725e+276]\n",
      "378 [1.76393313e+277 3.85620594e+277]\n",
      "379 [9.47634501e+277 2.07166231e+278]\n",
      "380 [5.09095913e+278 1.11295528e+279]\n",
      "381 [2.73500647e+279 5.97910884e+279]\n",
      "382 [1.46932242e+280 3.21214548e+280]\n",
      "383 [7.89361337e+280 1.72565492e+281]\n",
      "384 [4.24067117e+281 9.27070367e+281]\n",
      "385 [2.27820785e+282 4.98048282e+282]\n",
      "386 [1.22391735e+283 2.67565548e+283]\n",
      "387 [6.57522832e+283 1.43743740e+284]\n",
      "388 [3.53239763e+284 7.72231809e+284]\n",
      "389 [1.89770338e+285 4.14864652e+285]\n",
      "390 [1.01949964e+286 2.22876962e+286]\n",
      "391 [5.47703889e+286 1.19735774e+287]\n",
      "392 [2.94241937e+287 6.43254263e+287]\n",
      "393 [1.58075046e+288 3.45574287e+288]\n",
      "394 [8.49223620e+288 1.85652229e+289]\n",
      "395 [4.56226819e+289 9.97376000e+289]\n",
      "396 [2.45097882e+290 5.35818446e+290]\n",
      "397 [1.31673477e+291 2.87856743e+291]\n",
      "398 [7.07386963e+291 1.54644740e+292]\n",
      "399 [3.80028177e+292 8.30795050e+292]\n",
      "400 [2.04161828e+293 4.46326473e+293]\n",
      "401 [1.09681477e+294 2.39779138e+294]\n",
      "402 [5.89239753e+294 1.28816098e+295]\n",
      "403 [3.16556172e+295 6.92036319e+295]\n",
      "404 [1.70062881e+296 3.71781379e+296]\n",
      "405 [9.13625639e+296 1.99731416e+297]\n",
      "406 [4.90825396e+297 1.07301336e+298]\n",
      "407 [2.63685211e+298 5.76452961e+298]\n",
      "408 [1.41659113e+299 3.09686748e+299]\n",
      "409 [7.61032608e+299 1.66372433e+300]\n",
      "410 [4.08848127e+300 8.93799515e+300]\n",
      "411 [2.19644716e+301 4.80174244e+301]\n",
      "412 [1.17999320e+302 2.57963112e+302]\n",
      "413 [6.33925545e+302 1.38585041e+303]\n",
      "414 [3.40562636e+303 7.44517827e+303]\n",
      "415 [1.82959829e+304 3.99975922e+304]\n",
      "416 [9.82911673e+304 2.14878318e+305]\n",
      "417 [5.28047802e+305 1.15438677e+306]\n",
      "418 [2.83682135e+306 6.20169049e+306]\n",
      "419 [1.52402023e+307 3.33172260e+307]\n",
      "420 [8.18746544e+307 1.78989511e+308]\n",
      "421 [inf inf]\n",
      "422 [inf inf]\n",
      "423 [inf inf]\n",
      "424 [inf inf]\n",
      "425 [inf inf]\n",
      "426 [inf inf]\n",
      "427 [inf inf]\n",
      "428 [inf inf]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 [inf inf]\r\n",
      "430 [inf inf]\r\n",
      "431 [inf inf]\r\n",
      "432 [inf inf]\r\n",
      "433 [inf inf]\r\n",
      "434 [inf inf]\r\n",
      "435 [inf inf]\r\n",
      "436 [inf inf]\r\n",
      "437 [inf inf]\r\n",
      "438 [inf inf]\r\n",
      "439 [inf inf]\r\n",
      "440 [inf inf]\r\n",
      "441 [inf inf]\r\n",
      "442 [inf inf]\r\n",
      "443 [inf inf]\r\n",
      "444 [inf inf]\r\n",
      "445 [inf inf]\r\n",
      "446 [inf inf]\r\n",
      "447 [inf inf]\r\n",
      "448 [inf inf]\r\n",
      "449 [inf inf]\r\n",
      "450 [inf inf]\r\n",
      "451 [inf inf]\r\n",
      "452 [inf inf]\r\n",
      "453 [inf inf]\r\n",
      "454 [inf inf]\r\n",
      "455 [inf inf]\r\n",
      "456 [inf inf]\r\n",
      "457 [inf inf]\r\n",
      "458 [inf inf]\r\n",
      "459 [inf inf]\r\n",
      "460 [inf inf]\r\n",
      "461 [inf inf]\r\n",
      "462 [inf inf]\r\n",
      "463 [inf inf]\r\n",
      "464 [inf inf]\r\n",
      "465 [inf inf]\r\n",
      "466 [inf inf]\r\n",
      "467 [inf inf]\r\n",
      "468 [inf inf]\r\n",
      "469 [inf inf]\r\n",
      "470 [inf inf]\r\n",
      "471 [inf inf]\r\n",
      "472 [inf inf]\r\n",
      "473 [inf inf]\r\n",
      "474 [inf inf]\r\n",
      "475 [inf inf]\r\n",
      "476 [inf inf]\r\n",
      "477 [inf inf]\r\n",
      "478 [inf inf]\r\n",
      "479 [inf inf]\r\n",
      "480 [inf inf]\r\n",
      "481 [inf inf]\r\n",
      "482 [inf inf]\r\n",
      "483 [inf inf]\r\n",
      "484 [inf inf]\r\n",
      "485 [inf inf]\r\n",
      "486 [inf inf]\r\n",
      "487 [inf inf]\r\n",
      "488 [inf inf]\r\n",
      "489 [inf inf]\r\n",
      "490 [inf inf]\r\n",
      "491 [inf inf]\r\n",
      "492 [inf inf]\r\n",
      "493 [inf inf]\r\n",
      "494 [inf inf]\r\n",
      "495 [inf inf]\r\n",
      "496 [inf inf]\r\n",
      "497 [inf inf]\r\n",
      "498 [inf inf]\r\n",
      "499 [inf inf]\r\n",
      "500 [inf inf]\r\n",
      "501 [inf inf]\r\n",
      "502 [inf inf]\r\n",
      "503 [inf inf]\r\n",
      "504 [inf inf]\r\n",
      "505 [inf inf]\r\n",
      "506 [inf inf]\r\n",
      "507 [inf inf]\r\n",
      "508 [inf inf]\r\n",
      "509 [inf inf]\r\n",
      "510 [inf inf]\r\n",
      "511 [inf inf]\r\n",
      "512 [inf inf]\r\n",
      "513 [inf inf]\r\n",
      "514 [inf inf]\r\n",
      "515 [inf inf]\r\n",
      "516 [inf inf]\r\n",
      "517 [inf inf]\r\n",
      "518 [inf inf]\r\n",
      "519 [inf inf]\r\n",
      "520 [inf inf]\r\n",
      "521 [inf inf]\r\n",
      "522 [inf inf]\r\n",
      "523 [inf inf]\r\n",
      "524 [inf inf]\r\n",
      "525 [inf inf]\r\n",
      "526 [inf inf]\r\n",
      "527 [inf inf]\r\n",
      "528 [inf inf]\r\n",
      "529 [inf inf]\r\n",
      "530 [inf inf]\r\n",
      "531 [inf inf]\r\n",
      "532 [inf inf]\r\n",
      "533 [inf inf]\r\n",
      "534 [inf inf]\r\n",
      "535 [inf inf]\r\n",
      "536 [inf inf]\r\n",
      "537 [inf inf]\r\n",
      "538 [inf inf]\r\n",
      "539 [inf inf]\r\n",
      "540 [inf inf]\r\n",
      "541 [inf inf]\r\n",
      "542 [inf inf]\r\n",
      "543 [inf inf]\r\n",
      "544 [inf inf]\r\n",
      "545 [inf inf]\r\n",
      "546 [inf inf]\r\n",
      "547 [inf inf]\r\n",
      "548 [inf inf]\r\n",
      "549 [inf inf]\r\n",
      "550 [inf inf]\r\n",
      "551 [inf inf]\r\n",
      "552 [inf inf]\r\n",
      "553 [inf inf]\r\n",
      "554 [inf inf]\r\n",
      "555 [inf inf]\r\n",
      "556 [inf inf]\r\n",
      "557 [inf inf]\r\n",
      "558 [inf inf]\r\n",
      "559 [inf inf]\r\n",
      "560 [inf inf]\r\n",
      "561 [inf inf]\r\n",
      "562 [inf inf]\r\n",
      "563 [inf inf]\r\n",
      "564 [inf inf]\r\n",
      "565 [inf inf]\r\n",
      "566 [inf inf]\r\n",
      "567 [inf inf]\r\n",
      "568 [inf inf]\r\n",
      "569 [inf inf]\r\n",
      "570 [inf inf]\r\n",
      "571 [inf inf]\r\n",
      "572 [inf inf]\r\n",
      "573 [inf inf]\r\n",
      "574 [inf inf]\r\n",
      "575 [inf inf]\r\n",
      "576 [inf inf]\r\n",
      "577 [inf inf]\r\n",
      "578 [inf inf]\r\n",
      "579 [inf inf]\r\n",
      "580 [inf inf]\r\n",
      "581 [inf inf]\r\n",
      "582 [inf inf]\r\n",
      "583 [inf inf]\r\n",
      "584 [inf inf]\r\n",
      "585 [inf inf]\r\n",
      "586 [inf inf]\r\n",
      "587 [inf inf]\r\n",
      "588 [inf inf]\r\n",
      "589 [inf inf]\r\n",
      "590 [inf inf]\r\n",
      "591 [inf inf]\r\n",
      "592 [inf inf]\r\n",
      "593 [inf inf]\r\n",
      "594 [inf inf]\r\n",
      "595 [inf inf]\r\n",
      "596 [inf inf]\r\n",
      "597 [inf inf]\r\n",
      "598 [inf inf]\r\n",
      "599 [inf inf]\r\n",
      "600 [inf inf]\r\n",
      "601 [inf inf]\r\n",
      "602 [inf inf]\r\n",
      "603 [inf inf]\r\n",
      "604 [inf inf]\r\n",
      "605 [inf inf]\r\n",
      "606 [inf inf]\r\n",
      "607 [inf inf]\r\n",
      "608 [inf inf]\r\n",
      "609 [inf inf]\r\n",
      "610 [inf inf]\r\n",
      "611 [inf inf]\r\n",
      "612 [inf inf]\r\n",
      "613 [inf inf]\r\n",
      "614 [inf inf]\r\n",
      "615 [inf inf]\r\n",
      "616 [inf inf]\r\n",
      "617 [inf inf]\r\n",
      "618 [inf inf]\r\n",
      "619 [inf inf]\r\n",
      "620 [inf inf]\r\n",
      "621 [inf inf]\r\n",
      "622 [inf inf]\r\n",
      "623 [inf inf]\r\n",
      "624 [inf inf]\r\n",
      "625 [inf inf]\r\n",
      "626 [inf inf]\r\n",
      "627 [inf inf]\r\n",
      "628 [inf inf]\r\n",
      "629 [inf inf]\r\n",
      "630 [inf inf]\r\n",
      "631 [inf inf]\r\n",
      "632 [inf inf]\r\n",
      "633 [inf inf]\r\n",
      "634 [inf inf]\r\n",
      "635 [inf inf]\r\n",
      "636 [inf inf]\r\n",
      "637 [inf inf]\r\n",
      "638 [inf inf]\r\n",
      "639 [inf inf]\r\n",
      "640 [inf inf]\r\n",
      "641 [inf inf]\r\n",
      "642 [inf inf]\r\n",
      "643 [inf inf]\r\n",
      "644 [inf inf]\r\n",
      "645 [inf inf]\r\n",
      "646 [inf inf]\r\n",
      "647 [inf inf]\r\n",
      "648 [inf inf]\r\n",
      "649 [inf inf]\r\n",
      "650 [inf inf]\r\n",
      "651 [inf inf]\r\n",
      "652 [inf inf]\r\n",
      "653 [inf inf]\r\n",
      "654 [inf inf]\r\n",
      "655 [inf inf]\r\n",
      "656 [inf inf]\r\n",
      "657 [inf inf]\r\n",
      "658 [inf inf]\r\n",
      "659 [inf inf]\r\n",
      "660 [inf inf]\r\n",
      "661 [inf inf]\r\n",
      "662 [inf inf]\r\n",
      "663 [inf inf]\r\n",
      "664 [inf inf]\r\n",
      "665 [inf inf]\r\n",
      "666 [inf inf]\r\n",
      "667 [inf inf]\r\n",
      "668 [inf inf]\r\n",
      "669 [inf inf]\r\n",
      "670 [inf inf]\r\n",
      "671 [inf inf]\r\n",
      "672 [inf inf]\r\n",
      "673 [inf inf]\r\n",
      "674 [inf inf]\r\n",
      "675 [inf inf]\r\n",
      "676 [inf inf]\r\n",
      "677 [inf inf]\r\n",
      "678 [inf inf]\r\n",
      "679 [inf inf]\r\n",
      "680 [inf inf]\r\n",
      "681 [inf inf]\r\n",
      "682 [inf inf]\r\n",
      "683 [inf inf]\r\n",
      "684 [inf inf]\r\n",
      "685 [inf inf]\r\n",
      "686 [inf inf]\r\n",
      "687 [inf inf]\r\n",
      "688 [inf inf]\r\n",
      "689 [inf inf]\r\n",
      "690 [inf inf]\r\n",
      "691 [inf inf]\r\n",
      "692 [inf inf]\r\n",
      "693 [inf inf]\r\n",
      "694 [inf inf]\r\n",
      "695 [inf inf]\r\n",
      "696 [inf inf]\r\n",
      "697 [inf inf]\r\n",
      "698 [inf inf]\r\n",
      "699 [inf inf]\r\n",
      "700 [inf inf]\r\n",
      "701 [inf inf]\r\n",
      "702 [inf inf]\r\n",
      "703 [inf inf]\r\n",
      "704 [inf inf]\r\n",
      "705 [inf inf]\r\n",
      "706 [inf inf]\r\n",
      "707 [inf inf]\r\n",
      "708 [inf inf]\r\n",
      "709 [inf inf]\r\n",
      "710 [inf inf]\r\n",
      "711 [inf inf]\r\n",
      "712 [inf inf]\r\n",
      "713 [inf inf]\r\n",
      "714 [inf inf]\r\n",
      "715 [inf inf]\r\n",
      "716 [inf inf]\r\n",
      "717 [inf inf]\r\n",
      "718 [inf inf]\r\n",
      "719 [inf inf]\r\n",
      "720 [inf inf]\r\n",
      "721 [inf inf]\r\n",
      "722 [inf inf]\r\n",
      "723 [inf inf]\r\n",
      "724 [inf inf]\r\n",
      "725 [inf inf]\r\n",
      "726 [inf inf]\r\n",
      "727 [inf inf]\r\n",
      "728 [inf inf]\r\n",
      "729 [inf inf]\r\n",
      "730 [inf inf]\r\n",
      "731 [inf inf]\r\n",
      "732 [inf inf]\r\n",
      "733 [inf inf]\r\n",
      "734 [inf inf]\r\n",
      "735 [inf inf]\r\n",
      "736 [inf inf]\r\n",
      "737 [inf inf]\r\n",
      "738 [inf inf]\r\n",
      "739 [inf inf]\r\n",
      "740 [inf inf]\r\n",
      "741 [inf inf]\r\n",
      "742 [inf inf]\r\n",
      "743 [inf inf]\r\n",
      "744 [inf inf]\r\n",
      "745 [inf inf]\r\n",
      "746 [inf inf]\r\n",
      "747 [inf inf]\r\n",
      "748 [inf inf]\r\n",
      "749 [inf inf]\r\n",
      "750 [inf inf]\r\n",
      "751 [inf inf]\r\n",
      "752 [inf inf]\r\n",
      "753 [inf inf]\r\n",
      "754 [inf inf]\r\n",
      "755 [inf inf]\r\n",
      "756 [inf inf]\r\n",
      "757 [inf inf]\r\n",
      "758 [inf inf]\r\n",
      "759 [inf inf]\r\n",
      "760 [inf inf]\r\n",
      "761 [inf inf]\r\n",
      "762 [inf inf]\r\n",
      "763 [inf inf]\r\n",
      "764 [inf inf]\r\n",
      "765 [inf inf]\r\n",
      "766 [inf inf]\r\n",
      "767 [inf inf]\r\n",
      "768 [inf inf]\r\n",
      "769 [inf inf]\r\n",
      "770 [inf inf]\r\n",
      "771 [inf inf]\r\n",
      "772 [inf inf]\r\n",
      "773 [inf inf]\r\n",
      "774 [inf inf]\r\n",
      "775 [inf inf]\r\n",
      "776 [inf inf]\r\n",
      "777 [inf inf]\r\n",
      "778 [inf inf]\r\n",
      "779 [inf inf]\r\n",
      "780 [inf inf]\r\n",
      "781 [inf inf]\r\n",
      "782 [inf inf]\r\n",
      "783 [inf inf]\r\n",
      "784 [inf inf]\r\n",
      "785 [inf inf]\r\n",
      "786 [inf inf]\r\n",
      "787 [inf inf]\r\n",
      "788 [inf inf]\r\n",
      "789 [inf inf]\r\n",
      "790 [inf inf]\r\n",
      "791 [inf inf]\r\n",
      "792 [inf inf]\r\n",
      "793 [inf inf]\r\n",
      "794 [inf inf]\r\n",
      "795 [inf inf]\r\n",
      "796 [inf inf]\r\n",
      "797 [inf inf]\r\n",
      "798 [inf inf]\r\n",
      "799 [inf inf]\r\n",
      "800 [inf inf]\r\n",
      "801 [inf inf]\r\n",
      "802 [inf inf]\r\n",
      "803 [inf inf]\r\n",
      "804 [inf inf]\r\n",
      "805 [inf inf]\r\n",
      "806 [inf inf]\r\n",
      "807 [inf inf]\r\n",
      "808 [inf inf]\r\n",
      "809 [inf inf]\r\n",
      "810 [inf inf]\r\n",
      "811 [inf inf]\r\n",
      "812 [inf inf]\r\n",
      "813 [inf inf]\r\n",
      "814 [inf inf]\r\n",
      "815 [inf inf]\r\n",
      "816 [inf inf]\r\n",
      "817 [inf inf]\r\n",
      "818 [inf inf]\r\n",
      "819 [inf inf]\r\n",
      "820 [inf inf]\r\n",
      "821 [inf inf]\r\n",
      "822 [inf inf]\r\n",
      "823 [inf inf]\r\n",
      "824 [inf inf]\r\n",
      "825 [inf inf]\r\n",
      "826 [inf inf]\r\n",
      "827 [inf inf]\r\n",
      "828 [inf inf]\r\n",
      "829 [inf inf]\r\n",
      "830 [inf inf]\r\n",
      "831 [inf inf]\r\n",
      "832 [inf inf]\r\n",
      "833 [inf inf]\r\n",
      "834 [inf inf]\r\n",
      "835 [inf inf]\r\n",
      "836 [inf inf]\r\n",
      "837 [inf inf]\r\n",
      "838 [inf inf]\r\n",
      "839 [inf inf]\r\n",
      "840 [inf inf]\r\n",
      "841 [inf inf]\r\n",
      "842 [inf inf]\r\n",
      "843 [inf inf]\r\n",
      "844 [inf inf]\r\n",
      "845 [inf inf]\r\n",
      "846 [inf inf]\r\n",
      "847 [inf inf]\r\n",
      "848 [inf inf]\r\n",
      "849 [inf inf]\r\n",
      "850 [inf inf]\r\n",
      "851 [inf inf]\r\n",
      "852 [inf inf]\r\n",
      "853 [inf inf]\r\n",
      "854 [inf inf]\r\n",
      "855 [inf inf]\r\n",
      "856 [inf inf]\r\n",
      "857 [inf inf]\r\n",
      "858 [inf inf]\r\n",
      "859 [inf inf]\r\n",
      "860 [inf inf]\r\n",
      "861 [inf inf]\r\n",
      "862 [inf inf]\r\n",
      "863 [inf inf]\r\n",
      "864 [inf inf]\r\n",
      "865 [inf inf]\r\n",
      "866 [inf inf]\r\n",
      "867 [inf inf]\r\n",
      "868 [inf inf]\r\n",
      "869 [inf inf]\r\n",
      "870 [inf inf]\r\n",
      "871 [inf inf]\r\n",
      "872 [inf inf]\r\n",
      "873 [inf inf]\r\n",
      "874 [inf inf]\r\n",
      "875 [inf inf]\r\n",
      "876 [inf inf]\r\n",
      "877 [inf inf]\r\n",
      "878 [inf inf]\r\n",
      "879 [inf inf]\r\n",
      "880 [inf inf]\r\n",
      "881 [inf inf]\r\n",
      "882 [inf inf]\r\n",
      "883 [inf inf]\r\n",
      "884 [inf inf]\r\n",
      "885 [inf inf]\r\n",
      "886 [inf inf]\r\n",
      "887 [inf inf]\r\n",
      "888 [inf inf]\r\n",
      "889 [inf inf]\r\n",
      "890 [inf inf]\r\n",
      "891 [inf inf]\r\n",
      "892 [inf inf]\r\n",
      "893 [inf inf]\r\n",
      "894 [inf inf]\r\n",
      "895 [inf inf]\r\n",
      "896 [inf inf]\r\n",
      "897 [inf inf]\r\n",
      "898 [inf inf]\r\n",
      "899 [inf inf]\r\n",
      "900 [inf inf]\r\n",
      "901 [inf inf]\r\n",
      "902 [inf inf]\r\n",
      "903 [inf inf]\r\n",
      "904 [inf inf]\r\n",
      "905 [inf inf]\r\n",
      "906 [inf inf]\r\n",
      "907 [inf inf]\r\n",
      "908 [inf inf]\r\n",
      "909 [inf inf]\r\n",
      "910 [inf inf]\r\n",
      "911 [inf inf]\r\n",
      "912 [inf inf]\r\n",
      "913 [inf inf]\r\n",
      "914 [inf inf]\r\n",
      "915 [inf inf]\r\n",
      "916 [inf inf]\r\n",
      "917 [inf inf]\r\n",
      "918 [inf inf]\r\n",
      "919 [inf inf]\r\n",
      "920 [inf inf]\r\n",
      "921 [inf inf]\r\n",
      "922 [inf inf]\r\n",
      "923 [inf inf]\r\n",
      "924 [inf inf]\r\n",
      "925 [inf inf]\r\n",
      "926 [inf inf]\r\n",
      "927 [inf inf]\r\n",
      "928 [inf inf]\r\n",
      "929 [inf inf]\r\n",
      "930 [inf inf]\r\n",
      "931 [inf inf]\r\n",
      "932 [inf inf]\r\n",
      "933 [inf inf]\r\n",
      "934 [inf inf]\r\n",
      "935 [inf inf]\r\n",
      "936 [inf inf]\r\n",
      "937 [inf inf]\r\n",
      "938 [inf inf]\r\n",
      "939 [inf inf]\r\n",
      "940 [inf inf]\r\n",
      "941 [inf inf]\r\n",
      "942 [inf inf]\r\n",
      "943 [inf inf]\r\n",
      "944 [inf inf]\r\n",
      "945 [inf inf]\r\n",
      "946 [inf inf]\r\n",
      "947 [inf inf]\r\n",
      "948 [inf inf]\r\n",
      "949 [inf inf]\r\n",
      "950 [inf inf]\r\n",
      "951 [inf inf]\r\n",
      "952 [inf inf]\r\n",
      "953 [inf inf]\r\n",
      "954 [inf inf]\r\n",
      "955 [inf inf]\r\n",
      "956 [inf inf]\r\n",
      "957 [inf inf]\r\n",
      "958 [inf inf]\r\n",
      "959 [inf inf]\r\n",
      "960 [inf inf]\r\n",
      "961 [inf inf]\r\n",
      "962 [inf inf]\r\n",
      "963 [inf inf]\r\n",
      "964 [inf inf]\r\n",
      "965 [inf inf]\r\n",
      "966 [inf inf]\r\n",
      "967 [inf inf]\r\n",
      "968 [inf inf]\r\n",
      "969 [inf inf]\r\n",
      "970 [inf inf]\r\n",
      "971 [inf inf]\r\n",
      "972 [inf inf]\r\n",
      "973 [inf inf]\r\n",
      "974 [inf inf]\r\n",
      "975 [inf inf]\r\n",
      "976 [inf inf]\r\n",
      "977 [inf inf]\r\n",
      "978 [inf inf]\r\n",
      "979 [inf inf]\r\n",
      "980 [inf inf]\r\n",
      "981 [inf inf]\r\n",
      "982 [inf inf]\r\n",
      "983 [inf inf]\r\n",
      "984 [inf inf]\r\n",
      "985 [inf inf]\r\n",
      "986 [inf inf]\r\n",
      "987 [inf inf]\r\n",
      "988 [inf inf]\r\n",
      "989 [inf inf]\r\n",
      "990 [inf inf]\r\n",
      "991 [inf inf]\r\n",
      "992 [inf inf]\r\n",
      "993 [inf inf]\r\n",
      "994 [inf inf]\r\n",
      "995 [inf inf]\r\n",
      "996 [inf inf]\r\n",
      "997 [inf inf]\r\n",
      "998 [inf inf]\r\n",
      "999 [inf inf]\r\n",
      "[[17 23  9]\r\n",
      " [39 53 27]]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 302958.14it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:07:34.038606: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:07:34.419891: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:07:34.419930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:07:34.643265: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:07:35.250323: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 91559.70it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 110,882\n",
      "Trainable params: 110,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 221.0004 - pos_accuracy: 0.0000e+00 - val_loss: 123.8639 - val_pos_accuracy: 0.0024\n",
      "Epoch 2/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 50.2797 - pos_accuracy: 0.0031 - val_loss: 32.3421 - val_pos_accuracy: 0.0120\n",
      "Epoch 3/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 32.2409 - pos_accuracy: 0.0088 - val_loss: 27.1573 - val_pos_accuracy: 0.0168\n",
      "Epoch 4/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 24.5648 - pos_accuracy: 0.0213 - val_loss: 17.0348 - val_pos_accuracy: 0.0288\n",
      "Epoch 5/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 11.0082 - pos_accuracy: 0.0594 - val_loss: 5.8072 - val_pos_accuracy: 0.1466\n",
      "Epoch 6/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 3.8151 - pos_accuracy: 0.1269 - val_loss: 3.2192 - val_pos_accuracy: 0.1707\n",
      "Epoch 7/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.3083 - pos_accuracy: 0.1750 - val_loss: 2.3874 - val_pos_accuracy: 0.2067\n",
      "Epoch 8/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.7568 - pos_accuracy: 0.2087 - val_loss: 2.0526 - val_pos_accuracy: 0.1803\n",
      "Epoch 9/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.4616 - pos_accuracy: 0.2262 - val_loss: 1.7673 - val_pos_accuracy: 0.2139\n",
      "Epoch 10/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.2700 - pos_accuracy: 0.2406 - val_loss: 1.5819 - val_pos_accuracy: 0.2115\n",
      "Epoch 11/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.1286 - pos_accuracy: 0.2644 - val_loss: 1.4504 - val_pos_accuracy: 0.2212\n",
      "Epoch 12/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.0231 - pos_accuracy: 0.2775 - val_loss: 1.3368 - val_pos_accuracy: 0.2308\n",
      "Epoch 13/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.9311 - pos_accuracy: 0.3006 - val_loss: 1.2621 - val_pos_accuracy: 0.2332\n",
      "Epoch 14/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.8549 - pos_accuracy: 0.3156 - val_loss: 1.1822 - val_pos_accuracy: 0.2428\n",
      "Epoch 15/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.7844 - pos_accuracy: 0.3319 - val_loss: 1.1675 - val_pos_accuracy: 0.2524\n",
      "Epoch 16/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.7305 - pos_accuracy: 0.3394 - val_loss: 1.0717 - val_pos_accuracy: 0.2692\n",
      "Epoch 17/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6799 - pos_accuracy: 0.3681 - val_loss: 1.0017 - val_pos_accuracy: 0.2957\n",
      "Epoch 18/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6374 - pos_accuracy: 0.3837 - val_loss: 0.9849 - val_pos_accuracy: 0.3005\n",
      "Epoch 19/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5969 - pos_accuracy: 0.4013 - val_loss: 0.9295 - val_pos_accuracy: 0.3221\n",
      "Epoch 20/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5653 - pos_accuracy: 0.4225 - val_loss: 0.9061 - val_pos_accuracy: 0.3341\n",
      "Epoch 21/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5353 - pos_accuracy: 0.4431 - val_loss: 0.8576 - val_pos_accuracy: 0.3462\n",
      "Epoch 22/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5044 - pos_accuracy: 0.4519 - val_loss: 0.8268 - val_pos_accuracy: 0.3630\n",
      "Epoch 23/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4812 - pos_accuracy: 0.4750 - val_loss: 0.7944 - val_pos_accuracy: 0.3774\n",
      "Epoch 24/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4549 - pos_accuracy: 0.4856 - val_loss: 0.7687 - val_pos_accuracy: 0.3894\n",
      "Epoch 25/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4328 - pos_accuracy: 0.4938 - val_loss: 0.7442 - val_pos_accuracy: 0.3966\n",
      "Epoch 26/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4154 - pos_accuracy: 0.5069 - val_loss: 0.7186 - val_pos_accuracy: 0.4111\n",
      "Epoch 27/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3974 - pos_accuracy: 0.5163 - val_loss: 0.7000 - val_pos_accuracy: 0.4062\n",
      "Epoch 28/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3806 - pos_accuracy: 0.5387 - val_loss: 0.6891 - val_pos_accuracy: 0.4135\n",
      "Epoch 29/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3634 - pos_accuracy: 0.5537 - val_loss: 0.6634 - val_pos_accuracy: 0.4399\n",
      "Epoch 30/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3502 - pos_accuracy: 0.5725 - val_loss: 0.6506 - val_pos_accuracy: 0.4471\n",
      "Epoch 31/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3346 - pos_accuracy: 0.5813 - val_loss: 0.6291 - val_pos_accuracy: 0.4543\n",
      "Epoch 32/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3236 - pos_accuracy: 0.5900 - val_loss: 0.6143 - val_pos_accuracy: 0.4712\n",
      "Epoch 33/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3108 - pos_accuracy: 0.6044 - val_loss: 0.6033 - val_pos_accuracy: 0.4663\n",
      "Epoch 34/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2973 - pos_accuracy: 0.6181 - val_loss: 0.5954 - val_pos_accuracy: 0.4712\n",
      "Epoch 35/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2875 - pos_accuracy: 0.6263 - val_loss: 0.5744 - val_pos_accuracy: 0.4832\n",
      "Epoch 36/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2780 - pos_accuracy: 0.6356 - val_loss: 0.5619 - val_pos_accuracy: 0.4904\n",
      "Epoch 37/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2683 - pos_accuracy: 0.6475 - val_loss: 0.5513 - val_pos_accuracy: 0.5072\n",
      "Epoch 38/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2590 - pos_accuracy: 0.6519 - val_loss: 0.5424 - val_pos_accuracy: 0.5000\n",
      "Epoch 39/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2506 - pos_accuracy: 0.6606 - val_loss: 0.5322 - val_pos_accuracy: 0.5120\n",
      "Epoch 40/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2411 - pos_accuracy: 0.6700 - val_loss: 0.5286 - val_pos_accuracy: 0.5192\n",
      "Epoch 41/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2357 - pos_accuracy: 0.6712 - val_loss: 0.5137 - val_pos_accuracy: 0.5385\n",
      "Epoch 42/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2277 - pos_accuracy: 0.6806 - val_loss: 0.5041 - val_pos_accuracy: 0.5385\n",
      "Epoch 43/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2198 - pos_accuracy: 0.6969 - val_loss: 0.4966 - val_pos_accuracy: 0.5409\n",
      "Epoch 44/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2117 - pos_accuracy: 0.6975 - val_loss: 0.4880 - val_pos_accuracy: 0.5481\n",
      "Epoch 45/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2076 - pos_accuracy: 0.7038 - val_loss: 0.4785 - val_pos_accuracy: 0.5505\n",
      "Epoch 46/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2007 - pos_accuracy: 0.7188 - val_loss: 0.4723 - val_pos_accuracy: 0.5577\n",
      "Epoch 47/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1928 - pos_accuracy: 0.7225 - val_loss: 0.4647 - val_pos_accuracy: 0.5817\n",
      "Epoch 48/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1883 - pos_accuracy: 0.7325 - val_loss: 0.4589 - val_pos_accuracy: 0.5817\n",
      "Epoch 49/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1832 - pos_accuracy: 0.7425 - val_loss: 0.4498 - val_pos_accuracy: 0.5841\n",
      "Epoch 50/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1779 - pos_accuracy: 0.7556 - val_loss: 0.4421 - val_pos_accuracy: 0.6034\n",
      "Epoch 51/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1720 - pos_accuracy: 0.7606 - val_loss: 0.4546 - val_pos_accuracy: 0.5721\n",
      "Epoch 52/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1678 - pos_accuracy: 0.7619 - val_loss: 0.4360 - val_pos_accuracy: 0.6250\n",
      "Epoch 53/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1622 - pos_accuracy: 0.7725 - val_loss: 0.4265 - val_pos_accuracy: 0.6106\n",
      "Epoch 54/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1583 - pos_accuracy: 0.7756 - val_loss: 0.4196 - val_pos_accuracy: 0.6178\n",
      "Epoch 55/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1540 - pos_accuracy: 0.7825 - val_loss: 0.4107 - val_pos_accuracy: 0.6346\n",
      "Epoch 56/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1500 - pos_accuracy: 0.7837 - val_loss: 0.4084 - val_pos_accuracy: 0.6587\n",
      "Epoch 57/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1459 - pos_accuracy: 0.7925 - val_loss: 0.4048 - val_pos_accuracy: 0.6587\n",
      "Epoch 58/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1417 - pos_accuracy: 0.7981 - val_loss: 0.3987 - val_pos_accuracy: 0.6538\n",
      "Epoch 59/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1391 - pos_accuracy: 0.8019 - val_loss: 0.3916 - val_pos_accuracy: 0.6659\n",
      "Epoch 60/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1352 - pos_accuracy: 0.8087 - val_loss: 0.3989 - val_pos_accuracy: 0.6466\n",
      "Epoch 61/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1313 - pos_accuracy: 0.8119 - val_loss: 0.3847 - val_pos_accuracy: 0.6803\n",
      "Epoch 62/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1281 - pos_accuracy: 0.8144 - val_loss: 0.3820 - val_pos_accuracy: 0.6803\n",
      "Epoch 63/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1253 - pos_accuracy: 0.8125 - val_loss: 0.3741 - val_pos_accuracy: 0.6851\n",
      "Epoch 64/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1212 - pos_accuracy: 0.8169 - val_loss: 0.3718 - val_pos_accuracy: 0.6899\n",
      "Epoch 65/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1191 - pos_accuracy: 0.8219 - val_loss: 0.3676 - val_pos_accuracy: 0.6875\n",
      "Epoch 66/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1159 - pos_accuracy: 0.8288 - val_loss: 0.3702 - val_pos_accuracy: 0.6899\n",
      "Epoch 67/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1136 - pos_accuracy: 0.8313 - val_loss: 0.3626 - val_pos_accuracy: 0.6875\n",
      "Epoch 68/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1107 - pos_accuracy: 0.8319 - val_loss: 0.3599 - val_pos_accuracy: 0.7019\n",
      "Epoch 69/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1085 - pos_accuracy: 0.8344 - val_loss: 0.3581 - val_pos_accuracy: 0.7019\n",
      "Epoch 70/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1058 - pos_accuracy: 0.8425 - val_loss: 0.3518 - val_pos_accuracy: 0.7019\n",
      "Epoch 71/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1033 - pos_accuracy: 0.8475 - val_loss: 0.3441 - val_pos_accuracy: 0.7139\n",
      "Epoch 72/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1012 - pos_accuracy: 0.8550 - val_loss: 0.3430 - val_pos_accuracy: 0.7139\n",
      "Epoch 73/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0996 - pos_accuracy: 0.8537 - val_loss: 0.3409 - val_pos_accuracy: 0.7260\n",
      "Epoch 74/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0967 - pos_accuracy: 0.8594 - val_loss: 0.3371 - val_pos_accuracy: 0.7236\n",
      "Epoch 75/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0949 - pos_accuracy: 0.8606 - val_loss: 0.3348 - val_pos_accuracy: 0.7212\n",
      "Epoch 76/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0924 - pos_accuracy: 0.8650 - val_loss: 0.3309 - val_pos_accuracy: 0.7284\n",
      "Epoch 77/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0910 - pos_accuracy: 0.8694 - val_loss: 0.3267 - val_pos_accuracy: 0.7260\n",
      "Epoch 78/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0885 - pos_accuracy: 0.8712 - val_loss: 0.3278 - val_pos_accuracy: 0.7380\n",
      "Epoch 79/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0870 - pos_accuracy: 0.8719 - val_loss: 0.3254 - val_pos_accuracy: 0.7428\n",
      "Epoch 80/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0851 - pos_accuracy: 0.8725 - val_loss: 0.3252 - val_pos_accuracy: 0.7404\n",
      "Epoch 81/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0834 - pos_accuracy: 0.8806 - val_loss: 0.3197 - val_pos_accuracy: 0.7500\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0813 - pos_accuracy: 0.8838 - val_loss: 0.3138 - val_pos_accuracy: 0.7428\n",
      "Epoch 83/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0795 - pos_accuracy: 0.8838 - val_loss: 0.3139 - val_pos_accuracy: 0.7476\n",
      "Epoch 84/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0782 - pos_accuracy: 0.8825 - val_loss: 0.3188 - val_pos_accuracy: 0.7548\n",
      "Epoch 85/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0764 - pos_accuracy: 0.8894 - val_loss: 0.3088 - val_pos_accuracy: 0.7476\n",
      "Epoch 86/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0754 - pos_accuracy: 0.8900 - val_loss: 0.3070 - val_pos_accuracy: 0.7524\n",
      "Epoch 87/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0739 - pos_accuracy: 0.8906 - val_loss: 0.3049 - val_pos_accuracy: 0.7524\n",
      "Epoch 88/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0721 - pos_accuracy: 0.8888 - val_loss: 0.3052 - val_pos_accuracy: 0.7668\n",
      "Epoch 89/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0711 - pos_accuracy: 0.8963 - val_loss: 0.3033 - val_pos_accuracy: 0.7644\n",
      "Epoch 90/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0692 - pos_accuracy: 0.9013 - val_loss: 0.2988 - val_pos_accuracy: 0.7548\n",
      "Epoch 91/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0679 - pos_accuracy: 0.9038 - val_loss: 0.2982 - val_pos_accuracy: 0.7692\n",
      "Epoch 92/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0665 - pos_accuracy: 0.9031 - val_loss: 0.2962 - val_pos_accuracy: 0.7668\n",
      "Epoch 93/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0653 - pos_accuracy: 0.9062 - val_loss: 0.2922 - val_pos_accuracy: 0.7644\n",
      "Epoch 94/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0641 - pos_accuracy: 0.9075 - val_loss: 0.2910 - val_pos_accuracy: 0.7692\n",
      "Epoch 95/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0630 - pos_accuracy: 0.9131 - val_loss: 0.2893 - val_pos_accuracy: 0.7764\n",
      "Epoch 96/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0618 - pos_accuracy: 0.9144 - val_loss: 0.2872 - val_pos_accuracy: 0.7692\n",
      "Epoch 97/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0605 - pos_accuracy: 0.9144 - val_loss: 0.2838 - val_pos_accuracy: 0.7740\n",
      "Epoch 98/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0596 - pos_accuracy: 0.9144 - val_loss: 0.2835 - val_pos_accuracy: 0.7764\n",
      "Epoch 99/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0585 - pos_accuracy: 0.9169 - val_loss: 0.2820 - val_pos_accuracy: 0.7788\n",
      "Epoch 100/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0574 - pos_accuracy: 0.9169 - val_loss: 0.2791 - val_pos_accuracy: 0.7788\n",
      "Epoch 101/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0564 - pos_accuracy: 0.9187 - val_loss: 0.2766 - val_pos_accuracy: 0.7788\n",
      "Epoch 102/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0555 - pos_accuracy: 0.9187 - val_loss: 0.2784 - val_pos_accuracy: 0.7788\n",
      "Epoch 103/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0543 - pos_accuracy: 0.9206 - val_loss: 0.2756 - val_pos_accuracy: 0.7740\n",
      "Epoch 104/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0534 - pos_accuracy: 0.9212 - val_loss: 0.2761 - val_pos_accuracy: 0.7788\n",
      "Epoch 105/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0524 - pos_accuracy: 0.9200 - val_loss: 0.2738 - val_pos_accuracy: 0.7812\n",
      "Epoch 106/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0516 - pos_accuracy: 0.9225 - val_loss: 0.2707 - val_pos_accuracy: 0.7788\n",
      "Epoch 107/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0505 - pos_accuracy: 0.9219 - val_loss: 0.2725 - val_pos_accuracy: 0.7788\n",
      "Epoch 108/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0497 - pos_accuracy: 0.9219 - val_loss: 0.2708 - val_pos_accuracy: 0.7861\n",
      "Epoch 109/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0488 - pos_accuracy: 0.9262 - val_loss: 0.2703 - val_pos_accuracy: 0.7861\n",
      "Epoch 110/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0483 - pos_accuracy: 0.9244 - val_loss: 0.2676 - val_pos_accuracy: 0.7837\n",
      "Epoch 111/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0473 - pos_accuracy: 0.9250 - val_loss: 0.2634 - val_pos_accuracy: 0.7837\n",
      "Epoch 112/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0466 - pos_accuracy: 0.9269 - val_loss: 0.2650 - val_pos_accuracy: 0.7861\n",
      "Epoch 113/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0458 - pos_accuracy: 0.9287 - val_loss: 0.2623 - val_pos_accuracy: 0.7885\n",
      "Epoch 114/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0449 - pos_accuracy: 0.9294 - val_loss: 0.2620 - val_pos_accuracy: 0.7861\n",
      "Epoch 115/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0442 - pos_accuracy: 0.9325 - val_loss: 0.2599 - val_pos_accuracy: 0.7885\n",
      "Epoch 116/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0434 - pos_accuracy: 0.9344 - val_loss: 0.2618 - val_pos_accuracy: 0.7885\n",
      "Epoch 117/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0426 - pos_accuracy: 0.9325 - val_loss: 0.2578 - val_pos_accuracy: 0.7933\n",
      "Epoch 118/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0421 - pos_accuracy: 0.9344 - val_loss: 0.2584 - val_pos_accuracy: 0.7933\n",
      "Epoch 119/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0413 - pos_accuracy: 0.9337 - val_loss: 0.2565 - val_pos_accuracy: 0.7933\n",
      "Epoch 120/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0406 - pos_accuracy: 0.9344 - val_loss: 0.2560 - val_pos_accuracy: 0.7957\n",
      "Epoch 121/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0402 - pos_accuracy: 0.9369 - val_loss: 0.2528 - val_pos_accuracy: 0.7957\n",
      "Epoch 122/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0393 - pos_accuracy: 0.9381 - val_loss: 0.2526 - val_pos_accuracy: 0.7933\n",
      "Epoch 123/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0387 - pos_accuracy: 0.9394 - val_loss: 0.2528 - val_pos_accuracy: 0.7981\n",
      "Epoch 124/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0382 - pos_accuracy: 0.9406 - val_loss: 0.2486 - val_pos_accuracy: 0.7981\n",
      "Epoch 125/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0378 - pos_accuracy: 0.9413 - val_loss: 0.2498 - val_pos_accuracy: 0.7981\n",
      "Epoch 126/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0369 - pos_accuracy: 0.9425 - val_loss: 0.2530 - val_pos_accuracy: 0.8029\n",
      "Epoch 127/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0363 - pos_accuracy: 0.9419 - val_loss: 0.2469 - val_pos_accuracy: 0.8029\n",
      "Epoch 128/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0359 - pos_accuracy: 0.9425 - val_loss: 0.2459 - val_pos_accuracy: 0.8053\n",
      "Epoch 129/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0352 - pos_accuracy: 0.9438 - val_loss: 0.2467 - val_pos_accuracy: 0.8053\n",
      "Epoch 130/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0346 - pos_accuracy: 0.9438 - val_loss: 0.2446 - val_pos_accuracy: 0.8101\n",
      "Epoch 131/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0342 - pos_accuracy: 0.9469 - val_loss: 0.2448 - val_pos_accuracy: 0.8077\n",
      "Epoch 132/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0337 - pos_accuracy: 0.9469 - val_loss: 0.2422 - val_pos_accuracy: 0.8077\n",
      "Epoch 133/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0332 - pos_accuracy: 0.9456 - val_loss: 0.2415 - val_pos_accuracy: 0.8077\n",
      "Epoch 134/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0327 - pos_accuracy: 0.9475 - val_loss: 0.2440 - val_pos_accuracy: 0.8101\n",
      "Epoch 135/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0321 - pos_accuracy: 0.9469 - val_loss: 0.2419 - val_pos_accuracy: 0.8101\n",
      "Epoch 136/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0317 - pos_accuracy: 0.9488 - val_loss: 0.2394 - val_pos_accuracy: 0.8173\n",
      "Epoch 137/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0313 - pos_accuracy: 0.9481 - val_loss: 0.2393 - val_pos_accuracy: 0.8221\n",
      "Epoch 138/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0308 - pos_accuracy: 0.9519 - val_loss: 0.2382 - val_pos_accuracy: 0.8245\n",
      "Epoch 139/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0302 - pos_accuracy: 0.9538 - val_loss: 0.2382 - val_pos_accuracy: 0.8221\n",
      "Epoch 140/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0300 - pos_accuracy: 0.9525 - val_loss: 0.2372 - val_pos_accuracy: 0.8221\n",
      "Epoch 141/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0294 - pos_accuracy: 0.9538 - val_loss: 0.2381 - val_pos_accuracy: 0.8269\n",
      "Epoch 142/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0290 - pos_accuracy: 0.9538 - val_loss: 0.2372 - val_pos_accuracy: 0.8269\n",
      "Epoch 143/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0286 - pos_accuracy: 0.9556 - val_loss: 0.2390 - val_pos_accuracy: 0.8221\n",
      "Epoch 144/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0282 - pos_accuracy: 0.9556 - val_loss: 0.2363 - val_pos_accuracy: 0.8245\n",
      "Epoch 145/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0279 - pos_accuracy: 0.9550 - val_loss: 0.2339 - val_pos_accuracy: 0.8269\n",
      "Epoch 146/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0273 - pos_accuracy: 0.9563 - val_loss: 0.2346 - val_pos_accuracy: 0.8245\n",
      "Epoch 147/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0270 - pos_accuracy: 0.9563 - val_loss: 0.2320 - val_pos_accuracy: 0.8269\n",
      "Epoch 148/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0267 - pos_accuracy: 0.9581 - val_loss: 0.2324 - val_pos_accuracy: 0.8269\n",
      "Epoch 149/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0262 - pos_accuracy: 0.9594 - val_loss: 0.2299 - val_pos_accuracy: 0.8269\n",
      "Epoch 150/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0259 - pos_accuracy: 0.9594 - val_loss: 0.2314 - val_pos_accuracy: 0.8269\n",
      "Epoch 151/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0256 - pos_accuracy: 0.9600 - val_loss: 0.2327 - val_pos_accuracy: 0.8293\n",
      "Epoch 152/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0253 - pos_accuracy: 0.9600 - val_loss: 0.2302 - val_pos_accuracy: 0.8293\n",
      "Epoch 153/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0250 - pos_accuracy: 0.9600 - val_loss: 0.2291 - val_pos_accuracy: 0.8317\n",
      "Epoch 154/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0244 - pos_accuracy: 0.9631 - val_loss: 0.2310 - val_pos_accuracy: 0.8293\n",
      "Epoch 155/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0242 - pos_accuracy: 0.9619 - val_loss: 0.2305 - val_pos_accuracy: 0.8317\n",
      "Epoch 156/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0239 - pos_accuracy: 0.9650 - val_loss: 0.2263 - val_pos_accuracy: 0.8341\n",
      "Epoch 157/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0236 - pos_accuracy: 0.9644 - val_loss: 0.2270 - val_pos_accuracy: 0.8341\n",
      "Epoch 158/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0231 - pos_accuracy: 0.9663 - val_loss: 0.2274 - val_pos_accuracy: 0.8317\n",
      "Epoch 159/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0229 - pos_accuracy: 0.9663 - val_loss: 0.2269 - val_pos_accuracy: 0.8317\n",
      "Epoch 160/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0226 - pos_accuracy: 0.9681 - val_loss: 0.2259 - val_pos_accuracy: 0.8317\n",
      "Epoch 161/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0224 - pos_accuracy: 0.9688 - val_loss: 0.2257 - val_pos_accuracy: 0.8317\n",
      "Epoch 162/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0219 - pos_accuracy: 0.9712 - val_loss: 0.2252 - val_pos_accuracy: 0.8341\n",
      "Epoch 163/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0217 - pos_accuracy: 0.9681 - val_loss: 0.2254 - val_pos_accuracy: 0.8341\n",
      "Epoch 164/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0215 - pos_accuracy: 0.9694 - val_loss: 0.2252 - val_pos_accuracy: 0.8341\n",
      "Epoch 165/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0213 - pos_accuracy: 0.9700 - val_loss: 0.2241 - val_pos_accuracy: 0.8341\n",
      "Epoch 166/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0209 - pos_accuracy: 0.9694 - val_loss: 0.2257 - val_pos_accuracy: 0.8341\n",
      "Epoch 167/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0206 - pos_accuracy: 0.9712 - val_loss: 0.2218 - val_pos_accuracy: 0.8389\n",
      "Epoch 168/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0204 - pos_accuracy: 0.9706 - val_loss: 0.2208 - val_pos_accuracy: 0.8413\n",
      "Epoch 169/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0202 - pos_accuracy: 0.9694 - val_loss: 0.2216 - val_pos_accuracy: 0.8413\n",
      "Epoch 170/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0199 - pos_accuracy: 0.9719 - val_loss: 0.2217 - val_pos_accuracy: 0.8413\n",
      "Epoch 171/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0197 - pos_accuracy: 0.9719 - val_loss: 0.2204 - val_pos_accuracy: 0.8413\n",
      "Epoch 172/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0194 - pos_accuracy: 0.9719 - val_loss: 0.2202 - val_pos_accuracy: 0.8413\n",
      "Epoch 173/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0192 - pos_accuracy: 0.9725 - val_loss: 0.2212 - val_pos_accuracy: 0.8413\n",
      "Epoch 174/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0189 - pos_accuracy: 0.9725 - val_loss: 0.2186 - val_pos_accuracy: 0.8413\n",
      "Epoch 175/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0188 - pos_accuracy: 0.9725 - val_loss: 0.2206 - val_pos_accuracy: 0.8413\n",
      "Epoch 176/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0184 - pos_accuracy: 0.9737 - val_loss: 0.2177 - val_pos_accuracy: 0.8413\n",
      "Epoch 177/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0183 - pos_accuracy: 0.9737 - val_loss: 0.2183 - val_pos_accuracy: 0.8413\n",
      "Epoch 178/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0180 - pos_accuracy: 0.9731 - val_loss: 0.2159 - val_pos_accuracy: 0.8462\n",
      "Epoch 179/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0178 - pos_accuracy: 0.9750 - val_loss: 0.2175 - val_pos_accuracy: 0.8413\n",
      "Epoch 180/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9731 - val_loss: 0.2172 - val_pos_accuracy: 0.8413\n",
      "Epoch 181/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0174 - pos_accuracy: 0.9731 - val_loss: 0.2167 - val_pos_accuracy: 0.8438\n",
      "Epoch 182/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0172 - pos_accuracy: 0.9769 - val_loss: 0.2167 - val_pos_accuracy: 0.8413\n",
      "Epoch 183/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0170 - pos_accuracy: 0.9762 - val_loss: 0.2167 - val_pos_accuracy: 0.8413\n",
      "Epoch 184/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0168 - pos_accuracy: 0.9756 - val_loss: 0.2138 - val_pos_accuracy: 0.8486\n",
      "Epoch 185/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0166 - pos_accuracy: 0.9769 - val_loss: 0.2152 - val_pos_accuracy: 0.8486\n",
      "Epoch 186/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0164 - pos_accuracy: 0.9787 - val_loss: 0.2142 - val_pos_accuracy: 0.8558\n",
      "Epoch 187/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0162 - pos_accuracy: 0.9775 - val_loss: 0.2147 - val_pos_accuracy: 0.8534\n",
      "Epoch 188/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0160 - pos_accuracy: 0.9775 - val_loss: 0.2143 - val_pos_accuracy: 0.8510\n",
      "Epoch 189/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0158 - pos_accuracy: 0.9800 - val_loss: 0.2139 - val_pos_accuracy: 0.8534\n",
      "Epoch 190/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0156 - pos_accuracy: 0.9787 - val_loss: 0.2137 - val_pos_accuracy: 0.8582\n",
      "Epoch 191/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0155 - pos_accuracy: 0.9806 - val_loss: 0.2130 - val_pos_accuracy: 0.8606\n",
      "Epoch 192/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0153 - pos_accuracy: 0.9812 - val_loss: 0.2138 - val_pos_accuracy: 0.8606\n",
      "Epoch 193/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0151 - pos_accuracy: 0.9800 - val_loss: 0.2123 - val_pos_accuracy: 0.8606\n",
      "Epoch 194/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0149 - pos_accuracy: 0.9800 - val_loss: 0.2157 - val_pos_accuracy: 0.8534\n",
      "Epoch 195/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0148 - pos_accuracy: 0.9806 - val_loss: 0.2122 - val_pos_accuracy: 0.8606\n",
      "Epoch 196/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0145 - pos_accuracy: 0.9806 - val_loss: 0.2119 - val_pos_accuracy: 0.8606\n",
      "Epoch 197/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9812 - val_loss: 0.2111 - val_pos_accuracy: 0.8606\n",
      "Epoch 198/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0142 - pos_accuracy: 0.9819 - val_loss: 0.2110 - val_pos_accuracy: 0.8606\n",
      "Epoch 199/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0141 - pos_accuracy: 0.9812 - val_loss: 0.2116 - val_pos_accuracy: 0.8606\n",
      "Epoch 200/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0139 - pos_accuracy: 0.9819 - val_loss: 0.2107 - val_pos_accuracy: 0.8606\n",
      "Epoch 201/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0138 - pos_accuracy: 0.9819 - val_loss: 0.2102 - val_pos_accuracy: 0.8606\n",
      "Epoch 202/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0137 - pos_accuracy: 0.9819 - val_loss: 0.2093 - val_pos_accuracy: 0.8654\n",
      "Epoch 203/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0135 - pos_accuracy: 0.9825 - val_loss: 0.2100 - val_pos_accuracy: 0.8606\n",
      "Epoch 204/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0133 - pos_accuracy: 0.9831 - val_loss: 0.2091 - val_pos_accuracy: 0.8654\n",
      "Epoch 205/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0132 - pos_accuracy: 0.9837 - val_loss: 0.2089 - val_pos_accuracy: 0.8654\n",
      "Epoch 206/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0130 - pos_accuracy: 0.9825 - val_loss: 0.2080 - val_pos_accuracy: 0.8654\n",
      "Epoch 207/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0129 - pos_accuracy: 0.9844 - val_loss: 0.2078 - val_pos_accuracy: 0.8654\n",
      "Epoch 208/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0128 - pos_accuracy: 0.9844 - val_loss: 0.2073 - val_pos_accuracy: 0.8654\n",
      "Epoch 209/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0127 - pos_accuracy: 0.9831 - val_loss: 0.2073 - val_pos_accuracy: 0.8654\n",
      "Epoch 210/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0125 - pos_accuracy: 0.9844 - val_loss: 0.2068 - val_pos_accuracy: 0.8654\n",
      "Epoch 211/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0124 - pos_accuracy: 0.9844 - val_loss: 0.2079 - val_pos_accuracy: 0.8654\n",
      "Epoch 212/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0122 - pos_accuracy: 0.9844 - val_loss: 0.2069 - val_pos_accuracy: 0.8654\n",
      "Epoch 213/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0121 - pos_accuracy: 0.9844 - val_loss: 0.2072 - val_pos_accuracy: 0.8654\n",
      "Epoch 214/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0120 - pos_accuracy: 0.9844 - val_loss: 0.2077 - val_pos_accuracy: 0.8654\n",
      "Epoch 215/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0119 - pos_accuracy: 0.9844 - val_loss: 0.2063 - val_pos_accuracy: 0.8654\n",
      "Epoch 216/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0117 - pos_accuracy: 0.9844 - val_loss: 0.2074 - val_pos_accuracy: 0.8654\n",
      "Epoch 217/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0117 - pos_accuracy: 0.9844 - val_loss: 0.2054 - val_pos_accuracy: 0.8702\n",
      "Epoch 218/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0115 - pos_accuracy: 0.9844 - val_loss: 0.2059 - val_pos_accuracy: 0.8702\n",
      "Epoch 219/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0114 - pos_accuracy: 0.9844 - val_loss: 0.2065 - val_pos_accuracy: 0.8654\n",
      "Epoch 220/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0113 - pos_accuracy: 0.9850 - val_loss: 0.2058 - val_pos_accuracy: 0.8654\n",
      "Epoch 221/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0111 - pos_accuracy: 0.9850 - val_loss: 0.2051 - val_pos_accuracy: 0.8678\n",
      "Epoch 222/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0111 - pos_accuracy: 0.9844 - val_loss: 0.2044 - val_pos_accuracy: 0.8702\n",
      "Epoch 223/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0109 - pos_accuracy: 0.9850 - val_loss: 0.2042 - val_pos_accuracy: 0.8702\n",
      "Epoch 224/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0108 - pos_accuracy: 0.9850 - val_loss: 0.2032 - val_pos_accuracy: 0.8726\n",
      "Epoch 225/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0107 - pos_accuracy: 0.9850 - val_loss: 0.2051 - val_pos_accuracy: 0.8654\n",
      "Epoch 226/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0106 - pos_accuracy: 0.9850 - val_loss: 0.2036 - val_pos_accuracy: 0.8726\n",
      "Epoch 227/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0105 - pos_accuracy: 0.9850 - val_loss: 0.2035 - val_pos_accuracy: 0.8726\n",
      "Epoch 228/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0104 - pos_accuracy: 0.9850 - val_loss: 0.2039 - val_pos_accuracy: 0.8702\n",
      "Epoch 229/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0103 - pos_accuracy: 0.9856 - val_loss: 0.2028 - val_pos_accuracy: 0.8726\n",
      "Epoch 230/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0102 - pos_accuracy: 0.9850 - val_loss: 0.2036 - val_pos_accuracy: 0.8726\n",
      "Epoch 231/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0101 - pos_accuracy: 0.9856 - val_loss: 0.2040 - val_pos_accuracy: 0.8726\n",
      "Epoch 232/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0100 - pos_accuracy: 0.9856 - val_loss: 0.2028 - val_pos_accuracy: 0.8726\n",
      "Epoch 233/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0099 - pos_accuracy: 0.9850 - val_loss: 0.2026 - val_pos_accuracy: 0.8726\n",
      "Epoch 234/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0098 - pos_accuracy: 0.9856 - val_loss: 0.2021 - val_pos_accuracy: 0.8726\n",
      "Epoch 235/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0097 - pos_accuracy: 0.9856 - val_loss: 0.2028 - val_pos_accuracy: 0.8726\n",
      "Epoch 236/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0096 - pos_accuracy: 0.9856 - val_loss: 0.2028 - val_pos_accuracy: 0.8726\n",
      "Epoch 237/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9856 - val_loss: 0.2028 - val_pos_accuracy: 0.8726\n",
      "Epoch 238/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9856 - val_loss: 0.2024 - val_pos_accuracy: 0.8726\n",
      "Epoch 239/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0094 - pos_accuracy: 0.9856 - val_loss: 0.2027 - val_pos_accuracy: 0.8726\n",
      "Epoch 240/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0093 - pos_accuracy: 0.9856 - val_loss: 0.2016 - val_pos_accuracy: 0.8726\n",
      "Epoch 241/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0092 - pos_accuracy: 0.9856 - val_loss: 0.2019 - val_pos_accuracy: 0.8726\n",
      "Epoch 242/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0091 - pos_accuracy: 0.9862 - val_loss: 0.2009 - val_pos_accuracy: 0.8726\n",
      "Epoch 243/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0090 - pos_accuracy: 0.9862 - val_loss: 0.2006 - val_pos_accuracy: 0.8726\n",
      "Epoch 244/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0089 - pos_accuracy: 0.9862 - val_loss: 0.2000 - val_pos_accuracy: 0.8726\n",
      "Epoch 245/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0088 - pos_accuracy: 0.9869 - val_loss: 0.2003 - val_pos_accuracy: 0.8726\n",
      "Epoch 246/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9869 - val_loss: 0.2001 - val_pos_accuracy: 0.8726\n",
      "Epoch 247/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9869 - val_loss: 0.1998 - val_pos_accuracy: 0.8726\n",
      "Epoch 248/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0086 - pos_accuracy: 0.9869 - val_loss: 0.1998 - val_pos_accuracy: 0.8726\n",
      "Epoch 249/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0086 - pos_accuracy: 0.9869 - val_loss: 0.1997 - val_pos_accuracy: 0.8726\n",
      "Epoch 250/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0084 - pos_accuracy: 0.9869 - val_loss: 0.2002 - val_pos_accuracy: 0.8726\n",
      "Epoch 251/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0084 - pos_accuracy: 0.9869 - val_loss: 0.1992 - val_pos_accuracy: 0.8726\n",
      "Epoch 252/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9869 - val_loss: 0.1994 - val_pos_accuracy: 0.8726\n",
      "Epoch 253/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0082 - pos_accuracy: 0.9869 - val_loss: 0.1992 - val_pos_accuracy: 0.8726\n",
      "Epoch 254/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0081 - pos_accuracy: 0.9869 - val_loss: 0.1999 - val_pos_accuracy: 0.8726\n",
      "Epoch 255/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0081 - pos_accuracy: 0.9875 - val_loss: 0.1989 - val_pos_accuracy: 0.8726\n",
      "Epoch 256/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0080 - pos_accuracy: 0.9869 - val_loss: 0.1982 - val_pos_accuracy: 0.8726\n",
      "Epoch 257/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9869 - val_loss: 0.1981 - val_pos_accuracy: 0.8726\n",
      "Epoch 258/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9875 - val_loss: 0.1995 - val_pos_accuracy: 0.8726\n",
      "Epoch 259/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9875 - val_loss: 0.1995 - val_pos_accuracy: 0.8726\n",
      "Epoch 260/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0078 - pos_accuracy: 0.9875 - val_loss: 0.1984 - val_pos_accuracy: 0.8726\n",
      "Epoch 261/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0076 - pos_accuracy: 0.9875 - val_loss: 0.1977 - val_pos_accuracy: 0.8726\n",
      "Epoch 262/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0076 - pos_accuracy: 0.9869 - val_loss: 0.1975 - val_pos_accuracy: 0.8726\n",
      "Epoch 263/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0075 - pos_accuracy: 0.9875 - val_loss: 0.1989 - val_pos_accuracy: 0.8750\n",
      "Epoch 264/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0075 - pos_accuracy: 0.9875 - val_loss: 0.1981 - val_pos_accuracy: 0.8726\n",
      "Epoch 265/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0074 - pos_accuracy: 0.9875 - val_loss: 0.1975 - val_pos_accuracy: 0.8726\n",
      "Epoch 266/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0073 - pos_accuracy: 0.9875 - val_loss: 0.1976 - val_pos_accuracy: 0.8726\n",
      "Epoch 267/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0073 - pos_accuracy: 0.9875 - val_loss: 0.1975 - val_pos_accuracy: 0.8726\n",
      "Epoch 268/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0072 - pos_accuracy: 0.9875 - val_loss: 0.1972 - val_pos_accuracy: 0.8726\n",
      "Epoch 269/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0072 - pos_accuracy: 0.9875 - val_loss: 0.1970 - val_pos_accuracy: 0.8726\n",
      "Epoch 270/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0071 - pos_accuracy: 0.9875 - val_loss: 0.1970 - val_pos_accuracy: 0.8750\n",
      "Epoch 271/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0070 - pos_accuracy: 0.9875 - val_loss: 0.1968 - val_pos_accuracy: 0.8726\n",
      "Epoch 272/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9875 - val_loss: 0.1979 - val_pos_accuracy: 0.8750\n",
      "Epoch 273/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9875 - val_loss: 0.1964 - val_pos_accuracy: 0.8726\n",
      "Epoch 274/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9875 - val_loss: 0.1974 - val_pos_accuracy: 0.8750\n",
      "Epoch 275/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0068 - pos_accuracy: 0.9881 - val_loss: 0.1960 - val_pos_accuracy: 0.8750\n",
      "Epoch 276/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0068 - pos_accuracy: 0.9875 - val_loss: 0.1956 - val_pos_accuracy: 0.8726\n",
      "Epoch 277/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9875 - val_loss: 0.1956 - val_pos_accuracy: 0.8726\n",
      "Epoch 278/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9875 - val_loss: 0.1965 - val_pos_accuracy: 0.8726\n",
      "Epoch 279/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0066 - pos_accuracy: 0.9875 - val_loss: 0.1959 - val_pos_accuracy: 0.8726\n",
      "Epoch 280/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9881 - val_loss: 0.1965 - val_pos_accuracy: 0.8750\n",
      "Epoch 281/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9887 - val_loss: 0.1961 - val_pos_accuracy: 0.8750\n",
      "Epoch 282/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9887 - val_loss: 0.1964 - val_pos_accuracy: 0.8750\n",
      "Epoch 283/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9887 - val_loss: 0.1954 - val_pos_accuracy: 0.8726\n",
      "Epoch 284/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0063 - pos_accuracy: 0.9887 - val_loss: 0.1949 - val_pos_accuracy: 0.8750\n",
      "Epoch 285/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0063 - pos_accuracy: 0.9894 - val_loss: 0.1953 - val_pos_accuracy: 0.8750\n",
      "Epoch 286/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0062 - pos_accuracy: 0.9881 - val_loss: 0.1953 - val_pos_accuracy: 0.8750\n",
      "Epoch 287/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0062 - pos_accuracy: 0.9894 - val_loss: 0.1949 - val_pos_accuracy: 0.8750\n",
      "Epoch 288/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9894 - val_loss: 0.1957 - val_pos_accuracy: 0.8750\n",
      "Epoch 289/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9894 - val_loss: 0.1952 - val_pos_accuracy: 0.8750\n",
      "Epoch 290/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9894 - val_loss: 0.1949 - val_pos_accuracy: 0.8750\n",
      "Epoch 291/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9894 - val_loss: 0.1938 - val_pos_accuracy: 0.8750\n",
      "Epoch 292/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0059 - pos_accuracy: 0.9894 - val_loss: 0.1948 - val_pos_accuracy: 0.8750\n",
      "Epoch 293/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0059 - pos_accuracy: 0.9894 - val_loss: 0.1943 - val_pos_accuracy: 0.8750\n",
      "Epoch 294/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9894 - val_loss: 0.1951 - val_pos_accuracy: 0.8750\n",
      "Epoch 295/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9894 - val_loss: 0.1943 - val_pos_accuracy: 0.8750\n",
      "Epoch 296/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9894 - val_loss: 0.1935 - val_pos_accuracy: 0.8750\n",
      "Epoch 297/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9894 - val_loss: 0.1939 - val_pos_accuracy: 0.8750\n",
      "Epoch 298/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9894 - val_loss: 0.1946 - val_pos_accuracy: 0.8750\n",
      "Epoch 299/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9894 - val_loss: 0.1937 - val_pos_accuracy: 0.8750\n",
      "Epoch 300/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9894 - val_loss: 0.1952 - val_pos_accuracy: 0.8750\n",
      "Epoch 301/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9894 - val_loss: 0.1945 - val_pos_accuracy: 0.8750\n",
      "Epoch 302/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9900 - val_loss: 0.1936 - val_pos_accuracy: 0.8750\n",
      "Epoch 303/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9894 - val_loss: 0.1940 - val_pos_accuracy: 0.8750\n",
      "Epoch 304/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9900 - val_loss: 0.1944 - val_pos_accuracy: 0.8750\n",
      "Epoch 305/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9900 - val_loss: 0.1937 - val_pos_accuracy: 0.8750\n",
      "Epoch 306/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9900 - val_loss: 0.1937 - val_pos_accuracy: 0.8750\n",
      "Epoch 307/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9894 - val_loss: 0.1934 - val_pos_accuracy: 0.8750\n",
      "Epoch 308/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9900 - val_loss: 0.1935 - val_pos_accuracy: 0.8750\n",
      "Epoch 309/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9906 - val_loss: 0.1935 - val_pos_accuracy: 0.8750\n",
      "Epoch 310/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9894 - val_loss: 0.1936 - val_pos_accuracy: 0.8750\n",
      "Epoch 311/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9900 - val_loss: 0.1938 - val_pos_accuracy: 0.8750\n",
      "Epoch 312/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9906 - val_loss: 0.1925 - val_pos_accuracy: 0.8750\n",
      "Epoch 313/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9906 - val_loss: 0.1928 - val_pos_accuracy: 0.8750\n",
      "Epoch 314/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9906 - val_loss: 0.1930 - val_pos_accuracy: 0.8750\n",
      "Epoch 315/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9906 - val_loss: 0.1930 - val_pos_accuracy: 0.8750\n",
      "Epoch 316/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9906 - val_loss: 0.1928 - val_pos_accuracy: 0.8750\n",
      "Epoch 317/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9906 - val_loss: 0.1932 - val_pos_accuracy: 0.8750\n",
      "Epoch 318/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9906 - val_loss: 0.1926 - val_pos_accuracy: 0.8750\n",
      "Epoch 319/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9906 - val_loss: 0.1926 - val_pos_accuracy: 0.8750\n",
      "Epoch 320/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9906 - val_loss: 0.1924 - val_pos_accuracy: 0.8750\n",
      "Epoch 321/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9906 - val_loss: 0.1927 - val_pos_accuracy: 0.8750\n",
      "Epoch 322/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9906 - val_loss: 0.1924 - val_pos_accuracy: 0.8750\n",
      "Epoch 323/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9906 - val_loss: 0.1927 - val_pos_accuracy: 0.8750\n",
      "Epoch 324/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0046 - pos_accuracy: 0.9912 - val_loss: 0.1927 - val_pos_accuracy: 0.8750\n",
      "Epoch 325/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9912 - val_loss: 0.1925 - val_pos_accuracy: 0.8750\n",
      "Epoch 326/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9912 - val_loss: 0.1912 - val_pos_accuracy: 0.8750\n",
      "Epoch 327/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9925 - val_loss: 0.1923 - val_pos_accuracy: 0.8750\n",
      "Epoch 328/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9944 - val_loss: 0.1923 - val_pos_accuracy: 0.8750\n",
      "Epoch 329/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9937 - val_loss: 0.1919 - val_pos_accuracy: 0.8774\n",
      "Epoch 330/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9950 - val_loss: 0.1918 - val_pos_accuracy: 0.8774\n",
      "Epoch 331/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9950 - val_loss: 0.1917 - val_pos_accuracy: 0.8774\n",
      "Epoch 332/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9950 - val_loss: 0.1914 - val_pos_accuracy: 0.8774\n",
      "Epoch 333/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9950 - val_loss: 0.1919 - val_pos_accuracy: 0.8774\n",
      "Epoch 334/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9950 - val_loss: 0.1919 - val_pos_accuracy: 0.8774\n",
      "Epoch 335/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9950 - val_loss: 0.1913 - val_pos_accuracy: 0.8774\n",
      "Epoch 336/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9950 - val_loss: 0.1916 - val_pos_accuracy: 0.8774\n",
      "Epoch 337/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9950 - val_loss: 0.1910 - val_pos_accuracy: 0.8774\n",
      "Epoch 338/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9950 - val_loss: 0.1914 - val_pos_accuracy: 0.8774\n",
      "Epoch 339/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9950 - val_loss: 0.1910 - val_pos_accuracy: 0.8774\n",
      "Epoch 340/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9950 - val_loss: 0.1915 - val_pos_accuracy: 0.8774\n",
      "Epoch 341/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9950 - val_loss: 0.1911 - val_pos_accuracy: 0.8774\n",
      "Epoch 342/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9950 - val_loss: 0.1909 - val_pos_accuracy: 0.8774\n",
      "Epoch 343/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9950 - val_loss: 0.1912 - val_pos_accuracy: 0.8774\n",
      "Epoch 344/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9950 - val_loss: 0.1907 - val_pos_accuracy: 0.8774\n",
      "Epoch 345/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9950 - val_loss: 0.1908 - val_pos_accuracy: 0.8774\n",
      "Epoch 346/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9950 - val_loss: 0.1905 - val_pos_accuracy: 0.8774\n",
      "Epoch 347/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9956 - val_loss: 0.1907 - val_pos_accuracy: 0.8774\n",
      "Epoch 348/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9956 - val_loss: 0.1907 - val_pos_accuracy: 0.8774\n",
      "Epoch 349/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9950 - val_loss: 0.1900 - val_pos_accuracy: 0.8774\n",
      "Epoch 350/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9956 - val_loss: 0.1901 - val_pos_accuracy: 0.8774\n",
      "Epoch 351/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9956 - val_loss: 0.1905 - val_pos_accuracy: 0.8774\n",
      "Epoch 352/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9956 - val_loss: 0.1906 - val_pos_accuracy: 0.8774\n",
      "Epoch 353/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9956 - val_loss: 0.1902 - val_pos_accuracy: 0.8774\n",
      "Epoch 354/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9956 - val_loss: 0.1899 - val_pos_accuracy: 0.8774\n",
      "Epoch 355/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9956 - val_loss: 0.1904 - val_pos_accuracy: 0.8774\n",
      "Epoch 356/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9956 - val_loss: 0.1901 - val_pos_accuracy: 0.8774\n",
      "Epoch 357/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9956 - val_loss: 0.1901 - val_pos_accuracy: 0.8774\n",
      "Epoch 358/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9956 - val_loss: 0.1901 - val_pos_accuracy: 0.8774\n",
      "Epoch 359/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9956 - val_loss: 0.1899 - val_pos_accuracy: 0.8774\n",
      "Epoch 360/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9956 - val_loss: 0.1894 - val_pos_accuracy: 0.8774\n",
      "Epoch 361/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9956 - val_loss: 0.1903 - val_pos_accuracy: 0.8774\n",
      "Epoch 362/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9956 - val_loss: 0.1899 - val_pos_accuracy: 0.8774\n",
      "Epoch 363/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9956 - val_loss: 0.1896 - val_pos_accuracy: 0.8774\n",
      "Epoch 364/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9956 - val_loss: 0.1900 - val_pos_accuracy: 0.8774\n",
      "Epoch 365/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9956 - val_loss: 0.1895 - val_pos_accuracy: 0.8774\n",
      "Epoch 366/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9956 - val_loss: 0.1903 - val_pos_accuracy: 0.8774\n",
      "Epoch 367/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9956 - val_loss: 0.1896 - val_pos_accuracy: 0.8774\n",
      "Epoch 368/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9956 - val_loss: 0.1894 - val_pos_accuracy: 0.8774\n",
      "Epoch 369/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9956 - val_loss: 0.1894 - val_pos_accuracy: 0.8774\n",
      "Epoch 370/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9956 - val_loss: 0.1891 - val_pos_accuracy: 0.8774\n",
      "Epoch 371/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9956 - val_loss: 0.1889 - val_pos_accuracy: 0.8774\n",
      "Epoch 372/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9956 - val_loss: 0.1892 - val_pos_accuracy: 0.8774\n",
      "Epoch 373/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9956 - val_loss: 0.1896 - val_pos_accuracy: 0.8774\n",
      "Epoch 374/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9956 - val_loss: 0.1891 - val_pos_accuracy: 0.8774\n",
      "Epoch 375/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9956 - val_loss: 0.1889 - val_pos_accuracy: 0.8774\n",
      "Epoch 376/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9956 - val_loss: 0.1887 - val_pos_accuracy: 0.8774\n",
      "Epoch 377/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9956 - val_loss: 0.1887 - val_pos_accuracy: 0.8774\n",
      "Epoch 378/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9956 - val_loss: 0.1886 - val_pos_accuracy: 0.8774\n",
      "Epoch 379/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9956 - val_loss: 0.1898 - val_pos_accuracy: 0.8774\n",
      "Epoch 380/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9956 - val_loss: 0.1885 - val_pos_accuracy: 0.8774\n",
      "Epoch 381/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9956 - val_loss: 0.1889 - val_pos_accuracy: 0.8774\n",
      "Epoch 382/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9956 - val_loss: 0.1888 - val_pos_accuracy: 0.8774\n",
      "Epoch 383/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9962 - val_loss: 0.1886 - val_pos_accuracy: 0.8774\n",
      "Epoch 384/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9956 - val_loss: 0.1891 - val_pos_accuracy: 0.8774\n",
      "Epoch 385/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9956 - val_loss: 0.1887 - val_pos_accuracy: 0.8774\n",
      "Epoch 386/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9962 - val_loss: 0.1887 - val_pos_accuracy: 0.8774\n",
      "Epoch 387/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9962 - val_loss: 0.1888 - val_pos_accuracy: 0.8774\n",
      "Epoch 388/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9962 - val_loss: 0.1883 - val_pos_accuracy: 0.8774\n",
      "Epoch 389/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9962 - val_loss: 0.1883 - val_pos_accuracy: 0.8774\n",
      "Epoch 390/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9962 - val_loss: 0.1885 - val_pos_accuracy: 0.8774\n",
      "Epoch 391/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9962 - val_loss: 0.1884 - val_pos_accuracy: 0.8774\n",
      "Epoch 392/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9962 - val_loss: 0.1880 - val_pos_accuracy: 0.8774\n",
      "Epoch 393/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9969 - val_loss: 0.1884 - val_pos_accuracy: 0.8774\n",
      "Epoch 394/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9969 - val_loss: 0.1881 - val_pos_accuracy: 0.8774\n",
      "Epoch 395/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9962 - val_loss: 0.1883 - val_pos_accuracy: 0.8774\n",
      "Epoch 396/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9969 - val_loss: 0.1879 - val_pos_accuracy: 0.8774\n",
      "Epoch 397/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9975 - val_loss: 0.1883 - val_pos_accuracy: 0.8774\n",
      "Epoch 398/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9969 - val_loss: 0.1881 - val_pos_accuracy: 0.8774\n",
      "Epoch 399/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9975 - val_loss: 0.1879 - val_pos_accuracy: 0.8774\n",
      "Epoch 400/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9969 - val_loss: 0.1880 - val_pos_accuracy: 0.8774\n",
      "Epoch 401/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9975 - val_loss: 0.1882 - val_pos_accuracy: 0.8774\n",
      "Epoch 402/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9975 - val_loss: 0.1881 - val_pos_accuracy: 0.8774\n",
      "Epoch 403/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0028 - pos_accuracy: 0.9975 - val_loss: 0.1879 - val_pos_accuracy: 0.8774\n",
      "Epoch 404/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9981 - val_loss: 0.1875 - val_pos_accuracy: 0.8774\n",
      "Epoch 405/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9981 - val_loss: 0.1880 - val_pos_accuracy: 0.8774\n",
      "Epoch 406/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9981 - val_loss: 0.1880 - val_pos_accuracy: 0.8774\n",
      "Epoch 407/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9969 - val_loss: 0.1879 - val_pos_accuracy: 0.8774\n",
      "Epoch 408/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9981 - val_loss: 0.1881 - val_pos_accuracy: 0.8774\n",
      "Epoch 409/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9981 - val_loss: 0.1881 - val_pos_accuracy: 0.8774\n",
      "Epoch 410/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9981 - val_loss: 0.1877 - val_pos_accuracy: 0.8774\n",
      "Epoch 411/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0027 - pos_accuracy: 0.9981 - val_loss: 0.1876 - val_pos_accuracy: 0.8774\n",
      "Epoch 412/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9981 - val_loss: 0.1881 - val_pos_accuracy: 0.8774\n",
      "Epoch 413/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9981 - val_loss: 0.1872 - val_pos_accuracy: 0.8774\n",
      "Epoch 414/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9981 - val_loss: 0.1876 - val_pos_accuracy: 0.8774\n",
      "Epoch 415/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9981 - val_loss: 0.1876 - val_pos_accuracy: 0.8774\n",
      "Epoch 416/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9987 - val_loss: 0.1878 - val_pos_accuracy: 0.8774\n",
      "Epoch 417/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9981 - val_loss: 0.1869 - val_pos_accuracy: 0.8798\n",
      "Epoch 418/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9981 - val_loss: 0.1872 - val_pos_accuracy: 0.8798\n",
      "Epoch 419/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.1875 - val_pos_accuracy: 0.8798\n",
      "Epoch 420/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9981 - val_loss: 0.1878 - val_pos_accuracy: 0.8798\n",
      "Epoch 421/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.1870 - val_pos_accuracy: 0.8798\n",
      "Epoch 422/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.1874 - val_pos_accuracy: 0.8798\n",
      "Epoch 423/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.1865 - val_pos_accuracy: 0.8798\n",
      "Epoch 424/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9981 - val_loss: 0.1872 - val_pos_accuracy: 0.8798\n",
      "Epoch 425/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9981 - val_loss: 0.1872 - val_pos_accuracy: 0.8798\n",
      "Epoch 426/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.1871 - val_pos_accuracy: 0.8798\n",
      "Epoch 427/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9981 - val_loss: 0.1874 - val_pos_accuracy: 0.8798\n",
      "Epoch 428/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.1871 - val_pos_accuracy: 0.8798\n",
      "Epoch 429/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.1869 - val_pos_accuracy: 0.8798\n",
      "Epoch 430/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.1871 - val_pos_accuracy: 0.8774\n",
      "Epoch 431/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.1871 - val_pos_accuracy: 0.8798\n",
      "Epoch 432/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.1870 - val_pos_accuracy: 0.8798\n",
      "Epoch 433/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1872 - val_pos_accuracy: 0.8798\n",
      "Epoch 434/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1868 - val_pos_accuracy: 0.8798\n",
      "Epoch 435/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1867 - val_pos_accuracy: 0.8798\n",
      "Epoch 436/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1866 - val_pos_accuracy: 0.8798\n",
      "Epoch 437/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1870 - val_pos_accuracy: 0.8798\n",
      "Epoch 438/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1864 - val_pos_accuracy: 0.8798\n",
      "Epoch 439/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1867 - val_pos_accuracy: 0.8798\n",
      "Epoch 440/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1864 - val_pos_accuracy: 0.8798\n",
      "Epoch 441/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1866 - val_pos_accuracy: 0.8798\n",
      "Epoch 442/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1868 - val_pos_accuracy: 0.8798\n",
      "Epoch 443/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1869 - val_pos_accuracy: 0.8798\n",
      "Epoch 444/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1865 - val_pos_accuracy: 0.8798\n",
      "Epoch 445/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1865 - val_pos_accuracy: 0.8798\n",
      "Epoch 446/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1865 - val_pos_accuracy: 0.8798\n",
      "Epoch 447/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1864 - val_pos_accuracy: 0.8798\n",
      "Epoch 448/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1865 - val_pos_accuracy: 0.8798\n",
      "Epoch 449/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1865 - val_pos_accuracy: 0.8798\n",
      "Epoch 450/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1863 - val_pos_accuracy: 0.8798\n",
      "Epoch 451/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9987 - val_loss: 0.1863 - val_pos_accuracy: 0.8822\n",
      "Epoch 452/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1861 - val_pos_accuracy: 0.8822\n",
      "Epoch 453/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1862 - val_pos_accuracy: 0.8822\n",
      "Epoch 454/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1863 - val_pos_accuracy: 0.8822\n",
      "Epoch 455/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1861 - val_pos_accuracy: 0.8822\n",
      "Epoch 456/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1862 - val_pos_accuracy: 0.8822\n",
      "Epoch 457/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1859 - val_pos_accuracy: 0.8822\n",
      "Epoch 458/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1865 - val_pos_accuracy: 0.8822\n",
      "Epoch 459/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1864 - val_pos_accuracy: 0.8822\n",
      "Epoch 460/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1861 - val_pos_accuracy: 0.8822\n",
      "Epoch 461/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1861 - val_pos_accuracy: 0.8822\n",
      "Epoch 462/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1866 - val_pos_accuracy: 0.8798\n",
      "Epoch 463/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1859 - val_pos_accuracy: 0.8822\n",
      "Epoch 464/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1860 - val_pos_accuracy: 0.8822\n",
      "Epoch 465/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1860 - val_pos_accuracy: 0.8822\n",
      "Epoch 466/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1856 - val_pos_accuracy: 0.8822\n",
      "Epoch 467/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1860 - val_pos_accuracy: 0.8822\n",
      "Epoch 468/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1861 - val_pos_accuracy: 0.8822\n",
      "Epoch 469/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1860 - val_pos_accuracy: 0.8822\n",
      "Epoch 470/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1857 - val_pos_accuracy: 0.8822\n",
      "Epoch 471/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1857 - val_pos_accuracy: 0.8822\n",
      "Epoch 472/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1853 - val_pos_accuracy: 0.8822\n",
      "Epoch 473/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1858 - val_pos_accuracy: 0.8822\n",
      "Epoch 474/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1859 - val_pos_accuracy: 0.8822\n",
      "Epoch 475/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1856 - val_pos_accuracy: 0.8822\n",
      "Epoch 476/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1853 - val_pos_accuracy: 0.8822\n",
      "Epoch 477/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1854 - val_pos_accuracy: 0.8822\n",
      "Epoch 478/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1855 - val_pos_accuracy: 0.8822\n",
      "Epoch 479/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1851 - val_pos_accuracy: 0.8822\n",
      "Epoch 480/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1854 - val_pos_accuracy: 0.8822\n",
      "Epoch 481/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1856 - val_pos_accuracy: 0.8822\n",
      "Epoch 482/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1854 - val_pos_accuracy: 0.8822\n",
      "Epoch 483/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1853 - val_pos_accuracy: 0.8822\n",
      "Epoch 484/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1853 - val_pos_accuracy: 0.8822\n",
      "Epoch 485/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1853 - val_pos_accuracy: 0.8822\n",
      "Epoch 486/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1852 - val_pos_accuracy: 0.8822\n",
      "Epoch 487/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1853 - val_pos_accuracy: 0.8822\n",
      "Epoch 488/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1850 - val_pos_accuracy: 0.8822\n",
      "Epoch 489/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1850 - val_pos_accuracy: 0.8822\n",
      "Epoch 490/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1851 - val_pos_accuracy: 0.8822\n",
      "Epoch 491/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1855 - val_pos_accuracy: 0.8822\n",
      "Epoch 492/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1849 - val_pos_accuracy: 0.8822\n",
      "Epoch 493/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1851 - val_pos_accuracy: 0.8822\n",
      "Epoch 494/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1850 - val_pos_accuracy: 0.8822\n",
      "Epoch 495/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1851 - val_pos_accuracy: 0.8822\n",
      "Epoch 496/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1850 - val_pos_accuracy: 0.8822\n",
      "Epoch 497/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1851 - val_pos_accuracy: 0.8822\n",
      "Epoch 498/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1852 - val_pos_accuracy: 0.8798\n",
      "Epoch 499/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1849 - val_pos_accuracy: 0.8822\n",
      "Epoch 500/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1846 - val_pos_accuracy: 0.8822\n",
      "Epoch 501/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1847 - val_pos_accuracy: 0.8822\n",
      "Epoch 502/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1855 - val_pos_accuracy: 0.8822\n",
      "Epoch 503/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1852 - val_pos_accuracy: 0.8822\n",
      "Epoch 504/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1850 - val_pos_accuracy: 0.8822\n",
      "Epoch 505/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1848 - val_pos_accuracy: 0.8822\n",
      "Epoch 506/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1845 - val_pos_accuracy: 0.8822\n",
      "Epoch 507/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1848 - val_pos_accuracy: 0.8822\n",
      "Epoch 508/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1848 - val_pos_accuracy: 0.8822\n",
      "Epoch 509/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1849 - val_pos_accuracy: 0.8798\n",
      "Epoch 510/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1847 - val_pos_accuracy: 0.8798\n",
      "Epoch 511/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1845 - val_pos_accuracy: 0.8798\n",
      "Epoch 512/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1844 - val_pos_accuracy: 0.8822\n",
      "Epoch 513/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1843 - val_pos_accuracy: 0.8822\n",
      "Epoch 514/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1842 - val_pos_accuracy: 0.8822\n",
      "Epoch 515/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1844 - val_pos_accuracy: 0.8822\n",
      "Epoch 516/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1842 - val_pos_accuracy: 0.8822\n",
      "Epoch 517/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1846 - val_pos_accuracy: 0.8798\n",
      "Epoch 518/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1845 - val_pos_accuracy: 0.8822\n",
      "Epoch 519/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1847 - val_pos_accuracy: 0.8822\n",
      "Epoch 520/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1847 - val_pos_accuracy: 0.8822\n",
      "Epoch 521/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1844 - val_pos_accuracy: 0.8822\n",
      "Epoch 522/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1848 - val_pos_accuracy: 0.8798\n",
      "Epoch 523/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1839 - val_pos_accuracy: 0.8822\n",
      "Epoch 524/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1840 - val_pos_accuracy: 0.8822\n",
      "Epoch 525/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1841 - val_pos_accuracy: 0.8822\n",
      "Epoch 526/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1844 - val_pos_accuracy: 0.8798\n",
      "Epoch 527/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1845 - val_pos_accuracy: 0.8822\n",
      "Epoch 528/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1844 - val_pos_accuracy: 0.8822\n",
      "Epoch 529/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1843 - val_pos_accuracy: 0.8822\n",
      "Epoch 530/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1844 - val_pos_accuracy: 0.8822\n",
      "Epoch 531/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1842 - val_pos_accuracy: 0.8822\n",
      "Epoch 532/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1841 - val_pos_accuracy: 0.8822\n",
      "Epoch 533/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1842 - val_pos_accuracy: 0.8822\n",
      "Epoch 534/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1843 - val_pos_accuracy: 0.8822\n",
      "Epoch 535/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1840 - val_pos_accuracy: 0.8822\n",
      "Epoch 536/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1842 - val_pos_accuracy: 0.8822\n",
      "Epoch 537/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1842 - val_pos_accuracy: 0.8822\n",
      "Epoch 538/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1840 - val_pos_accuracy: 0.8822\n",
      "Epoch 539/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1839 - val_pos_accuracy: 0.8822\n",
      "Epoch 540/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1840 - val_pos_accuracy: 0.8822\n",
      "Epoch 541/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1839 - val_pos_accuracy: 0.8822\n",
      "Epoch 542/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1838 - val_pos_accuracy: 0.8822\n",
      "Epoch 543/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1840 - val_pos_accuracy: 0.8822\n",
      "Epoch 544/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1838 - val_pos_accuracy: 0.8822\n",
      "Epoch 545/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1839 - val_pos_accuracy: 0.8822\n",
      "Epoch 546/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1838 - val_pos_accuracy: 0.8822\n",
      "Epoch 547/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1838 - val_pos_accuracy: 0.8798\n",
      "Epoch 548/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1838 - val_pos_accuracy: 0.8822\n",
      "Epoch 549/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1838 - val_pos_accuracy: 0.8822\n",
      "Epoch 550/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1837 - val_pos_accuracy: 0.8822\n",
      "Epoch 551/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1837 - val_pos_accuracy: 0.8822\n",
      "Epoch 552/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1836 - val_pos_accuracy: 0.8822\n",
      "Epoch 553/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1837 - val_pos_accuracy: 0.8822\n",
      "Epoch 554/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1836 - val_pos_accuracy: 0.8822\n",
      "Epoch 555/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1840 - val_pos_accuracy: 0.8822\n",
      "Epoch 556/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1838 - val_pos_accuracy: 0.8822\n",
      "Epoch 557/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1839 - val_pos_accuracy: 0.8822\n",
      "Epoch 558/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1831 - val_pos_accuracy: 0.8822\n",
      "Epoch 559/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1836 - val_pos_accuracy: 0.8822\n",
      "Epoch 560/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1836 - val_pos_accuracy: 0.8822\n",
      "Epoch 561/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1835 - val_pos_accuracy: 0.8822\n",
      "Epoch 562/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1838 - val_pos_accuracy: 0.8798\n",
      "Epoch 563/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1835 - val_pos_accuracy: 0.8822\n",
      "Epoch 564/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1832 - val_pos_accuracy: 0.8822\n",
      "Epoch 565/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1837 - val_pos_accuracy: 0.8822\n",
      "Epoch 566/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1837 - val_pos_accuracy: 0.8798\n",
      "Epoch 567/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1834 - val_pos_accuracy: 0.8822\n",
      "Epoch 568/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1833 - val_pos_accuracy: 0.8822\n",
      "Epoch 569/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1830 - val_pos_accuracy: 0.8822\n",
      "Epoch 570/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1835 - val_pos_accuracy: 0.8798\n",
      "Epoch 571/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1834 - val_pos_accuracy: 0.8798\n",
      "Epoch 572/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1835 - val_pos_accuracy: 0.8822\n",
      "Epoch 573/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1834 - val_pos_accuracy: 0.8822\n",
      "Epoch 574/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1835 - val_pos_accuracy: 0.8798\n",
      "Epoch 575/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1833 - val_pos_accuracy: 0.8822\n",
      "Epoch 576/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1834 - val_pos_accuracy: 0.8822\n",
      "Epoch 577/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1832 - val_pos_accuracy: 0.8822\n",
      "Epoch 578/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1834 - val_pos_accuracy: 0.8798\n",
      "Epoch 579/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1832 - val_pos_accuracy: 0.8822\n",
      "Epoch 580/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1834 - val_pos_accuracy: 0.8798\n",
      "Epoch 581/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1831 - val_pos_accuracy: 0.8822\n",
      "Epoch 582/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1833 - val_pos_accuracy: 0.8822\n",
      "Epoch 583/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1831 - val_pos_accuracy: 0.8822\n",
      "Epoch 584/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1831 - val_pos_accuracy: 0.8822\n",
      "Epoch 585/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1832 - val_pos_accuracy: 0.8822\n",
      "Epoch 586/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1832 - val_pos_accuracy: 0.8822\n",
      "Epoch 587/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1832 - val_pos_accuracy: 0.8822\n",
      "Epoch 588/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1831 - val_pos_accuracy: 0.8822\n",
      "Epoch 589/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1831 - val_pos_accuracy: 0.8822\n",
      "Epoch 590/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1831 - val_pos_accuracy: 0.8822\n",
      "Epoch 591/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1829 - val_pos_accuracy: 0.8822\n",
      "Epoch 592/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1832 - val_pos_accuracy: 0.8798\n",
      "Epoch 593/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1830 - val_pos_accuracy: 0.8822\n",
      "Epoch 594/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1828 - val_pos_accuracy: 0.8822\n",
      "Epoch 595/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1831 - val_pos_accuracy: 0.8822\n",
      "Epoch 596/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1831 - val_pos_accuracy: 0.8822\n",
      "Epoch 597/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1829 - val_pos_accuracy: 0.8798\n",
      "Epoch 598/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1830 - val_pos_accuracy: 0.8822\n",
      "Epoch 599/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1830 - val_pos_accuracy: 0.8798\n",
      "Epoch 600/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1829 - val_pos_accuracy: 0.8798\n",
      "Epoch 601/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1830 - val_pos_accuracy: 0.8822\n",
      "Epoch 602/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1829 - val_pos_accuracy: 0.8822\n",
      "Epoch 603/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1829 - val_pos_accuracy: 0.8822\n",
      "Epoch 604/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1830 - val_pos_accuracy: 0.8822\n",
      "Epoch 605/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1827 - val_pos_accuracy: 0.8822\n",
      "Epoch 606/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1827 - val_pos_accuracy: 0.8822\n",
      "Epoch 607/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1828 - val_pos_accuracy: 0.8822\n",
      "Epoch 608/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1828 - val_pos_accuracy: 0.8798\n",
      "Epoch 609/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1828 - val_pos_accuracy: 0.8822\n",
      "Epoch 610/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1829 - val_pos_accuracy: 0.8822\n",
      "Epoch 611/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1830 - val_pos_accuracy: 0.8822\n",
      "Epoch 612/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1828 - val_pos_accuracy: 0.8822\n",
      "Epoch 613/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1828 - val_pos_accuracy: 0.8822\n",
      "Epoch 614/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1827 - val_pos_accuracy: 0.8822\n",
      "Epoch 615/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1827 - val_pos_accuracy: 0.8798\n",
      "Epoch 616/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1828 - val_pos_accuracy: 0.8822\n",
      "Epoch 617/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1825 - val_pos_accuracy: 0.8822\n",
      "Epoch 618/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1829 - val_pos_accuracy: 0.8822\n",
      "Epoch 619/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8822\n",
      "Epoch 620/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1825 - val_pos_accuracy: 0.8822\n",
      "Epoch 621/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1825 - val_pos_accuracy: 0.8822\n",
      "Epoch 622/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1830 - val_pos_accuracy: 0.8822\n",
      "Epoch 623/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1828 - val_pos_accuracy: 0.8822\n",
      "Epoch 624/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1826 - val_pos_accuracy: 0.8822\n",
      "Epoch 625/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0010 - pos_accuracy: 0.9994 - val_loss: 0.1823 - val_pos_accuracy: 0.8822\n",
      "Epoch 626/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.9552e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8822\n",
      "Epoch 627/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.9413e-04 - pos_accuracy: 0.9994 - val_loss: 0.1819 - val_pos_accuracy: 0.8822\n",
      "Epoch 628/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.9040e-04 - pos_accuracy: 0.9994 - val_loss: 0.1825 - val_pos_accuracy: 0.8822\n",
      "Epoch 629/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.8291e-04 - pos_accuracy: 0.9994 - val_loss: 0.1825 - val_pos_accuracy: 0.8822\n",
      "Epoch 630/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.8179e-04 - pos_accuracy: 0.9994 - val_loss: 0.1824 - val_pos_accuracy: 0.8822\n",
      "Epoch 631/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.7617e-04 - pos_accuracy: 0.9994 - val_loss: 0.1824 - val_pos_accuracy: 0.8822\n",
      "Epoch 632/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.7407e-04 - pos_accuracy: 0.9994 - val_loss: 0.1823 - val_pos_accuracy: 0.8822\n",
      "Epoch 633/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.6909e-04 - pos_accuracy: 0.9994 - val_loss: 0.1824 - val_pos_accuracy: 0.8822\n",
      "Epoch 634/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.6724e-04 - pos_accuracy: 0.9994 - val_loss: 0.1824 - val_pos_accuracy: 0.8822\n",
      "Epoch 635/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.6087e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8822\n",
      "Epoch 636/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.6172e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8798\n",
      "Epoch 637/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.5265e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8822\n",
      "Epoch 638/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.5449e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8822\n",
      "Epoch 639/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.4863e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8822\n",
      "Epoch 640/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.4569e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8822\n",
      "Epoch 641/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 9.4266e-04 - pos_accuracy: 0.9994 - val_loss: 0.1823 - val_pos_accuracy: 0.8798\n",
      "Epoch 642/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.3703e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8798\n",
      "Epoch 643/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.3298e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8822\n",
      "Epoch 644/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.3248e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8822\n",
      "Epoch 645/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.2443e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8822\n",
      "Epoch 646/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.1831e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8798\n",
      "Epoch 647/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.1634e-04 - pos_accuracy: 0.9994 - val_loss: 0.1825 - val_pos_accuracy: 0.8798\n",
      "Epoch 648/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.1405e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8822\n",
      "Epoch 649/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.0715e-04 - pos_accuracy: 0.9994 - val_loss: 0.1824 - val_pos_accuracy: 0.8798\n",
      "Epoch 650/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.0679e-04 - pos_accuracy: 0.9994 - val_loss: 0.1823 - val_pos_accuracy: 0.8822\n",
      "Epoch 651/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.0293e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8822\n",
      "Epoch 652/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.9921e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8822\n",
      "Epoch 653/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.9772e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8822\n",
      "Epoch 654/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.9444e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8822\n",
      "Epoch 655/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.9085e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8822\n",
      "Epoch 656/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.8843e-04 - pos_accuracy: 0.9994 - val_loss: 0.1819 - val_pos_accuracy: 0.8822\n",
      "Epoch 657/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.8592e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8822\n",
      "Epoch 658/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.8436e-04 - pos_accuracy: 0.9994 - val_loss: 0.1820 - val_pos_accuracy: 0.8822\n",
      "Epoch 659/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.7856e-04 - pos_accuracy: 0.9994 - val_loss: 0.1820 - val_pos_accuracy: 0.8822\n",
      "Epoch 660/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.7864e-04 - pos_accuracy: 0.9994 - val_loss: 0.1820 - val_pos_accuracy: 0.8798\n",
      "Epoch 661/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.7408e-04 - pos_accuracy: 0.9994 - val_loss: 0.1820 - val_pos_accuracy: 0.8822\n",
      "Epoch 662/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.7337e-04 - pos_accuracy: 0.9994 - val_loss: 0.1818 - val_pos_accuracy: 0.8822\n",
      "Epoch 663/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.6887e-04 - pos_accuracy: 0.9994 - val_loss: 0.1818 - val_pos_accuracy: 0.8822\n",
      "Epoch 664/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.6576e-04 - pos_accuracy: 0.9994 - val_loss: 0.1818 - val_pos_accuracy: 0.8798\n",
      "Epoch 665/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.6517e-04 - pos_accuracy: 0.9994 - val_loss: 0.1820 - val_pos_accuracy: 0.8822\n",
      "Epoch 666/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.6027e-04 - pos_accuracy: 0.9994 - val_loss: 0.1820 - val_pos_accuracy: 0.8822\n",
      "Epoch 667/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.5471e-04 - pos_accuracy: 0.9994 - val_loss: 0.1816 - val_pos_accuracy: 0.8822\n",
      "Epoch 668/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 8.5350e-04 - pos_accuracy: 0.9994 - val_loss: 0.1817 - val_pos_accuracy: 0.8822\n",
      "Epoch 669/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.4933e-04 - pos_accuracy: 0.9994 - val_loss: 0.1818 - val_pos_accuracy: 0.8822\n",
      "Epoch 670/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.4701e-04 - pos_accuracy: 0.9994 - val_loss: 0.1817 - val_pos_accuracy: 0.8822\n",
      "Epoch 671/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.4278e-04 - pos_accuracy: 0.9994 - val_loss: 0.1817 - val_pos_accuracy: 0.8822\n",
      "Epoch 672/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.4016e-04 - pos_accuracy: 0.9994 - val_loss: 0.1819 - val_pos_accuracy: 0.8822\n",
      "Epoch 673/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.3883e-04 - pos_accuracy: 0.9994 - val_loss: 0.1819 - val_pos_accuracy: 0.8798\n",
      "Epoch 674/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.3546e-04 - pos_accuracy: 0.9994 - val_loss: 0.1819 - val_pos_accuracy: 0.8798\n",
      "Epoch 675/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 8.3506e-04 - pos_accuracy: 0.9994 - val_loss: 0.1818 - val_pos_accuracy: 0.8822\n",
      "Epoch 676/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.2994e-04 - pos_accuracy: 0.9994 - val_loss: 0.1818 - val_pos_accuracy: 0.8822\n",
      "Epoch 677/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.2660e-04 - pos_accuracy: 0.9994 - val_loss: 0.1817 - val_pos_accuracy: 0.8822\n",
      "Epoch 678/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.2617e-04 - pos_accuracy: 0.9994 - val_loss: 0.1815 - val_pos_accuracy: 0.8822\n",
      "Epoch 679/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.2196e-04 - pos_accuracy: 0.9994 - val_loss: 0.1816 - val_pos_accuracy: 0.8822\n",
      "Epoch 680/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.2067e-04 - pos_accuracy: 0.9994 - val_loss: 0.1822 - val_pos_accuracy: 0.8798\n",
      "Epoch 681/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.1780e-04 - pos_accuracy: 0.9994 - val_loss: 0.1821 - val_pos_accuracy: 0.8798\n",
      "Epoch 682/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.1261e-04 - pos_accuracy: 0.9994 - val_loss: 0.1814 - val_pos_accuracy: 0.8822\n",
      "Epoch 683/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.1111e-04 - pos_accuracy: 0.9994 - val_loss: 0.1819 - val_pos_accuracy: 0.8798\n",
      "Epoch 684/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.0704e-04 - pos_accuracy: 0.9994 - val_loss: 0.1816 - val_pos_accuracy: 0.8822\n",
      "Epoch 685/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.0531e-04 - pos_accuracy: 0.9994 - val_loss: 0.1818 - val_pos_accuracy: 0.8798\n",
      "Epoch 686/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8.0366e-04 - pos_accuracy: 0.9994 - val_loss: 0.1819 - val_pos_accuracy: 0.8798\n",
      "Epoch 687/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.9966e-04 - pos_accuracy: 0.9994 - val_loss: 0.1818 - val_pos_accuracy: 0.8798\n",
      "Epoch 688/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.9209e-04 - pos_accuracy: 0.9994 - val_loss: 0.1816 - val_pos_accuracy: 0.8798\n",
      "Epoch 689/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.9194e-04 - pos_accuracy: 0.9994 - val_loss: 0.1816 - val_pos_accuracy: 0.8822\n",
      "Epoch 690/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.8775e-04 - pos_accuracy: 0.9994 - val_loss: 0.1815 - val_pos_accuracy: 0.8822\n",
      "Epoch 691/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.8480e-04 - pos_accuracy: 0.9994 - val_loss: 0.1814 - val_pos_accuracy: 0.8822\n",
      "Epoch 692/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.8114e-04 - pos_accuracy: 0.9994 - val_loss: 0.1817 - val_pos_accuracy: 0.8798\n",
      "Epoch 693/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.7894e-04 - pos_accuracy: 0.9994 - val_loss: 0.1815 - val_pos_accuracy: 0.8822\n",
      "Epoch 694/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 7.7781e-04 - pos_accuracy: 0.9994 - val_loss: 0.1816 - val_pos_accuracy: 0.8798\n",
      "Epoch 695/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.7509e-04 - pos_accuracy: 0.9994 - val_loss: 0.1816 - val_pos_accuracy: 0.8822\n",
      "Epoch 696/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.7408e-04 - pos_accuracy: 0.9994 - val_loss: 0.1815 - val_pos_accuracy: 0.8798\n",
      "Epoch 697/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.7153e-04 - pos_accuracy: 0.9994 - val_loss: 0.1814 - val_pos_accuracy: 0.8822\n",
      "Epoch 698/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.7302e-04 - pos_accuracy: 0.9994 - val_loss: 0.1813 - val_pos_accuracy: 0.8822\n",
      "Epoch 699/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.6991e-04 - pos_accuracy: 0.9994 - val_loss: 0.1816 - val_pos_accuracy: 0.8798\n",
      "Epoch 700/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.6850e-04 - pos_accuracy: 0.9994 - val_loss: 0.1813 - val_pos_accuracy: 0.8822\n",
      "Epoch 701/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.7350e-04 - pos_accuracy: 0.9994 - val_loss: 0.1813 - val_pos_accuracy: 0.8798\n",
      "Epoch 702/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.6203e-04 - pos_accuracy: 0.9994 - val_loss: 0.1818 - val_pos_accuracy: 0.8798\n",
      "Epoch 703/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.6265e-04 - pos_accuracy: 0.9994 - val_loss: 0.1819 - val_pos_accuracy: 0.8798\n",
      "Epoch 704/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.5772e-04 - pos_accuracy: 0.9994 - val_loss: 0.1815 - val_pos_accuracy: 0.8798\n",
      "Epoch 705/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.5590e-04 - pos_accuracy: 0.9994 - val_loss: 0.1814 - val_pos_accuracy: 0.8798\n",
      "Epoch 706/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.5207e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 707/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.5374e-04 - pos_accuracy: 0.9994 - val_loss: 0.1814 - val_pos_accuracy: 0.8798\n",
      "Epoch 708/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.5067e-04 - pos_accuracy: 0.9994 - val_loss: 0.1813 - val_pos_accuracy: 0.8798\n",
      "Epoch 709/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.4290e-04 - pos_accuracy: 0.9994 - val_loss: 0.1813 - val_pos_accuracy: 0.8798\n",
      "Epoch 710/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.4143e-04 - pos_accuracy: 0.9994 - val_loss: 0.1812 - val_pos_accuracy: 0.8798\n",
      "Epoch 711/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.3620e-04 - pos_accuracy: 0.9994 - val_loss: 0.1813 - val_pos_accuracy: 0.8798\n",
      "Epoch 712/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.3902e-04 - pos_accuracy: 0.9994 - val_loss: 0.1812 - val_pos_accuracy: 0.8798\n",
      "Epoch 713/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.3211e-04 - pos_accuracy: 0.9994 - val_loss: 0.1812 - val_pos_accuracy: 0.8798\n",
      "Epoch 714/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.3128e-04 - pos_accuracy: 0.9994 - val_loss: 0.1813 - val_pos_accuracy: 0.8798\n",
      "Epoch 715/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.3070e-04 - pos_accuracy: 0.9994 - val_loss: 0.1812 - val_pos_accuracy: 0.8798\n",
      "Epoch 716/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.2735e-04 - pos_accuracy: 0.9994 - val_loss: 0.1812 - val_pos_accuracy: 0.8798\n",
      "Epoch 717/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.2466e-04 - pos_accuracy: 0.9994 - val_loss: 0.1812 - val_pos_accuracy: 0.8798\n",
      "Epoch 718/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.2191e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8822\n",
      "Epoch 719/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.1883e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8822\n",
      "Epoch 720/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.1734e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 721/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 7.1550e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 722/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.1274e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 723/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.1054e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 724/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.0778e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 725/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.0243e-04 - pos_accuracy: 0.9994 - val_loss: 0.1813 - val_pos_accuracy: 0.8798\n",
      "Epoch 726/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.0341e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 727/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 7.0089e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 728/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.9863e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 729/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.9833e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 730/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.9499e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8822\n",
      "Epoch 731/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.9148e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8822\n",
      "Epoch 732/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.8762e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8822\n",
      "Epoch 733/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.8645e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8822\n",
      "Epoch 734/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.8650e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 735/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.8212e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 736/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.8236e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 737/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.7766e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 738/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.7523e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8822\n",
      "Epoch 739/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.7393e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 740/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.7373e-04 - pos_accuracy: 0.9994 - val_loss: 0.1812 - val_pos_accuracy: 0.8822\n",
      "Epoch 741/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.6990e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 742/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.6493e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8798\n",
      "Epoch 743/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.6653e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 744/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.6187e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8822\n",
      "Epoch 745/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.6220e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 746/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.5973e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8798\n",
      "Epoch 747/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.5421e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 748/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.5555e-04 - pos_accuracy: 0.9994 - val_loss: 0.1811 - val_pos_accuracy: 0.8822\n",
      "Epoch 749/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.5142e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 750/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.5043e-04 - pos_accuracy: 0.9994 - val_loss: 0.1807 - val_pos_accuracy: 0.8798\n",
      "Epoch 751/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.5099e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8822\n",
      "Epoch 752/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.4606e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 753/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.4301e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 754/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.4184e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 755/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.3964e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 756/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.3699e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 757/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.3299e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 758/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.3237e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 759/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.3093e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 760/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.3042e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 761/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.3003e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8798\n",
      "Epoch 762/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.2683e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 763/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.2412e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 764/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.2190e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8798\n",
      "Epoch 765/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.1824e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 766/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.1797e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8798\n",
      "Epoch 767/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.1836e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 768/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.1467e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8822\n",
      "Epoch 769/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.1171e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8798\n",
      "Epoch 770/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.1580e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8798\n",
      "Epoch 771/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.1221e-04 - pos_accuracy: 0.9994 - val_loss: 0.1807 - val_pos_accuracy: 0.8798\n",
      "Epoch 772/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.0769e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 773/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.1023e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8822\n",
      "Epoch 774/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.0357e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8822\n",
      "Epoch 775/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.0653e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 776/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.0445e-04 - pos_accuracy: 0.9994 - val_loss: 0.1807 - val_pos_accuracy: 0.8822\n",
      "Epoch 777/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.0408e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 778/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.0102e-04 - pos_accuracy: 0.9994 - val_loss: 0.1807 - val_pos_accuracy: 0.8798\n",
      "Epoch 779/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 5.9848e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8798\n",
      "Epoch 780/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.9654e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8798\n",
      "Epoch 781/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.9520e-04 - pos_accuracy: 0.9994 - val_loss: 0.1808 - val_pos_accuracy: 0.8798\n",
      "Epoch 782/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.9098e-04 - pos_accuracy: 0.9994 - val_loss: 0.1809 - val_pos_accuracy: 0.8822\n",
      "Epoch 783/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.8861e-04 - pos_accuracy: 0.9994 - val_loss: 0.1806 - val_pos_accuracy: 0.8822\n",
      "Epoch 784/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.8782e-04 - pos_accuracy: 0.9994 - val_loss: 0.1810 - val_pos_accuracy: 0.8798\n",
      "Epoch 785/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.8994e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8822\n",
      "Epoch 786/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.8640e-04 - pos_accuracy: 0.9994 - val_loss: 0.1807 - val_pos_accuracy: 0.8798\n",
      "Epoch 787/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.8321e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 788/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.8028e-04 - pos_accuracy: 0.9994 - val_loss: 0.1806 - val_pos_accuracy: 0.8822\n",
      "Epoch 789/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.7732e-04 - pos_accuracy: 0.9994 - val_loss: 0.1806 - val_pos_accuracy: 0.8798\n",
      "Epoch 790/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.7816e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 791/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.7824e-04 - pos_accuracy: 0.9994 - val_loss: 0.1806 - val_pos_accuracy: 0.8798\n",
      "Epoch 792/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.7728e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8822\n",
      "Epoch 793/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.7239e-04 - pos_accuracy: 0.9994 - val_loss: 0.1806 - val_pos_accuracy: 0.8798\n",
      "Epoch 794/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.7175e-04 - pos_accuracy: 0.9994 - val_loss: 0.1807 - val_pos_accuracy: 0.8798\n",
      "Epoch 795/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.7195e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 796/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.6838e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 797/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.6444e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 798/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.6209e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 799/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.6305e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 800/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.5840e-04 - pos_accuracy: 0.9994 - val_loss: 0.1806 - val_pos_accuracy: 0.8798\n",
      "Epoch 801/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.5773e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 802/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.5483e-04 - pos_accuracy: 0.9994 - val_loss: 0.1806 - val_pos_accuracy: 0.8798\n",
      "Epoch 803/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.5452e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 804/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4890e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 805/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4820e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 806/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4692e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 807/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4512e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 808/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4690e-04 - pos_accuracy: 0.9994 - val_loss: 0.1806 - val_pos_accuracy: 0.8798\n",
      "Epoch 809/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4381e-04 - pos_accuracy: 0.9994 - val_loss: 0.1806 - val_pos_accuracy: 0.8798\n",
      "Epoch 810/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4204e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 811/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.4065e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 812/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.3958e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 813/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.3818e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 814/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.3539e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 815/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.3324e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 816/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.3191e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 817/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.3196e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 818/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.2921e-04 - pos_accuracy: 0.9994 - val_loss: 0.1805 - val_pos_accuracy: 0.8798\n",
      "Epoch 819/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.2816e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 820/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.2701e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 821/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.2578e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 822/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.2476e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 823/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.2138e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 824/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.2088e-04 - pos_accuracy: 0.9994 - val_loss: 0.1802 - val_pos_accuracy: 0.8798\n",
      "Epoch 825/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.2194e-04 - pos_accuracy: 0.9994 - val_loss: 0.1804 - val_pos_accuracy: 0.8798\n",
      "Epoch 826/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.1894e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 827/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 5.1819e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 828/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.1606e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 829/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.1540e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 830/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.1165e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 831/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 5.1185e-04 - pos_accuracy: 0.9994 - val_loss: 0.1802 - val_pos_accuracy: 0.8798\n",
      "Epoch 832/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.1000e-04 - pos_accuracy: 0.9994 - val_loss: 0.1801 - val_pos_accuracy: 0.8798\n",
      "Epoch 833/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.1059e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 834/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.0716e-04 - pos_accuracy: 0.9994 - val_loss: 0.1803 - val_pos_accuracy: 0.8798\n",
      "Epoch 835/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.0677e-04 - pos_accuracy: 0.9994 - val_loss: 0.1802 - val_pos_accuracy: 0.8798\n",
      "Epoch 836/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.0595e-04 - pos_accuracy: 0.9994 - val_loss: 0.1802 - val_pos_accuracy: 0.8822\n",
      "Epoch 837/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.0453e-04 - pos_accuracy: 0.9994 - val_loss: 0.1801 - val_pos_accuracy: 0.8798\n",
      "Epoch 838/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.0368e-04 - pos_accuracy: 0.9994 - val_loss: 0.1801 - val_pos_accuracy: 0.8822\n",
      "Epoch 839/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5.0216e-04 - pos_accuracy: 0.9994 - val_loss: 0.1802 - val_pos_accuracy: 0.8798\n",
      "Epoch 840/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.9922e-04 - pos_accuracy: 0.9994 - val_loss: 0.1802 - val_pos_accuracy: 0.8798\n",
      "Epoch 841/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.9957e-04 - pos_accuracy: 0.9994 - val_loss: 0.1801 - val_pos_accuracy: 0.8798\n",
      "Epoch 842/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.9829e-04 - pos_accuracy: 0.9994 - val_loss: 0.1801 - val_pos_accuracy: 0.8798\n",
      "Epoch 843/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.9532e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 844/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.9505e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 845/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.9114e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 846/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.9123e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 847/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.8792e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 848/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.8739e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 849/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.8589e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 850/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.8719e-04 - pos_accuracy: 0.9994 - val_loss: 0.1801 - val_pos_accuracy: 0.8798\n",
      "Epoch 851/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.8391e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 852/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.8238e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 853/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.8046e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 854/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 4.7911e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 855/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.7744e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 856/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.7808e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 857/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.7626e-04 - pos_accuracy: 0.9994 - val_loss: 0.1801 - val_pos_accuracy: 0.8798\n",
      "Epoch 858/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.7255e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 859/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.7411e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 860/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.7357e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 861/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.7206e-04 - pos_accuracy: 0.9994 - val_loss: 0.1801 - val_pos_accuracy: 0.8798\n",
      "Epoch 862/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.6755e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 863/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.6740e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 864/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.6767e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 865/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.6572e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 866/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.6387e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 867/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.6225e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 868/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.6143e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 869/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.5860e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 870/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.5876e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 871/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.5641e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 872/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.5666e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 873/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.5592e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 874/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.5333e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 875/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.5286e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 876/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.4859e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 877/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.5002e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 878/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.4749e-04 - pos_accuracy: 0.9994 - val_loss: 0.1796 - val_pos_accuracy: 0.8798\n",
      "Epoch 879/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.4769e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 880/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.4666e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 881/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 4.4556e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 882/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.4296e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 883/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 4.4250e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 884/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.4027e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 885/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3958e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 886/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.4038e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 887/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3838e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 888/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3666e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 889/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3530e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 890/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3497e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 891/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3325e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 892/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3278e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8822\n",
      "Epoch 893/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3089e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 894/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2912e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 895/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3088e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8822\n",
      "Epoch 896/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2617e-04 - pos_accuracy: 0.9994 - val_loss: 0.1796 - val_pos_accuracy: 0.8798\n",
      "Epoch 897/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2680e-04 - pos_accuracy: 0.9994 - val_loss: 0.1796 - val_pos_accuracy: 0.8798\n",
      "Epoch 898/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2732e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 899/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2373e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8822\n",
      "Epoch 900/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2268e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 901/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2053e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8822\n",
      "Epoch 902/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1992e-04 - pos_accuracy: 0.9994 - val_loss: 0.1796 - val_pos_accuracy: 0.8798\n",
      "Epoch 903/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1986e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 904/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2024e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 905/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1927e-04 - pos_accuracy: 0.9994 - val_loss: 0.1796 - val_pos_accuracy: 0.8798\n",
      "Epoch 906/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1878e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 907/1000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 4.1698e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 908/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1686e-04 - pos_accuracy: 0.9994 - val_loss: 0.1800 - val_pos_accuracy: 0.8798\n",
      "Epoch 909/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1525e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 910/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1393e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 911/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1375e-04 - pos_accuracy: 0.9994 - val_loss: 0.1798 - val_pos_accuracy: 0.8798\n",
      "Epoch 912/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1330e-04 - pos_accuracy: 0.9994 - val_loss: 0.1799 - val_pos_accuracy: 0.8798\n",
      "Epoch 913/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.0906e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 914/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1076e-04 - pos_accuracy: 0.9994 - val_loss: 0.1796 - val_pos_accuracy: 0.8798\n",
      "Epoch 915/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.0568e-04 - pos_accuracy: 0.9994 - val_loss: 0.1796 - val_pos_accuracy: 0.8798\n",
      "Epoch 916/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.0498e-04 - pos_accuracy: 0.9994 - val_loss: 0.1796 - val_pos_accuracy: 0.8798\n",
      "Epoch 917/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.0437e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 918/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.0129e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 919/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.0041e-04 - pos_accuracy: 0.9994 - val_loss: 0.1796 - val_pos_accuracy: 0.8798\n",
      "Epoch 920/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9926e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 921/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9864e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 922/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9746e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 923/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9630e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 924/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9545e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 925/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9518e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 926/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9400e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 927/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9369e-04 - pos_accuracy: 0.9994 - val_loss: 0.1797 - val_pos_accuracy: 0.8798\n",
      "Epoch 928/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9253e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 929/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9150e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 930/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9162e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 931/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9099e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 932/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8905e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 933/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8899e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 934/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8627e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 935/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8626e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 936/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8515e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 937/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8383e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 938/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8146e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 939/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8078e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 940/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7995e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 941/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7988e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 942/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7731e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 943/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7707e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 944/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7644e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 945/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7573e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 946/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7382e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 947/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7190e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 948/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7312e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 949/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7110e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 950/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7191e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 951/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7117e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 952/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.6998e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 953/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.6777e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 954/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.6815e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 955/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.6545e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 956/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.6276e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 957/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.6180e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 958/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.6136e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 959/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.6079e-04 - pos_accuracy: 0.9994 - val_loss: 0.1795 - val_pos_accuracy: 0.8798\n",
      "Epoch 960/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5852e-04 - pos_accuracy: 0.9994 - val_loss: 0.1794 - val_pos_accuracy: 0.8798\n",
      "Epoch 961/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5791e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 962/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5712e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 963/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5587e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 964/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5460e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 965/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5342e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 966/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5264e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 967/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5376e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 968/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5226e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 969/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5137e-04 - pos_accuracy: 0.9994 - val_loss: 0.1793 - val_pos_accuracy: 0.8798\n",
      "Epoch 970/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5185e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 971/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4977e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 972/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4916e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 973/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4829e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 974/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4700e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 975/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4607e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 976/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4533e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 977/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4546e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 978/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4523e-04 - pos_accuracy: 0.9994 - val_loss: 0.1790 - val_pos_accuracy: 0.8798\n",
      "Epoch 979/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4288e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 980/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4234e-04 - pos_accuracy: 0.9994 - val_loss: 0.1790 - val_pos_accuracy: 0.8798\n",
      "Epoch 981/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4356e-04 - pos_accuracy: 0.9994 - val_loss: 0.1789 - val_pos_accuracy: 0.8798\n",
      "Epoch 982/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4067e-04 - pos_accuracy: 0.9994 - val_loss: 0.1789 - val_pos_accuracy: 0.8798\n",
      "Epoch 983/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3998e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 984/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3999e-04 - pos_accuracy: 0.9994 - val_loss: 0.1790 - val_pos_accuracy: 0.8798\n",
      "Epoch 985/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3997e-04 - pos_accuracy: 0.9994 - val_loss: 0.1790 - val_pos_accuracy: 0.8798\n",
      "Epoch 986/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3774e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 987/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/step - loss: 3.3635e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 988/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3830e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 989/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3674e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 990/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3473e-04 - pos_accuracy: 0.9994 - val_loss: 0.1792 - val_pos_accuracy: 0.8798\n",
      "Epoch 991/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3548e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 992/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3420e-04 - pos_accuracy: 0.9994 - val_loss: 0.1790 - val_pos_accuracy: 0.8798\n",
      "Epoch 993/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3266e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 994/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3126e-04 - pos_accuracy: 0.9994 - val_loss: 0.1791 - val_pos_accuracy: 0.8798\n",
      "Epoch 995/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.3017e-04 - pos_accuracy: 0.9994 - val_loss: 0.1790 - val_pos_accuracy: 0.8798\n",
      "Epoch 996/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.2892e-04 - pos_accuracy: 0.9994 - val_loss: 0.1790 - val_pos_accuracy: 0.8798\n",
      "Epoch 997/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.2890e-04 - pos_accuracy: 0.9994 - val_loss: 0.1790 - val_pos_accuracy: 0.8798\n",
      "Epoch 998/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.2717e-04 - pos_accuracy: 0.9994 - val_loss: 0.1789 - val_pos_accuracy: 0.8798\n",
      "Epoch 999/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.2650e-04 - pos_accuracy: 0.9994 - val_loss: 0.1789 - val_pos_accuracy: 0.8798\n",
      "Epoch 1000/1000\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.2649e-04 - pos_accuracy: 0.9994 - val_loss: 0.1790 - val_pos_accuracy: 0.8798\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:11:00.673697: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1693: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n(답변)\\ny = Ax + b 의 형태를 y = Mx와 같은 형태로 변경하기 위해 \\nA와 b를 열방향(axis=1)으로 합쳐주는 역할을 하고 있습니다.\\n\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\n(답변)\\n121, 122에서 \\n첫번째자리는 subplot의 행의 수(nrows)를 의미하고  \\n두번째 자리는 subplot의 열의 수(ncols)를 나타내고\\n세번째 자리는 인덱스(index)를 말하는 것으로\\n\\n121, 122의 앞의 두자리 12는 1행x2열 형태의 subplot에서 \\n121은 첫번째 자리에 122는 두번째 자리를 의미합니다.\\n\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n(답변)\\n텐서의 차원이 제거되고 요소들에 대한 결과가 T/F 논리값이 계산되어 나옴\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n(답변)\\n텐서의 차원이 제거되고 텐서 요소들의 평균값이 결과로 나옴\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n(답변)\\n최종 예측해야하는 값의 형태가 분류가 아니기 때문에\\n선형회귀와 같이 예측된 결과 자체가 필요하기 때문에 활성함수를 사용하지 않음\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n(답변)\\n다시 실행해도 동일한 난수가 생성되게 하도록 해서\\n동일 학습 데이터로 여러가지 방식을 적용해서 비교가 가능하도록 하기 위함임\\n\",\n",
      "        true,\n",
      "        \"\\n어려운 내용이지만, 수업 너무 재미있게 듣고 있습니다. 감사합니다!\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.8798\n",
      "}\"report1/CUIGUANGZHI_46005.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/CUIGUANGZHI_46005.ipynb to python\n",
      "[NbConvertApp] Writing 36957 bytes to report1/CUIGUANGZHI_46005.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:214: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "51\n",
      "[inf inf]\n",
      "421\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "[[19 21  8]\n",
      " [43 51 24]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 305262.30it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:11:08.954453: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:11:09.334742: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:11:09.334783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:11:09.559761: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:11:10.160820: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 105680.59it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 9.4030 - val_loss: 1.2405\n",
      "Epoch 2/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.9056 - val_loss: 0.8771\n",
      "Epoch 3/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6558 - val_loss: 0.6789\n",
      "Epoch 4/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5247 - val_loss: 0.5924\n",
      "Epoch 5/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4428 - val_loss: 0.5630\n",
      "Epoch 6/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4018 - val_loss: 0.6351\n",
      "Epoch 7/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3702 - val_loss: 0.5597\n",
      "Epoch 8/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3396 - val_loss: 0.5253\n",
      "Epoch 9/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3144 - val_loss: 0.5045\n",
      "Epoch 10/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2953 - val_loss: 0.5041\n",
      "Epoch 11/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2734 - val_loss: 0.5274\n",
      "Epoch 12/12\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2724 - val_loss: 0.4843\n",
      "['Model: \"model_1\"', '_________________________________________________________________', 'Layer (type)                 Output Shape              Param #   ', '=================================================================', 'input_2 (InputLayer)         [(None, 28, 28)]          0         ', '_________________________________________________________________', 'flatten_1 (Flatten)          (None, 784)               0         ', '_________________________________________________________________', 'dense_1 (Dense)              (None, 2)                 1570      ', '=================================================================', 'Total params: 1,570', 'Trainable params: 1,570', 'Non-trainable params: 0', '_________________________________________________________________']\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:11:13.981974: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "ans17 error\n",
      "답안을 확인하여 주세요\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1744: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\n정답 : np.concatenate의 역할은 numpy 배열을 하나로 합치는데 사용합니다.\\naxis=1의 의미는 여기서는 2차원 배열이라서 열 축 기준으로 두 numpy 배열을 하나로 합치는데 있습니다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\n정답 : 첫번째 숫자 1은 nrow파라미터에 해당되어 1개 행으로 그림을 표시하게 되고\\n        두번째 숫자 2는 nrow파라미터에 해당되어 2개 열로 그림을 표시하게 되며\\n        세번째 숫자 1,과 2는 index에 해당 되어 각각 첫번째 index 위치, 두번째 index 위치 차례대로 표시하게 된다.\\n        그러므로 3개 index를 합쳐서 한행으로 표시하되 한행에 두개 이미지를 표시하며 121 첫번째 이미지는 첫번째 위치,\\n        122 두번째 이미지는 2번째 위치로 표시한다.  \\n        \\n\",\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\n정답 : tf.reduce_all은 파라미터에 따라서 AND 연산을 진행한다.\\n    axis = 1은 행에 대해서 AND연산을 진행하는 의미이다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n\\n정답: tf.reduce_mean은 tensorflow의 평균 값을 구한다.\\n예를 들어 y_true값과 y_pred 값의 and 계산으로 1과0으로 조합 된 결과 즉 예측이 맞을때 1, 예측이 오류일때 0이라면\\n예측이 세개 맞고 세개가 오류일때 [1,1,1,0,0,0]이므로 이를 평균 값 계산하면 1+1+1+0+0+0 / 6 하여 0.5 값이 얻어지며\\n이 값인 즉 딥러닝 모듈로 학습하여 얻어진 예측 정확도이다. 해당 학습 모듈로 학습하고 6개을 예측 했을때 평균 적으로 3개 정확한 결과를 얻고 3개 부정확한 결과를 얻는 다는 뜻이다.  \\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n\\n정답 : 이번 학습 모델은 분류기와 달리 두개 숫자와 두개 숫자와의 관계를 분석하고 학습하는 모델이라서 \\n    활성화 함수를 사용하지 않습니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n\\n정답 : 동일한 세트의 난수를 생성하기 위해서이다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n강의내용은 소프트웨어 공학과 학생으로써 인공지능 트랙 기초지식 없이 처음 접하는 딥러닝이라서 지금까지 배운 과목중 최고 난이도 과목이라고 보고 있고\\n수학 미분 기초 지식이 많이 기억나지 않은 상태에서 배우기가 조금 어려운점 있었습니다. 강의하셔야 할 내용은 많으셔서 할수 없이 강의속도가 빠르게 진행되고 있지만 \\n코드에 대한 설명이나 중요한 지식점에 대하여 처음 접하는 분들께도 쉽게 다가갈수 있는 형식으로 진행되었으면 하는 바랩입니다. 딥러닝을 잘 배우고 싶습니다.\\n\\n과제는 난이도가 어렵지만 아주 재미있고 한가지 한가지 내용을 검색하면서 답을 얻을때 성취감이 있었으며 딥러닝이란 퍼즐을 하나하나 맞추어 가는 느낌이였습니다.\\n개인적인 사정으로 수업이 많이 밀리고 과제도 한꺼번에 많이 나온 바람에 딥러닝 과제에 다소 많은 시간을 투자하였지만 만족할만한 성과를 이루지 못했다고 생각합니다.\\n다음엔 꼭 바로바로 강의 수강하여 미루는 일이 없도록 해야 겠습니다. 전체적으로 과제 내용 재밌었고 유익했습니다. 교수님, 감사합니다. 더 노력하는 자신으로 다시 뵙겠습니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 87.87878787878788,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/우형욱_46053.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/우형욱_46053.ipynb to python\n",
      "[NbConvertApp] Writing 36305 bytes to report1/우형욱_46053.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:207: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:238: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 301672.53it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:11:20.536520: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:11:20.921545: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:11:20.921586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:11:21.146664: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/10\n",
      "2022-10-14 11:11:21.805887: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - pos_accuracy: 0.0000e+00 - val_loss: 212.3778 - val_pos_accuracy: 0.0045\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - pos_accuracy: 6.2500e-04 - val_loss: 195.9663 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 205.1136 - pos_accuracy: 0.0019 - val_loss: 180.9897 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 189.4806 - pos_accuracy: 0.0019 - val_loss: 167.3181 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 175.1996 - pos_accuracy: 6.2500e-04 - val_loss: 154.8309 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 162.1355 - pos_accuracy: 6.2500e-04 - val_loss: 143.4211 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 150.1897 - pos_accuracy: 6.2500e-04 - val_loss: 132.9914 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 139.2480 - pos_accuracy: 6.2500e-04 - val_loss: 123.4498 - val_pos_accuracy: 0.0022\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 129.2301 - pos_accuracy: 0.0012 - val_loss: 114.7210 - val_pos_accuracy: 0.0022\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 120.0508 - pos_accuracy: 6.2500e-04 - val_loss: 106.7253 - val_pos_accuracy: 0.0022\n",
      "(2, 2)\n",
      "(2,)\n",
      "[ True False]\n",
      "[1. 0.]\n",
      "0.5\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 94276.27it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 54,530\n",
      "Trainable params: 54,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 36.8712 - pos_accuracy: 0.0481 - val_loss: 2.3357 - val_pos_accuracy: 0.1707\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.6207 - pos_accuracy: 0.1925 - val_loss: 1.2060 - val_pos_accuracy: 0.3125\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.3395 - pos_accuracy: 0.2212 - val_loss: 2.2215 - val_pos_accuracy: 0.0962\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.8283 - pos_accuracy: 0.3131 - val_loss: 0.9550 - val_pos_accuracy: 0.2933\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.7088 - pos_accuracy: 0.3494 - val_loss: 0.8314 - val_pos_accuracy: 0.3245\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.6187 - pos_accuracy: 0.4144 - val_loss: 0.6360 - val_pos_accuracy: 0.4447\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4797 - pos_accuracy: 0.4706 - val_loss: 0.5475 - val_pos_accuracy: 0.4856\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3761 - pos_accuracy: 0.5119 - val_loss: 0.4656 - val_pos_accuracy: 0.5216\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3043 - pos_accuracy: 0.5925 - val_loss: 0.4097 - val_pos_accuracy: 0.5986\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2693 - pos_accuracy: 0.6131 - val_loss: 0.4587 - val_pos_accuracy: 0.5096\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2175 - pos_accuracy: 0.6762 - val_loss: 0.3782 - val_pos_accuracy: 0.6034\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1897 - pos_accuracy: 0.7025 - val_loss: 0.3133 - val_pos_accuracy: 0.6827\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1866 - pos_accuracy: 0.7069 - val_loss: 0.3136 - val_pos_accuracy: 0.7019\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1561 - pos_accuracy: 0.7656 - val_loss: 0.2796 - val_pos_accuracy: 0.7308\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1149 - pos_accuracy: 0.8369 - val_loss: 0.3680 - val_pos_accuracy: 0.6058\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1109 - pos_accuracy: 0.8381 - val_loss: 0.4278 - val_pos_accuracy: 0.5216\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1243 - pos_accuracy: 0.8069 - val_loss: 0.2961 - val_pos_accuracy: 0.6538\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0887 - pos_accuracy: 0.8788 - val_loss: 0.2476 - val_pos_accuracy: 0.7740\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0805 - pos_accuracy: 0.8919 - val_loss: 0.2564 - val_pos_accuracy: 0.7260\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0803 - pos_accuracy: 0.8863 - val_loss: 0.2436 - val_pos_accuracy: 0.7620\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0681 - pos_accuracy: 0.9112 - val_loss: 0.2179 - val_pos_accuracy: 0.8125\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0693 - pos_accuracy: 0.9019 - val_loss: 0.2078 - val_pos_accuracy: 0.8005\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0615 - pos_accuracy: 0.9287 - val_loss: 0.2249 - val_pos_accuracy: 0.7909\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0530 - pos_accuracy: 0.9381 - val_loss: 0.1993 - val_pos_accuracy: 0.8341\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0468 - pos_accuracy: 0.9538 - val_loss: 0.2341 - val_pos_accuracy: 0.7308\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0472 - pos_accuracy: 0.9575 - val_loss: 0.1987 - val_pos_accuracy: 0.8413\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0384 - pos_accuracy: 0.9613 - val_loss: 0.1845 - val_pos_accuracy: 0.8630\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0354 - pos_accuracy: 0.9644 - val_loss: 0.1941 - val_pos_accuracy: 0.8438\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0380 - pos_accuracy: 0.9581 - val_loss: 0.1844 - val_pos_accuracy: 0.8438\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0293 - pos_accuracy: 0.9719 - val_loss: 0.2064 - val_pos_accuracy: 0.8101\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0322 - pos_accuracy: 0.9663 - val_loss: 0.1736 - val_pos_accuracy: 0.8678\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0313 - pos_accuracy: 0.9681 - val_loss: 0.1906 - val_pos_accuracy: 0.8317\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0289 - pos_accuracy: 0.9725 - val_loss: 0.1697 - val_pos_accuracy: 0.8702\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0251 - pos_accuracy: 0.9744 - val_loss: 0.1662 - val_pos_accuracy: 0.8726\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0239 - pos_accuracy: 0.9781 - val_loss: 0.1619 - val_pos_accuracy: 0.8846\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0196 - pos_accuracy: 0.9812 - val_loss: 0.1617 - val_pos_accuracy: 0.8726\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0193 - pos_accuracy: 0.9800 - val_loss: 0.1621 - val_pos_accuracy: 0.8774\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0217 - pos_accuracy: 0.9819 - val_loss: 0.1603 - val_pos_accuracy: 0.8774\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0177 - pos_accuracy: 0.9862 - val_loss: 0.1660 - val_pos_accuracy: 0.8726\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0147 - pos_accuracy: 0.9869 - val_loss: 0.1604 - val_pos_accuracy: 0.8870\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0154 - pos_accuracy: 0.9850 - val_loss: 0.1615 - val_pos_accuracy: 0.8822\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0157 - pos_accuracy: 0.9856 - val_loss: 0.1582 - val_pos_accuracy: 0.8822\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0119 - pos_accuracy: 0.9906 - val_loss: 0.1534 - val_pos_accuracy: 0.8918\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0114 - pos_accuracy: 0.9900 - val_loss: 0.1539 - val_pos_accuracy: 0.8966\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0130 - pos_accuracy: 0.9912 - val_loss: 0.1525 - val_pos_accuracy: 0.8942\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0122 - pos_accuracy: 0.9912 - val_loss: 0.1504 - val_pos_accuracy: 0.8894\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0102 - pos_accuracy: 0.9925 - val_loss: 0.1579 - val_pos_accuracy: 0.8846\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0121 - pos_accuracy: 0.9937 - val_loss: 0.1519 - val_pos_accuracy: 0.8966\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0124 - pos_accuracy: 0.9931 - val_loss: 0.1485 - val_pos_accuracy: 0.8966\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9944 - val_loss: 0.1481 - val_pos_accuracy: 0.8966\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0085 - pos_accuracy: 0.9956 - val_loss: 0.1472 - val_pos_accuracy: 0.8966\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0090 - pos_accuracy: 0.9956 - val_loss: 0.1464 - val_pos_accuracy: 0.8990\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9956 - val_loss: 0.1480 - val_pos_accuracy: 0.9038\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0077 - pos_accuracy: 0.9962 - val_loss: 0.1490 - val_pos_accuracy: 0.8990\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9962 - val_loss: 0.1449 - val_pos_accuracy: 0.9038\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0072 - pos_accuracy: 0.9956 - val_loss: 0.1439 - val_pos_accuracy: 0.9014\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9969 - val_loss: 0.1440 - val_pos_accuracy: 0.9014\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0061 - pos_accuracy: 0.9969 - val_loss: 0.1438 - val_pos_accuracy: 0.8990\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0063 - pos_accuracy: 0.9969 - val_loss: 0.1428 - val_pos_accuracy: 0.9038\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9969 - val_loss: 0.1446 - val_pos_accuracy: 0.9038\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0076 - pos_accuracy: 0.9969 - val_loss: 0.1421 - val_pos_accuracy: 0.9014\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9969 - val_loss: 0.1409 - val_pos_accuracy: 0.9014\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9969 - val_loss: 0.1416 - val_pos_accuracy: 0.9038\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9969 - val_loss: 0.1432 - val_pos_accuracy: 0.9014\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9969 - val_loss: 0.1452 - val_pos_accuracy: 0.8990\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9969 - val_loss: 0.1410 - val_pos_accuracy: 0.8966\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9969 - val_loss: 0.1397 - val_pos_accuracy: 0.9038\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9969 - val_loss: 0.1393 - val_pos_accuracy: 0.8990\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9969 - val_loss: 0.1393 - val_pos_accuracy: 0.9038\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9969 - val_loss: 0.1392 - val_pos_accuracy: 0.9014\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9969 - val_loss: 0.1395 - val_pos_accuracy: 0.9014\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9969 - val_loss: 0.1378 - val_pos_accuracy: 0.8990\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9969 - val_loss: 0.1367 - val_pos_accuracy: 0.9014\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9975 - val_loss: 0.1374 - val_pos_accuracy: 0.8990\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9975 - val_loss: 0.1382 - val_pos_accuracy: 0.9038\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9975 - val_loss: 0.1389 - val_pos_accuracy: 0.9038\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9981 - val_loss: 0.1396 - val_pos_accuracy: 0.8966\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9987 - val_loss: 0.1370 - val_pos_accuracy: 0.9014\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9981 - val_loss: 0.1365 - val_pos_accuracy: 0.9014\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.1370 - val_pos_accuracy: 0.9014\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9987 - val_loss: 0.1361 - val_pos_accuracy: 0.9014\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1364 - val_pos_accuracy: 0.9038\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.1359 - val_pos_accuracy: 0.9014\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1368 - val_pos_accuracy: 0.9014\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9987 - val_loss: 0.1357 - val_pos_accuracy: 0.9014\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1356 - val_pos_accuracy: 0.9014\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1367 - val_pos_accuracy: 0.9038\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1380 - val_pos_accuracy: 0.9038\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1359 - val_pos_accuracy: 0.8966\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1365 - val_pos_accuracy: 0.9014\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1349 - val_pos_accuracy: 0.9014\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1347 - val_pos_accuracy: 0.9014\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1343 - val_pos_accuracy: 0.9014\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1348 - val_pos_accuracy: 0.9014\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1338 - val_pos_accuracy: 0.9014\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0016 - pos_accuracy: 0.9994 - val_loss: 0.1337 - val_pos_accuracy: 0.8990\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1348 - val_pos_accuracy: 0.9038\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1341 - val_pos_accuracy: 0.9014\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1334 - val_pos_accuracy: 0.9014\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1338 - val_pos_accuracy: 0.9014\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:11:44.507490: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1682: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할 : 선택한 축의 방향으로 배열을 연결해 주는 함수 \\naxis=1의 의미 : y축 방향, 즉 column방향으로 배열한다는 것을 의미함.\\n\",\n",
      "        \"\\nsubplot 121: 1x2그리드에서 첫번째 subplot\\nsubplot 122: 1x2그리드에서 두번째 subplot\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할 : And 연산자로서, 조건의 일치 여부에 따라, Dimension을 축소하는 역할\\naxis=1 : 열기준으로 Dimension을 축소하는 역할\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 역할 : accuracy의 평균값을 구하여 Dimension을 축소\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 : 값을 출력하는 함수로 항등함수\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 : '무작위'의 결과를 특정한 값으로 고정하는 것이 가능. 따라서 여러번 Run하여도 결과값이 변하지 않음.\\n\",\n",
      "        true,\n",
      "        \"\\n어렵지만 열심히 따라 하겠습니다. 감사합니다. \\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.9062\n",
      "}\"report1/구태홍_45996.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/구태홍_45996.ipynb to python\n",
      "[NbConvertApp] Writing 35287 bytes to report1/구태홍_45996.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:241: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)\n",
      "[ 0. nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 305963.75it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:11:51.008790: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:11:51.390210: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:11:51.390251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:11:51.649564: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:11:52.279099: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 191853.63it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 234,114\n",
      "Trainable params: 234,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 5ms/step - loss: 32.2965 - pos_accuracy: 0.0549 - val_loss: 3.4160 - val_pos_accuracy: 0.0857\n",
      "Epoch 2/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 1.5389 - pos_accuracy: 0.2185 - val_loss: 1.3438 - val_pos_accuracy: 0.2524\n",
      "Epoch 3/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.8450 - pos_accuracy: 0.3130 - val_loss: 0.8214 - val_pos_accuracy: 0.3619\n",
      "Epoch 4/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.6943 - pos_accuracy: 0.3556 - val_loss: 0.7248 - val_pos_accuracy: 0.3548\n",
      "Epoch 5/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.4867 - pos_accuracy: 0.4469 - val_loss: 0.5995 - val_pos_accuracy: 0.4333\n",
      "Epoch 6/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.3712 - pos_accuracy: 0.5549 - val_loss: 0.6009 - val_pos_accuracy: 0.4405\n",
      "Epoch 7/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2600 - pos_accuracy: 0.6599 - val_loss: 0.4997 - val_pos_accuracy: 0.4571\n",
      "Epoch 8/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2893 - pos_accuracy: 0.5988 - val_loss: 0.7858 - val_pos_accuracy: 0.2333\n",
      "Epoch 9/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2159 - pos_accuracy: 0.6759 - val_loss: 0.3842 - val_pos_accuracy: 0.5357\n",
      "Epoch 10/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1498 - pos_accuracy: 0.7920 - val_loss: 0.2807 - val_pos_accuracy: 0.7024\n",
      "Epoch 11/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1283 - pos_accuracy: 0.8191 - val_loss: 0.2662 - val_pos_accuracy: 0.7167\n",
      "Epoch 12/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.1153 - pos_accuracy: 0.8364 - val_loss: 0.2637 - val_pos_accuracy: 0.6690\n",
      "Epoch 13/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0940 - pos_accuracy: 0.8728 - val_loss: 0.2231 - val_pos_accuracy: 0.7690\n",
      "Epoch 14/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0955 - pos_accuracy: 0.8673 - val_loss: 0.3038 - val_pos_accuracy: 0.6190\n",
      "Epoch 15/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0808 - pos_accuracy: 0.8753 - val_loss: 0.7531 - val_pos_accuracy: 0.2548\n",
      "Epoch 16/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0838 - pos_accuracy: 0.8914 - val_loss: 0.1966 - val_pos_accuracy: 0.8310\n",
      "Epoch 17/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0591 - pos_accuracy: 0.9315 - val_loss: 0.2559 - val_pos_accuracy: 0.6714\n",
      "Epoch 18/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0531 - pos_accuracy: 0.9302 - val_loss: 0.1713 - val_pos_accuracy: 0.8476\n",
      "Epoch 19/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0380 - pos_accuracy: 0.9549 - val_loss: 0.2594 - val_pos_accuracy: 0.6429\n",
      "Epoch 20/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0366 - pos_accuracy: 0.9531 - val_loss: 0.1695 - val_pos_accuracy: 0.8452\n",
      "Epoch 21/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0336 - pos_accuracy: 0.9636 - val_loss: 0.1679 - val_pos_accuracy: 0.8548\n",
      "Epoch 22/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0287 - pos_accuracy: 0.9691 - val_loss: 0.1603 - val_pos_accuracy: 0.8476\n",
      "Epoch 23/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0243 - pos_accuracy: 0.9722 - val_loss: 0.1488 - val_pos_accuracy: 0.8714\n",
      "Epoch 24/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0234 - pos_accuracy: 0.9741 - val_loss: 0.1635 - val_pos_accuracy: 0.8595\n",
      "Epoch 25/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0210 - pos_accuracy: 0.9784 - val_loss: 0.1479 - val_pos_accuracy: 0.8643\n",
      "Epoch 26/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0180 - pos_accuracy: 0.9827 - val_loss: 0.1525 - val_pos_accuracy: 0.8738\n",
      "Epoch 27/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0189 - pos_accuracy: 0.9821 - val_loss: 0.1446 - val_pos_accuracy: 0.8762\n",
      "Epoch 28/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0143 - pos_accuracy: 0.9846 - val_loss: 0.1423 - val_pos_accuracy: 0.8810\n",
      "Epoch 29/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0136 - pos_accuracy: 0.9870 - val_loss: 0.1684 - val_pos_accuracy: 0.8548\n",
      "Epoch 30/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0145 - pos_accuracy: 0.9889 - val_loss: 0.1582 - val_pos_accuracy: 0.8524\n",
      "Epoch 31/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0150 - pos_accuracy: 0.9895 - val_loss: 0.1379 - val_pos_accuracy: 0.8810\n",
      "Epoch 32/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0104 - pos_accuracy: 0.9926 - val_loss: 0.1367 - val_pos_accuracy: 0.8881\n",
      "Epoch 33/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0106 - pos_accuracy: 0.9926 - val_loss: 0.1340 - val_pos_accuracy: 0.8857\n",
      "Epoch 34/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9926 - val_loss: 0.1324 - val_pos_accuracy: 0.8881\n",
      "Epoch 35/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0082 - pos_accuracy: 0.9926 - val_loss: 0.1321 - val_pos_accuracy: 0.8857\n",
      "Epoch 36/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0074 - pos_accuracy: 0.9932 - val_loss: 0.1347 - val_pos_accuracy: 0.8857\n",
      "Epoch 37/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0076 - pos_accuracy: 0.9932 - val_loss: 0.1336 - val_pos_accuracy: 0.8881\n",
      "Epoch 38/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0074 - pos_accuracy: 0.9938 - val_loss: 0.1318 - val_pos_accuracy: 0.8833\n",
      "Epoch 39/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9938 - val_loss: 0.1303 - val_pos_accuracy: 0.8905\n",
      "Epoch 40/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9938 - val_loss: 0.1308 - val_pos_accuracy: 0.8905\n",
      "Epoch 41/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9932 - val_loss: 0.1309 - val_pos_accuracy: 0.8881\n",
      "Epoch 42/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9951 - val_loss: 0.1297 - val_pos_accuracy: 0.8881\n",
      "Epoch 43/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9944 - val_loss: 0.1299 - val_pos_accuracy: 0.8833\n",
      "Epoch 44/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9963 - val_loss: 0.1296 - val_pos_accuracy: 0.8881\n",
      "Epoch 45/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9963 - val_loss: 0.1278 - val_pos_accuracy: 0.8857\n",
      "Epoch 46/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9975 - val_loss: 0.1280 - val_pos_accuracy: 0.8881\n",
      "Epoch 47/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9969 - val_loss: 0.1284 - val_pos_accuracy: 0.8905\n",
      "Epoch 48/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9969 - val_loss: 0.1286 - val_pos_accuracy: 0.8929\n",
      "Epoch 49/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9975 - val_loss: 0.1273 - val_pos_accuracy: 0.8929\n",
      "Epoch 50/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9981 - val_loss: 0.1263 - val_pos_accuracy: 0.8857\n",
      "Epoch 51/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9981 - val_loss: 0.1271 - val_pos_accuracy: 0.8905\n",
      "Epoch 52/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9981 - val_loss: 0.1274 - val_pos_accuracy: 0.8881\n",
      "Epoch 53/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9988 - val_loss: 0.1262 - val_pos_accuracy: 0.8857\n",
      "Epoch 54/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0023 - pos_accuracy: 0.9981 - val_loss: 0.1258 - val_pos_accuracy: 0.8881\n",
      "Epoch 55/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9994 - val_loss: 0.1240 - val_pos_accuracy: 0.8857\n",
      "Epoch 56/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9994 - val_loss: 0.1250 - val_pos_accuracy: 0.8905\n",
      "Epoch 57/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.1242 - val_pos_accuracy: 0.8857\n",
      "Epoch 58/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9994 - val_loss: 0.1252 - val_pos_accuracy: 0.8905\n",
      "Epoch 59/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0020 - pos_accuracy: 0.9994 - val_loss: 0.1253 - val_pos_accuracy: 0.8857\n",
      "Epoch 60/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1246 - val_pos_accuracy: 0.8905\n",
      "Epoch 61/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9994 - val_loss: 0.1237 - val_pos_accuracy: 0.8857\n",
      "Epoch 62/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1252 - val_pos_accuracy: 0.8881\n",
      "Epoch 63/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9994 - val_loss: 0.1238 - val_pos_accuracy: 0.8857\n",
      "Epoch 64/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1241 - val_pos_accuracy: 0.8857\n",
      "Epoch 65/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0014 - pos_accuracy: 0.9994 - val_loss: 0.1233 - val_pos_accuracy: 0.8857\n",
      "Epoch 66/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1234 - val_pos_accuracy: 0.8857\n",
      "Epoch 67/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0013 - pos_accuracy: 0.9994 - val_loss: 0.1234 - val_pos_accuracy: 0.8857\n",
      "Epoch 68/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1222 - val_pos_accuracy: 0.8857\n",
      "Epoch 69/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1220 - val_pos_accuracy: 0.8857\n",
      "Epoch 70/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1229 - val_pos_accuracy: 0.8857\n",
      "Epoch 71/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0011 - pos_accuracy: 0.9994 - val_loss: 0.1236 - val_pos_accuracy: 0.8905\n",
      "Epoch 72/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.0012 - pos_accuracy: 0.9994 - val_loss: 0.1231 - val_pos_accuracy: 0.8857\n",
      "Epoch 73/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 9.2022e-04 - pos_accuracy: 0.9994 - val_loss: 0.1218 - val_pos_accuracy: 0.8857\n",
      "Epoch 74/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 9.0413e-04 - pos_accuracy: 0.9994 - val_loss: 0.1230 - val_pos_accuracy: 0.8881\n",
      "Epoch 75/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 9.4778e-04 - pos_accuracy: 0.9994 - val_loss: 0.1221 - val_pos_accuracy: 0.8857\n",
      "Epoch 76/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 8.1441e-04 - pos_accuracy: 0.9994 - val_loss: 0.1221 - val_pos_accuracy: 0.8857\n",
      "Epoch 77/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 8.0143e-04 - pos_accuracy: 0.9994 - val_loss: 0.1221 - val_pos_accuracy: 0.8857\n",
      "Epoch 78/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 7.4596e-04 - pos_accuracy: 0.9994 - val_loss: 0.1223 - val_pos_accuracy: 0.8857\n",
      "Epoch 79/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 9.0544e-04 - pos_accuracy: 0.9994 - val_loss: 0.1218 - val_pos_accuracy: 0.8857\n",
      "Epoch 80/80\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 6.9436e-04 - pos_accuracy: 0.9994 - val_loss: 0.1214 - val_pos_accuracy: 0.8857\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:12:13.008670: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1654: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"np.concatenate 함수는 Numpy 배열들을 하나로 합치는데 쓰는 함수입니다. axis=1은 가로방향(열방향)으로 행렬을 합치는 것을 말합니다\",\n",
      "        \"121은 1행 2열 1번 그림, 122는 1행 2열 2번 그림을 말합니다.\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"tensor가 지정한 축 방향에 있는 각 요소의 논리와(and 연산)를 계산 합니다.axis=1은 가로방향을 뜻합니다.\",\n",
      "        \"tensor가 지정한 축(tensor의 특정한 차원)의 평균 값을 계산합니다\",\n",
      "        \"출력이 선형이나 분류가 아닌 x,y 좌표 숫자로 나오기 때문입니다.\",\n",
      "        \"재현 가능한 임의의 결과를 얻기 위하여 사용합니다.\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.8857\n",
      "}\"report1/송영석_46064.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/송영석_46064.ipynb to python\n",
      "[NbConvertApp] Writing 34396 bytes to report1/송영석_46064.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:209: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:240: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 297869.75it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:12:19.472691: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:12:19.860899: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:12:19.860938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:12:20.085898: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:12:20.719106: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 131956.52it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:12:22.967935: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1645: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 65.45454545454545,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/장철_45997.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/장철_45997.ipynb to python\n",
      "[NbConvertApp] Writing 36138 bytes to report1/장철_45997.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[1. 2.]\n",
      " [3. 4.]] \n",
      "-----\n",
      "[[5. 6.]\n",
      " [7. 8.]\n",
      " [9. 0.]] \n",
      "-----\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "[[5. 7. 9.]\n",
      " [6. 8. 0.]]\n",
      "2\n",
      "2\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 302215.95it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:12:29.356330: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:12:29.745228: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:12:29.745268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:12:29.970723: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:12:30.573924: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 4ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 120697.66it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 66.3126 - pos_accuracy: 0.0069 - val_loss: 7.0102 - val_pos_accuracy: 0.0240\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/step - loss: 2.8046 - pos_accuracy: 0.1262 - val_loss: 0.8812 - val_pos_accuracy: 0.3077\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.8657 - pos_accuracy: 0.2981 - val_loss: 0.7139 - val_pos_accuracy: 0.3774\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3748 - pos_accuracy: 0.4619 - val_loss: 0.6392 - val_pos_accuracy: 0.3053\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4596 - pos_accuracy: 0.4669 - val_loss: 0.2767 - val_pos_accuracy: 0.5962\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2405 - pos_accuracy: 0.6031 - val_loss: 0.3133 - val_pos_accuracy: 0.5745\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4567 - pos_accuracy: 0.5925 - val_loss: 0.1938 - val_pos_accuracy: 0.7620\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1690 - pos_accuracy: 0.7219 - val_loss: 0.2596 - val_pos_accuracy: 0.5962\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1006 - pos_accuracy: 0.8375 - val_loss: 0.1489 - val_pos_accuracy: 0.8005\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0893 - pos_accuracy: 0.8619 - val_loss: 0.1366 - val_pos_accuracy: 0.8389\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0607 - pos_accuracy: 0.9212 - val_loss: 0.1353 - val_pos_accuracy: 0.8534\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0543 - pos_accuracy: 0.9369 - val_loss: 0.1026 - val_pos_accuracy: 0.8726\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0485 - pos_accuracy: 0.9450 - val_loss: 0.1054 - val_pos_accuracy: 0.8798\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0396 - pos_accuracy: 0.9563 - val_loss: 0.1059 - val_pos_accuracy: 0.8822\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0304 - pos_accuracy: 0.9737 - val_loss: 0.0988 - val_pos_accuracy: 0.9087\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0288 - pos_accuracy: 0.9731 - val_loss: 0.0836 - val_pos_accuracy: 0.8990\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0278 - pos_accuracy: 0.9775 - val_loss: 0.0803 - val_pos_accuracy: 0.8918\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0266 - pos_accuracy: 0.9775 - val_loss: 0.0742 - val_pos_accuracy: 0.9159\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0239 - pos_accuracy: 0.9806 - val_loss: 0.0828 - val_pos_accuracy: 0.8990\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0219 - pos_accuracy: 0.9844 - val_loss: 0.0820 - val_pos_accuracy: 0.9087\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0209 - pos_accuracy: 0.9850 - val_loss: 0.0695 - val_pos_accuracy: 0.9351\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0175 - pos_accuracy: 0.9894 - val_loss: 0.0673 - val_pos_accuracy: 0.9327\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0139 - pos_accuracy: 0.9900 - val_loss: 0.0614 - val_pos_accuracy: 0.9207\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0123 - pos_accuracy: 0.9906 - val_loss: 0.0595 - val_pos_accuracy: 0.9351\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0113 - pos_accuracy: 0.9919 - val_loss: 0.0605 - val_pos_accuracy: 0.9279\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0104 - pos_accuracy: 0.9925 - val_loss: 0.0587 - val_pos_accuracy: 0.9183\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9937 - val_loss: 0.0654 - val_pos_accuracy: 0.9183\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9931 - val_loss: 0.0588 - val_pos_accuracy: 0.9351\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0082 - pos_accuracy: 0.9944 - val_loss: 0.0564 - val_pos_accuracy: 0.9231\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0072 - pos_accuracy: 0.9937 - val_loss: 0.0625 - val_pos_accuracy: 0.9423\n",
      "TensorShape([2, 2])\n",
      "TensorShape([2])\n",
      "array([ True, False])\n",
      "array([1., 0.], dtype=float32)\n",
      "0.5\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:12:38.335992: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1697: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할: A와 b 배열을 연결함.\\naxis=1의 의미: 열방향 (좌 -> 우)으로 연결함.\\n\",\n",
      "        \"\\n이미지를 1행 2열로 나타내고 이미지의 위치를 설정함. 1,2,1: 1행2 열의 1열에 이미지를 나타냄, 1,2,2: 1행 2열의 2열에 이미지를 나타냄.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n예측값과 실제값이 같은지 논리 연산을 계산함. 이때, 열을 기준으로 연산함. \\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다.\\nscore에 accuracy를 계산하기 위해 모든 값 (각각 0과 1)의 평균을 계산해줌.  \\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n분류의 결과가 연속적이기 때문. y값 2개중 택1 하는 것이 아님.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유\\n디버깅 등을 위해서 동일한 난수를 발생시키기 위함. \\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 90.0,\n",
      "    \"accuracy\": 0.9447\n",
      "}\"report1/이정휘_46029.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이정휘_46029.ipynb to python\n",
      "[NbConvertApp] Writing 35402 bytes to report1/이정휘_46029.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "ans01 = \n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "\n",
      "\n",
      "v = \n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "\n",
      "\n",
      "u = \n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "\n",
      "\n",
      "ans02 = \n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "\n",
      "\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:219: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:251: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "x shape =\n",
      "(3, 2)\n",
      "\n",
      "x.T shape =\n",
      "(2, 3)\n",
      "\n",
      "y shape =\n",
      "(2, 3)\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -10. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 297552.78it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:12:45.763525: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:12:46.137753: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:12:46.137792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:12:46.359589: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:12:46.959808: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 101451.37it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 27,362\n",
      "Trainable params: 27,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 100.6872 - pos_accuracy: 0.0063 - val_loss: 12.9238 - val_pos_accuracy: 0.0201\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 14.9495 - pos_accuracy: 0.0375 - val_loss: 8.2110 - val_pos_accuracy: 0.0446\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.5267 - pos_accuracy: 0.1319 - val_loss: 4.4084 - val_pos_accuracy: 0.0268\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2.3650 - pos_accuracy: 0.2125 - val_loss: 0.8165 - val_pos_accuracy: 0.3393\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3.0999 - pos_accuracy: 0.1075 - val_loss: 1.1561 - val_pos_accuracy: 0.1540\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6671 - pos_accuracy: 0.2956 - val_loss: 0.6292 - val_pos_accuracy: 0.4174\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4888 - pos_accuracy: 0.1762 - val_loss: 1.0789 - val_pos_accuracy: 0.1942\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5053 - pos_accuracy: 0.3931 - val_loss: 0.5222 - val_pos_accuracy: 0.3862\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2612 - pos_accuracy: 0.5706 - val_loss: 0.4208 - val_pos_accuracy: 0.5312\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2220 - pos_accuracy: 0.6456 - val_loss: 0.3575 - val_pos_accuracy: 0.5871\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3366 - pos_accuracy: 0.5312 - val_loss: 0.7785 - val_pos_accuracy: 0.2589\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3568 - pos_accuracy: 0.4694 - val_loss: 0.3343 - val_pos_accuracy: 0.5759\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1529 - pos_accuracy: 0.7594 - val_loss: 0.2853 - val_pos_accuracy: 0.6562\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1623 - pos_accuracy: 0.7212 - val_loss: 0.2704 - val_pos_accuracy: 0.6429\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1788 - pos_accuracy: 0.6831 - val_loss: 0.3332 - val_pos_accuracy: 0.6071\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3569 - pos_accuracy: 0.4750 - val_loss: 0.3869 - val_pos_accuracy: 0.3482\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1192 - pos_accuracy: 0.8094 - val_loss: 0.2197 - val_pos_accuracy: 0.7031\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1176 - pos_accuracy: 0.7981 - val_loss: 0.2159 - val_pos_accuracy: 0.7143\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1204 - pos_accuracy: 0.8050 - val_loss: 0.1888 - val_pos_accuracy: 0.7701\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1181 - pos_accuracy: 0.8069 - val_loss: 0.2218 - val_pos_accuracy: 0.7522\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1279 - pos_accuracy: 0.7819 - val_loss: 0.1942 - val_pos_accuracy: 0.7679\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1068 - pos_accuracy: 0.8369 - val_loss: 0.1766 - val_pos_accuracy: 0.7656\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0646 - pos_accuracy: 0.9112 - val_loss: 0.1545 - val_pos_accuracy: 0.8036\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0616 - pos_accuracy: 0.9125 - val_loss: 0.1528 - val_pos_accuracy: 0.7946\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0629 - pos_accuracy: 0.9106 - val_loss: 0.1568 - val_pos_accuracy: 0.8013\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0537 - pos_accuracy: 0.9306 - val_loss: 0.1442 - val_pos_accuracy: 0.8259\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0527 - pos_accuracy: 0.9312 - val_loss: 0.1469 - val_pos_accuracy: 0.8170\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1101 - pos_accuracy: 0.8069 - val_loss: 0.1466 - val_pos_accuracy: 0.8192\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0515 - pos_accuracy: 0.9269 - val_loss: 0.1373 - val_pos_accuracy: 0.8237\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0465 - pos_accuracy: 0.9400 - val_loss: 0.1930 - val_pos_accuracy: 0.7165\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1042 - pos_accuracy: 0.8294 - val_loss: 0.1334 - val_pos_accuracy: 0.8348\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0450 - pos_accuracy: 0.9325 - val_loss: 0.1277 - val_pos_accuracy: 0.8371\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0387 - pos_accuracy: 0.9438 - val_loss: 0.1363 - val_pos_accuracy: 0.8415\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0606 - pos_accuracy: 0.9081 - val_loss: 0.1188 - val_pos_accuracy: 0.8371\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0470 - pos_accuracy: 0.9344 - val_loss: 0.1445 - val_pos_accuracy: 0.8393\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0377 - pos_accuracy: 0.9475 - val_loss: 0.1177 - val_pos_accuracy: 0.8460\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0313 - pos_accuracy: 0.9544 - val_loss: 0.1091 - val_pos_accuracy: 0.8638\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0295 - pos_accuracy: 0.9575 - val_loss: 0.1107 - val_pos_accuracy: 0.8527\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0272 - pos_accuracy: 0.9606 - val_loss: 0.1093 - val_pos_accuracy: 0.8661\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0279 - pos_accuracy: 0.9644 - val_loss: 0.1148 - val_pos_accuracy: 0.8571\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0313 - pos_accuracy: 0.9625 - val_loss: 0.1037 - val_pos_accuracy: 0.8728\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0268 - pos_accuracy: 0.9675 - val_loss: 0.1161 - val_pos_accuracy: 0.8504\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0244 - pos_accuracy: 0.9744 - val_loss: 0.1028 - val_pos_accuracy: 0.8795\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0214 - pos_accuracy: 0.9737 - val_loss: 0.0987 - val_pos_accuracy: 0.8929\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0274 - pos_accuracy: 0.9700 - val_loss: 0.1190 - val_pos_accuracy: 0.8929\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0220 - pos_accuracy: 0.9744 - val_loss: 0.1001 - val_pos_accuracy: 0.8839\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0189 - pos_accuracy: 0.9787 - val_loss: 0.0968 - val_pos_accuracy: 0.8795\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0191 - pos_accuracy: 0.9794 - val_loss: 0.1005 - val_pos_accuracy: 0.8951\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0177 - pos_accuracy: 0.9806 - val_loss: 0.0995 - val_pos_accuracy: 0.8839\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0238 - pos_accuracy: 0.9781 - val_loss: 0.0972 - val_pos_accuracy: 0.8929\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0199 - pos_accuracy: 0.9806 - val_loss: 0.0959 - val_pos_accuracy: 0.8839\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0187 - pos_accuracy: 0.9819 - val_loss: 0.0949 - val_pos_accuracy: 0.8906\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0164 - pos_accuracy: 0.9844 - val_loss: 0.0933 - val_pos_accuracy: 0.8929\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0162 - pos_accuracy: 0.9819 - val_loss: 0.0915 - val_pos_accuracy: 0.8951\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0150 - pos_accuracy: 0.9856 - val_loss: 0.0950 - val_pos_accuracy: 0.8884\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0194 - pos_accuracy: 0.9875 - val_loss: 0.0956 - val_pos_accuracy: 0.9085\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0221 - pos_accuracy: 0.9856 - val_loss: 0.0901 - val_pos_accuracy: 0.8973\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9875 - val_loss: 0.0890 - val_pos_accuracy: 0.8973\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0123 - pos_accuracy: 0.9869 - val_loss: 0.0913 - val_pos_accuracy: 0.8996\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0174 - pos_accuracy: 0.9856 - val_loss: 0.1256 - val_pos_accuracy: 0.8616\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0979 - pos_accuracy: 0.8462 - val_loss: 0.1039 - val_pos_accuracy: 0.9152\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0260 - pos_accuracy: 0.9862 - val_loss: 0.1054 - val_pos_accuracy: 0.8951\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0391 - pos_accuracy: 0.9644 - val_loss: 0.0875 - val_pos_accuracy: 0.9018\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0126 - pos_accuracy: 0.9900 - val_loss: 0.0916 - val_pos_accuracy: 0.8951\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0224 - pos_accuracy: 0.9887 - val_loss: 0.0910 - val_pos_accuracy: 0.8996\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0118 - pos_accuracy: 0.9900 - val_loss: 0.0862 - val_pos_accuracy: 0.8996\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0099 - pos_accuracy: 0.9900 - val_loss: 0.0855 - val_pos_accuracy: 0.8996\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0097 - pos_accuracy: 0.9900 - val_loss: 0.0839 - val_pos_accuracy: 0.8996\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0099 - pos_accuracy: 0.9900 - val_loss: 0.0849 - val_pos_accuracy: 0.8996\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9906 - val_loss: 0.0869 - val_pos_accuracy: 0.8973\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0099 - pos_accuracy: 0.9912 - val_loss: 0.0837 - val_pos_accuracy: 0.8996\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0090 - pos_accuracy: 0.9906 - val_loss: 0.0873 - val_pos_accuracy: 0.8951\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0166 - pos_accuracy: 0.9912 - val_loss: 0.0828 - val_pos_accuracy: 0.9018\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9919 - val_loss: 0.0890 - val_pos_accuracy: 0.8951\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0155 - pos_accuracy: 0.9906 - val_loss: 0.0858 - val_pos_accuracy: 0.9018\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0141 - pos_accuracy: 0.9912 - val_loss: 0.0833 - val_pos_accuracy: 0.8996\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0111 - pos_accuracy: 0.9919 - val_loss: 0.0827 - val_pos_accuracy: 0.8996\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0073 - pos_accuracy: 0.9912 - val_loss: 0.0812 - val_pos_accuracy: 0.8996\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0069 - pos_accuracy: 0.9925 - val_loss: 0.0806 - val_pos_accuracy: 0.8996\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0069 - pos_accuracy: 0.9912 - val_loss: 0.0804 - val_pos_accuracy: 0.8996\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9931 - val_loss: 0.0802 - val_pos_accuracy: 0.9018\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9925 - val_loss: 0.0813 - val_pos_accuracy: 0.8996\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0073 - pos_accuracy: 0.9925 - val_loss: 0.0803 - val_pos_accuracy: 0.8996\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9925 - val_loss: 0.0797 - val_pos_accuracy: 0.9018\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9925 - val_loss: 0.0800 - val_pos_accuracy: 0.8996\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0078 - pos_accuracy: 0.9931 - val_loss: 0.0826 - val_pos_accuracy: 0.9018\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0063 - pos_accuracy: 0.9931 - val_loss: 0.0792 - val_pos_accuracy: 0.8973\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0056 - pos_accuracy: 0.9937 - val_loss: 0.0800 - val_pos_accuracy: 0.8996\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0086 - pos_accuracy: 0.9931 - val_loss: 0.0818 - val_pos_accuracy: 0.9040\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9950 - val_loss: 0.0790 - val_pos_accuracy: 0.9040\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9944 - val_loss: 0.0783 - val_pos_accuracy: 0.8996\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0063 - pos_accuracy: 0.9937 - val_loss: 0.0786 - val_pos_accuracy: 0.9018\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9944 - val_loss: 0.0776 - val_pos_accuracy: 0.9018\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9944 - val_loss: 0.0800 - val_pos_accuracy: 0.9018\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0075 - pos_accuracy: 0.9950 - val_loss: 0.0772 - val_pos_accuracy: 0.9018\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9950 - val_loss: 0.0766 - val_pos_accuracy: 0.8996\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9950 - val_loss: 0.0772 - val_pos_accuracy: 0.8996\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9950 - val_loss: 0.0781 - val_pos_accuracy: 0.9018\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0114 - pos_accuracy: 0.9950 - val_loss: 0.0873 - val_pos_accuracy: 0.9018\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9956 - val_loss: 0.0772 - val_pos_accuracy: 0.9018\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:13:00.137486: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1675: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\n- np.concatenate는 행렬과 행렬을 결합시키는 함수로, 2차행렬의 경우 행으로 확장, 열로 확장 2가지 방향으로 결합을 확장 시킬 수 있으므로, \\n옵션인 axis=를 이용하여 이를 지정할 수 있다.\\naxis=1은 열의 방향으로 행렬의 결합을 연산해준다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\nsubplot는 복수의 데이터를 그리는 함수로서, 옵션으로서 열의 수, 행의 수, 인덱스 3가지 패러미터를 받는다.\\n121- 1개의 열, 2개의 행을 가지는 도표에서 1번\\n122- 1개의 열, 2개의 행을 가지는 도표에서 2번\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n\\n비교대상 엘레멘트들을 AND연산을 시행하는 함수로, axis=0는 1차원 연산으로 같은 위치의 엘레멘트 행렬 요소를 연산하고(세로연산),\\naxis=1의 경우에는 한 행렬 엘레멘트들 내부 요소를 먼저 연산한다(가로연산).\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n\\n엘레멘트들의 평균을 내는 함수로, 차원을 하나 줄이고 그 평균을 낸다. 평균의 경우 전체를 총합하고 총 개수로 나눈다.\\n위의 경우를 포함하여 텐서플로우에서 대개 정확도를 계산하기 위해 사용한다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n이번 최종 출력층은 우리가 원하는 결과값이 분류나, 확률이 아닌 좌표이기에 활성화함수를 적용하지 않는다. 분류의 경우 softmax가 선호된다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n\\n시드 사용없이는 컴퓨터, 파이썬 내부함수의 특정한 랜덤 결과값으로 학습데이터를 작성하게되며, 이를 기초로 학습하였을 경우\\n동일한 샘플을 만들거나, 머신러닝에서 내부 랜덤값을 결정하는 규칙을 학습해버릴 가능성이 있다. \\n\",\n",
      "        false,\n",
      "        \"\\n아직까지는 감각적으로 패러미터를 넣고 있습니다. 많은 도구들이 이미 개발되어 간략하게 시험해볼 수 있음에도, 각종 패러미터들이 뜻하는 것과\\n어떤 학습모델에 더 유용한 건지 아직은 이해가 미치지 못해서 답답합니다만 이후 강의로 많은 부분이 해소될 것이라고 생각합니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 80.0,\n",
      "    \"accuracy\": 0.9085\n",
      "}\"report1/이창묵_46042.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이창묵_46042.ipynb to python\n",
      "[NbConvertApp] Writing 35223 bytes to report1/이창묵_46042.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 5.  7.]\n",
      " [11. 13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[3.53237216e+146 7.72226241e+146]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:252: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 298346.48it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:13:06.624831: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:13:07.004133: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:13:07.004170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:13:07.246948: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:13:07.885539: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 100714.46it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 25,186\n",
      "Trainable params: 25,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 6ms/step - loss: 63.8065 - accuracy: 0.8600 - val_loss: 35.4987 - val_accuracy: 0.9000\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.9806 - accuracy: 0.9413 - val_loss: 8.0157 - val_accuracy: 0.9675\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.8694 - accuracy: 0.9600 - val_loss: 18.2270 - val_accuracy: 0.8875\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.4030 - accuracy: 0.9650 - val_loss: 0.7662 - val_accuracy: 0.9800\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6173 - accuracy: 0.9756 - val_loss: 0.5546 - val_accuracy: 0.9725\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0421 - accuracy: 0.9706 - val_loss: 0.6457 - val_accuracy: 0.9800\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0533 - accuracy: 0.9750 - val_loss: 0.4288 - val_accuracy: 0.9725\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2651 - accuracy: 0.9762 - val_loss: 0.3394 - val_accuracy: 0.9850\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3789 - accuracy: 0.9769 - val_loss: 0.3250 - val_accuracy: 0.9700\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1832 - accuracy: 0.9744 - val_loss: 0.3284 - val_accuracy: 0.9850\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1526 - accuracy: 0.9775 - val_loss: 0.3068 - val_accuracy: 0.9825\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1438 - accuracy: 0.9750 - val_loss: 0.2487 - val_accuracy: 0.9800\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1115 - accuracy: 0.9781 - val_loss: 0.2347 - val_accuracy: 0.9750\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9769 - val_loss: 0.2150 - val_accuracy: 0.9750\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9762 - val_loss: 0.2338 - val_accuracy: 0.9750\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1165 - accuracy: 0.9725 - val_loss: 0.2755 - val_accuracy: 0.9625\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.9719 - val_loss: 0.2047 - val_accuracy: 0.9875\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9762 - val_loss: 0.1705 - val_accuracy: 0.9750\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.9787 - val_loss: 0.1786 - val_accuracy: 0.9775\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0520 - accuracy: 0.9756 - val_loss: 0.1696 - val_accuracy: 0.9775\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9744 - val_loss: 0.1786 - val_accuracy: 0.9825\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9794 - val_loss: 0.1546 - val_accuracy: 0.9675\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0419 - accuracy: 0.9756 - val_loss: 0.1547 - val_accuracy: 0.9650\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9731 - val_loss: 0.1450 - val_accuracy: 0.9825\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0420 - accuracy: 0.9756 - val_loss: 0.1533 - val_accuracy: 0.9925\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9775 - val_loss: 0.1476 - val_accuracy: 0.9900\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.9806 - val_loss: 0.1436 - val_accuracy: 0.9750\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.9787 - val_loss: 0.1454 - val_accuracy: 0.9650\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.9787 - val_loss: 0.1349 - val_accuracy: 0.9850\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.9775 - val_loss: 0.2227 - val_accuracy: 0.9925\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:13:13.053237: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1697: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnumpy패키지에 있는 np.concatenate의 역할은 행렬을 좌우나 위아래로 갖다 붙여주는 기능을 가짐\\n기본적으로는 세로로 붙는데 axis=1 주면 가로로 붙음\\n\",\n",
      "        \"\\nsubplot(121)은 1행과 2열로 이루어진칸에 1번째 그림을 넣는 의미\\nsubplot(122)                            2번째                 .\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\ntf.reduce_mean은 차원을 감소하여 평균을 구함\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유는 y= wx + b구조의 네트워크이기 때문임\\n\",\n",
      "        \"\\nrandom seed의 사용 이유는 동일한 난수의 조함을 고정시키기 위함임\\n\",\n",
      "        false,\n",
      "        \"\\n과제수행시 기초적인 부분이 많이 부족함을 느꼈습니다. 특히 5번문제는 강의사간에 자세한 설명을 한번 부탁드립니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 78.18181818181819,\n",
      "    \"accuracy\": 0.9925\n",
      "}\"report1/김기찬_46058.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/김기찬_46058.ipynb to python\n",
      "[NbConvertApp] Writing 35166 bytes to report1/김기찬_46058.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:242: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -10. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[-1.0000000e+00 -1.2246468e-16  2.0000000e+02]\n",
      " [ 1.2246468e-16  1.0000000e+00  0.0000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 300462.34it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:13:19.494300: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:13:19.872356: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:13:19.872396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:13:20.116816: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:13:20.749420: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 118830.59it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 104.2139 - pos_accuracy: 0.0250 - val_loss: 5.2700 - val_pos_accuracy: 0.0491\n",
      "Epoch 2/93\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.5185 - pos_accuracy: 0.0694 - val_loss: 3.7802 - val_pos_accuracy: 0.0513\n",
      "Epoch 3/93\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.0052 - pos_accuracy: 0.0781 - val_loss: 1.8581 - val_pos_accuracy: 0.0848\n",
      "Epoch 4/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1717 - pos_accuracy: 0.0962 - val_loss: 1.3032 - val_pos_accuracy: 0.1406\n",
      "Epoch 5/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4026 - pos_accuracy: 0.0900 - val_loss: 2.1062 - val_pos_accuracy: 0.0759\n",
      "Epoch 6/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8717 - pos_accuracy: 0.2281 - val_loss: 1.1338 - val_pos_accuracy: 0.1920\n",
      "Epoch 7/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6820 - pos_accuracy: 0.2850 - val_loss: 0.7450 - val_pos_accuracy: 0.3170\n",
      "Epoch 8/93\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5056 - pos_accuracy: 0.3681 - val_loss: 0.8191 - val_pos_accuracy: 0.2500\n",
      "Epoch 9/93\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7560 - pos_accuracy: 0.2362 - val_loss: 0.6902 - val_pos_accuracy: 0.3259\n",
      "Epoch 10/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4966 - pos_accuracy: 0.3237 - val_loss: 0.6459 - val_pos_accuracy: 0.3281\n",
      "Epoch 11/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7427 - pos_accuracy: 0.2688 - val_loss: 0.6623 - val_pos_accuracy: 0.3348\n",
      "Epoch 12/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3587 - pos_accuracy: 0.4550 - val_loss: 0.5573 - val_pos_accuracy: 0.3638\n",
      "Epoch 13/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7127 - pos_accuracy: 0.2763 - val_loss: 0.9812 - val_pos_accuracy: 0.1942\n",
      "Epoch 14/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5670 - pos_accuracy: 0.2988 - val_loss: 0.5633 - val_pos_accuracy: 0.4062\n",
      "Epoch 15/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9419 - pos_accuracy: 0.2294 - val_loss: 1.6788 - val_pos_accuracy: 0.1027\n",
      "Epoch 16/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1860 - pos_accuracy: 0.1587 - val_loss: 2.4840 - val_pos_accuracy: 0.0893\n",
      "Epoch 17/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4676 - pos_accuracy: 0.3913 - val_loss: 0.7970 - val_pos_accuracy: 0.2411\n",
      "Epoch 18/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3931 - pos_accuracy: 0.3850 - val_loss: 0.7054 - val_pos_accuracy: 0.2768\n",
      "Epoch 19/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3559 - pos_accuracy: 0.4169 - val_loss: 0.5768 - val_pos_accuracy: 0.4263\n",
      "Epoch 20/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4215 - pos_accuracy: 0.3719 - val_loss: 0.7509 - val_pos_accuracy: 0.3013\n",
      "Epoch 21/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2798 - pos_accuracy: 0.5094 - val_loss: 0.8260 - val_pos_accuracy: 0.2433\n",
      "Epoch 22/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3614 - pos_accuracy: 0.4181 - val_loss: 0.7310 - val_pos_accuracy: 0.3147\n",
      "Epoch 23/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3090 - pos_accuracy: 0.4331 - val_loss: 0.5763 - val_pos_accuracy: 0.4263\n",
      "Epoch 24/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5728 - pos_accuracy: 0.3137 - val_loss: 0.6056 - val_pos_accuracy: 0.3170\n",
      "Epoch 25/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2719 - pos_accuracy: 0.5225 - val_loss: 0.7448 - val_pos_accuracy: 0.2321\n",
      "Epoch 26/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2196 - pos_accuracy: 0.5925 - val_loss: 0.5571 - val_pos_accuracy: 0.4665\n",
      "Epoch 27/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2171 - pos_accuracy: 0.5700 - val_loss: 0.5813 - val_pos_accuracy: 0.4420\n",
      "Epoch 28/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2066 - pos_accuracy: 0.6094 - val_loss: 0.8495 - val_pos_accuracy: 0.1652\n",
      "Epoch 29/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3146 - pos_accuracy: 0.4550 - val_loss: 0.5519 - val_pos_accuracy: 0.4598\n",
      "Epoch 30/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2262 - pos_accuracy: 0.5788 - val_loss: 0.6740 - val_pos_accuracy: 0.3460\n",
      "Epoch 31/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3115 - pos_accuracy: 0.4969 - val_loss: 0.5250 - val_pos_accuracy: 0.5357\n",
      "Epoch 32/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2391 - pos_accuracy: 0.5462 - val_loss: 0.5241 - val_pos_accuracy: 0.5424\n",
      "Epoch 33/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1785 - pos_accuracy: 0.6631 - val_loss: 0.5442 - val_pos_accuracy: 0.4598\n",
      "Epoch 34/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2484 - pos_accuracy: 0.5306 - val_loss: 0.5219 - val_pos_accuracy: 0.5491\n",
      "Epoch 35/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2333 - pos_accuracy: 0.5763 - val_loss: 0.6822 - val_pos_accuracy: 0.3460\n",
      "Epoch 36/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1563 - pos_accuracy: 0.7013 - val_loss: 0.5654 - val_pos_accuracy: 0.4129\n",
      "Epoch 37/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2655 - pos_accuracy: 0.4988 - val_loss: 0.5499 - val_pos_accuracy: 0.5179\n",
      "Epoch 38/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2097 - pos_accuracy: 0.5875 - val_loss: 0.5288 - val_pos_accuracy: 0.6004\n",
      "Epoch 39/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2973 - pos_accuracy: 0.4688 - val_loss: 0.6094 - val_pos_accuracy: 0.4576\n",
      "Epoch 40/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2677 - pos_accuracy: 0.5025 - val_loss: 0.5330 - val_pos_accuracy: 0.5781\n",
      "Epoch 41/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1404 - pos_accuracy: 0.7350 - val_loss: 0.5689 - val_pos_accuracy: 0.5536\n",
      "Epoch 42/93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1688 - pos_accuracy: 0.6500 - val_loss: 0.6201 - val_pos_accuracy: 0.4397\n",
      "Epoch 43/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1496 - pos_accuracy: 0.7169 - val_loss: 0.5093 - val_pos_accuracy: 0.6763\n",
      "Epoch 44/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1204 - pos_accuracy: 0.7588 - val_loss: 0.5376 - val_pos_accuracy: 0.6295\n",
      "Epoch 45/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1300 - pos_accuracy: 0.7437 - val_loss: 0.6284 - val_pos_accuracy: 0.3683\n",
      "Epoch 46/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2423 - pos_accuracy: 0.5231 - val_loss: 0.6722 - val_pos_accuracy: 0.3884\n",
      "Epoch 47/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2406 - pos_accuracy: 0.5294 - val_loss: 0.5487 - val_pos_accuracy: 0.5848\n",
      "Epoch 48/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1365 - pos_accuracy: 0.7219 - val_loss: 0.5796 - val_pos_accuracy: 0.5714\n",
      "Epoch 49/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1679 - pos_accuracy: 0.6525 - val_loss: 0.6262 - val_pos_accuracy: 0.4397\n",
      "Epoch 50/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2352 - pos_accuracy: 0.5437 - val_loss: 0.5180 - val_pos_accuracy: 0.6987\n",
      "Epoch 51/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2003 - pos_accuracy: 0.6263 - val_loss: 0.5582 - val_pos_accuracy: 0.6183\n",
      "Epoch 52/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1657 - pos_accuracy: 0.7044 - val_loss: 0.8604 - val_pos_accuracy: 0.2455\n",
      "Epoch 53/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1753 - pos_accuracy: 0.6444 - val_loss: 0.5337 - val_pos_accuracy: 0.6496\n",
      "Epoch 54/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1105 - pos_accuracy: 0.7869 - val_loss: 0.5317 - val_pos_accuracy: 0.6875\n",
      "Epoch 55/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0863 - pos_accuracy: 0.8637 - val_loss: 0.5201 - val_pos_accuracy: 0.6853\n",
      "Epoch 56/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1056 - pos_accuracy: 0.7962 - val_loss: 0.5397 - val_pos_accuracy: 0.6808\n",
      "Epoch 57/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0830 - pos_accuracy: 0.8694 - val_loss: 0.5441 - val_pos_accuracy: 0.6964\n",
      "Epoch 58/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0773 - pos_accuracy: 0.8781 - val_loss: 0.5249 - val_pos_accuracy: 0.7277\n",
      "Epoch 59/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1011 - pos_accuracy: 0.8125 - val_loss: 0.5736 - val_pos_accuracy: 0.6004\n",
      "Epoch 60/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2025 - pos_accuracy: 0.5337 - val_loss: 0.6042 - val_pos_accuracy: 0.4911\n",
      "Epoch 61/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1372 - pos_accuracy: 0.7088 - val_loss: 0.6052 - val_pos_accuracy: 0.5379\n",
      "Epoch 62/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1064 - pos_accuracy: 0.8000 - val_loss: 0.5766 - val_pos_accuracy: 0.6071\n",
      "Epoch 63/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1058 - pos_accuracy: 0.8069 - val_loss: 0.5470 - val_pos_accuracy: 0.6250\n",
      "Epoch 64/93\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0750 - pos_accuracy: 0.8769 - val_loss: 0.5293 - val_pos_accuracy: 0.7299\n",
      "Epoch 65/93\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0952 - pos_accuracy: 0.8150 - val_loss: 0.5878 - val_pos_accuracy: 0.6094\n",
      "Epoch 66/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0832 - pos_accuracy: 0.8594 - val_loss: 0.5608 - val_pos_accuracy: 0.6920\n",
      "Epoch 67/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1011 - pos_accuracy: 0.8112 - val_loss: 0.6123 - val_pos_accuracy: 0.5915\n",
      "Epoch 68/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0858 - pos_accuracy: 0.8506 - val_loss: 0.5457 - val_pos_accuracy: 0.7009\n",
      "Epoch 69/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0859 - pos_accuracy: 0.8475 - val_loss: 0.5468 - val_pos_accuracy: 0.6987\n",
      "Epoch 70/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0773 - pos_accuracy: 0.8763 - val_loss: 0.6009 - val_pos_accuracy: 0.6049\n",
      "Epoch 71/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1140 - pos_accuracy: 0.7700 - val_loss: 0.5884 - val_pos_accuracy: 0.6473\n",
      "Epoch 72/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1121 - pos_accuracy: 0.7912 - val_loss: 0.6375 - val_pos_accuracy: 0.5201\n",
      "Epoch 73/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3921 - pos_accuracy: 0.4781 - val_loss: 0.5683 - val_pos_accuracy: 0.6741\n",
      "Epoch 74/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0637 - pos_accuracy: 0.9100 - val_loss: 0.5465 - val_pos_accuracy: 0.7165\n",
      "Epoch 75/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0986 - pos_accuracy: 0.8181 - val_loss: 0.5403 - val_pos_accuracy: 0.7500\n",
      "Epoch 76/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0829 - pos_accuracy: 0.8512 - val_loss: 0.5525 - val_pos_accuracy: 0.7366\n",
      "Epoch 77/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0745 - pos_accuracy: 0.8831 - val_loss: 0.5957 - val_pos_accuracy: 0.5379\n",
      "Epoch 78/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0849 - pos_accuracy: 0.8506 - val_loss: 0.5317 - val_pos_accuracy: 0.7679\n",
      "Epoch 79/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0784 - pos_accuracy: 0.8650 - val_loss: 0.6811 - val_pos_accuracy: 0.5290\n",
      "Epoch 80/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1385 - pos_accuracy: 0.6831 - val_loss: 0.6671 - val_pos_accuracy: 0.3661\n",
      "Epoch 81/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1173 - pos_accuracy: 0.7412 - val_loss: 0.5591 - val_pos_accuracy: 0.7366\n",
      "Epoch 82/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0557 - pos_accuracy: 0.9269 - val_loss: 0.5390 - val_pos_accuracy: 0.7589\n",
      "Epoch 83/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1158 - pos_accuracy: 0.7619 - val_loss: 0.7065 - val_pos_accuracy: 0.4219\n",
      "Epoch 84/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1038 - pos_accuracy: 0.8188 - val_loss: 0.5537 - val_pos_accuracy: 0.7433\n",
      "Epoch 85/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1122 - pos_accuracy: 0.7500 - val_loss: 0.6444 - val_pos_accuracy: 0.4621\n",
      "Epoch 86/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1203 - pos_accuracy: 0.7275 - val_loss: 0.7162 - val_pos_accuracy: 0.4107\n",
      "Epoch 87/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1168 - pos_accuracy: 0.7425 - val_loss: 0.6271 - val_pos_accuracy: 0.5469\n",
      "Epoch 88/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0792 - pos_accuracy: 0.8425 - val_loss: 0.6332 - val_pos_accuracy: 0.5982\n",
      "Epoch 89/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0837 - pos_accuracy: 0.8438 - val_loss: 0.6887 - val_pos_accuracy: 0.5335\n",
      "Epoch 90/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0892 - pos_accuracy: 0.8325 - val_loss: 0.5594 - val_pos_accuracy: 0.7500\n",
      "Epoch 91/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0614 - pos_accuracy: 0.8881 - val_loss: 0.5895 - val_pos_accuracy: 0.6830\n",
      "Epoch 92/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0828 - pos_accuracy: 0.8544 - val_loss: 0.5888 - val_pos_accuracy: 0.7121\n",
      "Epoch 93/93\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0590 - pos_accuracy: 0.9144 - val_loss: 0.5526 - val_pos_accuracy: 0.7879\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:13:31.851662: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1667: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concatenate = 기존축을 따라 배열들을 결합해주는 함수\\naxis=1의 의미 = 열을 따라서 동작하라는 매개변수\\n\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다. = 열을 기준으로 축소연산 하는 역할입니다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. = 매개변수에 따라 열 혹은 행의 평균을 내주는 역할입니다\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유  = 다른 출력층에 비해 정확도가 매우 높아지고 손실도가 매우 낮아지기 때문입니다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 = 고정된 결과값을 받기 위해서 입니다.\\n\",\n",
      "        false,\n",
      "        \"\\n어렵지만 쉽게 알려주시려는 교수님의 수업 잘 듣고있습니다! 앞으로도 좋은 수업 부탁드립니다! \\n\"\n",
      "    ],\n",
      "    \"score\": 74.54545454545455,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/권범수_46040.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/권범수_46040.ipynb to python\n",
      "[NbConvertApp] Writing 36646 bytes to report1/권범수_46040.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "51\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:256: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "421\n",
      "421\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:279: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 314050.69it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:13:38.355001: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:13:38.731929: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:13:38.731970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:13:38.955115: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:13:39.580233: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 165818.81it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 52,882\n",
      "Trainable params: 52,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 164.6052 - pos_accuracy: 0.0012 - val_loss: 97.6475 - val_pos_accuracy: 0.0039\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 40.1568 - pos_accuracy: 0.0024 - val_loss: 31.3603 - val_pos_accuracy: 0.0039\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.2470 - pos_accuracy: 0.0343 - val_loss: 6.4240 - val_pos_accuracy: 0.0176\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.7716 - pos_accuracy: 0.0343 - val_loss: 6.0779 - val_pos_accuracy: 0.0215\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 7.1867 - pos_accuracy: 0.0312 - val_loss: 3.3792 - val_pos_accuracy: 0.0762\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 7.2616 - pos_accuracy: 0.0294 - val_loss: 2.0266 - val_pos_accuracy: 0.1855\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.8953 - pos_accuracy: 0.0950 - val_loss: 2.5889 - val_pos_accuracy: 0.0820\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5301 - pos_accuracy: 0.0715 - val_loss: 4.0810 - val_pos_accuracy: 0.0684\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.0847 - pos_accuracy: 0.0619 - val_loss: 1.1390 - val_pos_accuracy: 0.1797\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.7678 - pos_accuracy: 0.2831 - val_loss: 0.7861 - val_pos_accuracy: 0.4277\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5666 - pos_accuracy: 0.3738 - val_loss: 0.9640 - val_pos_accuracy: 0.3008\n",
      "Epoch 12/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 5ms/step - loss: 0.7657 - pos_accuracy: 0.2686 - val_loss: 0.7661 - val_pos_accuracy: 0.3672\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.7101 - pos_accuracy: 0.2969 - val_loss: 0.6562 - val_pos_accuracy: 0.4590\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5428 - pos_accuracy: 0.3672 - val_loss: 0.6706 - val_pos_accuracy: 0.3965\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5329 - pos_accuracy: 0.3413 - val_loss: 0.7882 - val_pos_accuracy: 0.2793\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.9527 - pos_accuracy: 0.1947 - val_loss: 0.7144 - val_pos_accuracy: 0.3477\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5475 - pos_accuracy: 0.3311 - val_loss: 0.8008 - val_pos_accuracy: 0.2812\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1.1929 - pos_accuracy: 0.1220 - val_loss: 1.7811 - val_pos_accuracy: 0.2188\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1.2451 - pos_accuracy: 0.1364 - val_loss: 1.4370 - val_pos_accuracy: 0.1523\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.7183 - pos_accuracy: 0.2788 - val_loss: 1.3783 - val_pos_accuracy: 0.1953\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1.2076 - pos_accuracy: 0.1977 - val_loss: 0.5414 - val_pos_accuracy: 0.4277\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.3137 - pos_accuracy: 0.5661 - val_loss: 0.4338 - val_pos_accuracy: 0.5586\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.2353 - pos_accuracy: 0.6611 - val_loss: 0.4478 - val_pos_accuracy: 0.5039\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2755 - pos_accuracy: 0.5583 - val_loss: 0.4272 - val_pos_accuracy: 0.5215\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2338 - pos_accuracy: 0.6460 - val_loss: 0.5235 - val_pos_accuracy: 0.3965\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3177 - pos_accuracy: 0.4916 - val_loss: 0.6677 - val_pos_accuracy: 0.2695\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3416 - pos_accuracy: 0.4934 - val_loss: 0.3513 - val_pos_accuracy: 0.6699\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1951 - pos_accuracy: 0.7236 - val_loss: 0.3338 - val_pos_accuracy: 0.6758\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1818 - pos_accuracy: 0.7326 - val_loss: 0.3326 - val_pos_accuracy: 0.6914\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1695 - pos_accuracy: 0.7644 - val_loss: 0.3940 - val_pos_accuracy: 0.5547\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1794 - pos_accuracy: 0.7169 - val_loss: 0.3117 - val_pos_accuracy: 0.7090\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.2523 - pos_accuracy: 0.5944 - val_loss: 0.4103 - val_pos_accuracy: 0.4492\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2247 - pos_accuracy: 0.5962 - val_loss: 0.3097 - val_pos_accuracy: 0.6777\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1509 - pos_accuracy: 0.7812 - val_loss: 0.2921 - val_pos_accuracy: 0.7031\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1995 - pos_accuracy: 0.6971 - val_loss: 0.4625 - val_pos_accuracy: 0.4668\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2745 - pos_accuracy: 0.5577 - val_loss: 0.3968 - val_pos_accuracy: 0.5000\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2501 - pos_accuracy: 0.5282 - val_loss: 0.2798 - val_pos_accuracy: 0.6973\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1280 - pos_accuracy: 0.8179 - val_loss: 0.2633 - val_pos_accuracy: 0.7344\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1321 - pos_accuracy: 0.8053 - val_loss: 0.2842 - val_pos_accuracy: 0.6973\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1688 - pos_accuracy: 0.7115 - val_loss: 0.3310 - val_pos_accuracy: 0.6172\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1765 - pos_accuracy: 0.6881 - val_loss: 0.3237 - val_pos_accuracy: 0.6074\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.2927 - pos_accuracy: 0.4706 - val_loss: 0.4731 - val_pos_accuracy: 0.4141\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2131 - pos_accuracy: 0.5847 - val_loss: 0.5508 - val_pos_accuracy: 0.2949\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.2941 - pos_accuracy: 0.4952 - val_loss: 0.2896 - val_pos_accuracy: 0.6816\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1479 - pos_accuracy: 0.7278 - val_loss: 0.3629 - val_pos_accuracy: 0.5234\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1710 - pos_accuracy: 0.6839 - val_loss: 0.2772 - val_pos_accuracy: 0.6738\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1062 - pos_accuracy: 0.8431 - val_loss: 0.2344 - val_pos_accuracy: 0.7559\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1139 - pos_accuracy: 0.8401 - val_loss: 0.2238 - val_pos_accuracy: 0.7715\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1150 - pos_accuracy: 0.8353 - val_loss: 0.2290 - val_pos_accuracy: 0.7617\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0840 - pos_accuracy: 0.8804 - val_loss: 0.2212 - val_pos_accuracy: 0.7852\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0813 - pos_accuracy: 0.8900 - val_loss: 0.2336 - val_pos_accuracy: 0.7227\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0820 - pos_accuracy: 0.8816 - val_loss: 0.2400 - val_pos_accuracy: 0.7461\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0925 - pos_accuracy: 0.8792 - val_loss: 0.2211 - val_pos_accuracy: 0.7559\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1276 - pos_accuracy: 0.7542 - val_loss: 0.2298 - val_pos_accuracy: 0.7402\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1237 - pos_accuracy: 0.7632 - val_loss: 0.2438 - val_pos_accuracy: 0.6973\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1092 - pos_accuracy: 0.8293 - val_loss: 0.2677 - val_pos_accuracy: 0.6289\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1331 - pos_accuracy: 0.7272 - val_loss: 0.1996 - val_pos_accuracy: 0.8027\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0813 - pos_accuracy: 0.8792 - val_loss: 0.2418 - val_pos_accuracy: 0.7148\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1244 - pos_accuracy: 0.7861 - val_loss: 0.2152 - val_pos_accuracy: 0.7676\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0936 - pos_accuracy: 0.8660 - val_loss: 0.2328 - val_pos_accuracy: 0.7246\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1206 - pos_accuracy: 0.7945 - val_loss: 0.2234 - val_pos_accuracy: 0.7812\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0959 - pos_accuracy: 0.8510 - val_loss: 0.2154 - val_pos_accuracy: 0.7988\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1058 - pos_accuracy: 0.8371 - val_loss: 0.1958 - val_pos_accuracy: 0.7969\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0615 - pos_accuracy: 0.9201 - val_loss: 0.2007 - val_pos_accuracy: 0.7949\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0749 - pos_accuracy: 0.8924 - val_loss: 0.1969 - val_pos_accuracy: 0.7969\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0621 - pos_accuracy: 0.9243 - val_loss: 0.1878 - val_pos_accuracy: 0.8105\n",
      "Epoch 67/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0735 - pos_accuracy: 0.9008 - val_loss: 0.2322 - val_pos_accuracy: 0.7188\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1150 - pos_accuracy: 0.7656 - val_loss: 0.2062 - val_pos_accuracy: 0.8125\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0960 - pos_accuracy: 0.8377 - val_loss: 0.2284 - val_pos_accuracy: 0.7070\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1545 - pos_accuracy: 0.6617 - val_loss: 0.2446 - val_pos_accuracy: 0.6387\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1957 - pos_accuracy: 0.5487 - val_loss: 0.2187 - val_pos_accuracy: 0.7188\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0852 - pos_accuracy: 0.8504 - val_loss: 0.1861 - val_pos_accuracy: 0.8223\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0973 - pos_accuracy: 0.8143 - val_loss: 0.2330 - val_pos_accuracy: 0.6836\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1128 - pos_accuracy: 0.8029 - val_loss: 0.2543 - val_pos_accuracy: 0.6602\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1287 - pos_accuracy: 0.7344 - val_loss: 0.1943 - val_pos_accuracy: 0.7871\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0562 - pos_accuracy: 0.9309 - val_loss: 0.1669 - val_pos_accuracy: 0.8281\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0461 - pos_accuracy: 0.9411 - val_loss: 0.1712 - val_pos_accuracy: 0.8379\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0702 - pos_accuracy: 0.9044 - val_loss: 0.2163 - val_pos_accuracy: 0.6953\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1504 - pos_accuracy: 0.6749 - val_loss: 0.1759 - val_pos_accuracy: 0.8379\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0542 - pos_accuracy: 0.9369 - val_loss: 0.1741 - val_pos_accuracy: 0.8496\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0783 - pos_accuracy: 0.8996 - val_loss: 0.1684 - val_pos_accuracy: 0.8281\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0547 - pos_accuracy: 0.9429 - val_loss: 0.1610 - val_pos_accuracy: 0.8438\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0392 - pos_accuracy: 0.9507 - val_loss: 0.1595 - val_pos_accuracy: 0.8418\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0442 - pos_accuracy: 0.9501 - val_loss: 0.1604 - val_pos_accuracy: 0.8594\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0437 - pos_accuracy: 0.9471 - val_loss: 0.1594 - val_pos_accuracy: 0.8574\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0399 - pos_accuracy: 0.9519 - val_loss: 0.1858 - val_pos_accuracy: 0.8164\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0719 - pos_accuracy: 0.9026 - val_loss: 0.1707 - val_pos_accuracy: 0.8516\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0667 - pos_accuracy: 0.9093 - val_loss: 0.1844 - val_pos_accuracy: 0.8145\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0615 - pos_accuracy: 0.9387 - val_loss: 0.1735 - val_pos_accuracy: 0.8555\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0578 - pos_accuracy: 0.9429 - val_loss: 0.1916 - val_pos_accuracy: 0.7969\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0684 - pos_accuracy: 0.8948 - val_loss: 0.1611 - val_pos_accuracy: 0.8398\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0619 - pos_accuracy: 0.9321 - val_loss: 0.1700 - val_pos_accuracy: 0.8418\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0453 - pos_accuracy: 0.9531 - val_loss: 0.1482 - val_pos_accuracy: 0.8594\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0331 - pos_accuracy: 0.9621 - val_loss: 0.1592 - val_pos_accuracy: 0.8574\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0499 - pos_accuracy: 0.9489 - val_loss: 0.1486 - val_pos_accuracy: 0.8691\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0447 - pos_accuracy: 0.9519 - val_loss: 0.1487 - val_pos_accuracy: 0.8613\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0491 - pos_accuracy: 0.9399 - val_loss: 0.1894 - val_pos_accuracy: 0.7812\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0715 - pos_accuracy: 0.8666 - val_loss: 0.2038 - val_pos_accuracy: 0.7402\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0873 - pos_accuracy: 0.8143 - val_loss: 0.1747 - val_pos_accuracy: 0.8242\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0545 - pos_accuracy: 0.9447 - val_loss: 0.1651 - val_pos_accuracy: 0.8379\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0414 - pos_accuracy: 0.9651 - val_loss: 0.1501 - val_pos_accuracy: 0.8672\n",
      "Epoch 102/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0364 - pos_accuracy: 0.9651 - val_loss: 0.1473 - val_pos_accuracy: 0.8633\n",
      "Epoch 103/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0300 - pos_accuracy: 0.9675 - val_loss: 0.1401 - val_pos_accuracy: 0.8691\n",
      "Epoch 104/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0250 - pos_accuracy: 0.9712 - val_loss: 0.1435 - val_pos_accuracy: 0.8613\n",
      "Epoch 105/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0280 - pos_accuracy: 0.9694 - val_loss: 0.1443 - val_pos_accuracy: 0.8711\n",
      "Epoch 106/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0298 - pos_accuracy: 0.9688 - val_loss: 0.1378 - val_pos_accuracy: 0.8613\n",
      "Epoch 107/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0257 - pos_accuracy: 0.9675 - val_loss: 0.1412 - val_pos_accuracy: 0.8691\n",
      "Epoch 108/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0279 - pos_accuracy: 0.9706 - val_loss: 0.1539 - val_pos_accuracy: 0.8613\n",
      "Epoch 109/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0542 - pos_accuracy: 0.9453 - val_loss: 0.1637 - val_pos_accuracy: 0.8477\n",
      "Epoch 110/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0757 - pos_accuracy: 0.9044 - val_loss: 0.2097 - val_pos_accuracy: 0.7734\n",
      "Epoch 111/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0605 - pos_accuracy: 0.9453 - val_loss: 0.1443 - val_pos_accuracy: 0.8711\n",
      "Epoch 112/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0301 - pos_accuracy: 0.9700 - val_loss: 0.1683 - val_pos_accuracy: 0.8457\n",
      "Epoch 113/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0475 - pos_accuracy: 0.9555 - val_loss: 0.1607 - val_pos_accuracy: 0.8594\n",
      "Epoch 114/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0482 - pos_accuracy: 0.9585 - val_loss: 0.1484 - val_pos_accuracy: 0.8613\n",
      "Epoch 115/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0281 - pos_accuracy: 0.9712 - val_loss: 0.1385 - val_pos_accuracy: 0.8730\n",
      "Epoch 116/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0305 - pos_accuracy: 0.9712 - val_loss: 0.1502 - val_pos_accuracy: 0.8613\n",
      "Epoch 117/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0365 - pos_accuracy: 0.9688 - val_loss: 0.1457 - val_pos_accuracy: 0.8770\n",
      "Epoch 118/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0508 - pos_accuracy: 0.9393 - val_loss: 0.1576 - val_pos_accuracy: 0.8633\n",
      "Epoch 119/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0280 - pos_accuracy: 0.9742 - val_loss: 0.1325 - val_pos_accuracy: 0.8770\n",
      "Epoch 120/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0345 - pos_accuracy: 0.9681 - val_loss: 0.1492 - val_pos_accuracy: 0.8594\n",
      "Epoch 121/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0333 - pos_accuracy: 0.9694 - val_loss: 0.1451 - val_pos_accuracy: 0.8672\n",
      "Epoch 122/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0442 - pos_accuracy: 0.9675 - val_loss: 0.1662 - val_pos_accuracy: 0.8633\n",
      "Epoch 123/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0672 - pos_accuracy: 0.9417 - val_loss: 0.1851 - val_pos_accuracy: 0.8145\n",
      "Epoch 124/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1206 - pos_accuracy: 0.8161 - val_loss: 0.1833 - val_pos_accuracy: 0.8242\n",
      "Epoch 125/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0732 - pos_accuracy: 0.9315 - val_loss: 0.1676 - val_pos_accuracy: 0.8281\n",
      "Epoch 126/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0720 - pos_accuracy: 0.9020 - val_loss: 0.1553 - val_pos_accuracy: 0.8438\n",
      "Epoch 127/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0438 - pos_accuracy: 0.9645 - val_loss: 0.1315 - val_pos_accuracy: 0.8809\n",
      "Epoch 128/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0326 - pos_accuracy: 0.9724 - val_loss: 0.1423 - val_pos_accuracy: 0.8691\n",
      "Epoch 129/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0234 - pos_accuracy: 0.9754 - val_loss: 0.1290 - val_pos_accuracy: 0.8750\n",
      "Epoch 130/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0223 - pos_accuracy: 0.9748 - val_loss: 0.1338 - val_pos_accuracy: 0.8691\n",
      "Epoch 131/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0345 - pos_accuracy: 0.9718 - val_loss: 0.1280 - val_pos_accuracy: 0.8711\n",
      "Epoch 132/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0183 - pos_accuracy: 0.9766 - val_loss: 0.1250 - val_pos_accuracy: 0.8711\n",
      "Epoch 133/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0173 - pos_accuracy: 0.9790 - val_loss: 0.1271 - val_pos_accuracy: 0.8691\n",
      "Epoch 134/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0301 - pos_accuracy: 0.9766 - val_loss: 0.1374 - val_pos_accuracy: 0.8770\n",
      "Epoch 135/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0192 - pos_accuracy: 0.9802 - val_loss: 0.1267 - val_pos_accuracy: 0.8691\n",
      "Epoch 136/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0155 - pos_accuracy: 0.9802 - val_loss: 0.1249 - val_pos_accuracy: 0.8691\n",
      "Epoch 137/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0202 - pos_accuracy: 0.9778 - val_loss: 0.1286 - val_pos_accuracy: 0.8691\n",
      "Epoch 138/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0150 - pos_accuracy: 0.9808 - val_loss: 0.1233 - val_pos_accuracy: 0.8730\n",
      "Epoch 139/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0156 - pos_accuracy: 0.9826 - val_loss: 0.1248 - val_pos_accuracy: 0.8730\n",
      "Epoch 140/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0158 - pos_accuracy: 0.9808 - val_loss: 0.1238 - val_pos_accuracy: 0.8750\n",
      "Epoch 141/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0150 - pos_accuracy: 0.9814 - val_loss: 0.1247 - val_pos_accuracy: 0.8789\n",
      "Epoch 142/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0183 - pos_accuracy: 0.9796 - val_loss: 0.1302 - val_pos_accuracy: 0.8711\n",
      "Epoch 143/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0198 - pos_accuracy: 0.9796 - val_loss: 0.1372 - val_pos_accuracy: 0.8809\n",
      "Epoch 144/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0340 - pos_accuracy: 0.9724 - val_loss: 0.1423 - val_pos_accuracy: 0.8887\n",
      "Epoch 145/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0363 - pos_accuracy: 0.9694 - val_loss: 0.1335 - val_pos_accuracy: 0.8770\n",
      "Epoch 146/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0224 - pos_accuracy: 0.9808 - val_loss: 0.1308 - val_pos_accuracy: 0.8965\n",
      "Epoch 147/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0230 - pos_accuracy: 0.9790 - val_loss: 0.1396 - val_pos_accuracy: 0.8730\n",
      "Epoch 148/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0547 - pos_accuracy: 0.9117 - val_loss: 0.1398 - val_pos_accuracy: 0.8867\n",
      "Epoch 149/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0325 - pos_accuracy: 0.9790 - val_loss: 0.1332 - val_pos_accuracy: 0.8750\n",
      "Epoch 150/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0193 - pos_accuracy: 0.9772 - val_loss: 0.1276 - val_pos_accuracy: 0.8965\n",
      "Epoch 151/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0172 - pos_accuracy: 0.9814 - val_loss: 0.1211 - val_pos_accuracy: 0.8828\n",
      "Epoch 152/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0139 - pos_accuracy: 0.9838 - val_loss: 0.1233 - val_pos_accuracy: 0.8770\n",
      "Epoch 153/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0199 - pos_accuracy: 0.9826 - val_loss: 0.1325 - val_pos_accuracy: 0.8945\n",
      "Epoch 154/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0283 - pos_accuracy: 0.9760 - val_loss: 0.1697 - val_pos_accuracy: 0.8477\n",
      "Epoch 155/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0609 - pos_accuracy: 0.9243 - val_loss: 0.1354 - val_pos_accuracy: 0.8965\n",
      "Epoch 156/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0178 - pos_accuracy: 0.9838 - val_loss: 0.1253 - val_pos_accuracy: 0.8809\n",
      "Epoch 157/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0168 - pos_accuracy: 0.9838 - val_loss: 0.1247 - val_pos_accuracy: 0.8789\n",
      "Epoch 158/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0158 - pos_accuracy: 0.9808 - val_loss: 0.1280 - val_pos_accuracy: 0.8789\n",
      "Epoch 159/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0177 - pos_accuracy: 0.9844 - val_loss: 0.1192 - val_pos_accuracy: 0.8848\n",
      "Epoch 160/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0137 - pos_accuracy: 0.9838 - val_loss: 0.1213 - val_pos_accuracy: 0.8809\n",
      "Epoch 161/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0134 - pos_accuracy: 0.9856 - val_loss: 0.1229 - val_pos_accuracy: 0.8984\n",
      "Epoch 162/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0177 - pos_accuracy: 0.9844 - val_loss: 0.1213 - val_pos_accuracy: 0.8848\n",
      "Epoch 163/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0135 - pos_accuracy: 0.9844 - val_loss: 0.1184 - val_pos_accuracy: 0.9004\n",
      "Epoch 164/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0112 - pos_accuracy: 0.9862 - val_loss: 0.1223 - val_pos_accuracy: 0.9023\n",
      "Epoch 165/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0176 - pos_accuracy: 0.9856 - val_loss: 0.1292 - val_pos_accuracy: 0.8867\n",
      "Epoch 166/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0255 - pos_accuracy: 0.9826 - val_loss: 0.1243 - val_pos_accuracy: 0.9023\n",
      "Epoch 167/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0159 - pos_accuracy: 0.9862 - val_loss: 0.1200 - val_pos_accuracy: 0.8867\n",
      "Epoch 168/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0120 - pos_accuracy: 0.9862 - val_loss: 0.1245 - val_pos_accuracy: 0.9023\n",
      "Epoch 169/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0186 - pos_accuracy: 0.9868 - val_loss: 0.1268 - val_pos_accuracy: 0.8906\n",
      "Epoch 170/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0254 - pos_accuracy: 0.9838 - val_loss: 0.1373 - val_pos_accuracy: 0.8945\n",
      "Epoch 171/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0223 - pos_accuracy: 0.9862 - val_loss: 0.1374 - val_pos_accuracy: 0.8906\n",
      "Epoch 172/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0346 - pos_accuracy: 0.9844 - val_loss: 0.1265 - val_pos_accuracy: 0.9023\n",
      "Epoch 173/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0187 - pos_accuracy: 0.9856 - val_loss: 0.1218 - val_pos_accuracy: 0.8906\n",
      "Epoch 174/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0183 - pos_accuracy: 0.9856 - val_loss: 0.1239 - val_pos_accuracy: 0.9004\n",
      "Epoch 175/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0165 - pos_accuracy: 0.9862 - val_loss: 0.1222 - val_pos_accuracy: 0.8867\n",
      "Epoch 176/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0140 - pos_accuracy: 0.9874 - val_loss: 0.1202 - val_pos_accuracy: 0.9004\n",
      "Epoch 177/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0175 - pos_accuracy: 0.9862 - val_loss: 0.1354 - val_pos_accuracy: 0.8809\n",
      "Epoch 178/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0180 - pos_accuracy: 0.9868 - val_loss: 0.1311 - val_pos_accuracy: 0.9004\n",
      "Epoch 179/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0197 - pos_accuracy: 0.9838 - val_loss: 0.1391 - val_pos_accuracy: 0.8730\n",
      "Epoch 180/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0305 - pos_accuracy: 0.9802 - val_loss: 0.1291 - val_pos_accuracy: 0.8965\n",
      "Epoch 181/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0364 - pos_accuracy: 0.9675 - val_loss: 0.1379 - val_pos_accuracy: 0.8770\n",
      "Epoch 182/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0436 - pos_accuracy: 0.9706 - val_loss: 0.1490 - val_pos_accuracy: 0.8848\n",
      "Epoch 183/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0522 - pos_accuracy: 0.9213 - val_loss: 0.1539 - val_pos_accuracy: 0.8027\n",
      "Epoch 184/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0144 - pos_accuracy: 0.9856 - val_loss: 0.1146 - val_pos_accuracy: 0.9023\n",
      "Epoch 185/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0101 - pos_accuracy: 0.9880 - val_loss: 0.1155 - val_pos_accuracy: 0.8867\n",
      "Epoch 186/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0101 - pos_accuracy: 0.9886 - val_loss: 0.1143 - val_pos_accuracy: 0.9023\n",
      "Epoch 187/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0085 - pos_accuracy: 0.9868 - val_loss: 0.1128 - val_pos_accuracy: 0.9043\n",
      "Epoch 188/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0089 - pos_accuracy: 0.9886 - val_loss: 0.1153 - val_pos_accuracy: 0.9023\n",
      "Epoch 189/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0090 - pos_accuracy: 0.9886 - val_loss: 0.1143 - val_pos_accuracy: 0.9102\n",
      "Epoch 190/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0110 - pos_accuracy: 0.9880 - val_loss: 0.1157 - val_pos_accuracy: 0.9043\n",
      "Epoch 191/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0104 - pos_accuracy: 0.9874 - val_loss: 0.1172 - val_pos_accuracy: 0.8867\n",
      "Epoch 192/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0111 - pos_accuracy: 0.9886 - val_loss: 0.1129 - val_pos_accuracy: 0.9023\n",
      "Epoch 193/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0086 - pos_accuracy: 0.9886 - val_loss: 0.1134 - val_pos_accuracy: 0.9043\n",
      "Epoch 194/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0085 - pos_accuracy: 0.9880 - val_loss: 0.1134 - val_pos_accuracy: 0.9082\n",
      "Epoch 195/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0087 - pos_accuracy: 0.9880 - val_loss: 0.1127 - val_pos_accuracy: 0.9043\n",
      "Epoch 196/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0087 - pos_accuracy: 0.9880 - val_loss: 0.1156 - val_pos_accuracy: 0.9062\n",
      "Epoch 197/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0089 - pos_accuracy: 0.9880 - val_loss: 0.1144 - val_pos_accuracy: 0.9023\n",
      "Epoch 198/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0099 - pos_accuracy: 0.9892 - val_loss: 0.1125 - val_pos_accuracy: 0.9043\n",
      "Epoch 199/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0119 - pos_accuracy: 0.9868 - val_loss: 0.1205 - val_pos_accuracy: 0.9043\n",
      "Epoch 200/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0108 - pos_accuracy: 0.9880 - val_loss: 0.1134 - val_pos_accuracy: 0.9082\n",
      "Epoch 201/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0118 - pos_accuracy: 0.9886 - val_loss: 0.1169 - val_pos_accuracy: 0.9023\n",
      "Epoch 202/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0162 - pos_accuracy: 0.9886 - val_loss: 0.1210 - val_pos_accuracy: 0.9141\n",
      "Epoch 203/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0358 - pos_accuracy: 0.9880 - val_loss: 0.1336 - val_pos_accuracy: 0.9023\n",
      "Epoch 204/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0337 - pos_accuracy: 0.9850 - val_loss: 0.1237 - val_pos_accuracy: 0.9062\n",
      "Epoch 205/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0222 - pos_accuracy: 0.9880 - val_loss: 0.1255 - val_pos_accuracy: 0.9062\n",
      "Epoch 206/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0236 - pos_accuracy: 0.9850 - val_loss: 0.1315 - val_pos_accuracy: 0.8945\n",
      "Epoch 207/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0451 - pos_accuracy: 0.9405 - val_loss: 0.1316 - val_pos_accuracy: 0.9023\n",
      "Epoch 208/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0348 - pos_accuracy: 0.9645 - val_loss: 0.1416 - val_pos_accuracy: 0.8789\n",
      "Epoch 209/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0490 - pos_accuracy: 0.9261 - val_loss: 0.1570 - val_pos_accuracy: 0.8086\n",
      "Epoch 210/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0328 - pos_accuracy: 0.9724 - val_loss: 0.1444 - val_pos_accuracy: 0.8672\n",
      "Epoch 211/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0749 - pos_accuracy: 0.8161 - val_loss: 0.1412 - val_pos_accuracy: 0.8770\n",
      "Epoch 212/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0439 - pos_accuracy: 0.9339 - val_loss: 0.1171 - val_pos_accuracy: 0.9141\n",
      "Epoch 213/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0101 - pos_accuracy: 0.9892 - val_loss: 0.1115 - val_pos_accuracy: 0.9043\n",
      "Epoch 214/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0077 - pos_accuracy: 0.9880 - val_loss: 0.1101 - val_pos_accuracy: 0.9062\n",
      "Epoch 215/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0076 - pos_accuracy: 0.9892 - val_loss: 0.1119 - val_pos_accuracy: 0.9043\n",
      "Epoch 216/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0112 - pos_accuracy: 0.9904 - val_loss: 0.1121 - val_pos_accuracy: 0.9082\n",
      "Epoch 217/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0110 - pos_accuracy: 0.9910 - val_loss: 0.1117 - val_pos_accuracy: 0.9023\n",
      "Epoch 218/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0089 - pos_accuracy: 0.9898 - val_loss: 0.1141 - val_pos_accuracy: 0.9062\n",
      "Epoch 219/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0089 - pos_accuracy: 0.9910 - val_loss: 0.1099 - val_pos_accuracy: 0.9062\n",
      "Epoch 220/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0105 - pos_accuracy: 0.9898 - val_loss: 0.1128 - val_pos_accuracy: 0.9062\n",
      "Epoch 221/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0135 - pos_accuracy: 0.9904 - val_loss: 0.1181 - val_pos_accuracy: 0.9141\n",
      "Epoch 222/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0200 - pos_accuracy: 0.9892 - val_loss: 0.1396 - val_pos_accuracy: 0.8770\n",
      "Epoch 223/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0395 - pos_accuracy: 0.9651 - val_loss: 0.1236 - val_pos_accuracy: 0.9043\n",
      "Epoch 224/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0276 - pos_accuracy: 0.9916 - val_loss: 0.1323 - val_pos_accuracy: 0.8887\n",
      "Epoch 225/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0236 - pos_accuracy: 0.9862 - val_loss: 0.1182 - val_pos_accuracy: 0.9102\n",
      "Epoch 226/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0120 - pos_accuracy: 0.9928 - val_loss: 0.1101 - val_pos_accuracy: 0.9082\n",
      "Epoch 227/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0086 - pos_accuracy: 0.9934 - val_loss: 0.1111 - val_pos_accuracy: 0.9102\n",
      "Epoch 228/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0074 - pos_accuracy: 0.9922 - val_loss: 0.1104 - val_pos_accuracy: 0.9102\n",
      "Epoch 229/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0062 - pos_accuracy: 0.9934 - val_loss: 0.1089 - val_pos_accuracy: 0.9102\n",
      "Epoch 230/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0068 - pos_accuracy: 0.9928 - val_loss: 0.1125 - val_pos_accuracy: 0.9160\n",
      "Epoch 231/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0117 - pos_accuracy: 0.9922 - val_loss: 0.1137 - val_pos_accuracy: 0.9082\n",
      "Epoch 232/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0101 - pos_accuracy: 0.9934 - val_loss: 0.1117 - val_pos_accuracy: 0.9141\n",
      "Epoch 233/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0080 - pos_accuracy: 0.9928 - val_loss: 0.1099 - val_pos_accuracy: 0.9102\n",
      "Epoch 234/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0089 - pos_accuracy: 0.9934 - val_loss: 0.1085 - val_pos_accuracy: 0.9102\n",
      "Epoch 235/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0089 - pos_accuracy: 0.9934 - val_loss: 0.1102 - val_pos_accuracy: 0.9082\n",
      "Epoch 236/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0088 - pos_accuracy: 0.9940 - val_loss: 0.1084 - val_pos_accuracy: 0.9082\n",
      "Epoch 237/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0096 - pos_accuracy: 0.9946 - val_loss: 0.1149 - val_pos_accuracy: 0.9082\n",
      "Epoch 238/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0129 - pos_accuracy: 0.9940 - val_loss: 0.1096 - val_pos_accuracy: 0.9141\n",
      "Epoch 239/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0119 - pos_accuracy: 0.9946 - val_loss: 0.1110 - val_pos_accuracy: 0.9102\n",
      "Epoch 240/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0111 - pos_accuracy: 0.9934 - val_loss: 0.1159 - val_pos_accuracy: 0.9199\n",
      "Epoch 241/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0199 - pos_accuracy: 0.9940 - val_loss: 0.1205 - val_pos_accuracy: 0.9082\n",
      "Epoch 242/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0315 - pos_accuracy: 0.9718 - val_loss: 0.1205 - val_pos_accuracy: 0.8887\n",
      "Epoch 243/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0133 - pos_accuracy: 0.9946 - val_loss: 0.1203 - val_pos_accuracy: 0.9121\n",
      "Epoch 244/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0210 - pos_accuracy: 0.9946 - val_loss: 0.1265 - val_pos_accuracy: 0.8887\n",
      "Epoch 245/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0209 - pos_accuracy: 0.9922 - val_loss: 0.1161 - val_pos_accuracy: 0.9102\n",
      "Epoch 246/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0146 - pos_accuracy: 0.9958 - val_loss: 0.1104 - val_pos_accuracy: 0.9141\n",
      "Epoch 247/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0070 - pos_accuracy: 0.9946 - val_loss: 0.1084 - val_pos_accuracy: 0.9102\n",
      "Epoch 248/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0080 - pos_accuracy: 0.9946 - val_loss: 0.1135 - val_pos_accuracy: 0.9141\n",
      "Epoch 249/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0122 - pos_accuracy: 0.9952 - val_loss: 0.1106 - val_pos_accuracy: 0.9082\n",
      "Epoch 250/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0079 - pos_accuracy: 0.9946 - val_loss: 0.1082 - val_pos_accuracy: 0.9160\n",
      "Epoch 251/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0062 - pos_accuracy: 0.9952 - val_loss: 0.1092 - val_pos_accuracy: 0.9102\n",
      "Epoch 252/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0062 - pos_accuracy: 0.9952 - val_loss: 0.1070 - val_pos_accuracy: 0.9180\n",
      "Epoch 253/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0060 - pos_accuracy: 0.9946 - val_loss: 0.1087 - val_pos_accuracy: 0.9082\n",
      "Epoch 254/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0074 - pos_accuracy: 0.9946 - val_loss: 0.1063 - val_pos_accuracy: 0.9121\n",
      "Epoch 255/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0064 - pos_accuracy: 0.9946 - val_loss: 0.1084 - val_pos_accuracy: 0.9160\n",
      "Epoch 256/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0063 - pos_accuracy: 0.9952 - val_loss: 0.1078 - val_pos_accuracy: 0.9102\n",
      "Epoch 257/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0059 - pos_accuracy: 0.9940 - val_loss: 0.1075 - val_pos_accuracy: 0.9160\n",
      "Epoch 258/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0064 - pos_accuracy: 0.9952 - val_loss: 0.1074 - val_pos_accuracy: 0.9102\n",
      "Epoch 259/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0071 - pos_accuracy: 0.9958 - val_loss: 0.1066 - val_pos_accuracy: 0.9199\n",
      "Epoch 260/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0047 - pos_accuracy: 0.9952 - val_loss: 0.1075 - val_pos_accuracy: 0.9121\n",
      "Epoch 261/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0056 - pos_accuracy: 0.9946 - val_loss: 0.1075 - val_pos_accuracy: 0.9219\n",
      "Epoch 262/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0066 - pos_accuracy: 0.9958 - val_loss: 0.1102 - val_pos_accuracy: 0.9082\n",
      "Epoch 263/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0064 - pos_accuracy: 0.9946 - val_loss: 0.1076 - val_pos_accuracy: 0.9160\n",
      "Epoch 264/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0064 - pos_accuracy: 0.9952 - val_loss: 0.1093 - val_pos_accuracy: 0.9102\n",
      "Epoch 265/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0061 - pos_accuracy: 0.9934 - val_loss: 0.1080 - val_pos_accuracy: 0.9238\n",
      "Epoch 266/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0061 - pos_accuracy: 0.9952 - val_loss: 0.1101 - val_pos_accuracy: 0.9102\n",
      "Epoch 267/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0087 - pos_accuracy: 0.9964 - val_loss: 0.1063 - val_pos_accuracy: 0.9160\n",
      "Epoch 268/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0065 - pos_accuracy: 0.9946 - val_loss: 0.1072 - val_pos_accuracy: 0.9102\n",
      "Epoch 269/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0065 - pos_accuracy: 0.9952 - val_loss: 0.1107 - val_pos_accuracy: 0.9238\n",
      "Epoch 270/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0133 - pos_accuracy: 0.9952 - val_loss: 0.1131 - val_pos_accuracy: 0.9102\n",
      "Epoch 271/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0077 - pos_accuracy: 0.9940 - val_loss: 0.1064 - val_pos_accuracy: 0.9121\n",
      "Epoch 272/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0052 - pos_accuracy: 0.9958 - val_loss: 0.1089 - val_pos_accuracy: 0.9102\n",
      "Epoch 273/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0070 - pos_accuracy: 0.9964 - val_loss: 0.1069 - val_pos_accuracy: 0.9238\n",
      "Epoch 274/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0058 - pos_accuracy: 0.9958 - val_loss: 0.1055 - val_pos_accuracy: 0.9160\n",
      "Epoch 275/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0043 - pos_accuracy: 0.9946 - val_loss: 0.1049 - val_pos_accuracy: 0.9141\n",
      "Epoch 276/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0051 - pos_accuracy: 0.9964 - val_loss: 0.1069 - val_pos_accuracy: 0.9102\n",
      "Epoch 277/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0050 - pos_accuracy: 0.9970 - val_loss: 0.1053 - val_pos_accuracy: 0.9180\n",
      "Epoch 278/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0059 - pos_accuracy: 0.9964 - val_loss: 0.1085 - val_pos_accuracy: 0.9102\n",
      "Epoch 279/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0079 - pos_accuracy: 0.9964 - val_loss: 0.1053 - val_pos_accuracy: 0.9199\n",
      "Epoch 280/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0048 - pos_accuracy: 0.9964 - val_loss: 0.1054 - val_pos_accuracy: 0.9121\n",
      "Epoch 281/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0040 - pos_accuracy: 0.9970 - val_loss: 0.1046 - val_pos_accuracy: 0.9121\n",
      "Epoch 282/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0046 - pos_accuracy: 0.9964 - val_loss: 0.1055 - val_pos_accuracy: 0.9160\n",
      "Epoch 283/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0048 - pos_accuracy: 0.9970 - val_loss: 0.1065 - val_pos_accuracy: 0.9121\n",
      "Epoch 284/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0071 - pos_accuracy: 0.9958 - val_loss: 0.1064 - val_pos_accuracy: 0.9160\n",
      "Epoch 285/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0065 - pos_accuracy: 0.9952 - val_loss: 0.1084 - val_pos_accuracy: 0.9121\n",
      "Epoch 286/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0092 - pos_accuracy: 0.9964 - val_loss: 0.1062 - val_pos_accuracy: 0.9199\n",
      "Epoch 287/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0092 - pos_accuracy: 0.9976 - val_loss: 0.1078 - val_pos_accuracy: 0.9102\n",
      "Epoch 288/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0075 - pos_accuracy: 0.9970 - val_loss: 0.1077 - val_pos_accuracy: 0.9160\n",
      "Epoch 289/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0063 - pos_accuracy: 0.9970 - val_loss: 0.1061 - val_pos_accuracy: 0.9121\n",
      "Epoch 290/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0047 - pos_accuracy: 0.9976 - val_loss: 0.1045 - val_pos_accuracy: 0.9180\n",
      "Epoch 291/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0049 - pos_accuracy: 0.9976 - val_loss: 0.1064 - val_pos_accuracy: 0.9121\n",
      "Epoch 292/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0064 - pos_accuracy: 0.9976 - val_loss: 0.1073 - val_pos_accuracy: 0.9199\n",
      "Epoch 293/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0157 - pos_accuracy: 0.9970 - val_loss: 0.1203 - val_pos_accuracy: 0.9082\n",
      "Epoch 294/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0182 - pos_accuracy: 0.9970 - val_loss: 0.1156 - val_pos_accuracy: 0.9160\n",
      "Epoch 295/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0289 - pos_accuracy: 0.9946 - val_loss: 0.1243 - val_pos_accuracy: 0.9062\n",
      "Epoch 296/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0182 - pos_accuracy: 0.9946 - val_loss: 0.1119 - val_pos_accuracy: 0.9160\n",
      "Epoch 297/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0142 - pos_accuracy: 0.9976 - val_loss: 0.1184 - val_pos_accuracy: 0.9102\n",
      "Epoch 298/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0241 - pos_accuracy: 0.9946 - val_loss: 0.1157 - val_pos_accuracy: 0.9238\n",
      "Epoch 299/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0143 - pos_accuracy: 0.9970 - val_loss: 0.1061 - val_pos_accuracy: 0.9121\n",
      "Epoch 300/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0046 - pos_accuracy: 0.9976 - val_loss: 0.1044 - val_pos_accuracy: 0.9160\n",
      "Epoch 301/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0045 - pos_accuracy: 0.9976 - val_loss: 0.1050 - val_pos_accuracy: 0.9121\n",
      "Epoch 302/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0044 - pos_accuracy: 0.9976 - val_loss: 0.1028 - val_pos_accuracy: 0.9180\n",
      "Epoch 303/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0036 - pos_accuracy: 0.9970 - val_loss: 0.1039 - val_pos_accuracy: 0.9141\n",
      "Epoch 304/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0034 - pos_accuracy: 0.9976 - val_loss: 0.1034 - val_pos_accuracy: 0.9141\n",
      "Epoch 305/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0040 - pos_accuracy: 0.9976 - val_loss: 0.1029 - val_pos_accuracy: 0.9141\n",
      "Epoch 306/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0037 - pos_accuracy: 0.9976 - val_loss: 0.1035 - val_pos_accuracy: 0.9141\n",
      "Epoch 307/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0037 - pos_accuracy: 0.9976 - val_loss: 0.1025 - val_pos_accuracy: 0.9121\n",
      "Epoch 308/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0036 - pos_accuracy: 0.9976 - val_loss: 0.1030 - val_pos_accuracy: 0.9160\n",
      "Epoch 309/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0032 - pos_accuracy: 0.9976 - val_loss: 0.1045 - val_pos_accuracy: 0.9160\n",
      "Epoch 310/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0036 - pos_accuracy: 0.9970 - val_loss: 0.1026 - val_pos_accuracy: 0.9141\n",
      "Epoch 311/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0034 - pos_accuracy: 0.9970 - val_loss: 0.1042 - val_pos_accuracy: 0.9141\n",
      "Epoch 312/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0034 - pos_accuracy: 0.9976 - val_loss: 0.1023 - val_pos_accuracy: 0.9141\n",
      "Epoch 313/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0036 - pos_accuracy: 0.9976 - val_loss: 0.1037 - val_pos_accuracy: 0.9121\n",
      "Epoch 314/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0038 - pos_accuracy: 0.9976 - val_loss: 0.1023 - val_pos_accuracy: 0.9160\n",
      "Epoch 315/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0032 - pos_accuracy: 0.9976 - val_loss: 0.1046 - val_pos_accuracy: 0.9141\n",
      "Epoch 316/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0059 - pos_accuracy: 0.9970 - val_loss: 0.1025 - val_pos_accuracy: 0.9160\n",
      "Epoch 317/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0049 - pos_accuracy: 0.9988 - val_loss: 0.1027 - val_pos_accuracy: 0.9160\n",
      "Epoch 318/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0035 - pos_accuracy: 0.9982 - val_loss: 0.1022 - val_pos_accuracy: 0.9160\n",
      "Epoch 319/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0031 - pos_accuracy: 0.9988 - val_loss: 0.1045 - val_pos_accuracy: 0.9160\n",
      "Epoch 320/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0040 - pos_accuracy: 0.9988 - val_loss: 0.1034 - val_pos_accuracy: 0.9141\n",
      "Epoch 321/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0043 - pos_accuracy: 0.9988 - val_loss: 0.1034 - val_pos_accuracy: 0.9199\n",
      "Epoch 322/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0038 - pos_accuracy: 0.9988 - val_loss: 0.1040 - val_pos_accuracy: 0.9141\n",
      "Epoch 323/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0043 - pos_accuracy: 0.9982 - val_loss: 0.1047 - val_pos_accuracy: 0.9180\n",
      "Epoch 324/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0041 - pos_accuracy: 0.9988 - val_loss: 0.1023 - val_pos_accuracy: 0.9160\n",
      "Epoch 325/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0031 - pos_accuracy: 0.9988 - val_loss: 0.1021 - val_pos_accuracy: 0.9180\n",
      "Epoch 326/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0030 - pos_accuracy: 0.9988 - val_loss: 0.1031 - val_pos_accuracy: 0.9141\n",
      "Epoch 327/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0037 - pos_accuracy: 0.9988 - val_loss: 0.1036 - val_pos_accuracy: 0.9180\n",
      "Epoch 328/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0053 - pos_accuracy: 0.9988 - val_loss: 0.1054 - val_pos_accuracy: 0.9141\n",
      "Epoch 329/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0039 - pos_accuracy: 0.9988 - val_loss: 0.1026 - val_pos_accuracy: 0.9199\n",
      "Epoch 330/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0039 - pos_accuracy: 0.9982 - val_loss: 0.1026 - val_pos_accuracy: 0.9141\n",
      "Epoch 331/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0030 - pos_accuracy: 0.9988 - val_loss: 0.1029 - val_pos_accuracy: 0.9199\n",
      "Epoch 332/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0047 - pos_accuracy: 0.9988 - val_loss: 0.1035 - val_pos_accuracy: 0.9141\n",
      "Epoch 333/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0062 - pos_accuracy: 0.9988 - val_loss: 0.1102 - val_pos_accuracy: 0.9180\n",
      "Epoch 334/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0079 - pos_accuracy: 0.9988 - val_loss: 0.1100 - val_pos_accuracy: 0.9141\n",
      "Epoch 335/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0087 - pos_accuracy: 0.9988 - val_loss: 0.1078 - val_pos_accuracy: 0.9219\n",
      "Epoch 336/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0171 - pos_accuracy: 0.9976 - val_loss: 0.1145 - val_pos_accuracy: 0.9141\n",
      "Epoch 337/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0114 - pos_accuracy: 0.9988 - val_loss: 0.1094 - val_pos_accuracy: 0.9219\n",
      "Epoch 338/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0076 - pos_accuracy: 0.9988 - val_loss: 0.1028 - val_pos_accuracy: 0.9141\n",
      "Epoch 339/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0027 - pos_accuracy: 0.9988 - val_loss: 0.1014 - val_pos_accuracy: 0.9160\n",
      "Epoch 340/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0061 - pos_accuracy: 0.9988 - val_loss: 0.1047 - val_pos_accuracy: 0.9199\n",
      "Epoch 341/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0042 - pos_accuracy: 0.9988 - val_loss: 0.1011 - val_pos_accuracy: 0.9160\n",
      "Epoch 342/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0029 - pos_accuracy: 0.9988 - val_loss: 0.1019 - val_pos_accuracy: 0.9180\n",
      "Epoch 343/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0026 - pos_accuracy: 0.9988 - val_loss: 0.1010 - val_pos_accuracy: 0.9160\n",
      "Epoch 344/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0026 - pos_accuracy: 0.9988 - val_loss: 0.1009 - val_pos_accuracy: 0.9160\n",
      "Epoch 345/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0024 - pos_accuracy: 0.9988 - val_loss: 0.1008 - val_pos_accuracy: 0.9180\n",
      "Epoch 346/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0025 - pos_accuracy: 0.9988 - val_loss: 0.1007 - val_pos_accuracy: 0.9160\n",
      "Epoch 347/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0026 - pos_accuracy: 0.9988 - val_loss: 0.1019 - val_pos_accuracy: 0.9160\n",
      "Epoch 348/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0028 - pos_accuracy: 0.9988 - val_loss: 0.1011 - val_pos_accuracy: 0.9160\n",
      "Epoch 349/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0028 - pos_accuracy: 0.9988 - val_loss: 0.1013 - val_pos_accuracy: 0.9160\n",
      "Epoch 350/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0025 - pos_accuracy: 0.9988 - val_loss: 0.1004 - val_pos_accuracy: 0.9180\n",
      "Epoch 351/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0026 - pos_accuracy: 0.9988 - val_loss: 0.1017 - val_pos_accuracy: 0.9160\n",
      "Epoch 352/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0030 - pos_accuracy: 0.9982 - val_loss: 0.1012 - val_pos_accuracy: 0.9160\n",
      "Epoch 353/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0031 - pos_accuracy: 0.9988 - val_loss: 0.1019 - val_pos_accuracy: 0.9180\n",
      "Epoch 354/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0028 - pos_accuracy: 0.9988 - val_loss: 0.1003 - val_pos_accuracy: 0.9160\n",
      "Epoch 355/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0025 - pos_accuracy: 0.9988 - val_loss: 0.1009 - val_pos_accuracy: 0.9180\n",
      "Epoch 356/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0027 - pos_accuracy: 0.9988 - val_loss: 0.1011 - val_pos_accuracy: 0.9160\n",
      "Epoch 357/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0029 - pos_accuracy: 0.9988 - val_loss: 0.1012 - val_pos_accuracy: 0.9180\n",
      "Epoch 358/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0024 - pos_accuracy: 0.9988 - val_loss: 0.1004 - val_pos_accuracy: 0.9160\n",
      "Epoch 359/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0023 - pos_accuracy: 0.9988 - val_loss: 0.1013 - val_pos_accuracy: 0.9180\n",
      "Epoch 360/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0028 - pos_accuracy: 0.9988 - val_loss: 0.1008 - val_pos_accuracy: 0.9121\n",
      "Epoch 361/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0043 - pos_accuracy: 0.9988 - val_loss: 0.1033 - val_pos_accuracy: 0.9160\n",
      "Epoch 362/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0048 - pos_accuracy: 0.9988 - val_loss: 0.1019 - val_pos_accuracy: 0.9141\n",
      "Epoch 363/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0052 - pos_accuracy: 0.9988 - val_loss: 0.1041 - val_pos_accuracy: 0.9180\n",
      "Epoch 364/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0042 - pos_accuracy: 0.9988 - val_loss: 0.1008 - val_pos_accuracy: 0.9141\n",
      "Epoch 365/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0044 - pos_accuracy: 0.9988 - val_loss: 0.1022 - val_pos_accuracy: 0.9180\n",
      "Epoch 366/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0030 - pos_accuracy: 0.9988 - val_loss: 0.1006 - val_pos_accuracy: 0.9160\n",
      "Epoch 367/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0035 - pos_accuracy: 0.9988 - val_loss: 0.1035 - val_pos_accuracy: 0.9180\n",
      "Epoch 368/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0040 - pos_accuracy: 0.9988 - val_loss: 0.1039 - val_pos_accuracy: 0.9121\n",
      "Epoch 369/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0100 - pos_accuracy: 0.9988 - val_loss: 0.1097 - val_pos_accuracy: 0.9160\n",
      "Epoch 370/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0100 - pos_accuracy: 0.9988 - val_loss: 0.1047 - val_pos_accuracy: 0.9141\n",
      "Epoch 371/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0053 - pos_accuracy: 0.9988 - val_loss: 0.1031 - val_pos_accuracy: 0.9180\n",
      "Epoch 372/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0041 - pos_accuracy: 0.9988 - val_loss: 0.1017 - val_pos_accuracy: 0.9141\n",
      "Epoch 373/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0040 - pos_accuracy: 0.9988 - val_loss: 0.1027 - val_pos_accuracy: 0.9219\n",
      "Epoch 374/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0043 - pos_accuracy: 0.9988 - val_loss: 0.1012 - val_pos_accuracy: 0.9160\n",
      "Epoch 375/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9988 - val_loss: 0.1004 - val_pos_accuracy: 0.9219\n",
      "Epoch 376/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9988 - val_loss: 0.1010 - val_pos_accuracy: 0.9141\n",
      "Epoch 377/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0026 - pos_accuracy: 0.9982 - val_loss: 0.0995 - val_pos_accuracy: 0.9180\n",
      "Epoch 378/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0026 - pos_accuracy: 0.9988 - val_loss: 0.0999 - val_pos_accuracy: 0.9160\n",
      "Epoch 379/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0026 - pos_accuracy: 0.9988 - val_loss: 0.0994 - val_pos_accuracy: 0.9219\n",
      "Epoch 380/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0029 - pos_accuracy: 0.9988 - val_loss: 0.1024 - val_pos_accuracy: 0.9141\n",
      "Epoch 381/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0038 - pos_accuracy: 0.9988 - val_loss: 0.1019 - val_pos_accuracy: 0.9199\n",
      "Epoch 382/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0041 - pos_accuracy: 0.9988 - val_loss: 0.1049 - val_pos_accuracy: 0.9141\n",
      "Epoch 383/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0040 - pos_accuracy: 0.9988 - val_loss: 0.1010 - val_pos_accuracy: 0.9199\n",
      "Epoch 384/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0054 - pos_accuracy: 0.9988 - val_loss: 0.1002 - val_pos_accuracy: 0.9141\n",
      "Epoch 385/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0022 - pos_accuracy: 0.9988 - val_loss: 0.0994 - val_pos_accuracy: 0.9180\n",
      "Epoch 386/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0018 - pos_accuracy: 0.9988 - val_loss: 0.0990 - val_pos_accuracy: 0.9180\n",
      "Epoch 387/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0026 - pos_accuracy: 0.9988 - val_loss: 0.1009 - val_pos_accuracy: 0.9180\n",
      "Epoch 388/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0024 - pos_accuracy: 0.9988 - val_loss: 0.0992 - val_pos_accuracy: 0.9180\n",
      "Epoch 389/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0020 - pos_accuracy: 0.9988 - val_loss: 0.0996 - val_pos_accuracy: 0.9180\n",
      "Epoch 390/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0021 - pos_accuracy: 0.9988 - val_loss: 0.0995 - val_pos_accuracy: 0.9180\n",
      "Epoch 391/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0028 - pos_accuracy: 0.9988 - val_loss: 0.1026 - val_pos_accuracy: 0.9141\n",
      "Epoch 392/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0034 - pos_accuracy: 0.9988 - val_loss: 0.0993 - val_pos_accuracy: 0.9180\n",
      "Epoch 393/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0036 - pos_accuracy: 0.9988 - val_loss: 0.1018 - val_pos_accuracy: 0.9141\n",
      "Epoch 394/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0030 - pos_accuracy: 0.9988 - val_loss: 0.0991 - val_pos_accuracy: 0.9180\n",
      "Epoch 395/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0038 - pos_accuracy: 0.9988 - val_loss: 0.1021 - val_pos_accuracy: 0.9141\n",
      "Epoch 396/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0023 - pos_accuracy: 0.9988 - val_loss: 0.0986 - val_pos_accuracy: 0.9180\n",
      "Epoch 397/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0037 - pos_accuracy: 0.9988 - val_loss: 0.1013 - val_pos_accuracy: 0.9160\n",
      "Epoch 398/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0037 - pos_accuracy: 0.9988 - val_loss: 0.1008 - val_pos_accuracy: 0.9199\n",
      "Epoch 399/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0112 - pos_accuracy: 0.9988 - val_loss: 0.1011 - val_pos_accuracy: 0.9160\n",
      "Epoch 400/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0039 - pos_accuracy: 0.9988 - val_loss: 0.0994 - val_pos_accuracy: 0.9219\n",
      "Epoch 401/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0029 - pos_accuracy: 0.9988 - val_loss: 0.0999 - val_pos_accuracy: 0.9141\n",
      "Epoch 402/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0022 - pos_accuracy: 0.9988 - val_loss: 0.0992 - val_pos_accuracy: 0.9180\n",
      "Epoch 403/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0032 - pos_accuracy: 0.9988 - val_loss: 0.1019 - val_pos_accuracy: 0.9160\n",
      "Epoch 404/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0037 - pos_accuracy: 0.9988 - val_loss: 0.0998 - val_pos_accuracy: 0.9219\n",
      "Epoch 405/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0047 - pos_accuracy: 0.9988 - val_loss: 0.1010 - val_pos_accuracy: 0.9160\n",
      "Epoch 406/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0040 - pos_accuracy: 0.9988 - val_loss: 0.1026 - val_pos_accuracy: 0.9199\n",
      "Epoch 407/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0091 - pos_accuracy: 0.9988 - val_loss: 0.1130 - val_pos_accuracy: 0.9141\n",
      "Epoch 408/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0168 - pos_accuracy: 0.9964 - val_loss: 0.1191 - val_pos_accuracy: 0.9023\n",
      "Epoch 409/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0360 - pos_accuracy: 0.9844 - val_loss: 0.1406 - val_pos_accuracy: 0.8867\n",
      "Epoch 410/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0356 - pos_accuracy: 0.9760 - val_loss: 0.1152 - val_pos_accuracy: 0.9023\n",
      "Epoch 411/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0133 - pos_accuracy: 0.9988 - val_loss: 0.1179 - val_pos_accuracy: 0.9141\n",
      "Epoch 412/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0244 - pos_accuracy: 0.9934 - val_loss: 0.1067 - val_pos_accuracy: 0.9238\n",
      "Epoch 413/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0152 - pos_accuracy: 0.9988 - val_loss: 0.1122 - val_pos_accuracy: 0.9141\n",
      "Epoch 414/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0153 - pos_accuracy: 0.9976 - val_loss: 0.1108 - val_pos_accuracy: 0.9023\n",
      "Epoch 415/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0130 - pos_accuracy: 0.9988 - val_loss: 0.1162 - val_pos_accuracy: 0.9141\n",
      "Epoch 416/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0207 - pos_accuracy: 0.9964 - val_loss: 0.1228 - val_pos_accuracy: 0.8945\n",
      "Epoch 417/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0291 - pos_accuracy: 0.9844 - val_loss: 0.1216 - val_pos_accuracy: 0.9082\n",
      "Epoch 418/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0234 - pos_accuracy: 0.9976 - val_loss: 0.1163 - val_pos_accuracy: 0.8984\n",
      "Epoch 419/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0415 - pos_accuracy: 0.9507 - val_loss: 0.1385 - val_pos_accuracy: 0.8809\n",
      "Epoch 420/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0393 - pos_accuracy: 0.9657 - val_loss: 0.1652 - val_pos_accuracy: 0.7793\n",
      "Epoch 421/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0563 - pos_accuracy: 0.9105 - val_loss: 0.1684 - val_pos_accuracy: 0.7500\n",
      "Epoch 422/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0416 - pos_accuracy: 0.9339 - val_loss: 0.1057 - val_pos_accuracy: 0.9023\n",
      "Epoch 423/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0052 - pos_accuracy: 0.9988 - val_loss: 0.1007 - val_pos_accuracy: 0.9141\n",
      "Epoch 424/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0052 - pos_accuracy: 0.9988 - val_loss: 0.0981 - val_pos_accuracy: 0.9180\n",
      "Epoch 425/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0023 - pos_accuracy: 0.9988 - val_loss: 0.0992 - val_pos_accuracy: 0.9160\n",
      "Epoch 426/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0021 - pos_accuracy: 0.9988 - val_loss: 0.0978 - val_pos_accuracy: 0.9160\n",
      "Epoch 427/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0028 - pos_accuracy: 0.9988 - val_loss: 0.0992 - val_pos_accuracy: 0.9180\n",
      "Epoch 428/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0020 - pos_accuracy: 0.9988 - val_loss: 0.0979 - val_pos_accuracy: 0.9160\n",
      "Epoch 429/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0018 - pos_accuracy: 0.9988 - val_loss: 0.0990 - val_pos_accuracy: 0.9180\n",
      "Epoch 430/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0020 - pos_accuracy: 0.9988 - val_loss: 0.0980 - val_pos_accuracy: 0.9219\n",
      "Epoch 431/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 0.9988 - val_loss: 0.0991 - val_pos_accuracy: 0.9180\n",
      "Epoch 432/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0025 - pos_accuracy: 0.9988 - val_loss: 0.0977 - val_pos_accuracy: 0.9199\n",
      "Epoch 433/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0016 - pos_accuracy: 0.9988 - val_loss: 0.0981 - val_pos_accuracy: 0.9180\n",
      "Epoch 434/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0016 - pos_accuracy: 0.9988 - val_loss: 0.0976 - val_pos_accuracy: 0.9160\n",
      "Epoch 435/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0015 - pos_accuracy: 0.9988 - val_loss: 0.0978 - val_pos_accuracy: 0.9160\n",
      "Epoch 436/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0016 - pos_accuracy: 0.9982 - val_loss: 0.0981 - val_pos_accuracy: 0.9160\n",
      "Epoch 437/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0030 - pos_accuracy: 0.9988 - val_loss: 0.1004 - val_pos_accuracy: 0.9180\n",
      "Epoch 438/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0053 - pos_accuracy: 0.9988 - val_loss: 0.0990 - val_pos_accuracy: 0.9141\n",
      "Epoch 439/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0025 - pos_accuracy: 0.9988 - val_loss: 0.0989 - val_pos_accuracy: 0.9199\n",
      "Epoch 440/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0024 - pos_accuracy: 0.9988 - val_loss: 0.0983 - val_pos_accuracy: 0.9160\n",
      "Epoch 441/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0018 - pos_accuracy: 0.9988 - val_loss: 0.0980 - val_pos_accuracy: 0.9180\n",
      "Epoch 442/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0017 - pos_accuracy: 0.9988 - val_loss: 0.0984 - val_pos_accuracy: 0.9160\n",
      "Epoch 443/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0017 - pos_accuracy: 0.9988 - val_loss: 0.0981 - val_pos_accuracy: 0.9199\n",
      "Epoch 444/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 0.9988 - val_loss: 0.0974 - val_pos_accuracy: 0.9160\n",
      "Epoch 445/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0017 - pos_accuracy: 0.9988 - val_loss: 0.0980 - val_pos_accuracy: 0.9180\n",
      "Epoch 446/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 0.9988 - val_loss: 0.0979 - val_pos_accuracy: 0.9141\n",
      "Epoch 447/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 0.9988 - val_loss: 0.0983 - val_pos_accuracy: 0.9180\n",
      "Epoch 448/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0021 - pos_accuracy: 0.9988 - val_loss: 0.0980 - val_pos_accuracy: 0.9160\n",
      "Epoch 449/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0023 - pos_accuracy: 0.9982 - val_loss: 0.0987 - val_pos_accuracy: 0.9199\n",
      "Epoch 450/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0029 - pos_accuracy: 0.9988 - val_loss: 0.0989 - val_pos_accuracy: 0.9141\n",
      "Epoch 451/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0026 - pos_accuracy: 0.9988 - val_loss: 0.0981 - val_pos_accuracy: 0.9199\n",
      "Epoch 452/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 0.9988 - val_loss: 0.0981 - val_pos_accuracy: 0.9180\n",
      "Epoch 453/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0022 - pos_accuracy: 0.9988 - val_loss: 0.0998 - val_pos_accuracy: 0.9199\n",
      "Epoch 454/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0053 - pos_accuracy: 0.9988 - val_loss: 0.0996 - val_pos_accuracy: 0.9160\n",
      "Epoch 455/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0023 - pos_accuracy: 0.9988 - val_loss: 0.0982 - val_pos_accuracy: 0.9180\n",
      "Epoch 456/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0018 - pos_accuracy: 0.9988 - val_loss: 0.0971 - val_pos_accuracy: 0.9160\n",
      "Epoch 457/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0016 - pos_accuracy: 0.9988 - val_loss: 0.0985 - val_pos_accuracy: 0.9180\n",
      "Epoch 458/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0017 - pos_accuracy: 0.9988 - val_loss: 0.0972 - val_pos_accuracy: 0.9160\n",
      "Epoch 459/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0018 - pos_accuracy: 0.9988 - val_loss: 0.0974 - val_pos_accuracy: 0.9180\n",
      "Epoch 460/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0017 - pos_accuracy: 0.9988 - val_loss: 0.0970 - val_pos_accuracy: 0.9160\n",
      "Epoch 461/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0016 - pos_accuracy: 0.9988 - val_loss: 0.0978 - val_pos_accuracy: 0.9180\n",
      "Epoch 462/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0017 - pos_accuracy: 0.9982 - val_loss: 0.0975 - val_pos_accuracy: 0.9160\n",
      "Epoch 463/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0019 - pos_accuracy: 0.9988 - val_loss: 0.0977 - val_pos_accuracy: 0.9180\n",
      "Epoch 464/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0017 - pos_accuracy: 0.9988 - val_loss: 0.0974 - val_pos_accuracy: 0.9180\n",
      "Epoch 465/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0018 - pos_accuracy: 0.9988 - val_loss: 0.0978 - val_pos_accuracy: 0.9180\n",
      "Epoch 466/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0017 - pos_accuracy: 0.9988 - val_loss: 0.0970 - val_pos_accuracy: 0.9160\n",
      "Epoch 467/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0018 - pos_accuracy: 0.9988 - val_loss: 0.0976 - val_pos_accuracy: 0.9180\n",
      "Epoch 468/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0017 - pos_accuracy: 0.9988 - val_loss: 0.0967 - val_pos_accuracy: 0.9160\n",
      "Epoch 469/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0017 - pos_accuracy: 0.9988 - val_loss: 0.0986 - val_pos_accuracy: 0.9199\n",
      "Epoch 470/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0026 - pos_accuracy: 0.9988 - val_loss: 0.0979 - val_pos_accuracy: 0.9141\n",
      "Epoch 471/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0041 - pos_accuracy: 0.9988 - val_loss: 0.0985 - val_pos_accuracy: 0.9199\n",
      "Epoch 472/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0022 - pos_accuracy: 0.9988 - val_loss: 0.0986 - val_pos_accuracy: 0.9141\n",
      "Epoch 473/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0047 - pos_accuracy: 0.9988 - val_loss: 0.1007 - val_pos_accuracy: 0.9199\n",
      "Epoch 474/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0049 - pos_accuracy: 0.9988 - val_loss: 0.0984 - val_pos_accuracy: 0.9180\n",
      "Epoch 475/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0032 - pos_accuracy: 0.9988 - val_loss: 0.0987 - val_pos_accuracy: 0.9180\n",
      "Epoch 476/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0038 - pos_accuracy: 0.9988 - val_loss: 0.0975 - val_pos_accuracy: 0.9180\n",
      "Epoch 477/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0037 - pos_accuracy: 0.9988 - val_loss: 0.0981 - val_pos_accuracy: 0.9199\n",
      "Epoch 478/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0019 - pos_accuracy: 0.9982 - val_loss: 0.0969 - val_pos_accuracy: 0.9160\n",
      "Epoch 479/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0012 - pos_accuracy: 0.9988 - val_loss: 0.0967 - val_pos_accuracy: 0.9160\n",
      "Epoch 480/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0967 - val_pos_accuracy: 0.9160\n",
      "Epoch 481/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0968 - val_pos_accuracy: 0.9180\n",
      "Epoch 482/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0012 - pos_accuracy: 0.9988 - val_loss: 0.0967 - val_pos_accuracy: 0.9180\n",
      "Epoch 483/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0968 - val_pos_accuracy: 0.9180\n",
      "Epoch 484/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0966 - val_pos_accuracy: 0.9180\n",
      "Epoch 485/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0965 - val_pos_accuracy: 0.9180\n",
      "Epoch 486/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0964 - val_pos_accuracy: 0.9180\n",
      "Epoch 487/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0965 - val_pos_accuracy: 0.9160\n",
      "Epoch 488/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0965 - val_pos_accuracy: 0.9180\n",
      "Epoch 489/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0964 - val_pos_accuracy: 0.9160\n",
      "Epoch 490/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0965 - val_pos_accuracy: 0.9180\n",
      "Epoch 491/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0963 - val_pos_accuracy: 0.9180\n",
      "Epoch 492/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0963 - val_pos_accuracy: 0.9160\n",
      "Epoch 493/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0964 - val_pos_accuracy: 0.9160\n",
      "Epoch 494/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0964 - val_pos_accuracy: 0.9180\n",
      "Epoch 495/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0010 - pos_accuracy: 0.9988 - val_loss: 0.0965 - val_pos_accuracy: 0.9180\n",
      "Epoch 496/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0010 - pos_accuracy: 0.9988 - val_loss: 0.0963 - val_pos_accuracy: 0.9180\n",
      "Epoch 497/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0010 - pos_accuracy: 0.9988 - val_loss: 0.0964 - val_pos_accuracy: 0.9180\n",
      "Epoch 498/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0010 - pos_accuracy: 0.9982 - val_loss: 0.0963 - val_pos_accuracy: 0.9160\n",
      "Epoch 499/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0011 - pos_accuracy: 0.9988 - val_loss: 0.0965 - val_pos_accuracy: 0.9180\n",
      "Epoch 500/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0012 - pos_accuracy: 0.9988 - val_loss: 0.0968 - val_pos_accuracy: 0.9160\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:14:22.255256: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1730: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\\nnp.concateneate 메소드는 행렬을 연결해주는 메소드이다.\\n이때 axis는 행렬을 연결할 방향(평행, 수직)을 지정해주는 변수이다.\\naxis의 값이 0일때 기존 행렬의 평행 방향(오른쪽에서)으로 연결시키고\\n1일때는 수직 방향(아래에서) 연결시킨다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\\nsubplot 메소드는 현재 figure에 축을 추가하거나 정의하는 메소드이다.\\n이때 입력 파라메터로 지정되는 세자리의 정수값은 머리부터 각각 '행', '열', '인덱스'를 의미한다.\\nsubplot(121)의 의미는 현재 figure에 1x2행렬 중 1번째의 축의 값을 정의하기 위해서 사용된다.\\nsubplot(122)는 같은 행렬(1x2)의 2번째 축의 값을 정의하기 위해 사용되었다. \\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\nmodel.fit함수의 metrix 콜백함수로 pos_accuracy를 넘겨 준 경우,\\ny_true값과 x_pred값의 논리합 연산을 수행해 pos_accuracy의 값을 추정하기 위한 처리.\\n이때 axis가 1일 경우 수평방향으로 논리합의 연산자를 대입하게 된다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n한 에포크에서 실시한 학습 횟수의 평균값을 산출하여, pos_accuracy로 출력하기 위해서\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n은닉층에서 연산된 아날로그 값이 최중 출력층을 통해 출력될 때 디지털 값으로 변환시키기 위해서.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n모델의 과적합을 피하기 위해서\\n정해진 시험데이터만을 가지고 모델을 학습시키다 보면, 시험 데이터에 최적화되지만, 실제 데이터를 응요할 때 무의미한 모델이 되버린다.\\n이를 방지하기 위해서 매 학습 시에 랜덤한 값을 생성하여 이를 통해 무작위 데이터를 통한 모델 학습을 실시하기 위함.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.916\n",
      "}\"report1/장경찬_46049.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/장경찬_46049.ipynb to python\n",
      "[NbConvertApp] Writing 36792 bytes to report1/장경찬_46049.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[5. 6.]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:214: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:249: RuntimeWarning: overflow encountered in matmul\n",
      "  x = np.matmul(w, x.transpose())\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:257: RuntimeWarning: overflow encountered in matmul\n",
      "  x = np.matmul(w, x.transpose())\n",
      "52\n",
      "422\n",
      "(3, 2)\n",
      "(2, 3)\n",
      "(2, 3)\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -10. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00 -6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 290494.44it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:14:29.707946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:14:30.134993: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:14:30.135033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:14:30.359535: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:14:30.971511: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 4ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 114006.63it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,386\n",
      "Trainable params: 52,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "(32,)\n",
      "(32,)\n",
      "50/50 [==============================] - ETA: 0s - loss: 36.3811 - pos_accuracy: 0.0538    (None,)\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 36.3811 - pos_accuracy: 0.0538 - val_loss: 2.3778 - val_pos_accuracy: 0.1707\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.6415 - pos_accuracy: 0.1625 - val_loss: 1.2366 - val_pos_accuracy: 0.2788\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/step - loss: 1.4005 - pos_accuracy: 0.2125 - val_loss: 2.2521 - val_pos_accuracy: 0.1178\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.8798 - pos_accuracy: 0.3044 - val_loss: 0.9561 - val_pos_accuracy: 0.2957\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.7480 - pos_accuracy: 0.3400 - val_loss: 0.9755 - val_pos_accuracy: 0.2692\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5764 - pos_accuracy: 0.4250 - val_loss: 0.6174 - val_pos_accuracy: 0.4231\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4400 - pos_accuracy: 0.4656 - val_loss: 0.5035 - val_pos_accuracy: 0.5120\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3797 - pos_accuracy: 0.4956 - val_loss: 0.5183 - val_pos_accuracy: 0.5312\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.3255 - pos_accuracy: 0.5756 - val_loss: 0.4384 - val_pos_accuracy: 0.5745\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2704 - pos_accuracy: 0.6187 - val_loss: 0.4327 - val_pos_accuracy: 0.4952\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2142 - pos_accuracy: 0.6869 - val_loss: 0.3792 - val_pos_accuracy: 0.5938\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.2056 - pos_accuracy: 0.6750 - val_loss: 0.3033 - val_pos_accuracy: 0.6851\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1810 - pos_accuracy: 0.7181 - val_loss: 0.3242 - val_pos_accuracy: 0.6514\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1701 - pos_accuracy: 0.7181 - val_loss: 0.2832 - val_pos_accuracy: 0.7163\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1233 - pos_accuracy: 0.8281 - val_loss: 0.3196 - val_pos_accuracy: 0.6755\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1179 - pos_accuracy: 0.8194 - val_loss: 0.4098 - val_pos_accuracy: 0.5889\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1384 - pos_accuracy: 0.7975 - val_loss: 0.2801 - val_pos_accuracy: 0.6971\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0960 - pos_accuracy: 0.8775 - val_loss: 0.2499 - val_pos_accuracy: 0.7909\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0864 - pos_accuracy: 0.8813 - val_loss: 0.2505 - val_pos_accuracy: 0.7692\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0803 - pos_accuracy: 0.8919 - val_loss: 0.2455 - val_pos_accuracy: 0.7885\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0770 - pos_accuracy: 0.8994 - val_loss: 0.2185 - val_pos_accuracy: 0.8197\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0782 - pos_accuracy: 0.8856 - val_loss: 0.2178 - val_pos_accuracy: 0.8077\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0624 - pos_accuracy: 0.9244 - val_loss: 0.2601 - val_pos_accuracy: 0.7067\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0550 - pos_accuracy: 0.9381 - val_loss: 0.2115 - val_pos_accuracy: 0.8389\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0492 - pos_accuracy: 0.9438 - val_loss: 0.2335 - val_pos_accuracy: 0.7837\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0509 - pos_accuracy: 0.9444 - val_loss: 0.2073 - val_pos_accuracy: 0.8510\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0390 - pos_accuracy: 0.9613 - val_loss: 0.2008 - val_pos_accuracy: 0.8438\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0390 - pos_accuracy: 0.9588 - val_loss: 0.1919 - val_pos_accuracy: 0.8510\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0367 - pos_accuracy: 0.9619 - val_loss: 0.1889 - val_pos_accuracy: 0.8534\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0339 - pos_accuracy: 0.9638 - val_loss: 0.2346 - val_pos_accuracy: 0.7620\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0391 - pos_accuracy: 0.9500 - val_loss: 0.1832 - val_pos_accuracy: 0.8582\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0318 - pos_accuracy: 0.9650 - val_loss: 0.1916 - val_pos_accuracy: 0.8582\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0312 - pos_accuracy: 0.9663 - val_loss: 0.1799 - val_pos_accuracy: 0.8606\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0253 - pos_accuracy: 0.9756 - val_loss: 0.1751 - val_pos_accuracy: 0.8582\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0248 - pos_accuracy: 0.9719 - val_loss: 0.1775 - val_pos_accuracy: 0.8606\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0242 - pos_accuracy: 0.9750 - val_loss: 0.1787 - val_pos_accuracy: 0.8630\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0227 - pos_accuracy: 0.9775 - val_loss: 0.1735 - val_pos_accuracy: 0.8654\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0254 - pos_accuracy: 0.9794 - val_loss: 0.1740 - val_pos_accuracy: 0.8726\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0203 - pos_accuracy: 0.9781 - val_loss: 0.1796 - val_pos_accuracy: 0.8654\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0172 - pos_accuracy: 0.9812 - val_loss: 0.1719 - val_pos_accuracy: 0.8750\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0161 - pos_accuracy: 0.9800 - val_loss: 0.1685 - val_pos_accuracy: 0.8702\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0154 - pos_accuracy: 0.9869 - val_loss: 0.1699 - val_pos_accuracy: 0.8726\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9856 - val_loss: 0.1660 - val_pos_accuracy: 0.8726\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0134 - pos_accuracy: 0.9881 - val_loss: 0.1668 - val_pos_accuracy: 0.8774\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0143 - pos_accuracy: 0.9894 - val_loss: 0.1643 - val_pos_accuracy: 0.8822\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0135 - pos_accuracy: 0.9894 - val_loss: 0.1696 - val_pos_accuracy: 0.8822\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0116 - pos_accuracy: 0.9919 - val_loss: 0.1675 - val_pos_accuracy: 0.8822\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0128 - pos_accuracy: 0.9912 - val_loss: 0.1636 - val_pos_accuracy: 0.8822\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9912 - val_loss: 0.1627 - val_pos_accuracy: 0.8846\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0103 - pos_accuracy: 0.9919 - val_loss: 0.1613 - val_pos_accuracy: 0.8870\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9937 - val_loss: 0.1626 - val_pos_accuracy: 0.8918\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0102 - pos_accuracy: 0.9925 - val_loss: 0.1597 - val_pos_accuracy: 0.8846\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9956 - val_loss: 0.1616 - val_pos_accuracy: 0.8846\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0085 - pos_accuracy: 0.9950 - val_loss: 0.1604 - val_pos_accuracy: 0.8894\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9950 - val_loss: 0.1600 - val_pos_accuracy: 0.8918\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9956 - val_loss: 0.1585 - val_pos_accuracy: 0.8942\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9956 - val_loss: 0.1582 - val_pos_accuracy: 0.8942\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9956 - val_loss: 0.1579 - val_pos_accuracy: 0.8894\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0077 - pos_accuracy: 0.9956 - val_loss: 0.1573 - val_pos_accuracy: 0.8918\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0068 - pos_accuracy: 0.9962 - val_loss: 0.1593 - val_pos_accuracy: 0.8894\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9962 - val_loss: 0.1564 - val_pos_accuracy: 0.8942\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9969 - val_loss: 0.1578 - val_pos_accuracy: 0.8894\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9962 - val_loss: 0.1550 - val_pos_accuracy: 0.8894\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9969 - val_loss: 0.1579 - val_pos_accuracy: 0.8918\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0053 - pos_accuracy: 0.9969 - val_loss: 0.1568 - val_pos_accuracy: 0.8894\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9969 - val_loss: 0.1561 - val_pos_accuracy: 0.8942\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9969 - val_loss: 0.1544 - val_pos_accuracy: 0.8942\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9969 - val_loss: 0.1539 - val_pos_accuracy: 0.8942\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9962 - val_loss: 0.1535 - val_pos_accuracy: 0.8942\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9969 - val_loss: 0.1539 - val_pos_accuracy: 0.8942\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9969 - val_loss: 0.1535 - val_pos_accuracy: 0.8942\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9969 - val_loss: 0.1522 - val_pos_accuracy: 0.8942\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9975 - val_loss: 0.1513 - val_pos_accuracy: 0.8942\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9969 - val_loss: 0.1527 - val_pos_accuracy: 0.8894\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9975 - val_loss: 0.1512 - val_pos_accuracy: 0.8942\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9975 - val_loss: 0.1550 - val_pos_accuracy: 0.8894\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9975 - val_loss: 0.1540 - val_pos_accuracy: 0.8894\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9987 - val_loss: 0.1512 - val_pos_accuracy: 0.8942\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9987 - val_loss: 0.1517 - val_pos_accuracy: 0.8942\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0029 - pos_accuracy: 0.9987 - val_loss: 0.1511 - val_pos_accuracy: 0.8918\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0030 - pos_accuracy: 0.9987 - val_loss: 0.1505 - val_pos_accuracy: 0.8942\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0028 - pos_accuracy: 0.9987 - val_loss: 0.1508 - val_pos_accuracy: 0.8942\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9987 - val_loss: 0.1509 - val_pos_accuracy: 0.8942\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9987 - val_loss: 0.1509 - val_pos_accuracy: 0.8942\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0026 - pos_accuracy: 0.9981 - val_loss: 0.1498 - val_pos_accuracy: 0.8942\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.1502 - val_pos_accuracy: 0.8942\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0025 - pos_accuracy: 0.9987 - val_loss: 0.1543 - val_pos_accuracy: 0.8942\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0024 - pos_accuracy: 0.9987 - val_loss: 0.1510 - val_pos_accuracy: 0.8918\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0023 - pos_accuracy: 0.9987 - val_loss: 0.1500 - val_pos_accuracy: 0.8942\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0022 - pos_accuracy: 0.9987 - val_loss: 0.1497 - val_pos_accuracy: 0.8918\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0021 - pos_accuracy: 0.9987 - val_loss: 0.1493 - val_pos_accuracy: 0.8942\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9987 - val_loss: 0.1486 - val_pos_accuracy: 0.8942\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9987 - val_loss: 0.1484 - val_pos_accuracy: 0.8942\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9987 - val_loss: 0.1486 - val_pos_accuracy: 0.8942\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9987 - val_loss: 0.1491 - val_pos_accuracy: 0.8942\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0019 - pos_accuracy: 0.9987 - val_loss: 0.1476 - val_pos_accuracy: 0.8942\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0018 - pos_accuracy: 0.9987 - val_loss: 0.1490 - val_pos_accuracy: 0.8942\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9987 - val_loss: 0.1481 - val_pos_accuracy: 0.8942\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0017 - pos_accuracy: 0.9987 - val_loss: 0.1481 - val_pos_accuracy: 0.8942\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0015 - pos_accuracy: 0.9987 - val_loss: 0.1477 - val_pos_accuracy: 0.8942\n",
      "tf.Tensor(\n",
      "[[False  True]\n",
      " [ True False]], shape=(2, 2), dtype=bool)\n",
      "(2, 2)\n",
      "(2,)\n",
      "[False False]\n",
      "[0. 0.]\n",
      "0.0\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:14:52.036746: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1710: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate : axis축에 따라 배열을 순서대로 합친다.\\naxis=1 : n+1 번째 dimension 기준으로 배열을 합친다. 따라서 A = (2,2) b = (2,1) 이므로 2 번째 dimension(행) 기준으로 합쳐서 M = (2,3)이 된다.\\n\",\n",
      "        \"\\n정수 자릿수에 의미는 row,col,index 이고, 따라서 plot을 담는 (1,2) 배열을 생성하고 121는 1번 index(1,1), 122는 2번 index(1,2)에 plot을 담는다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nlabel_true == label_pred 연산을 하면 참값과 예측값에 각각 원소(x,y)가 같으면 true 다르면 false로 저장되므로, \\n모든 원소가 true 이여야 좌표값이 같다. 따라서 reduce_all을 사용해 각행에 원소들을 접근하여 AND 연산을 해서 예측값과 참값에 좌표가 같은지 체크한다.\\n\\naxis=1는 2번째 dimension 기준(각각 행에 포함된 좌표 x,y 값)으로 AND 연산을 수행한다.\\n\",\n",
      "        \"\\n예측값과 참값에 좌표가 같은 img들에(배치사이즈 만큼) 평균을 구해서 pos_accuracy 값을 구한다.\\n\",\n",
      "        \"\\n출력값이 2차원 좌표(x,y)에 위치하는 x,y 값을 예측하는 회귀문제이므로 최종 출력층은 None을 사용하는게 적합하다.\\n\",\n",
      "        \"\\nrandom seed를 설정하지 않으면 실행될때마다 좌표값이 달라서 성능에 차이가 난다. 따라서,\\nrandom seed로 좌표값을 고정시켜 하이퍼파라미터를 변경할때마다 학습시키는 데이터들에 무결성을 보장해준다.\\n\",\n",
      "        true,\n",
      "        \"\\n\"\n",
      "    ],\n",
      "    \"score\": 100.0,\n",
      "    \"accuracy\": 0.8918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\"report1/전수인_46015.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/전수인_46015.ipynb to python\n",
      "[NbConvertApp] Writing 37847 bytes to report1/전수인_46015.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[2.2 2.4 2.6]\n",
      " [3.  3.2 3.4]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:211: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:245: RuntimeWarning: invalid value encountered in subtract\n",
      "  y = x - x\n",
      "temp32: 51\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:251: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:264: RuntimeWarning: invalid value encountered in subtract\n",
      "  y = x - x\n",
      "temp64: 421\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:270: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "ans03 : 51     ans04: 421\n",
      "w: [[1 2]\n",
      " [3 4]]\n",
      "w.shape: (2, 2)\n",
      "x: [[5 6]\n",
      " [7 8]\n",
      " [9 0]]\n",
      "x.shape: (3, 2)\n",
      "x.T: [[5 7 9]\n",
      " [6 8 0]]\n",
      "x.T.shape: (2, 3)\n",
      "y: [[17 23  9]\n",
      " [39 53 27]]\n",
      "y.shape: (2, 3)\n",
      "ans05: [[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "ans09: \n",
      "np.concatenate의 역할: 축을 따라 배열 시퀀스를 결합한다.\n",
      "axis=1의 의미: axis는 배열이 결합될 축으로\n",
      "               0은 행 기준, 1은 열 기준으로 결합한다.\n",
      "               axis=1일 경우 열 기준이므로 옆으로 배열을 추가시켜 생성된다.\n",
      "\n",
      "ans10: \n",
      "subplot에서 121, 122의 의미: \n",
      "    121,122 세개의 정수는 subplot의 행의 수, 열의 수, index를 나타내고 따라서 121은 전체 1행 2열 중 첫번째, 122는 전체 1행 2열 중 두번째를 나타낸다.\n",
      "\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 294864.78it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:14:58.555088: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:14:58.954759: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:14:58.954802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:14:59.186610: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:14:59.812671: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 88445.44it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "score.numpy(): 0.5\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 50,778\n",
      "Trainable params: 50,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 18.7041 - pos_accuracy: 0.0656 - val_loss: 5.2996 - val_pos_accuracy: 0.0275\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8773 - pos_accuracy: 0.2156 - val_loss: 2.0597 - val_pos_accuracy: 0.1550\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0428 - pos_accuracy: 0.3094 - val_loss: 0.6876 - val_pos_accuracy: 0.4475\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.1604 - pos_accuracy: 0.3800 - val_loss: 6.3029 - val_pos_accuracy: 0.0525\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4797 - pos_accuracy: 0.4762 - val_loss: 0.3096 - val_pos_accuracy: 0.6725\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3034 - pos_accuracy: 0.6056 - val_loss: 0.3308 - val_pos_accuracy: 0.6725\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2492 - pos_accuracy: 0.6669 - val_loss: 0.3054 - val_pos_accuracy: 0.6750\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2230 - pos_accuracy: 0.6938 - val_loss: 0.3430 - val_pos_accuracy: 0.6725\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1904 - pos_accuracy: 0.7244 - val_loss: 0.2093 - val_pos_accuracy: 0.8275\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0984 - pos_accuracy: 0.8394 - val_loss: 0.2017 - val_pos_accuracy: 0.8375\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1035 - pos_accuracy: 0.8531 - val_loss: 0.3237 - val_pos_accuracy: 0.6575\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0909 - pos_accuracy: 0.8612 - val_loss: 0.1771 - val_pos_accuracy: 0.8625\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0692 - pos_accuracy: 0.8963 - val_loss: 0.1662 - val_pos_accuracy: 0.8575\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0577 - pos_accuracy: 0.9200 - val_loss: 0.1677 - val_pos_accuracy: 0.8825\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0379 - pos_accuracy: 0.9594 - val_loss: 0.2423 - val_pos_accuracy: 0.6950\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0385 - pos_accuracy: 0.9513 - val_loss: 0.1578 - val_pos_accuracy: 0.8750\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0389 - pos_accuracy: 0.9500 - val_loss: 0.1412 - val_pos_accuracy: 0.8675\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0248 - pos_accuracy: 0.9775 - val_loss: 0.1586 - val_pos_accuracy: 0.8800\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0250 - pos_accuracy: 0.9812 - val_loss: 0.1484 - val_pos_accuracy: 0.8950\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0214 - pos_accuracy: 0.9831 - val_loss: 0.1374 - val_pos_accuracy: 0.8975\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0203 - pos_accuracy: 0.9856 - val_loss: 0.1313 - val_pos_accuracy: 0.9000\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0198 - pos_accuracy: 0.9850 - val_loss: 0.1245 - val_pos_accuracy: 0.9025\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0181 - pos_accuracy: 0.9869 - val_loss: 0.1229 - val_pos_accuracy: 0.9150\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0192 - pos_accuracy: 0.9887 - val_loss: 0.1187 - val_pos_accuracy: 0.9125\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0148 - pos_accuracy: 0.9925 - val_loss: 0.1182 - val_pos_accuracy: 0.9150\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0095 - pos_accuracy: 0.9962 - val_loss: 0.1259 - val_pos_accuracy: 0.9100\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0075 - pos_accuracy: 0.9994 - val_loss: 0.1170 - val_pos_accuracy: 0.9100\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0085 - pos_accuracy: 0.9956 - val_loss: 0.1162 - val_pos_accuracy: 0.9075\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0066 - pos_accuracy: 0.9987 - val_loss: 0.1169 - val_pos_accuracy: 0.9175\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0056 - pos_accuracy: 0.9994 - val_loss: 0.1199 - val_pos_accuracy: 0.9150\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0079 - pos_accuracy: 0.9987 - val_loss: 0.1097 - val_pos_accuracy: 0.9075\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0066 - pos_accuracy: 0.9981 - val_loss: 0.1115 - val_pos_accuracy: 0.9175\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0043 - pos_accuracy: 0.9994 - val_loss: 0.1116 - val_pos_accuracy: 0.9150\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0038 - pos_accuracy: 0.9994 - val_loss: 0.1093 - val_pos_accuracy: 0.9100\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0041 - pos_accuracy: 0.9994 - val_loss: 0.1080 - val_pos_accuracy: 0.9250\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0031 - pos_accuracy: 0.9994 - val_loss: 0.1083 - val_pos_accuracy: 0.9175\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0039 - pos_accuracy: 1.0000 - val_loss: 0.1073 - val_pos_accuracy: 0.9275\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0035 - pos_accuracy: 1.0000 - val_loss: 0.1068 - val_pos_accuracy: 0.9250\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0031 - pos_accuracy: 1.0000 - val_loss: 0.1070 - val_pos_accuracy: 0.9225\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0030 - pos_accuracy: 1.0000 - val_loss: 0.1068 - val_pos_accuracy: 0.9225\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0026 - pos_accuracy: 1.0000 - val_loss: 0.1058 - val_pos_accuracy: 0.9225\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0020 - pos_accuracy: 1.0000 - val_loss: 0.1048 - val_pos_accuracy: 0.9250\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0018 - pos_accuracy: 1.0000 - val_loss: 0.1045 - val_pos_accuracy: 0.9175\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0017 - pos_accuracy: 1.0000 - val_loss: 0.1057 - val_pos_accuracy: 0.9175\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0025 - pos_accuracy: 1.0000 - val_loss: 0.1045 - val_pos_accuracy: 0.9150\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0018 - pos_accuracy: 1.0000 - val_loss: 0.1052 - val_pos_accuracy: 0.9250\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0013 - pos_accuracy: 1.0000 - val_loss: 0.1046 - val_pos_accuracy: 0.9225\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0018 - pos_accuracy: 1.0000 - val_loss: 0.1061 - val_pos_accuracy: 0.9100\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0018 - pos_accuracy: 1.0000 - val_loss: 0.1029 - val_pos_accuracy: 0.9250\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0013 - pos_accuracy: 1.0000 - val_loss: 0.1050 - val_pos_accuracy: 0.9100\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 9.6577e-04 - pos_accuracy: 1.0000 - val_loss: 0.1024 - val_pos_accuracy: 0.9225\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 8.6246e-04 - pos_accuracy: 1.0000 - val_loss: 0.1036 - val_pos_accuracy: 0.9200\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 8.7486e-04 - pos_accuracy: 1.0000 - val_loss: 0.1037 - val_pos_accuracy: 0.9200\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 9.6906e-04 - pos_accuracy: 1.0000 - val_loss: 0.1027 - val_pos_accuracy: 0.9225\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0010 - pos_accuracy: 1.0000 - val_loss: 0.1021 - val_pos_accuracy: 0.9275\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 8.3590e-04 - pos_accuracy: 1.0000 - val_loss: 0.1018 - val_pos_accuracy: 0.9250\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 8.8109e-04 - pos_accuracy: 1.0000 - val_loss: 0.1019 - val_pos_accuracy: 0.9250\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.8616e-04 - pos_accuracy: 1.0000 - val_loss: 0.1018 - val_pos_accuracy: 0.9275\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.9216e-04 - pos_accuracy: 1.0000 - val_loss: 0.1021 - val_pos_accuracy: 0.9250\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.1000e-04 - pos_accuracy: 1.0000 - val_loss: 0.1036 - val_pos_accuracy: 0.9100\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 6.0170e-04 - pos_accuracy: 1.0000 - val_loss: 0.1016 - val_pos_accuracy: 0.9225\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.1973e-04 - pos_accuracy: 1.0000 - val_loss: 0.1017 - val_pos_accuracy: 0.9175\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.0668e-04 - pos_accuracy: 1.0000 - val_loss: 0.1012 - val_pos_accuracy: 0.9250\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.1047e-04 - pos_accuracy: 1.0000 - val_loss: 0.1016 - val_pos_accuracy: 0.9250\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.6264e-04 - pos_accuracy: 1.0000 - val_loss: 0.1007 - val_pos_accuracy: 0.9250\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.6682e-04 - pos_accuracy: 1.0000 - val_loss: 0.1010 - val_pos_accuracy: 0.9250\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 5.8214e-04 - pos_accuracy: 1.0000 - val_loss: 0.1017 - val_pos_accuracy: 0.9250\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 4.6807e-04 - pos_accuracy: 1.0000 - val_loss: 0.1011 - val_pos_accuracy: 0.9250\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 3ms/step - loss: 4.5901e-04 - pos_accuracy: 1.0000 - val_loss: 0.1007 - val_pos_accuracy: 0.9200\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9012e-04 - pos_accuracy: 1.0000 - val_loss: 0.1001 - val_pos_accuracy: 0.9250\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.4612e-04 - pos_accuracy: 1.0000 - val_loss: 0.1006 - val_pos_accuracy: 0.9225\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 3.9832e-04 - pos_accuracy: 1.0000 - val_loss: 0.1007 - val_pos_accuracy: 0.9275\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.8795e-04 - pos_accuracy: 1.0000 - val_loss: 0.0998 - val_pos_accuracy: 0.9275\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3771e-04 - pos_accuracy: 1.0000 - val_loss: 0.1005 - val_pos_accuracy: 0.9175\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2961e-04 - pos_accuracy: 1.0000 - val_loss: 0.1000 - val_pos_accuracy: 0.9250\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1374e-04 - pos_accuracy: 1.0000 - val_loss: 0.1004 - val_pos_accuracy: 0.9200\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1719e-04 - pos_accuracy: 1.0000 - val_loss: 0.1003 - val_pos_accuracy: 0.9275\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3399e-04 - pos_accuracy: 1.0000 - val_loss: 0.0998 - val_pos_accuracy: 0.9250\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.7939e-04 - pos_accuracy: 1.0000 - val_loss: 0.1001 - val_pos_accuracy: 0.9200\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9063e-04 - pos_accuracy: 1.0000 - val_loss: 0.1002 - val_pos_accuracy: 0.9225\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.5395e-04 - pos_accuracy: 1.0000 - val_loss: 0.0994 - val_pos_accuracy: 0.9275\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.5716e-04 - pos_accuracy: 1.0000 - val_loss: 0.0996 - val_pos_accuracy: 0.9250\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.6230e-04 - pos_accuracy: 1.0000 - val_loss: 0.0998 - val_pos_accuracy: 0.9250\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8369e-04 - pos_accuracy: 1.0000 - val_loss: 0.1007 - val_pos_accuracy: 0.9275\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.0109e-04 - pos_accuracy: 1.0000 - val_loss: 0.0997 - val_pos_accuracy: 0.9250\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.7178e-04 - pos_accuracy: 1.0000 - val_loss: 0.0998 - val_pos_accuracy: 0.9275\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8819e-04 - pos_accuracy: 1.0000 - val_loss: 0.0995 - val_pos_accuracy: 0.9275\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1208e-04 - pos_accuracy: 1.0000 - val_loss: 0.0995 - val_pos_accuracy: 0.9200\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.1269e-04 - pos_accuracy: 1.0000 - val_loss: 0.0992 - val_pos_accuracy: 0.9225\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2006e-04 - pos_accuracy: 1.0000 - val_loss: 0.1000 - val_pos_accuracy: 0.9175\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9712e-04 - pos_accuracy: 1.0000 - val_loss: 0.0994 - val_pos_accuracy: 0.9275\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.8666e-04 - pos_accuracy: 1.0000 - val_loss: 0.0995 - val_pos_accuracy: 0.9200\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3957e-04 - pos_accuracy: 1.0000 - val_loss: 0.0991 - val_pos_accuracy: 0.9275\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2684e-04 - pos_accuracy: 1.0000 - val_loss: 0.0993 - val_pos_accuracy: 0.9250\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2365e-04 - pos_accuracy: 1.0000 - val_loss: 0.0994 - val_pos_accuracy: 0.9175\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.9008e-04 - pos_accuracy: 1.0000 - val_loss: 0.0997 - val_pos_accuracy: 0.9275\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.9403e-04 - pos_accuracy: 1.0000 - val_loss: 0.0994 - val_pos_accuracy: 0.9225\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.4668e-04 - pos_accuracy: 1.0000 - val_loss: 0.1000 - val_pos_accuracy: 0.9200\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.3428e-04 - pos_accuracy: 1.0000 - val_loss: 0.1000 - val_pos_accuracy: 0.9200\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.6938e-04 - pos_accuracy: 1.0000 - val_loss: 0.1003 - val_pos_accuracy: 0.9275\n",
      "ans11: [64, 8]\n",
      "ans12: ['Model: \"model_1\"', '_________________________________________________________________', 'Layer (type)                 Output Shape              Param #   ', '=================================================================', 'input_2 (InputLayer)         [(None, 28, 28)]          0         ', '_________________________________________________________________', 'flatten_1 (Flatten)          (None, 784)               0         ', '_________________________________________________________________', 'dense_1 (Dense)              (None, 64)                50240     ', '_________________________________________________________________', 'dense_2 (Dense)              (None, 8)                 520       ', '_________________________________________________________________', 'dense_3 (Dense)              (None, 2)                 18        ', '=================================================================', 'Total params: 50,778', 'Trainable params: 50,778', 'Non-trainable params: 0', '_________________________________________________________________']\n",
      "ans13: 0.01\n",
      "ans14: 8\n",
      "ans15: 100\n",
      "ans16: 0.0995\n",
      "ans17: 0.925\n",
      "ans18: \n",
      "tf.reduce_all의 역할은 무엇인지, \n",
      "    차원을 AND연산으로 축소한다.\n",
      "axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\n",
      "    axis=1은 2차원을 제거하고 차원을 축소한다.\n",
      "\n",
      "ans19: \n",
      "tf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \n",
      "    배열 전체 원소의 합을 원소 개수로 나누어 계산하여 평균을 구하며\n",
      "    제거할 차원의 axis를 입력하여 차원을 줄여서 평균을 구한다.\n",
      "\n",
      "ans20: \n",
      "최종 출력층의 activation=None인 이유 \n",
      "    활성함수가 없을 경우 신경망 학습은 선형회귀와 동일하다.\n",
      "    즉, 회귀의 출력은 임의의 어떤 숫자이므로 활성화 함수를 적용할 필요가 없이 \n",
      "    출력층의 선형 방정식의 계산을 그대로 출력한다.\n",
      "\n",
      "ans21: \n",
      "random seed의 사용 이유 \n",
      "    여러번 실행하더라도 동일한 결과 값을 갖기 위해서 사용한다.\n",
      "    즉, 동일한 실험 결과를 얻기 위해서는 난수 생성기의 시드(seed)를 일정한 값으로\n",
      "    유지시켜 프로그램 실행 중 동일한 지점에 다른 값이 대입되는 것을 막아야 한다.\n",
      "\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:16:08.900327: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "[[0.40392157 0.40392157 0.40392157]\n",
      " [0.40392157 0.40392157 0.42352941]\n",
      " [0.41176471 0.42352941 0.44705882]]\n",
      "0.4139433551198257\n",
      "0.41394338\n",
      "ans22: 0.41394338\n",
      "ans23: \n",
      "    여기에 적어주세요. \n",
      "    어렵지만 강의 재밌게 듣고 있습니다. 이해하기 쉽게 설명해 주셔서 감사합니다.\n",
      "    수업과 관련된 실습코드의 종류를 다양하게 참조할 수 있게 해주시면 어떨까요?\n",
      "    수업에서 다루는 코드도 완전히 이해를 못하는 수준이지만 다양하게 접해보고 싶어서\n",
      "    말씀드려 봅니다.\n",
      "\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1766: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할: 축을 따라 배열 시퀀스를 결합한다.\\naxis=1의 의미: axis는 배열이 결합될 축으로\\n               0은 행 기준, 1은 열 기준으로 결합한다.\\n               axis=1일 경우 열 기준이므로 옆으로 배열을 추가시켜 생성된다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미: \\n    121,122 세개의 정수는 subplot의 행의 수, 열의 수, index를 나타내고 따라서 121은 전체 1행 2열 중 첫번째, 122는 전체 1행 2열 중 두번째를 나타낸다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, \\n    차원을 AND연산으로 축소한다.\\naxis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n    axis=1은 2차원을 제거하고 차원을 축소한다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n    배열 전체 원소의 합을 원소 개수로 나누어 계산하여 평균을 구하며\\n    제거할 차원의 axis를 입력하여 차원을 줄여서 평균을 구한다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n    활성함수가 없을 경우 신경망 학습은 선형회귀와 동일하다.\\n    즉, 회귀의 출력은 임의의 어떤 숫자이므로 활성화 함수를 적용할 필요가 없이 \\n    출력층의 선형 방정식의 계산을 그대로 출력한다.\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n    여러번 실행하더라도 동일한 결과 값을 갖기 위해서 사용한다.\\n    즉, 동일한 실험 결과를 얻기 위해서는 난수 생성기의 시드(seed)를 일정한 값으로\\n    유지시켜 프로그램 실행 중 동일한 지점에 다른 값이 대입되는 것을 막아야 한다.\\n\",\n",
      "        false,\n",
      "        \"\\n    여기에 적어주세요. \\n    어렵지만 강의 재밌게 듣고 있습니다. 이해하기 쉽게 설명해 주셔서 감사합니다.\\n    수업과 관련된 실습코드의 종류를 다양하게 참조할 수 있게 해주시면 어떨까요?\\n    수업에서 다루는 코드도 완전히 이해를 못하는 수준이지만 다양하게 접해보고 싶어서\\n    말씀드려 봅니다.\\n\"\n",
      "    ],\n",
      "    \"score\": 70.0,\n",
      "    \"accuracy\": 0.925\n",
      "}\"report1/이재인_46024.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이재인_46024.ipynb to python\n",
      "[NbConvertApp] Writing 35814 bytes to report1/이재인_46024.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:210: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[3.0519118e+38           inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:244: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x1 - x1)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[ 0. nan]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:261: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x2 - x2)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "x:\n",
      " [[5 6]\n",
      " [7 8]\n",
      " [9 0]]\n",
      "x.shape:\n",
      " (3, 2)\n",
      "x.T.shape:\n",
      " (2, 3)\n",
      "y:\n",
      " [[17. 23.  9.]\n",
      " [39. 53. 27.]]\n",
      "y.shape:\n",
      " (2, 3)\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  1.2   0.  -15. ]\n",
      " [  0.    1.2 -25. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ 6.123234e-17 -1.000000e+00  2.000000e+02]\n",
      " [ 1.000000e+00  6.123234e-17  0.000000e+00]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 306064.21it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:16:17.224692: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:16:17.606995: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:16:17.607055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:16:17.845330: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:16:18.485076: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 6ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 111995.94it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 117,250\n",
      "Trainable params: 117,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 37.0826 - pos_accuracy: 0.0781 - val_loss: 2.2985 - val_pos_accuracy: 0.2043\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.7940 - pos_accuracy: 0.1725 - val_loss: 1.3813 - val_pos_accuracy: 0.2452\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1.2843 - pos_accuracy: 0.2400 - val_loss: 1.7439 - val_pos_accuracy: 0.1274\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.7737 - pos_accuracy: 0.3444 - val_loss: 0.7811 - val_pos_accuracy: 0.4159\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5756 - pos_accuracy: 0.4200 - val_loss: 1.1700 - val_pos_accuracy: 0.2091\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:16:21.589549: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1699: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate은 지정된 축에 따라 배열을 합치는 역할을 하고, axis=1은 열을 의미한다.\\n즉 np.concatenate에서 axis=1으로 지정되어 있을 때는 열 방향으로 배열을 합친다는 뜻이다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122는 여러 개의 subplot을 그릴 때 행,열,인덱스의 순으로 위치를 표시하는 것이며,\\n121, 122는 가로로 1x2 행렬 형태로 plot이 놓일 때 각각 첫 번째, 두 번째 plot을 의미한다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all은 주어진 축을 따라 tensor의 논리합을 계산해주며 axis=1은 열 방향 축을 의미한다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 주어진 tensor 요소의 평균을 계산해준다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유 \\n\\n\",\n",
      "        \"\\nrandom seed는 난수를 고정하여 결과의 재현하기 위해 사용한다.\\n\",\n",
      "        false,\n",
      "        \"\\n틈틈이 리마인더도 문자로 주시고 섬세함에 감동받았습니다!\\n과제의 양이 다소 벅차긴 했지만\\n꼼꼼하게 차근차근 공부할 수 있도록 해주셔서 너무 좋은 것 같습니다!\\n끝나고도 다시 봐야하겠습니다. 감사합니다 ^^\\n\"\n",
      "    ],\n",
      "    \"score\": 78.18181818181819,\n",
      "    \"accuracy\": 0.4183\n",
      "}\"report1/정일영_45999.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/정일영_45999.ipynb to python\n",
      "[NbConvertApp] Writing 35233 bytes to report1/정일영_45999.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:212: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "[inf inf]\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "Requirement already satisfied: matplotlib==3.1.3 in /home/kotech/venv-tensor2/lib/python3.8/site-packages (3.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/kotech/venv-tensor2/lib/python3.8/site-packages (from matplotlib==3.1.3) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/kotech/venv-tensor2/lib/python3.8/site-packages (from matplotlib==3.1.3) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/kotech/venv-tensor2/lib/python3.8/site-packages (from matplotlib==3.1.3) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kotech/venv-tensor2/lib/python3.8/site-packages (from matplotlib==3.1.3) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/kotech/venv-tensor2/lib/python3.8/site-packages (from matplotlib==3.1.3) (1.19.5)\n",
      "Requirement already satisfied: six in /home/kotech/venv-tensor2/lib/python3.8/site-packages (from cycler>=0.10->matplotlib==3.1.3) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 151679.02it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:16:30.779549: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:16:31.190081: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:16:31.190135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:16:31.421875: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "2022-10-14 11:16:32.046450: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "25/25 [==============================] - 1s 7ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/2\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 144840.94it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 240.9341 - val_loss: 212.3778\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2086 - val_loss: 195.9663\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 205.1136 - val_loss: 180.9897\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 189.4806 - val_loss: 167.3181\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 175.1996 - val_loss: 154.8309\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 162.1355 - val_loss: 143.4211\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 150.1897 - val_loss: 132.9914\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 139.2480 - val_loss: 123.4498\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 129.2301 - val_loss: 114.7210\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 120.0508 - val_loss: 106.7253\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 111.6396 - val_loss: 99.4010\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 103.9199 - val_loss: 92.6875\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 96.8369 - val_loss: 86.5300\n",
      "Epoch 14/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 90.3335 - val_loss: 80.8783\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 84.3564 - val_loss: 75.6882\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 78.8640 - val_loss: 70.9168\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 73.8082 - val_loss: 66.5271\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 69.1541 - val_loss: 62.4886\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 64.8669 - val_loss: 58.7688\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 60.9129 - val_loss: 55.3396\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 57.2655 - val_loss: 52.1754\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 53.9000 - val_loss: 49.2539\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 50.7896 - val_loss: 46.5551\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 47.9120 - val_loss: 44.0591\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 45.2511 - val_loss: 41.7481\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 42.7870 - val_loss: 39.6074\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 40.5004 - val_loss: 37.6223\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 38.3828 - val_loss: 35.7793\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 36.4151 - val_loss: 34.0677\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 34.5862 - val_loss: 32.4754\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 32.8852 - val_loss: 30.9928\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 31.3009 - val_loss: 29.6120\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 29.8260 - val_loss: 28.3235\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 28.4511 - val_loss: 27.1211\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 27.1661 - val_loss: 25.9959\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 25.9665 - val_loss: 24.9443\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 24.8447 - val_loss: 23.9595\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 23.7953 - val_loss: 23.0365\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 22.8118 - val_loss: 22.1704\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 21.8888 - val_loss: 21.3576\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 21.0239 - val_loss: 20.5921\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 20.2100 - val_loss: 19.8724\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 19.4461 - val_loss: 19.1937\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 18.7258 - val_loss: 18.5540\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 18.0480 - val_loss: 17.9503\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 17.4086 - val_loss: 17.3803\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 16.8061 - val_loss: 16.8404\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 16.2372 - val_loss: 16.3301\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 15.6984 - val_loss: 15.8457\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 15.1892 - val_loss: 15.3877\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 14.7071 - val_loss: 14.9510\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 14.2505 - val_loss: 14.5375\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 13.8168 - val_loss: 14.1441\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 13.4055 - val_loss: 13.7700\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 13.0151 - val_loss: 13.4138\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 12.6437 - val_loss: 13.0747\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 12.2898 - val_loss: 12.7502\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 11.9535 - val_loss: 12.4415\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 11.6336 - val_loss: 12.1466\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 11.3274 - val_loss: 11.8638\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 11.0361 - val_loss: 11.5941\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 10.7576 - val_loss: 11.3344\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 10.4917 - val_loss: 11.0872\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 10.2374 - val_loss: 10.8499\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.9944 - val_loss: 10.6221\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.7615 - val_loss: 10.4041\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 9.5387 - val_loss: 10.1938\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 9.3251 - val_loss: 9.9928\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.1203 - val_loss: 9.7989\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 8.9240 - val_loss: 9.6127\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 8.7350 - val_loss: 9.4337\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 8.5540 - val_loss: 9.2608\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 8.3797 - val_loss: 9.0939\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 8.2124 - val_loss: 8.9346\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 8.0512 - val_loss: 8.7801\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.8965 - val_loss: 8.6306\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.7472 - val_loss: 8.4869\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.6036 - val_loss: 8.3482\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.4654 - val_loss: 8.2141\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.3315 - val_loss: 8.0842\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 7.2029 - val_loss: 7.9593\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 7.0786 - val_loss: 7.8378\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.9586 - val_loss: 7.7202\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.8429 - val_loss: 7.6076\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.7308 - val_loss: 7.4974\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.6228 - val_loss: 7.3910\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.5180 - val_loss: 7.2878\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.4168 - val_loss: 7.1879\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.3189 - val_loss: 7.0913\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 6.2246 - val_loss: 6.9972\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.1327 - val_loss: 6.9061\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 6.0436 - val_loss: 6.8180\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.9578 - val_loss: 6.7319\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.8742 - val_loss: 6.6484\n",
      "Epoch 95/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 5.7934 - val_loss: 6.5675\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.7149 - val_loss: 6.4888\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.6389 - val_loss: 6.4124\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.5651 - val_loss: 6.3381\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.4934 - val_loss: 6.2658\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.4238 - val_loss: 6.1955\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.3561 - val_loss: 6.1269\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.2904 - val_loss: 6.0601\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.2265 - val_loss: 5.9955\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.1642 - val_loss: 5.9324\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5.1039 - val_loss: 5.8706\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5.0450 - val_loss: 5.8106\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.9879 - val_loss: 5.7523\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.9322 - val_loss: 5.6950\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.8781 - val_loss: 5.6394\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.8251 - val_loss: 5.5853\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.7736 - val_loss: 5.5323\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.7236 - val_loss: 5.4804\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.6748 - val_loss: 5.4302\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.6269 - val_loss: 5.3807\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.5805 - val_loss: 5.3329\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.5353 - val_loss: 5.2856\n",
      "Epoch 117/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.4909 - val_loss: 5.2397\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.4478 - val_loss: 5.1950\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.4056 - val_loss: 5.1510\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.3644 - val_loss: 5.1081\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.3242 - val_loss: 5.0659\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.2851 - val_loss: 5.0249\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.2466 - val_loss: 4.9850\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.2090 - val_loss: 4.9456\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.1723 - val_loss: 4.9069\n",
      "Epoch 126/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.1366 - val_loss: 4.8689\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.1015 - val_loss: 4.8321\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.0670 - val_loss: 4.7961\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.0335 - val_loss: 4.7606\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4.0007 - val_loss: 4.7258\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.9685 - val_loss: 4.6917\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.9369 - val_loss: 4.6585\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.9062 - val_loss: 4.6256\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.8761 - val_loss: 4.5935\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.8461 - val_loss: 4.5620\n",
      "Epoch 136/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.8172 - val_loss: 4.5311\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.7887 - val_loss: 4.5004\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.7608 - val_loss: 4.4709\n",
      "Epoch 139/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.7335 - val_loss: 4.4415\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.7066 - val_loss: 4.4128\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.6803 - val_loss: 4.3846\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.6545 - val_loss: 4.3567\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.6290 - val_loss: 4.3296\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.6043 - val_loss: 4.3027\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.5799 - val_loss: 4.2764\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.5557 - val_loss: 4.2505\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.5320 - val_loss: 4.2249\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.5089 - val_loss: 4.2001\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.4862 - val_loss: 4.1755\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.4638 - val_loss: 4.1508\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.4416 - val_loss: 4.1273\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.4203 - val_loss: 4.1037\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.3991 - val_loss: 4.0807\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.3779 - val_loss: 4.0582\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.3575 - val_loss: 4.0357\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.3373 - val_loss: 4.0137\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.3174 - val_loss: 3.9921\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.2978 - val_loss: 3.9707\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.2785 - val_loss: 3.9495\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.2595 - val_loss: 3.9288\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.2409 - val_loss: 3.9083\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.2224 - val_loss: 3.8881\n",
      "Epoch 163/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.2043 - val_loss: 3.8684\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.1864 - val_loss: 3.8486\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.1689 - val_loss: 3.8292\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.1516 - val_loss: 3.8100\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.1345 - val_loss: 3.7914\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.1177 - val_loss: 3.7728\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.1012 - val_loss: 3.7545\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.0848 - val_loss: 3.7365\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.0688 - val_loss: 3.7186\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.0529 - val_loss: 3.7013\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.0374 - val_loss: 3.6838\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.0218 - val_loss: 3.6668\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3.0066 - val_loss: 3.6498\n",
      "Epoch 176/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 2.9916 - val_loss: 3.6332\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.9768 - val_loss: 3.6166\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.9622 - val_loss: 3.6002\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.9477 - val_loss: 3.5843\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.9334 - val_loss: 3.5683\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.9193 - val_loss: 3.5527\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.9055 - val_loss: 3.5372\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8917 - val_loss: 3.5220\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8782 - val_loss: 3.5066\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8648 - val_loss: 3.4917\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8516 - val_loss: 3.4768\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8386 - val_loss: 3.4623\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8258 - val_loss: 3.4476\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8130 - val_loss: 3.4334\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8004 - val_loss: 3.4194\n",
      "Epoch 191/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.7880 - val_loss: 3.4053\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.7757 - val_loss: 3.3918\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.7636 - val_loss: 3.3778\n",
      "Epoch 194/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.7516 - val_loss: 3.3645\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.7398 - val_loss: 3.3510\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.7282 - val_loss: 3.3378\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.7165 - val_loss: 3.3246\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.7050 - val_loss: 3.3117\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.6936 - val_loss: 3.2989\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6825 - val_loss: 3.2862\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6714 - val_loss: 3.2735\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6605 - val_loss: 3.2614\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6497 - val_loss: 3.2490\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.6389 - val_loss: 3.2367\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6284 - val_loss: 3.2247\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.6178 - val_loss: 3.2128\n",
      "Epoch 207/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.6073 - val_loss: 3.2010\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5971 - val_loss: 3.1893\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5869 - val_loss: 3.1775\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5769 - val_loss: 3.1662\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5669 - val_loss: 3.1548\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5571 - val_loss: 3.1435\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.5473 - val_loss: 3.1322\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.5376 - val_loss: 3.1211\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.5280 - val_loss: 3.1101\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5185 - val_loss: 3.0995\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5092 - val_loss: 3.0888\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4999 - val_loss: 3.0783\n",
      "Epoch 219/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4907 - val_loss: 3.0677\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4816 - val_loss: 3.0569\n",
      "Epoch 221/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4725 - val_loss: 3.0465\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4636 - val_loss: 3.0362\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4548 - val_loss: 3.0262\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4460 - val_loss: 3.0160\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4373 - val_loss: 3.0059\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4287 - val_loss: 2.9962\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4202 - val_loss: 2.9862\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4117 - val_loss: 2.9766\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.4034 - val_loss: 2.9668\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3950 - val_loss: 2.9574\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3868 - val_loss: 2.9478\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3786 - val_loss: 2.9383\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3706 - val_loss: 2.9289\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.3625 - val_loss: 2.9197\n",
      "Epoch 235/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.3546 - val_loss: 2.9105\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3467 - val_loss: 2.9014\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3389 - val_loss: 2.8924\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.3312 - val_loss: 2.8835\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3235 - val_loss: 2.8742\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3158 - val_loss: 2.8655\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3083 - val_loss: 2.8568\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3007 - val_loss: 2.8481\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2935 - val_loss: 2.8394\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2862 - val_loss: 2.8308\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2788 - val_loss: 2.8224\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2715 - val_loss: 2.8139\n",
      "Epoch 247/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2643 - val_loss: 2.8056\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2573 - val_loss: 2.7973\n",
      "Epoch 249/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2501 - val_loss: 2.7891\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2433 - val_loss: 2.7809\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2362 - val_loss: 2.7729\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2293 - val_loss: 2.7647\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2225 - val_loss: 2.7567\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2158 - val_loss: 2.7487\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2089 - val_loss: 2.7408\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2023 - val_loss: 2.7329\n",
      "Epoch 257/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1957 - val_loss: 2.7253\n",
      "Epoch 258/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1890 - val_loss: 2.7176\n",
      "Epoch 259/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1826 - val_loss: 2.7098\n",
      "Epoch 260/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1761 - val_loss: 2.7022\n",
      "Epoch 261/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1697 - val_loss: 2.6947\n",
      "Epoch 262/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1633 - val_loss: 2.6872\n",
      "Epoch 263/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1569 - val_loss: 2.6798\n",
      "Epoch 264/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1506 - val_loss: 2.6725\n",
      "Epoch 265/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1445 - val_loss: 2.6652\n",
      "Epoch 266/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1382 - val_loss: 2.6579\n",
      "Epoch 267/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1321 - val_loss: 2.6506\n",
      "Epoch 268/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1260 - val_loss: 2.6434\n",
      "Epoch 269/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1198 - val_loss: 2.6365\n",
      "Epoch 270/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1140 - val_loss: 2.6291\n",
      "Epoch 271/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1079 - val_loss: 2.6221\n",
      "Epoch 272/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1020 - val_loss: 2.6155\n",
      "Epoch 273/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0961 - val_loss: 2.6082\n",
      "Epoch 274/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0903 - val_loss: 2.6015\n",
      "Epoch 275/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0846 - val_loss: 2.5946\n",
      "Epoch 276/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0787 - val_loss: 2.5877\n",
      "Epoch 277/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0731 - val_loss: 2.5808\n",
      "Epoch 278/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0673 - val_loss: 2.5743\n",
      "Epoch 279/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0616 - val_loss: 2.5677\n",
      "Epoch 280/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0561 - val_loss: 2.5609\n",
      "Epoch 281/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0505 - val_loss: 2.5543\n",
      "Epoch 282/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0450 - val_loss: 2.5478\n",
      "Epoch 283/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0395 - val_loss: 2.5412\n",
      "Epoch 284/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0341 - val_loss: 2.5347\n",
      "Epoch 285/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0285 - val_loss: 2.5286\n",
      "Epoch 286/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0233 - val_loss: 2.5221\n",
      "Epoch 287/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0179 - val_loss: 2.5158\n",
      "Epoch 288/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0126 - val_loss: 2.5095\n",
      "Epoch 289/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0073 - val_loss: 2.5032\n",
      "Epoch 290/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0022 - val_loss: 2.4970\n",
      "Epoch 291/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9969 - val_loss: 2.4906\n",
      "Epoch 292/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9917 - val_loss: 2.4848\n",
      "Epoch 293/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9867 - val_loss: 2.4785\n",
      "Epoch 294/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9816 - val_loss: 2.4725\n",
      "Epoch 295/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9765 - val_loss: 2.4665\n",
      "Epoch 296/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9714 - val_loss: 2.4603\n",
      "Epoch 297/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9665 - val_loss: 2.4546\n",
      "Epoch 298/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9614 - val_loss: 2.4485\n",
      "Epoch 299/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9565 - val_loss: 2.4428\n",
      "Epoch 300/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9517 - val_loss: 2.4370\n",
      "Epoch 301/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9468 - val_loss: 2.4311\n",
      "Epoch 302/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9420 - val_loss: 2.4254\n",
      "Epoch 303/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9372 - val_loss: 2.4195\n",
      "Epoch 304/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9323 - val_loss: 2.4139\n",
      "Epoch 305/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9275 - val_loss: 2.4081\n",
      "Epoch 306/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9228 - val_loss: 2.4026\n",
      "Epoch 307/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9181 - val_loss: 2.3970\n",
      "Epoch 308/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.9135 - val_loss: 2.3914\n",
      "Epoch 309/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9089 - val_loss: 2.3861\n",
      "Epoch 310/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9042 - val_loss: 2.3803\n",
      "Epoch 311/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8996 - val_loss: 2.3749\n",
      "Epoch 312/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8950 - val_loss: 2.3695\n",
      "Epoch 313/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8906 - val_loss: 2.3639\n",
      "Epoch 314/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8861 - val_loss: 2.3586\n",
      "Epoch 315/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8817 - val_loss: 2.3534\n",
      "Epoch 316/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8772 - val_loss: 2.3479\n",
      "Epoch 317/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8727 - val_loss: 2.3428\n",
      "Epoch 318/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8682 - val_loss: 2.3374\n",
      "Epoch 319/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8640 - val_loss: 2.3322\n",
      "Epoch 320/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8597 - val_loss: 2.3269\n",
      "Epoch 321/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8553 - val_loss: 2.3218\n",
      "Epoch 322/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8510 - val_loss: 2.3170\n",
      "Epoch 323/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8468 - val_loss: 2.3117\n",
      "Epoch 324/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8425 - val_loss: 2.3066\n",
      "Epoch 325/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8384 - val_loss: 2.3013\n",
      "Epoch 326/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8341 - val_loss: 2.2963\n",
      "Epoch 327/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8300 - val_loss: 2.2911\n",
      "Epoch 328/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8258 - val_loss: 2.2864\n",
      "Epoch 329/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8217 - val_loss: 2.2814\n",
      "Epoch 330/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8175 - val_loss: 2.2767\n",
      "Epoch 331/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8135 - val_loss: 2.2715\n",
      "Epoch 332/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8094 - val_loss: 2.2668\n",
      "Epoch 333/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8054 - val_loss: 2.2617\n",
      "Epoch 334/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8014 - val_loss: 2.2571\n",
      "Epoch 335/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7974 - val_loss: 2.2523\n",
      "Epoch 336/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7935 - val_loss: 2.2476\n",
      "Epoch 337/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7896 - val_loss: 2.2428\n",
      "Epoch 338/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7856 - val_loss: 2.2384\n",
      "Epoch 339/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7817 - val_loss: 2.2335\n",
      "Epoch 340/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7779 - val_loss: 2.2289\n",
      "Epoch 341/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7740 - val_loss: 2.2240\n",
      "Epoch 342/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7702 - val_loss: 2.2196\n",
      "Epoch 343/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7664 - val_loss: 2.2148\n",
      "Epoch 344/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7626 - val_loss: 2.2104\n",
      "Epoch 345/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7588 - val_loss: 2.2058\n",
      "Epoch 346/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7550 - val_loss: 2.2014\n",
      "Epoch 347/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7513 - val_loss: 2.1968\n",
      "Epoch 348/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7476 - val_loss: 2.1924\n",
      "Epoch 349/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7440 - val_loss: 2.1880\n",
      "Epoch 350/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7403 - val_loss: 2.1835\n",
      "Epoch 351/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7366 - val_loss: 2.1790\n",
      "Epoch 352/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7330 - val_loss: 2.1746\n",
      "Epoch 353/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7293 - val_loss: 2.1702\n",
      "Epoch 354/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7257 - val_loss: 2.1660\n",
      "Epoch 355/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7222 - val_loss: 2.1615\n",
      "Epoch 356/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7186 - val_loss: 2.1572\n",
      "Epoch 357/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7151 - val_loss: 2.1531\n",
      "Epoch 358/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7116 - val_loss: 2.1488\n",
      "Epoch 359/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7080 - val_loss: 2.1446\n",
      "Epoch 360/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.7046 - val_loss: 2.1404\n",
      "Epoch 361/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7011 - val_loss: 2.1364\n",
      "Epoch 362/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6978 - val_loss: 2.1319\n",
      "Epoch 363/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6943 - val_loss: 2.1278\n",
      "Epoch 364/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6909 - val_loss: 2.1237\n",
      "Epoch 365/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6875 - val_loss: 2.1198\n",
      "Epoch 366/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6840 - val_loss: 2.1155\n",
      "Epoch 367/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6807 - val_loss: 2.1114\n",
      "Epoch 368/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6774 - val_loss: 2.1075\n",
      "Epoch 369/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6740 - val_loss: 2.1032\n",
      "Epoch 370/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6706 - val_loss: 2.0994\n",
      "Epoch 371/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6674 - val_loss: 2.0952\n",
      "Epoch 372/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6642 - val_loss: 2.0914\n",
      "Epoch 373/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6609 - val_loss: 2.0874\n",
      "Epoch 374/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6577 - val_loss: 2.0835\n",
      "Epoch 375/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6544 - val_loss: 2.0795\n",
      "Epoch 376/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6511 - val_loss: 2.0757\n",
      "Epoch 377/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6479 - val_loss: 2.0717\n",
      "Epoch 378/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6447 - val_loss: 2.0680\n",
      "Epoch 379/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6417 - val_loss: 2.0641\n",
      "Epoch 380/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6384 - val_loss: 2.0602\n",
      "Epoch 381/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6354 - val_loss: 2.0565\n",
      "Epoch 382/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6322 - val_loss: 2.0526\n",
      "Epoch 383/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6291 - val_loss: 2.0490\n",
      "Epoch 384/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6260 - val_loss: 2.0451\n",
      "Epoch 385/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6229 - val_loss: 2.0413\n",
      "Epoch 386/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6199 - val_loss: 2.0376\n",
      "Epoch 387/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6168 - val_loss: 2.0339\n",
      "Epoch 388/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6138 - val_loss: 2.0302\n",
      "Epoch 389/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6109 - val_loss: 2.0265\n",
      "Epoch 390/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6077 - val_loss: 2.0229\n",
      "Epoch 391/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6047 - val_loss: 2.0193\n",
      "Epoch 392/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6017 - val_loss: 2.0157\n",
      "Epoch 393/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5988 - val_loss: 2.0122\n",
      "Epoch 394/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5958 - val_loss: 2.0087\n",
      "Epoch 395/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5930 - val_loss: 2.0048\n",
      "Epoch 396/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5900 - val_loss: 2.0012\n",
      "Epoch 397/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5871 - val_loss: 1.9980\n",
      "Epoch 398/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5842 - val_loss: 1.9944\n",
      "Epoch 399/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5813 - val_loss: 1.9908\n",
      "Epoch 400/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5784 - val_loss: 1.9874\n",
      "Epoch 401/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5756 - val_loss: 1.9837\n",
      "Epoch 402/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5728 - val_loss: 1.9803\n",
      "Epoch 403/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5700 - val_loss: 1.9770\n",
      "Epoch 404/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5671 - val_loss: 1.9735\n",
      "Epoch 405/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5643 - val_loss: 1.9701\n",
      "Epoch 406/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5615 - val_loss: 1.9667\n",
      "Epoch 407/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5587 - val_loss: 1.9634\n",
      "Epoch 408/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5560 - val_loss: 1.9599\n",
      "Epoch 409/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5531 - val_loss: 1.9565\n",
      "Epoch 410/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5504 - val_loss: 1.9534\n",
      "Epoch 411/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5477 - val_loss: 1.9499\n",
      "Epoch 412/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5450 - val_loss: 1.9467\n",
      "Epoch 413/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5423 - val_loss: 1.9433\n",
      "Epoch 414/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5396 - val_loss: 1.9399\n",
      "Epoch 415/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5369 - val_loss: 1.9368\n",
      "Epoch 416/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5342 - val_loss: 1.9334\n",
      "Epoch 417/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5316 - val_loss: 1.9301\n",
      "Epoch 418/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5289 - val_loss: 1.9269\n",
      "Epoch 419/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5263 - val_loss: 1.9239\n",
      "Epoch 420/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5237 - val_loss: 1.9205\n",
      "Epoch 421/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5210 - val_loss: 1.9173\n",
      "Epoch 422/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5185 - val_loss: 1.9143\n",
      "Epoch 423/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5158 - val_loss: 1.9111\n",
      "Epoch 424/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5133 - val_loss: 1.9079\n",
      "Epoch 425/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5107 - val_loss: 1.9047\n",
      "Epoch 426/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5081 - val_loss: 1.9016\n",
      "Epoch 427/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5055 - val_loss: 1.8986\n",
      "Epoch 428/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5030 - val_loss: 1.8956\n",
      "Epoch 429/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5005 - val_loss: 1.8922\n",
      "Epoch 430/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4980 - val_loss: 1.8894\n",
      "Epoch 431/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4955 - val_loss: 1.8862\n",
      "Epoch 432/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4930 - val_loss: 1.8831\n",
      "Epoch 433/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4905 - val_loss: 1.8799\n",
      "Epoch 434/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4881 - val_loss: 1.8772\n",
      "Epoch 435/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4855 - val_loss: 1.8743\n",
      "Epoch 436/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4830 - val_loss: 1.8710\n",
      "Epoch 437/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4807 - val_loss: 1.8682\n",
      "Epoch 438/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4782 - val_loss: 1.8651\n",
      "Epoch 439/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4759 - val_loss: 1.8623\n",
      "Epoch 440/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4734 - val_loss: 1.8592\n",
      "Epoch 441/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4710 - val_loss: 1.8564\n",
      "Epoch 442/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4686 - val_loss: 1.8534\n",
      "Epoch 443/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4662 - val_loss: 1.8505\n",
      "Epoch 444/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4639 - val_loss: 1.8477\n",
      "Epoch 445/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4615 - val_loss: 1.8448\n",
      "Epoch 446/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4591 - val_loss: 1.8418\n",
      "Epoch 447/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4568 - val_loss: 1.8392\n",
      "Epoch 448/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4545 - val_loss: 1.8361\n",
      "Epoch 449/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4522 - val_loss: 1.8332\n",
      "Epoch 450/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4499 - val_loss: 1.8305\n",
      "Epoch 451/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4475 - val_loss: 1.8277\n",
      "Epoch 452/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4452 - val_loss: 1.8250\n",
      "Epoch 453/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4429 - val_loss: 1.8219\n",
      "Epoch 454/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4407 - val_loss: 1.8192\n",
      "Epoch 455/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4384 - val_loss: 1.8166\n",
      "Epoch 456/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4362 - val_loss: 1.8137\n",
      "Epoch 457/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4339 - val_loss: 1.8110\n",
      "Epoch 458/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4316 - val_loss: 1.8082\n",
      "Epoch 459/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4294 - val_loss: 1.8054\n",
      "Epoch 460/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4272 - val_loss: 1.8028\n",
      "Epoch 461/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4249 - val_loss: 1.8000\n",
      "Epoch 462/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4227 - val_loss: 1.7974\n",
      "Epoch 463/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4206 - val_loss: 1.7947\n",
      "Epoch 464/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4184 - val_loss: 1.7919\n",
      "Epoch 465/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4162 - val_loss: 1.7892\n",
      "Epoch 466/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4140 - val_loss: 1.7867\n",
      "Epoch 467/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4119 - val_loss: 1.7840\n",
      "Epoch 468/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4097 - val_loss: 1.7812\n",
      "Epoch 469/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4076 - val_loss: 1.7785\n",
      "Epoch 470/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4054 - val_loss: 1.7762\n",
      "Epoch 471/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4032 - val_loss: 1.7735\n",
      "Epoch 472/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4011 - val_loss: 1.7708\n",
      "Epoch 473/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3990 - val_loss: 1.7683\n",
      "Epoch 474/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3969 - val_loss: 1.7658\n",
      "Epoch 475/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3948 - val_loss: 1.7632\n",
      "Epoch 476/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3928 - val_loss: 1.7605\n",
      "Epoch 477/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3906 - val_loss: 1.7581\n",
      "Epoch 478/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3886 - val_loss: 1.7555\n",
      "Epoch 479/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3865 - val_loss: 1.7528\n",
      "Epoch 480/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3845 - val_loss: 1.7504\n",
      "Epoch 481/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3824 - val_loss: 1.7479\n",
      "Epoch 482/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3803 - val_loss: 1.7455\n",
      "Epoch 483/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3783 - val_loss: 1.7428\n",
      "Epoch 484/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3762 - val_loss: 1.7404\n",
      "Epoch 485/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3743 - val_loss: 1.7379\n",
      "Epoch 486/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3723 - val_loss: 1.7355\n",
      "Epoch 487/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3702 - val_loss: 1.7329\n",
      "Epoch 488/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3682 - val_loss: 1.7305\n",
      "Epoch 489/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3663 - val_loss: 1.7283\n",
      "Epoch 490/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3642 - val_loss: 1.7257\n",
      "Epoch 491/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3623 - val_loss: 1.7232\n",
      "Epoch 492/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3603 - val_loss: 1.7208\n",
      "Epoch 493/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3584 - val_loss: 1.7185\n",
      "Epoch 494/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3564 - val_loss: 1.7160\n",
      "Epoch 495/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3544 - val_loss: 1.7136\n",
      "Epoch 496/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3524 - val_loss: 1.7113\n",
      "Epoch 497/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3505 - val_loss: 1.7088\n",
      "Epoch 498/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3486 - val_loss: 1.7065\n",
      "Epoch 499/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3467 - val_loss: 1.7043\n",
      "Epoch 500/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3448 - val_loss: 1.7017\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:17:18.950006: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "9 pixles average2 = 0.8640522875816994\n",
      "convolution result = 0.795207\n",
      "convolution result2 = 0.86405236\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1682: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        false,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n\",\n",
      "        true,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\n축에 주어진 차원을 따라 input_tensor를 줄입니다. keepdims가 true가 아니면 텐서의 순위는 축의 각 항목에 대해 1씩 감소하며, 이 항목은 고유해야 합니다. Keepdims가 true이면 축소된 차원이 길이 1로 유지됩니다.\\n\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n축의 차원에 걸쳐 요소의 평균을 계산하여 축에 지정된 차원을 따라 input_tensor를 줄입니다.\\n\",\n",
      "        false,\n",
      "        \"\\nrandom seed의 사용 이유 \\n테스트 데이터의 과적합을 테스트 하기 위해서\\n\",\n",
      "        false,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 62.54545454545455,\n",
      "    \"accuracy\": 0.0\n",
      "}\"report1/이순광_46084.ipynb\"\n",
      "[NbConvertApp] Converting notebook report1/이순광_46084.ipynb to python\n",
      "[NbConvertApp] Writing 37280 bytes to report1/이순광_46084.py\n",
      "rm: 'result.json'를 지울 수 없음: 그런 파일이나 디렉터리가 없습니다\n",
      "\u001b[22;0t\u001b]0;IPython: deep/exam_2022\u0007[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "[[2. 3.]\n",
      " [5. 6.]]\n",
      "[[ 4.  6.]\n",
      " [10. 12.]]\n",
      "[[ 7.]\n",
      " [13.]]\n",
      "w의 복사본을 ans01변수에 저장합니다.\n",
      "[[0.2 0.1 0.3 0.4]\n",
      " [0.5 0.6 0.7 0.8]\n",
      " [0.9 1.  1.1 1.2]\n",
      " [1.3 1.4 1.5 1.6]]\n",
      "위 행렬에서 노란색 부분을 뽑아서 v에 저장합니다.\n",
      "[[0.6 0.7 0.8]\n",
      " [1.  1.1 1.2]\n",
      " [1.4 1.5 1.6]]\n",
      "v의 전체 값을 두배로 하여 u에 저장합니다.\n",
      "[[1.2 1.4 1.6]\n",
      " [2.  2.2 2.4]\n",
      " [2.8 3.  3.2]]\n",
      "u의 1,2행(두번째, 세번째 행)에만 1을 더해서 a에 저장합니다.\n",
      "a의 복사본을 ans02변수에 저장합니다.\n",
      "[[3.  3.2 3.4]\n",
      " [3.8 4.  4.2]]\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:216: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "무한대 도달 시 n_iter_32 :  11\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:253: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "무한대 도달 시 n_iter_64 :  30\n",
      "[inf inf]\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:271: RuntimeWarning: invalid value encountered in subtract\n",
      "  print(x - x)    # inf를 연산하면 invalid값인 nan이 발생할 수 있습니다.\n",
      "[nan nan]\n",
      "x shape :  (3, 2)\n",
      "x.T shape :  (2, 3)\n",
      "[[17 23  9]\n",
      " [39 53 27]]\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "[[1.2 0.  0. ]\n",
      " [0.  1.2 0. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "이미지 해상도 :  (200, 200, 3)\n",
      "[[  1.2   0.  -20. ]\n",
      " [  0.    1.2 -20. ]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[  0.  -1. 200.]\n",
      " [  1.   0.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "[[ -1.   0. 200.]\n",
      " [  0.   1.   0.]]\n",
      "<Figure size 432x288 with 2 Axes>\n",
      "(400, 200, 3)\n",
      "<Figure size 432x288 with 1 Axes>\n",
      "(200, 400, 3)\n",
      "<Figure size 432x288 with 1 Axes>\n",
      "(200, 200, 6)\n",
      "100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 304674.68it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "2022-10-14 11:17:25.568968: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 11:17:25.944197: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-14 11:17:25.944237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18616 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:b5:00.0, compute capability: 8.6\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1570      \n",
      "=================================================================\n",
      "Total params: 1,570\n",
      "Trainable params: 1,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-14 11:17:26.165368: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Epoch 1/2\n",
      "Tensor(\"Round:0\", shape=(32, 2), dtype=float32)\n",
      "Tensor(\"Round_1:0\", shape=(32, 2), dtype=float32)\n",
      "Tensor(\"All:0\", shape=(32,), dtype=bool)\n",
      "Tensor(\"Round:0\", shape=(32, 2), dtype=float32)\n",
      "Tensor(\"Round_1:0\", shape=(32, 2), dtype=float32)\n",
      "Tensor(\"All:0\", shape=(32,), dtype=bool)\n",
      "2022-10-14 11:17:26.816697: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 242.5508 - pos_accuracy: 0.0000e+00 Tensor(\"Round:0\", shape=(None, 2), dtype=float32)\n",
      "Tensor(\"Round_1:0\", shape=(None, 2), dtype=float32)\n",
      "Tensor(\"All:0\", shape=(None,), dtype=bool)\n",
      "50/50 [==============================] - 1s 5ms/step - loss: 231.6624 - pos_accuracy: 0.0000e+00 - val_loss: 195.9575 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 197.3638 - pos_accuracy: 0.0019 - val_loss: 167.3064 - val_pos_accuracy: 0.0000e+00\n",
      "100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 99654.39it/s]\n",
      "<Figure size 720x720 with 25 Axes>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 13,170\n",
      "Trainable params: 13,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 230.1856 - pos_accuracy: 0.0025 - val_loss: 205.9118 - val_pos_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 135.7861 - pos_accuracy: 0.0012 - val_loss: 45.1705 - val_pos_accuracy: 0.0022\n",
      "Epoch 3/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 37.4863 - pos_accuracy: 0.0081 - val_loss: 32.2771 - val_pos_accuracy: 0.0045\n",
      "Epoch 4/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 29.0005 - pos_accuracy: 0.0175 - val_loss: 24.7006 - val_pos_accuracy: 0.0112\n",
      "Epoch 5/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 19.4584 - pos_accuracy: 0.0269 - val_loss: 13.6079 - val_pos_accuracy: 0.0424\n",
      "Epoch 6/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 9.5821 - pos_accuracy: 0.0556 - val_loss: 6.0044 - val_pos_accuracy: 0.0826\n",
      "Epoch 7/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4.8327 - pos_accuracy: 0.1056 - val_loss: 3.4458 - val_pos_accuracy: 0.1496\n",
      "Epoch 8/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3.2587 - pos_accuracy: 0.1456 - val_loss: 2.4958 - val_pos_accuracy: 0.1674\n",
      "Epoch 9/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5448 - pos_accuracy: 0.1819 - val_loss: 2.0023 - val_pos_accuracy: 0.2054\n",
      "Epoch 10/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.1278 - pos_accuracy: 0.1863 - val_loss: 1.7356 - val_pos_accuracy: 0.2299\n",
      "Epoch 11/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8553 - pos_accuracy: 0.2087 - val_loss: 1.5529 - val_pos_accuracy: 0.2054\n",
      "Epoch 12/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1.6568 - pos_accuracy: 0.2138 - val_loss: 1.4217 - val_pos_accuracy: 0.2321\n",
      "Epoch 13/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4995 - pos_accuracy: 0.2225 - val_loss: 1.3283 - val_pos_accuracy: 0.2098\n",
      "Epoch 14/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3838 - pos_accuracy: 0.2294 - val_loss: 1.2472 - val_pos_accuracy: 0.2143\n",
      "Epoch 15/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2872 - pos_accuracy: 0.2463 - val_loss: 1.1886 - val_pos_accuracy: 0.2076\n",
      "Epoch 16/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1987 - pos_accuracy: 0.2431 - val_loss: 1.1264 - val_pos_accuracy: 0.2210\n",
      "Epoch 17/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1299 - pos_accuracy: 0.2675 - val_loss: 1.0822 - val_pos_accuracy: 0.2121\n",
      "Epoch 18/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0647 - pos_accuracy: 0.2694 - val_loss: 1.0415 - val_pos_accuracy: 0.2098\n",
      "Epoch 19/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0134 - pos_accuracy: 0.2663 - val_loss: 1.0021 - val_pos_accuracy: 0.2321\n",
      "Epoch 20/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9633 - pos_accuracy: 0.2738 - val_loss: 0.9721 - val_pos_accuracy: 0.2433\n",
      "Epoch 21/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9182 - pos_accuracy: 0.2812 - val_loss: 0.9357 - val_pos_accuracy: 0.2902\n",
      "Epoch 22/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.8806 - pos_accuracy: 0.2906 - val_loss: 0.9006 - val_pos_accuracy: 0.2679\n",
      "Epoch 23/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8421 - pos_accuracy: 0.3019 - val_loss: 0.8804 - val_pos_accuracy: 0.2924\n",
      "Epoch 24/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8106 - pos_accuracy: 0.3013 - val_loss: 0.8492 - val_pos_accuracy: 0.2902\n",
      "Epoch 25/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7820 - pos_accuracy: 0.3137 - val_loss: 0.8241 - val_pos_accuracy: 0.3192\n",
      "Epoch 26/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7509 - pos_accuracy: 0.3219 - val_loss: 0.8107 - val_pos_accuracy: 0.2746\n",
      "Epoch 27/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.7244 - pos_accuracy: 0.3281 - val_loss: 0.7822 - val_pos_accuracy: 0.3259\n",
      "Epoch 28/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6998 - pos_accuracy: 0.3475 - val_loss: 0.7732 - val_pos_accuracy: 0.2924\n",
      "Epoch 29/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6808 - pos_accuracy: 0.3469 - val_loss: 0.7502 - val_pos_accuracy: 0.3460\n",
      "Epoch 30/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6594 - pos_accuracy: 0.3600 - val_loss: 0.7335 - val_pos_accuracy: 0.3504\n",
      "Epoch 31/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6387 - pos_accuracy: 0.3638 - val_loss: 0.7199 - val_pos_accuracy: 0.3348\n",
      "Epoch 32/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6225 - pos_accuracy: 0.3738 - val_loss: 0.7035 - val_pos_accuracy: 0.3616\n",
      "Epoch 33/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.6056 - pos_accuracy: 0.3775 - val_loss: 0.6878 - val_pos_accuracy: 0.3571\n",
      "Epoch 34/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5884 - pos_accuracy: 0.3713 - val_loss: 0.6819 - val_pos_accuracy: 0.3549\n",
      "Epoch 35/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5718 - pos_accuracy: 0.3819 - val_loss: 0.6652 - val_pos_accuracy: 0.3594\n",
      "Epoch 36/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5572 - pos_accuracy: 0.3913 - val_loss: 0.6562 - val_pos_accuracy: 0.3661\n",
      "Epoch 37/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5414 - pos_accuracy: 0.3981 - val_loss: 0.6500 - val_pos_accuracy: 0.3839\n",
      "Epoch 38/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5298 - pos_accuracy: 0.4137 - val_loss: 0.6325 - val_pos_accuracy: 0.3728\n",
      "Epoch 39/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5165 - pos_accuracy: 0.4112 - val_loss: 0.6234 - val_pos_accuracy: 0.3772\n",
      "Epoch 40/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.5077 - pos_accuracy: 0.4181 - val_loss: 0.6120 - val_pos_accuracy: 0.3839\n",
      "Epoch 41/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4952 - pos_accuracy: 0.4200 - val_loss: 0.6091 - val_pos_accuracy: 0.4152\n",
      "Epoch 42/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4841 - pos_accuracy: 0.4269 - val_loss: 0.6014 - val_pos_accuracy: 0.3817\n",
      "Epoch 43/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4741 - pos_accuracy: 0.4387 - val_loss: 0.5894 - val_pos_accuracy: 0.3884\n",
      "Epoch 44/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4629 - pos_accuracy: 0.4425 - val_loss: 0.5893 - val_pos_accuracy: 0.3973\n",
      "Epoch 45/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4519 - pos_accuracy: 0.4450 - val_loss: 0.5731 - val_pos_accuracy: 0.4018\n",
      "Epoch 46/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4450 - pos_accuracy: 0.4556 - val_loss: 0.5686 - val_pos_accuracy: 0.4263\n",
      "Epoch 47/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4351 - pos_accuracy: 0.4631 - val_loss: 0.5630 - val_pos_accuracy: 0.4152\n",
      "Epoch 48/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4273 - pos_accuracy: 0.4762 - val_loss: 0.5532 - val_pos_accuracy: 0.4129\n",
      "Epoch 49/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4194 - pos_accuracy: 0.4688 - val_loss: 0.5457 - val_pos_accuracy: 0.4353\n",
      "Epoch 50/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4127 - pos_accuracy: 0.4819 - val_loss: 0.5408 - val_pos_accuracy: 0.4263\n",
      "Epoch 51/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4031 - pos_accuracy: 0.4869 - val_loss: 0.5353 - val_pos_accuracy: 0.4397\n",
      "Epoch 52/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3980 - pos_accuracy: 0.4988 - val_loss: 0.5284 - val_pos_accuracy: 0.4464\n",
      "Epoch 53/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3895 - pos_accuracy: 0.4956 - val_loss: 0.5217 - val_pos_accuracy: 0.4330\n",
      "Epoch 54/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3829 - pos_accuracy: 0.5106 - val_loss: 0.5195 - val_pos_accuracy: 0.4330\n",
      "Epoch 55/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3747 - pos_accuracy: 0.5081 - val_loss: 0.5120 - val_pos_accuracy: 0.4509\n",
      "Epoch 56/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3705 - pos_accuracy: 0.5081 - val_loss: 0.5083 - val_pos_accuracy: 0.4487\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3630 - pos_accuracy: 0.5206 - val_loss: 0.5021 - val_pos_accuracy: 0.4531\n",
      "Epoch 58/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3574 - pos_accuracy: 0.5231 - val_loss: 0.4997 - val_pos_accuracy: 0.4598\n",
      "Epoch 59/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3521 - pos_accuracy: 0.5288 - val_loss: 0.4951 - val_pos_accuracy: 0.4353\n",
      "Epoch 60/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3460 - pos_accuracy: 0.5331 - val_loss: 0.4893 - val_pos_accuracy: 0.4442\n",
      "Epoch 61/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3410 - pos_accuracy: 0.5362 - val_loss: 0.4861 - val_pos_accuracy: 0.4263\n",
      "Epoch 62/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3356 - pos_accuracy: 0.5344 - val_loss: 0.4789 - val_pos_accuracy: 0.4576\n",
      "Epoch 63/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3301 - pos_accuracy: 0.5381 - val_loss: 0.4736 - val_pos_accuracy: 0.4531\n",
      "Epoch 64/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3244 - pos_accuracy: 0.5487 - val_loss: 0.4716 - val_pos_accuracy: 0.4464\n",
      "Epoch 65/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3201 - pos_accuracy: 0.5587 - val_loss: 0.4690 - val_pos_accuracy: 0.4442\n",
      "Epoch 66/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3138 - pos_accuracy: 0.5587 - val_loss: 0.4639 - val_pos_accuracy: 0.4554\n",
      "Epoch 67/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3105 - pos_accuracy: 0.5512 - val_loss: 0.4604 - val_pos_accuracy: 0.4665\n",
      "Epoch 68/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3060 - pos_accuracy: 0.5600 - val_loss: 0.4576 - val_pos_accuracy: 0.4554\n",
      "Epoch 69/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3013 - pos_accuracy: 0.5688 - val_loss: 0.4540 - val_pos_accuracy: 0.4665\n",
      "Epoch 70/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2975 - pos_accuracy: 0.5731 - val_loss: 0.4575 - val_pos_accuracy: 0.4397\n",
      "Epoch 71/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2944 - pos_accuracy: 0.5700 - val_loss: 0.4484 - val_pos_accuracy: 0.4576\n",
      "Epoch 72/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2885 - pos_accuracy: 0.5781 - val_loss: 0.4434 - val_pos_accuracy: 0.4598\n",
      "Epoch 73/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2850 - pos_accuracy: 0.5825 - val_loss: 0.4421 - val_pos_accuracy: 0.4621\n",
      "Epoch 74/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2817 - pos_accuracy: 0.5925 - val_loss: 0.4393 - val_pos_accuracy: 0.4509\n",
      "Epoch 75/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2781 - pos_accuracy: 0.5925 - val_loss: 0.4344 - val_pos_accuracy: 0.4665\n",
      "Epoch 76/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2738 - pos_accuracy: 0.5913 - val_loss: 0.4309 - val_pos_accuracy: 0.4754\n",
      "Epoch 77/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2701 - pos_accuracy: 0.6044 - val_loss: 0.4320 - val_pos_accuracy: 0.4509\n",
      "Epoch 78/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2663 - pos_accuracy: 0.5981 - val_loss: 0.4264 - val_pos_accuracy: 0.4688\n",
      "Epoch 79/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2621 - pos_accuracy: 0.6106 - val_loss: 0.4297 - val_pos_accuracy: 0.4643\n",
      "Epoch 80/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2601 - pos_accuracy: 0.6069 - val_loss: 0.4204 - val_pos_accuracy: 0.4598\n",
      "Epoch 81/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2563 - pos_accuracy: 0.6219 - val_loss: 0.4197 - val_pos_accuracy: 0.4688\n",
      "Epoch 82/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2535 - pos_accuracy: 0.6081 - val_loss: 0.4137 - val_pos_accuracy: 0.4665\n",
      "Epoch 83/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2497 - pos_accuracy: 0.6225 - val_loss: 0.4106 - val_pos_accuracy: 0.4754\n",
      "Epoch 84/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2466 - pos_accuracy: 0.6162 - val_loss: 0.4112 - val_pos_accuracy: 0.4554\n",
      "Epoch 85/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2441 - pos_accuracy: 0.6212 - val_loss: 0.4055 - val_pos_accuracy: 0.4688\n",
      "Epoch 86/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2418 - pos_accuracy: 0.6212 - val_loss: 0.4026 - val_pos_accuracy: 0.4866\n",
      "Epoch 87/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2366 - pos_accuracy: 0.6250 - val_loss: 0.4046 - val_pos_accuracy: 0.4754\n",
      "Epoch 88/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2346 - pos_accuracy: 0.6319 - val_loss: 0.3993 - val_pos_accuracy: 0.4754\n",
      "Epoch 89/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2324 - pos_accuracy: 0.6400 - val_loss: 0.3968 - val_pos_accuracy: 0.4821\n",
      "Epoch 90/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2289 - pos_accuracy: 0.6363 - val_loss: 0.3961 - val_pos_accuracy: 0.4799\n",
      "Epoch 91/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2265 - pos_accuracy: 0.6344 - val_loss: 0.3950 - val_pos_accuracy: 0.4933\n",
      "Epoch 92/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2244 - pos_accuracy: 0.6444 - val_loss: 0.3922 - val_pos_accuracy: 0.4888\n",
      "Epoch 93/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2205 - pos_accuracy: 0.6475 - val_loss: 0.3885 - val_pos_accuracy: 0.4866\n",
      "Epoch 94/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2190 - pos_accuracy: 0.6488 - val_loss: 0.3931 - val_pos_accuracy: 0.4888\n",
      "Epoch 95/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2164 - pos_accuracy: 0.6500 - val_loss: 0.3829 - val_pos_accuracy: 0.4933\n",
      "Epoch 96/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.2129 - pos_accuracy: 0.6550 - val_loss: 0.3819 - val_pos_accuracy: 0.5045\n",
      "Epoch 97/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2108 - pos_accuracy: 0.6513 - val_loss: 0.3800 - val_pos_accuracy: 0.5045\n",
      "Epoch 98/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2077 - pos_accuracy: 0.6587 - val_loss: 0.3782 - val_pos_accuracy: 0.5000\n",
      "Epoch 99/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2063 - pos_accuracy: 0.6587 - val_loss: 0.3750 - val_pos_accuracy: 0.5067\n",
      "Epoch 100/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2033 - pos_accuracy: 0.6600 - val_loss: 0.3754 - val_pos_accuracy: 0.5045\n",
      "Epoch 101/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2015 - pos_accuracy: 0.6650 - val_loss: 0.3718 - val_pos_accuracy: 0.5156\n",
      "Epoch 102/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1993 - pos_accuracy: 0.6662 - val_loss: 0.3716 - val_pos_accuracy: 0.5156\n",
      "Epoch 103/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1976 - pos_accuracy: 0.6694 - val_loss: 0.3666 - val_pos_accuracy: 0.5179\n",
      "Epoch 104/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1948 - pos_accuracy: 0.6750 - val_loss: 0.3686 - val_pos_accuracy: 0.5179\n",
      "Epoch 105/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1929 - pos_accuracy: 0.6769 - val_loss: 0.3687 - val_pos_accuracy: 0.5246\n",
      "Epoch 106/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1896 - pos_accuracy: 0.6744 - val_loss: 0.3644 - val_pos_accuracy: 0.5246\n",
      "Epoch 107/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1883 - pos_accuracy: 0.6819 - val_loss: 0.3600 - val_pos_accuracy: 0.5290\n",
      "Epoch 108/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1857 - pos_accuracy: 0.6900 - val_loss: 0.3599 - val_pos_accuracy: 0.5268\n",
      "Epoch 109/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1841 - pos_accuracy: 0.6837 - val_loss: 0.3582 - val_pos_accuracy: 0.5246\n",
      "Epoch 110/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1824 - pos_accuracy: 0.6919 - val_loss: 0.3585 - val_pos_accuracy: 0.5290\n",
      "Epoch 111/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1800 - pos_accuracy: 0.6925 - val_loss: 0.3565 - val_pos_accuracy: 0.5312\n",
      "Epoch 112/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1791 - pos_accuracy: 0.6969 - val_loss: 0.3540 - val_pos_accuracy: 0.5335\n",
      "Epoch 113/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1762 - pos_accuracy: 0.7050 - val_loss: 0.3516 - val_pos_accuracy: 0.5424\n",
      "Epoch 114/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1743 - pos_accuracy: 0.7013 - val_loss: 0.3490 - val_pos_accuracy: 0.5469\n",
      "Epoch 115/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1733 - pos_accuracy: 0.7113 - val_loss: 0.3496 - val_pos_accuracy: 0.5469\n",
      "Epoch 116/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1711 - pos_accuracy: 0.7094 - val_loss: 0.3487 - val_pos_accuracy: 0.5424\n",
      "Epoch 117/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1694 - pos_accuracy: 0.7094 - val_loss: 0.3447 - val_pos_accuracy: 0.5558\n",
      "Epoch 118/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1683 - pos_accuracy: 0.7131 - val_loss: 0.3453 - val_pos_accuracy: 0.5446\n",
      "Epoch 119/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1660 - pos_accuracy: 0.7262 - val_loss: 0.3430 - val_pos_accuracy: 0.5491\n",
      "Epoch 120/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1645 - pos_accuracy: 0.7244 - val_loss: 0.3419 - val_pos_accuracy: 0.5558\n",
      "Epoch 121/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1625 - pos_accuracy: 0.7325 - val_loss: 0.3433 - val_pos_accuracy: 0.5558\n",
      "Epoch 122/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1607 - pos_accuracy: 0.7344 - val_loss: 0.3401 - val_pos_accuracy: 0.5580\n",
      "Epoch 123/1000\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1588 - pos_accuracy: 0.7331 - val_loss: 0.3413 - val_pos_accuracy: 0.5580\n",
      "Epoch 124/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1576 - pos_accuracy: 0.7312 - val_loss: 0.3372 - val_pos_accuracy: 0.5536\n",
      "Epoch 125/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1557 - pos_accuracy: 0.7444 - val_loss: 0.3347 - val_pos_accuracy: 0.5558\n",
      "Epoch 126/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1551 - pos_accuracy: 0.7425 - val_loss: 0.3332 - val_pos_accuracy: 0.5580\n",
      "Epoch 127/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1530 - pos_accuracy: 0.7444 - val_loss: 0.3310 - val_pos_accuracy: 0.5826\n",
      "Epoch 128/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1515 - pos_accuracy: 0.7494 - val_loss: 0.3328 - val_pos_accuracy: 0.5603\n",
      "Epoch 129/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1506 - pos_accuracy: 0.7525 - val_loss: 0.3304 - val_pos_accuracy: 0.5647\n",
      "Epoch 130/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1489 - pos_accuracy: 0.7500 - val_loss: 0.3304 - val_pos_accuracy: 0.5625\n",
      "Epoch 131/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1472 - pos_accuracy: 0.7550 - val_loss: 0.3268 - val_pos_accuracy: 0.5781\n",
      "Epoch 132/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1458 - pos_accuracy: 0.7544 - val_loss: 0.3300 - val_pos_accuracy: 0.5625\n",
      "Epoch 133/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1445 - pos_accuracy: 0.7656 - val_loss: 0.3258 - val_pos_accuracy: 0.5670\n",
      "Epoch 134/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1427 - pos_accuracy: 0.7613 - val_loss: 0.3280 - val_pos_accuracy: 0.5759\n",
      "Epoch 135/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1418 - pos_accuracy: 0.7656 - val_loss: 0.3216 - val_pos_accuracy: 0.5737\n",
      "Epoch 136/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1412 - pos_accuracy: 0.7681 - val_loss: 0.3238 - val_pos_accuracy: 0.5737\n",
      "Epoch 137/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1391 - pos_accuracy: 0.7706 - val_loss: 0.3207 - val_pos_accuracy: 0.5759\n",
      "Epoch 138/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1378 - pos_accuracy: 0.7744 - val_loss: 0.3227 - val_pos_accuracy: 0.5893\n",
      "Epoch 139/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1372 - pos_accuracy: 0.7719 - val_loss: 0.3180 - val_pos_accuracy: 0.5871\n",
      "Epoch 140/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1352 - pos_accuracy: 0.7763 - val_loss: 0.3169 - val_pos_accuracy: 0.5714\n",
      "Epoch 141/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1343 - pos_accuracy: 0.7781 - val_loss: 0.3148 - val_pos_accuracy: 0.5848\n",
      "Epoch 142/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1330 - pos_accuracy: 0.7769 - val_loss: 0.3149 - val_pos_accuracy: 0.5960\n",
      "Epoch 143/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1319 - pos_accuracy: 0.7844 - val_loss: 0.3142 - val_pos_accuracy: 0.5804\n",
      "Epoch 144/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1303 - pos_accuracy: 0.7869 - val_loss: 0.3129 - val_pos_accuracy: 0.5915\n",
      "Epoch 145/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1295 - pos_accuracy: 0.7900 - val_loss: 0.3148 - val_pos_accuracy: 0.5960\n",
      "Epoch 146/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1284 - pos_accuracy: 0.7956 - val_loss: 0.3095 - val_pos_accuracy: 0.5893\n",
      "Epoch 147/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1268 - pos_accuracy: 0.7906 - val_loss: 0.3076 - val_pos_accuracy: 0.5826\n",
      "Epoch 148/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1262 - pos_accuracy: 0.7969 - val_loss: 0.3078 - val_pos_accuracy: 0.5982\n",
      "Epoch 149/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1250 - pos_accuracy: 0.7962 - val_loss: 0.3084 - val_pos_accuracy: 0.6004\n",
      "Epoch 150/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1240 - pos_accuracy: 0.7987 - val_loss: 0.3078 - val_pos_accuracy: 0.5982\n",
      "Epoch 151/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1225 - pos_accuracy: 0.8069 - val_loss: 0.3049 - val_pos_accuracy: 0.6049\n",
      "Epoch 152/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1213 - pos_accuracy: 0.8062 - val_loss: 0.3047 - val_pos_accuracy: 0.6116\n",
      "Epoch 153/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1205 - pos_accuracy: 0.8075 - val_loss: 0.3025 - val_pos_accuracy: 0.5982\n",
      "Epoch 154/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1194 - pos_accuracy: 0.8094 - val_loss: 0.3052 - val_pos_accuracy: 0.6027\n",
      "Epoch 155/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1186 - pos_accuracy: 0.8075 - val_loss: 0.3029 - val_pos_accuracy: 0.6027\n",
      "Epoch 156/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1176 - pos_accuracy: 0.8169 - val_loss: 0.2995 - val_pos_accuracy: 0.6071\n",
      "Epoch 157/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1163 - pos_accuracy: 0.8144 - val_loss: 0.3020 - val_pos_accuracy: 0.5938\n",
      "Epoch 158/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1155 - pos_accuracy: 0.8169 - val_loss: 0.2995 - val_pos_accuracy: 0.6116\n",
      "Epoch 159/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1146 - pos_accuracy: 0.8194 - val_loss: 0.3001 - val_pos_accuracy: 0.6004\n",
      "Epoch 160/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1136 - pos_accuracy: 0.8231 - val_loss: 0.2974 - val_pos_accuracy: 0.6116\n",
      "Epoch 161/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1126 - pos_accuracy: 0.8275 - val_loss: 0.2962 - val_pos_accuracy: 0.6183\n",
      "Epoch 162/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1121 - pos_accuracy: 0.8256 - val_loss: 0.2951 - val_pos_accuracy: 0.6116\n",
      "Epoch 163/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1110 - pos_accuracy: 0.8263 - val_loss: 0.2943 - val_pos_accuracy: 0.6138\n",
      "Epoch 164/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1102 - pos_accuracy: 0.8238 - val_loss: 0.2926 - val_pos_accuracy: 0.6228\n",
      "Epoch 165/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1089 - pos_accuracy: 0.8319 - val_loss: 0.2933 - val_pos_accuracy: 0.6161\n",
      "Epoch 166/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1081 - pos_accuracy: 0.8306 - val_loss: 0.2919 - val_pos_accuracy: 0.6272\n",
      "Epoch 167/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1074 - pos_accuracy: 0.8294 - val_loss: 0.2939 - val_pos_accuracy: 0.6228\n",
      "Epoch 168/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1068 - pos_accuracy: 0.8319 - val_loss: 0.2906 - val_pos_accuracy: 0.6138\n",
      "Epoch 169/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1055 - pos_accuracy: 0.8325 - val_loss: 0.2901 - val_pos_accuracy: 0.6183\n",
      "Epoch 170/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1048 - pos_accuracy: 0.8356 - val_loss: 0.2903 - val_pos_accuracy: 0.6161\n",
      "Epoch 171/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1037 - pos_accuracy: 0.8419 - val_loss: 0.2901 - val_pos_accuracy: 0.6183\n",
      "Epoch 172/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1028 - pos_accuracy: 0.8338 - val_loss: 0.2891 - val_pos_accuracy: 0.6250\n",
      "Epoch 173/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1027 - pos_accuracy: 0.8475 - val_loss: 0.2879 - val_pos_accuracy: 0.6183\n",
      "Epoch 174/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1013 - pos_accuracy: 0.8462 - val_loss: 0.2856 - val_pos_accuracy: 0.6317\n",
      "Epoch 175/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1007 - pos_accuracy: 0.8431 - val_loss: 0.2839 - val_pos_accuracy: 0.6339\n",
      "Epoch 176/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0998 - pos_accuracy: 0.8506 - val_loss: 0.2833 - val_pos_accuracy: 0.6317\n",
      "Epoch 177/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0991 - pos_accuracy: 0.8500 - val_loss: 0.2829 - val_pos_accuracy: 0.6339\n",
      "Epoch 178/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0985 - pos_accuracy: 0.8475 - val_loss: 0.2832 - val_pos_accuracy: 0.6362\n",
      "Epoch 179/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0973 - pos_accuracy: 0.8500 - val_loss: 0.2839 - val_pos_accuracy: 0.6272\n",
      "Epoch 180/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0966 - pos_accuracy: 0.8544 - val_loss: 0.2809 - val_pos_accuracy: 0.6429\n",
      "Epoch 181/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0960 - pos_accuracy: 0.8537 - val_loss: 0.2796 - val_pos_accuracy: 0.6406\n",
      "Epoch 182/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0954 - pos_accuracy: 0.8531 - val_loss: 0.2795 - val_pos_accuracy: 0.6406\n",
      "Epoch 183/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0947 - pos_accuracy: 0.8544 - val_loss: 0.2817 - val_pos_accuracy: 0.6362\n",
      "Epoch 184/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0939 - pos_accuracy: 0.8600 - val_loss: 0.2788 - val_pos_accuracy: 0.6429\n",
      "Epoch 185/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0932 - pos_accuracy: 0.8556 - val_loss: 0.2776 - val_pos_accuracy: 0.6362\n",
      "Epoch 186/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0924 - pos_accuracy: 0.8569 - val_loss: 0.2767 - val_pos_accuracy: 0.6473\n",
      "Epoch 187/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0917 - pos_accuracy: 0.8587 - val_loss: 0.2759 - val_pos_accuracy: 0.6406\n",
      "Epoch 188/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0910 - pos_accuracy: 0.8600 - val_loss: 0.2745 - val_pos_accuracy: 0.6473\n",
      "Epoch 189/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0909 - pos_accuracy: 0.8587 - val_loss: 0.2749 - val_pos_accuracy: 0.6429\n",
      "Epoch 190/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0896 - pos_accuracy: 0.8612 - val_loss: 0.2743 - val_pos_accuracy: 0.6562\n",
      "Epoch 191/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0890 - pos_accuracy: 0.8606 - val_loss: 0.2731 - val_pos_accuracy: 0.6496\n",
      "Epoch 192/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0885 - pos_accuracy: 0.8687 - val_loss: 0.2723 - val_pos_accuracy: 0.6496\n",
      "Epoch 193/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0879 - pos_accuracy: 0.8637 - val_loss: 0.2715 - val_pos_accuracy: 0.6518\n",
      "Epoch 194/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0872 - pos_accuracy: 0.8662 - val_loss: 0.2714 - val_pos_accuracy: 0.6429\n",
      "Epoch 195/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0864 - pos_accuracy: 0.8681 - val_loss: 0.2703 - val_pos_accuracy: 0.6473\n",
      "Epoch 196/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0860 - pos_accuracy: 0.8662 - val_loss: 0.2692 - val_pos_accuracy: 0.6518\n",
      "Epoch 197/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0854 - pos_accuracy: 0.8644 - val_loss: 0.2691 - val_pos_accuracy: 0.6473\n",
      "Epoch 198/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0848 - pos_accuracy: 0.8675 - val_loss: 0.2691 - val_pos_accuracy: 0.6518\n",
      "Epoch 199/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0838 - pos_accuracy: 0.8675 - val_loss: 0.2693 - val_pos_accuracy: 0.6496\n",
      "Epoch 200/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0838 - pos_accuracy: 0.8694 - val_loss: 0.2676 - val_pos_accuracy: 0.6585\n",
      "Epoch 201/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0827 - pos_accuracy: 0.8731 - val_loss: 0.2673 - val_pos_accuracy: 0.6562\n",
      "Epoch 202/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0821 - pos_accuracy: 0.8687 - val_loss: 0.2667 - val_pos_accuracy: 0.6540\n",
      "Epoch 203/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0816 - pos_accuracy: 0.8731 - val_loss: 0.2647 - val_pos_accuracy: 0.6629\n",
      "Epoch 204/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0812 - pos_accuracy: 0.8744 - val_loss: 0.2650 - val_pos_accuracy: 0.6562\n",
      "Epoch 205/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0805 - pos_accuracy: 0.8781 - val_loss: 0.2645 - val_pos_accuracy: 0.6540\n",
      "Epoch 206/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0801 - pos_accuracy: 0.8756 - val_loss: 0.2633 - val_pos_accuracy: 0.6540\n",
      "Epoch 207/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0793 - pos_accuracy: 0.8756 - val_loss: 0.2632 - val_pos_accuracy: 0.6562\n",
      "Epoch 208/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0787 - pos_accuracy: 0.8769 - val_loss: 0.2611 - val_pos_accuracy: 0.6629\n",
      "Epoch 209/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0783 - pos_accuracy: 0.8744 - val_loss: 0.2616 - val_pos_accuracy: 0.6674\n",
      "Epoch 210/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0777 - pos_accuracy: 0.8806 - val_loss: 0.2618 - val_pos_accuracy: 0.6562\n",
      "Epoch 211/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0774 - pos_accuracy: 0.8763 - val_loss: 0.2608 - val_pos_accuracy: 0.6585\n",
      "Epoch 212/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0770 - pos_accuracy: 0.8794 - val_loss: 0.2597 - val_pos_accuracy: 0.6585\n",
      "Epoch 213/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0763 - pos_accuracy: 0.8794 - val_loss: 0.2601 - val_pos_accuracy: 0.6652\n",
      "Epoch 214/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0757 - pos_accuracy: 0.8813 - val_loss: 0.2587 - val_pos_accuracy: 0.6562\n",
      "Epoch 215/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0751 - pos_accuracy: 0.8781 - val_loss: 0.2580 - val_pos_accuracy: 0.6607\n",
      "Epoch 216/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0749 - pos_accuracy: 0.8825 - val_loss: 0.2576 - val_pos_accuracy: 0.6719\n",
      "Epoch 217/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0743 - pos_accuracy: 0.8800 - val_loss: 0.2574 - val_pos_accuracy: 0.6629\n",
      "Epoch 218/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0737 - pos_accuracy: 0.8819 - val_loss: 0.2575 - val_pos_accuracy: 0.6652\n",
      "Epoch 219/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0733 - pos_accuracy: 0.8813 - val_loss: 0.2576 - val_pos_accuracy: 0.6629\n",
      "Epoch 220/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0727 - pos_accuracy: 0.8831 - val_loss: 0.2558 - val_pos_accuracy: 0.6741\n",
      "Epoch 221/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0722 - pos_accuracy: 0.8881 - val_loss: 0.2546 - val_pos_accuracy: 0.6741\n",
      "Epoch 222/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0716 - pos_accuracy: 0.8844 - val_loss: 0.2539 - val_pos_accuracy: 0.6808\n",
      "Epoch 223/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0711 - pos_accuracy: 0.8881 - val_loss: 0.2531 - val_pos_accuracy: 0.6786\n",
      "Epoch 224/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0710 - pos_accuracy: 0.8869 - val_loss: 0.2543 - val_pos_accuracy: 0.6830\n",
      "Epoch 225/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0702 - pos_accuracy: 0.8906 - val_loss: 0.2535 - val_pos_accuracy: 0.6987\n",
      "Epoch 226/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0699 - pos_accuracy: 0.8900 - val_loss: 0.2528 - val_pos_accuracy: 0.6763\n",
      "Epoch 227/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0693 - pos_accuracy: 0.8863 - val_loss: 0.2517 - val_pos_accuracy: 0.6763\n",
      "Epoch 228/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0687 - pos_accuracy: 0.8919 - val_loss: 0.2518 - val_pos_accuracy: 0.6786\n",
      "Epoch 229/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0684 - pos_accuracy: 0.8900 - val_loss: 0.2517 - val_pos_accuracy: 0.7054\n",
      "Epoch 230/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0680 - pos_accuracy: 0.8938 - val_loss: 0.2508 - val_pos_accuracy: 0.6897\n",
      "Epoch 231/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0673 - pos_accuracy: 0.8913 - val_loss: 0.2517 - val_pos_accuracy: 0.6964\n",
      "Epoch 232/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0671 - pos_accuracy: 0.8888 - val_loss: 0.2497 - val_pos_accuracy: 0.6853\n",
      "Epoch 233/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0666 - pos_accuracy: 0.8919 - val_loss: 0.2508 - val_pos_accuracy: 0.6830\n",
      "Epoch 234/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0664 - pos_accuracy: 0.8919 - val_loss: 0.2481 - val_pos_accuracy: 0.6830\n",
      "Epoch 235/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0657 - pos_accuracy: 0.8925 - val_loss: 0.2482 - val_pos_accuracy: 0.6897\n",
      "Epoch 236/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0653 - pos_accuracy: 0.8950 - val_loss: 0.2471 - val_pos_accuracy: 0.6875\n",
      "Epoch 237/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0651 - pos_accuracy: 0.8950 - val_loss: 0.2457 - val_pos_accuracy: 0.6897\n",
      "Epoch 238/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0647 - pos_accuracy: 0.8925 - val_loss: 0.2466 - val_pos_accuracy: 0.6897\n",
      "Epoch 239/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0640 - pos_accuracy: 0.8956 - val_loss: 0.2461 - val_pos_accuracy: 0.7076\n",
      "Epoch 240/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0636 - pos_accuracy: 0.8956 - val_loss: 0.2451 - val_pos_accuracy: 0.7076\n",
      "Epoch 241/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0638 - pos_accuracy: 0.8969 - val_loss: 0.2446 - val_pos_accuracy: 0.6920\n",
      "Epoch 242/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0629 - pos_accuracy: 0.8963 - val_loss: 0.2443 - val_pos_accuracy: 0.7121\n",
      "Epoch 243/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0623 - pos_accuracy: 0.8969 - val_loss: 0.2451 - val_pos_accuracy: 0.6964\n",
      "Epoch 244/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0622 - pos_accuracy: 0.8994 - val_loss: 0.2441 - val_pos_accuracy: 0.6920\n",
      "Epoch 245/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0616 - pos_accuracy: 0.8956 - val_loss: 0.2438 - val_pos_accuracy: 0.7121\n",
      "Epoch 246/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0615 - pos_accuracy: 0.8975 - val_loss: 0.2423 - val_pos_accuracy: 0.7098\n",
      "Epoch 247/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0610 - pos_accuracy: 0.8975 - val_loss: 0.2427 - val_pos_accuracy: 0.7121\n",
      "Epoch 248/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0607 - pos_accuracy: 0.9019 - val_loss: 0.2435 - val_pos_accuracy: 0.7098\n",
      "Epoch 249/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0601 - pos_accuracy: 0.9038 - val_loss: 0.2414 - val_pos_accuracy: 0.7009\n",
      "Epoch 250/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0598 - pos_accuracy: 0.9006 - val_loss: 0.2408 - val_pos_accuracy: 0.7165\n",
      "Epoch 251/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0594 - pos_accuracy: 0.9019 - val_loss: 0.2408 - val_pos_accuracy: 0.7210\n",
      "Epoch 252/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0590 - pos_accuracy: 0.9056 - val_loss: 0.2402 - val_pos_accuracy: 0.7188\n",
      "Epoch 253/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0587 - pos_accuracy: 0.9025 - val_loss: 0.2393 - val_pos_accuracy: 0.7254\n",
      "Epoch 254/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0584 - pos_accuracy: 0.9056 - val_loss: 0.2394 - val_pos_accuracy: 0.7299\n",
      "Epoch 255/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0581 - pos_accuracy: 0.9050 - val_loss: 0.2390 - val_pos_accuracy: 0.7366\n",
      "Epoch 256/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0577 - pos_accuracy: 0.9094 - val_loss: 0.2396 - val_pos_accuracy: 0.7143\n",
      "Epoch 257/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0572 - pos_accuracy: 0.9106 - val_loss: 0.2382 - val_pos_accuracy: 0.7321\n",
      "Epoch 258/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0570 - pos_accuracy: 0.9100 - val_loss: 0.2378 - val_pos_accuracy: 0.7277\n",
      "Epoch 259/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0565 - pos_accuracy: 0.9125 - val_loss: 0.2383 - val_pos_accuracy: 0.7210\n",
      "Epoch 260/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0565 - pos_accuracy: 0.9137 - val_loss: 0.2371 - val_pos_accuracy: 0.7344\n",
      "Epoch 261/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0558 - pos_accuracy: 0.9144 - val_loss: 0.2367 - val_pos_accuracy: 0.7411\n",
      "Epoch 262/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0556 - pos_accuracy: 0.9162 - val_loss: 0.2363 - val_pos_accuracy: 0.7455\n",
      "Epoch 263/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0553 - pos_accuracy: 0.9156 - val_loss: 0.2356 - val_pos_accuracy: 0.7388\n",
      "Epoch 264/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0550 - pos_accuracy: 0.9150 - val_loss: 0.2352 - val_pos_accuracy: 0.7545\n",
      "Epoch 265/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0548 - pos_accuracy: 0.9200 - val_loss: 0.2346 - val_pos_accuracy: 0.7388\n",
      "Epoch 266/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0542 - pos_accuracy: 0.9181 - val_loss: 0.2353 - val_pos_accuracy: 0.7455\n",
      "Epoch 267/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0539 - pos_accuracy: 0.9219 - val_loss: 0.2339 - val_pos_accuracy: 0.7433\n",
      "Epoch 268/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0537 - pos_accuracy: 0.9187 - val_loss: 0.2333 - val_pos_accuracy: 0.7522\n",
      "Epoch 269/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0534 - pos_accuracy: 0.9200 - val_loss: 0.2339 - val_pos_accuracy: 0.7388\n",
      "Epoch 270/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0530 - pos_accuracy: 0.9275 - val_loss: 0.2356 - val_pos_accuracy: 0.7366\n",
      "Epoch 271/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0525 - pos_accuracy: 0.9262 - val_loss: 0.2333 - val_pos_accuracy: 0.7411\n",
      "Epoch 272/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0523 - pos_accuracy: 0.9294 - val_loss: 0.2326 - val_pos_accuracy: 0.7634\n",
      "Epoch 273/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0520 - pos_accuracy: 0.9294 - val_loss: 0.2332 - val_pos_accuracy: 0.7500\n",
      "Epoch 274/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0515 - pos_accuracy: 0.9306 - val_loss: 0.2329 - val_pos_accuracy: 0.7455\n",
      "Epoch 275/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0515 - pos_accuracy: 0.9287 - val_loss: 0.2309 - val_pos_accuracy: 0.7589\n",
      "Epoch 276/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0511 - pos_accuracy: 0.9331 - val_loss: 0.2304 - val_pos_accuracy: 0.7589\n",
      "Epoch 277/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0507 - pos_accuracy: 0.9337 - val_loss: 0.2308 - val_pos_accuracy: 0.7500\n",
      "Epoch 278/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0504 - pos_accuracy: 0.9319 - val_loss: 0.2302 - val_pos_accuracy: 0.7567\n",
      "Epoch 279/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0501 - pos_accuracy: 0.9344 - val_loss: 0.2334 - val_pos_accuracy: 0.7522\n",
      "Epoch 280/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0500 - pos_accuracy: 0.9319 - val_loss: 0.2299 - val_pos_accuracy: 0.7545\n",
      "Epoch 281/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0497 - pos_accuracy: 0.9350 - val_loss: 0.2288 - val_pos_accuracy: 0.7634\n",
      "Epoch 282/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0494 - pos_accuracy: 0.9356 - val_loss: 0.2287 - val_pos_accuracy: 0.7545\n",
      "Epoch 283/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0490 - pos_accuracy: 0.9325 - val_loss: 0.2288 - val_pos_accuracy: 0.7567\n",
      "Epoch 284/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0487 - pos_accuracy: 0.9350 - val_loss: 0.2283 - val_pos_accuracy: 0.7656\n",
      "Epoch 285/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0485 - pos_accuracy: 0.9369 - val_loss: 0.2279 - val_pos_accuracy: 0.7679\n",
      "Epoch 286/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0482 - pos_accuracy: 0.9394 - val_loss: 0.2280 - val_pos_accuracy: 0.7589\n",
      "Epoch 287/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0480 - pos_accuracy: 0.9381 - val_loss: 0.2272 - val_pos_accuracy: 0.7589\n",
      "Epoch 288/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0475 - pos_accuracy: 0.9400 - val_loss: 0.2266 - val_pos_accuracy: 0.7656\n",
      "Epoch 289/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0474 - pos_accuracy: 0.9394 - val_loss: 0.2264 - val_pos_accuracy: 0.7612\n",
      "Epoch 290/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0470 - pos_accuracy: 0.9406 - val_loss: 0.2267 - val_pos_accuracy: 0.7723\n",
      "Epoch 291/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0467 - pos_accuracy: 0.9438 - val_loss: 0.2263 - val_pos_accuracy: 0.7701\n",
      "Epoch 292/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0466 - pos_accuracy: 0.9400 - val_loss: 0.2265 - val_pos_accuracy: 0.7679\n",
      "Epoch 293/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0462 - pos_accuracy: 0.9425 - val_loss: 0.2251 - val_pos_accuracy: 0.7701\n",
      "Epoch 294/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0461 - pos_accuracy: 0.9450 - val_loss: 0.2252 - val_pos_accuracy: 0.7679\n",
      "Epoch 295/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0459 - pos_accuracy: 0.9413 - val_loss: 0.2240 - val_pos_accuracy: 0.7723\n",
      "Epoch 296/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0454 - pos_accuracy: 0.9431 - val_loss: 0.2241 - val_pos_accuracy: 0.7701\n",
      "Epoch 297/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0451 - pos_accuracy: 0.9438 - val_loss: 0.2235 - val_pos_accuracy: 0.7746\n",
      "Epoch 298/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0449 - pos_accuracy: 0.9456 - val_loss: 0.2245 - val_pos_accuracy: 0.7723\n",
      "Epoch 299/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0446 - pos_accuracy: 0.9438 - val_loss: 0.2225 - val_pos_accuracy: 0.7768\n",
      "Epoch 300/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0443 - pos_accuracy: 0.9463 - val_loss: 0.2234 - val_pos_accuracy: 0.7723\n",
      "Epoch 301/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0442 - pos_accuracy: 0.9463 - val_loss: 0.2224 - val_pos_accuracy: 0.7746\n",
      "Epoch 302/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0441 - pos_accuracy: 0.9469 - val_loss: 0.2236 - val_pos_accuracy: 0.7701\n",
      "Epoch 303/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0436 - pos_accuracy: 0.9456 - val_loss: 0.2229 - val_pos_accuracy: 0.7746\n",
      "Epoch 304/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0436 - pos_accuracy: 0.9463 - val_loss: 0.2219 - val_pos_accuracy: 0.7857\n",
      "Epoch 305/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0433 - pos_accuracy: 0.9469 - val_loss: 0.2212 - val_pos_accuracy: 0.7835\n",
      "Epoch 306/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0431 - pos_accuracy: 0.9469 - val_loss: 0.2219 - val_pos_accuracy: 0.7723\n",
      "Epoch 307/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0426 - pos_accuracy: 0.9500 - val_loss: 0.2220 - val_pos_accuracy: 0.7746\n",
      "Epoch 308/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0424 - pos_accuracy: 0.9475 - val_loss: 0.2205 - val_pos_accuracy: 0.7790\n",
      "Epoch 309/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0422 - pos_accuracy: 0.9481 - val_loss: 0.2204 - val_pos_accuracy: 0.7746\n",
      "Epoch 310/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0421 - pos_accuracy: 0.9500 - val_loss: 0.2209 - val_pos_accuracy: 0.7746\n",
      "Epoch 311/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0418 - pos_accuracy: 0.9475 - val_loss: 0.2203 - val_pos_accuracy: 0.7746\n",
      "Epoch 312/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0416 - pos_accuracy: 0.9500 - val_loss: 0.2195 - val_pos_accuracy: 0.7812\n",
      "Epoch 313/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0414 - pos_accuracy: 0.9488 - val_loss: 0.2195 - val_pos_accuracy: 0.7790\n",
      "Epoch 314/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0410 - pos_accuracy: 0.9500 - val_loss: 0.2184 - val_pos_accuracy: 0.7768\n",
      "Epoch 315/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0409 - pos_accuracy: 0.9481 - val_loss: 0.2189 - val_pos_accuracy: 0.7835\n",
      "Epoch 316/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0408 - pos_accuracy: 0.9506 - val_loss: 0.2184 - val_pos_accuracy: 0.7812\n",
      "Epoch 317/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0405 - pos_accuracy: 0.9488 - val_loss: 0.2185 - val_pos_accuracy: 0.7857\n",
      "Epoch 318/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0403 - pos_accuracy: 0.9500 - val_loss: 0.2175 - val_pos_accuracy: 0.7969\n",
      "Epoch 319/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0400 - pos_accuracy: 0.9488 - val_loss: 0.2174 - val_pos_accuracy: 0.7969\n",
      "Epoch 320/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0398 - pos_accuracy: 0.9531 - val_loss: 0.2175 - val_pos_accuracy: 0.7946\n",
      "Epoch 321/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0394 - pos_accuracy: 0.9519 - val_loss: 0.2179 - val_pos_accuracy: 0.7835\n",
      "Epoch 322/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0393 - pos_accuracy: 0.9544 - val_loss: 0.2170 - val_pos_accuracy: 0.7812\n",
      "Epoch 323/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0390 - pos_accuracy: 0.9519 - val_loss: 0.2162 - val_pos_accuracy: 0.7946\n",
      "Epoch 324/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0389 - pos_accuracy: 0.9519 - val_loss: 0.2164 - val_pos_accuracy: 0.7991\n",
      "Epoch 325/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0387 - pos_accuracy: 0.9544 - val_loss: 0.2155 - val_pos_accuracy: 0.7902\n",
      "Epoch 326/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0385 - pos_accuracy: 0.9550 - val_loss: 0.2159 - val_pos_accuracy: 0.7879\n",
      "Epoch 327/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0381 - pos_accuracy: 0.9563 - val_loss: 0.2148 - val_pos_accuracy: 0.8058\n",
      "Epoch 328/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0381 - pos_accuracy: 0.9544 - val_loss: 0.2161 - val_pos_accuracy: 0.7902\n",
      "Epoch 329/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0378 - pos_accuracy: 0.9544 - val_loss: 0.2148 - val_pos_accuracy: 0.7924\n",
      "Epoch 330/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0375 - pos_accuracy: 0.9556 - val_loss: 0.2155 - val_pos_accuracy: 0.8013\n",
      "Epoch 331/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0374 - pos_accuracy: 0.9581 - val_loss: 0.2147 - val_pos_accuracy: 0.7857\n",
      "Epoch 332/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0371 - pos_accuracy: 0.9556 - val_loss: 0.2143 - val_pos_accuracy: 0.8036\n",
      "Epoch 333/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0369 - pos_accuracy: 0.9569 - val_loss: 0.2147 - val_pos_accuracy: 0.7946\n",
      "Epoch 334/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0369 - pos_accuracy: 0.9544 - val_loss: 0.2140 - val_pos_accuracy: 0.7924\n",
      "Epoch 335/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0367 - pos_accuracy: 0.9588 - val_loss: 0.2145 - val_pos_accuracy: 0.7924\n",
      "Epoch 336/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0365 - pos_accuracy: 0.9550 - val_loss: 0.2125 - val_pos_accuracy: 0.8080\n",
      "Epoch 337/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0363 - pos_accuracy: 0.9594 - val_loss: 0.2127 - val_pos_accuracy: 0.8013\n",
      "Epoch 338/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0360 - pos_accuracy: 0.9594 - val_loss: 0.2123 - val_pos_accuracy: 0.8058\n",
      "Epoch 339/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0358 - pos_accuracy: 0.9606 - val_loss: 0.2124 - val_pos_accuracy: 0.8058\n",
      "Epoch 340/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0356 - pos_accuracy: 0.9594 - val_loss: 0.2123 - val_pos_accuracy: 0.8058\n",
      "Epoch 341/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0355 - pos_accuracy: 0.9606 - val_loss: 0.2111 - val_pos_accuracy: 0.8036\n",
      "Epoch 342/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0353 - pos_accuracy: 0.9588 - val_loss: 0.2118 - val_pos_accuracy: 0.8013\n",
      "Epoch 343/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0351 - pos_accuracy: 0.9588 - val_loss: 0.2116 - val_pos_accuracy: 0.8013\n",
      "Epoch 344/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0349 - pos_accuracy: 0.9600 - val_loss: 0.2109 - val_pos_accuracy: 0.8080\n",
      "Epoch 345/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0348 - pos_accuracy: 0.9606 - val_loss: 0.2110 - val_pos_accuracy: 0.8013\n",
      "Epoch 346/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0346 - pos_accuracy: 0.9619 - val_loss: 0.2103 - val_pos_accuracy: 0.8036\n",
      "Epoch 347/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0342 - pos_accuracy: 0.9613 - val_loss: 0.2110 - val_pos_accuracy: 0.8013\n",
      "Epoch 348/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0341 - pos_accuracy: 0.9625 - val_loss: 0.2101 - val_pos_accuracy: 0.8036\n",
      "Epoch 349/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0339 - pos_accuracy: 0.9606 - val_loss: 0.2106 - val_pos_accuracy: 0.8058\n",
      "Epoch 350/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0338 - pos_accuracy: 0.9619 - val_loss: 0.2099 - val_pos_accuracy: 0.8080\n",
      "Epoch 351/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0336 - pos_accuracy: 0.9631 - val_loss: 0.2095 - val_pos_accuracy: 0.8058\n",
      "Epoch 352/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0334 - pos_accuracy: 0.9638 - val_loss: 0.2100 - val_pos_accuracy: 0.8013\n",
      "Epoch 353/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0332 - pos_accuracy: 0.9631 - val_loss: 0.2093 - val_pos_accuracy: 0.8080\n",
      "Epoch 354/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0332 - pos_accuracy: 0.9644 - val_loss: 0.2090 - val_pos_accuracy: 0.8058\n",
      "Epoch 355/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0331 - pos_accuracy: 0.9606 - val_loss: 0.2096 - val_pos_accuracy: 0.8058\n",
      "Epoch 356/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0328 - pos_accuracy: 0.9650 - val_loss: 0.2085 - val_pos_accuracy: 0.8036\n",
      "Epoch 357/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0326 - pos_accuracy: 0.9644 - val_loss: 0.2089 - val_pos_accuracy: 0.8058\n",
      "Epoch 358/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0325 - pos_accuracy: 0.9631 - val_loss: 0.2085 - val_pos_accuracy: 0.8080\n",
      "Epoch 359/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0321 - pos_accuracy: 0.9650 - val_loss: 0.2090 - val_pos_accuracy: 0.8058\n",
      "Epoch 360/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0320 - pos_accuracy: 0.9650 - val_loss: 0.2081 - val_pos_accuracy: 0.8103\n",
      "Epoch 361/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0319 - pos_accuracy: 0.9638 - val_loss: 0.2078 - val_pos_accuracy: 0.8058\n",
      "Epoch 362/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0317 - pos_accuracy: 0.9656 - val_loss: 0.2070 - val_pos_accuracy: 0.8080\n",
      "Epoch 363/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0315 - pos_accuracy: 0.9650 - val_loss: 0.2084 - val_pos_accuracy: 0.8125\n",
      "Epoch 364/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0314 - pos_accuracy: 0.9663 - val_loss: 0.2079 - val_pos_accuracy: 0.8080\n",
      "Epoch 365/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0312 - pos_accuracy: 0.9681 - val_loss: 0.2066 - val_pos_accuracy: 0.8103\n",
      "Epoch 366/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0312 - pos_accuracy: 0.9663 - val_loss: 0.2065 - val_pos_accuracy: 0.8080\n",
      "Epoch 367/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0310 - pos_accuracy: 0.9663 - val_loss: 0.2066 - val_pos_accuracy: 0.8103\n",
      "Epoch 368/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0307 - pos_accuracy: 0.9681 - val_loss: 0.2058 - val_pos_accuracy: 0.8080\n",
      "Epoch 369/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0306 - pos_accuracy: 0.9681 - val_loss: 0.2062 - val_pos_accuracy: 0.8080\n",
      "Epoch 370/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0306 - pos_accuracy: 0.9656 - val_loss: 0.2059 - val_pos_accuracy: 0.8103\n",
      "Epoch 371/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0302 - pos_accuracy: 0.9669 - val_loss: 0.2064 - val_pos_accuracy: 0.8080\n",
      "Epoch 372/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0301 - pos_accuracy: 0.9681 - val_loss: 0.2059 - val_pos_accuracy: 0.8103\n",
      "Epoch 373/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0300 - pos_accuracy: 0.9675 - val_loss: 0.2053 - val_pos_accuracy: 0.8103\n",
      "Epoch 374/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0299 - pos_accuracy: 0.9688 - val_loss: 0.2049 - val_pos_accuracy: 0.8103\n",
      "Epoch 375/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0297 - pos_accuracy: 0.9675 - val_loss: 0.2057 - val_pos_accuracy: 0.8080\n",
      "Epoch 376/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0295 - pos_accuracy: 0.9681 - val_loss: 0.2045 - val_pos_accuracy: 0.8103\n",
      "Epoch 377/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0295 - pos_accuracy: 0.9681 - val_loss: 0.2045 - val_pos_accuracy: 0.8147\n",
      "Epoch 378/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0292 - pos_accuracy: 0.9688 - val_loss: 0.2038 - val_pos_accuracy: 0.8147\n",
      "Epoch 379/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0290 - pos_accuracy: 0.9688 - val_loss: 0.2040 - val_pos_accuracy: 0.8147\n",
      "Epoch 380/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0290 - pos_accuracy: 0.9700 - val_loss: 0.2038 - val_pos_accuracy: 0.8147\n",
      "Epoch 381/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0287 - pos_accuracy: 0.9688 - val_loss: 0.2036 - val_pos_accuracy: 0.8125\n",
      "Epoch 382/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0287 - pos_accuracy: 0.9700 - val_loss: 0.2041 - val_pos_accuracy: 0.8147\n",
      "Epoch 383/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0285 - pos_accuracy: 0.9712 - val_loss: 0.2030 - val_pos_accuracy: 0.8147\n",
      "Epoch 384/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0284 - pos_accuracy: 0.9706 - val_loss: 0.2035 - val_pos_accuracy: 0.8147\n",
      "Epoch 385/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0281 - pos_accuracy: 0.9719 - val_loss: 0.2038 - val_pos_accuracy: 0.8125\n",
      "Epoch 386/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0282 - pos_accuracy: 0.9694 - val_loss: 0.2029 - val_pos_accuracy: 0.8147\n",
      "Epoch 387/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0279 - pos_accuracy: 0.9719 - val_loss: 0.2029 - val_pos_accuracy: 0.8147\n",
      "Epoch 388/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0278 - pos_accuracy: 0.9712 - val_loss: 0.2023 - val_pos_accuracy: 0.8125\n",
      "Epoch 389/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0278 - pos_accuracy: 0.9700 - val_loss: 0.2023 - val_pos_accuracy: 0.8147\n",
      "Epoch 390/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0275 - pos_accuracy: 0.9712 - val_loss: 0.2017 - val_pos_accuracy: 0.8147\n",
      "Epoch 391/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0275 - pos_accuracy: 0.9706 - val_loss: 0.2013 - val_pos_accuracy: 0.8170\n",
      "Epoch 392/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0273 - pos_accuracy: 0.9712 - val_loss: 0.2012 - val_pos_accuracy: 0.8147\n",
      "Epoch 393/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0272 - pos_accuracy: 0.9719 - val_loss: 0.2025 - val_pos_accuracy: 0.8170\n",
      "Epoch 394/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0270 - pos_accuracy: 0.9700 - val_loss: 0.2019 - val_pos_accuracy: 0.8170\n",
      "Epoch 395/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0268 - pos_accuracy: 0.9719 - val_loss: 0.2014 - val_pos_accuracy: 0.8170\n",
      "Epoch 396/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0267 - pos_accuracy: 0.9737 - val_loss: 0.2010 - val_pos_accuracy: 0.8170\n",
      "Epoch 397/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0266 - pos_accuracy: 0.9725 - val_loss: 0.2014 - val_pos_accuracy: 0.8170\n",
      "Epoch 398/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0265 - pos_accuracy: 0.9719 - val_loss: 0.2002 - val_pos_accuracy: 0.8147\n",
      "Epoch 399/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0264 - pos_accuracy: 0.9737 - val_loss: 0.2003 - val_pos_accuracy: 0.8170\n",
      "Epoch 400/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0263 - pos_accuracy: 0.9737 - val_loss: 0.2004 - val_pos_accuracy: 0.8192\n",
      "Epoch 401/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0261 - pos_accuracy: 0.9731 - val_loss: 0.2004 - val_pos_accuracy: 0.8192\n",
      "Epoch 402/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0260 - pos_accuracy: 0.9737 - val_loss: 0.2002 - val_pos_accuracy: 0.8170\n",
      "Epoch 403/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0259 - pos_accuracy: 0.9744 - val_loss: 0.1996 - val_pos_accuracy: 0.8192\n",
      "Epoch 404/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0258 - pos_accuracy: 0.9731 - val_loss: 0.2001 - val_pos_accuracy: 0.8170\n",
      "Epoch 405/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0256 - pos_accuracy: 0.9750 - val_loss: 0.2001 - val_pos_accuracy: 0.8192\n",
      "Epoch 406/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0256 - pos_accuracy: 0.9737 - val_loss: 0.1988 - val_pos_accuracy: 0.8192\n",
      "Epoch 407/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0255 - pos_accuracy: 0.9744 - val_loss: 0.1991 - val_pos_accuracy: 0.8214\n",
      "Epoch 408/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0252 - pos_accuracy: 0.9756 - val_loss: 0.1989 - val_pos_accuracy: 0.8192\n",
      "Epoch 409/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0251 - pos_accuracy: 0.9756 - val_loss: 0.1998 - val_pos_accuracy: 0.8214\n",
      "Epoch 410/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0250 - pos_accuracy: 0.9750 - val_loss: 0.1991 - val_pos_accuracy: 0.8214\n",
      "Epoch 411/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0248 - pos_accuracy: 0.9744 - val_loss: 0.1980 - val_pos_accuracy: 0.8192\n",
      "Epoch 412/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0247 - pos_accuracy: 0.9750 - val_loss: 0.1981 - val_pos_accuracy: 0.8214\n",
      "Epoch 413/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0247 - pos_accuracy: 0.9756 - val_loss: 0.1978 - val_pos_accuracy: 0.8192\n",
      "Epoch 414/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0246 - pos_accuracy: 0.9756 - val_loss: 0.1981 - val_pos_accuracy: 0.8192\n",
      "Epoch 415/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0245 - pos_accuracy: 0.9762 - val_loss: 0.1977 - val_pos_accuracy: 0.8214\n",
      "Epoch 416/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0243 - pos_accuracy: 0.9756 - val_loss: 0.1979 - val_pos_accuracy: 0.8214\n",
      "Epoch 417/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0242 - pos_accuracy: 0.9756 - val_loss: 0.1977 - val_pos_accuracy: 0.8192\n",
      "Epoch 418/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0241 - pos_accuracy: 0.9769 - val_loss: 0.1973 - val_pos_accuracy: 0.8214\n",
      "Epoch 419/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0240 - pos_accuracy: 0.9762 - val_loss: 0.1974 - val_pos_accuracy: 0.8192\n",
      "Epoch 420/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0239 - pos_accuracy: 0.9756 - val_loss: 0.1972 - val_pos_accuracy: 0.8214\n",
      "Epoch 421/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0237 - pos_accuracy: 0.9762 - val_loss: 0.1967 - val_pos_accuracy: 0.8214\n",
      "Epoch 422/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0236 - pos_accuracy: 0.9769 - val_loss: 0.1973 - val_pos_accuracy: 0.8214\n",
      "Epoch 423/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0236 - pos_accuracy: 0.9781 - val_loss: 0.1965 - val_pos_accuracy: 0.8214\n",
      "Epoch 424/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0234 - pos_accuracy: 0.9781 - val_loss: 0.1967 - val_pos_accuracy: 0.8214\n",
      "Epoch 425/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0234 - pos_accuracy: 0.9781 - val_loss: 0.1966 - val_pos_accuracy: 0.8214\n",
      "Epoch 426/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0232 - pos_accuracy: 0.9781 - val_loss: 0.1962 - val_pos_accuracy: 0.8214\n",
      "Epoch 427/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0232 - pos_accuracy: 0.9775 - val_loss: 0.1959 - val_pos_accuracy: 0.8214\n",
      "Epoch 428/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0229 - pos_accuracy: 0.9775 - val_loss: 0.1953 - val_pos_accuracy: 0.8214\n",
      "Epoch 429/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0229 - pos_accuracy: 0.9781 - val_loss: 0.1956 - val_pos_accuracy: 0.8214\n",
      "Epoch 430/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0228 - pos_accuracy: 0.9775 - val_loss: 0.1955 - val_pos_accuracy: 0.8214\n",
      "Epoch 431/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0227 - pos_accuracy: 0.9775 - val_loss: 0.1953 - val_pos_accuracy: 0.8214\n",
      "Epoch 432/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0226 - pos_accuracy: 0.9775 - val_loss: 0.1948 - val_pos_accuracy: 0.8214\n",
      "Epoch 433/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0224 - pos_accuracy: 0.9781 - val_loss: 0.1948 - val_pos_accuracy: 0.8214\n",
      "Epoch 434/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0224 - pos_accuracy: 0.9781 - val_loss: 0.1948 - val_pos_accuracy: 0.8214\n",
      "Epoch 435/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0223 - pos_accuracy: 0.9781 - val_loss: 0.1950 - val_pos_accuracy: 0.8214\n",
      "Epoch 436/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0221 - pos_accuracy: 0.9781 - val_loss: 0.1947 - val_pos_accuracy: 0.8214\n",
      "Epoch 437/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0220 - pos_accuracy: 0.9781 - val_loss: 0.1944 - val_pos_accuracy: 0.8214\n",
      "Epoch 438/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0219 - pos_accuracy: 0.9775 - val_loss: 0.1945 - val_pos_accuracy: 0.8214\n",
      "Epoch 439/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0218 - pos_accuracy: 0.9787 - val_loss: 0.1940 - val_pos_accuracy: 0.8214\n",
      "Epoch 440/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0217 - pos_accuracy: 0.9787 - val_loss: 0.1939 - val_pos_accuracy: 0.8214\n",
      "Epoch 441/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0216 - pos_accuracy: 0.9787 - val_loss: 0.1936 - val_pos_accuracy: 0.8214\n",
      "Epoch 442/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0214 - pos_accuracy: 0.9787 - val_loss: 0.1937 - val_pos_accuracy: 0.8214\n",
      "Epoch 443/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0214 - pos_accuracy: 0.9787 - val_loss: 0.1935 - val_pos_accuracy: 0.8214\n",
      "Epoch 444/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0213 - pos_accuracy: 0.9787 - val_loss: 0.1937 - val_pos_accuracy: 0.8214\n",
      "Epoch 445/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0212 - pos_accuracy: 0.9787 - val_loss: 0.1929 - val_pos_accuracy: 0.8237\n",
      "Epoch 446/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0211 - pos_accuracy: 0.9787 - val_loss: 0.1925 - val_pos_accuracy: 0.8214\n",
      "Epoch 447/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0210 - pos_accuracy: 0.9787 - val_loss: 0.1927 - val_pos_accuracy: 0.8214\n",
      "Epoch 448/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0210 - pos_accuracy: 0.9787 - val_loss: 0.1925 - val_pos_accuracy: 0.8214\n",
      "Epoch 449/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0209 - pos_accuracy: 0.9787 - val_loss: 0.1927 - val_pos_accuracy: 0.8214\n",
      "Epoch 450/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0207 - pos_accuracy: 0.9787 - val_loss: 0.1930 - val_pos_accuracy: 0.8214\n",
      "Epoch 451/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0207 - pos_accuracy: 0.9787 - val_loss: 0.1920 - val_pos_accuracy: 0.8237\n",
      "Epoch 452/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0205 - pos_accuracy: 0.9794 - val_loss: 0.1923 - val_pos_accuracy: 0.8214\n",
      "Epoch 453/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0204 - pos_accuracy: 0.9794 - val_loss: 0.1921 - val_pos_accuracy: 0.8214\n",
      "Epoch 454/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0204 - pos_accuracy: 0.9787 - val_loss: 0.1919 - val_pos_accuracy: 0.8237\n",
      "Epoch 455/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0203 - pos_accuracy: 0.9794 - val_loss: 0.1920 - val_pos_accuracy: 0.8214\n",
      "Epoch 456/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0203 - pos_accuracy: 0.9794 - val_loss: 0.1915 - val_pos_accuracy: 0.8237\n",
      "Epoch 457/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0201 - pos_accuracy: 0.9787 - val_loss: 0.1921 - val_pos_accuracy: 0.8214\n",
      "Epoch 458/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0201 - pos_accuracy: 0.9787 - val_loss: 0.1920 - val_pos_accuracy: 0.8214\n",
      "Epoch 459/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0199 - pos_accuracy: 0.9794 - val_loss: 0.1916 - val_pos_accuracy: 0.8214\n",
      "Epoch 460/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0199 - pos_accuracy: 0.9800 - val_loss: 0.1915 - val_pos_accuracy: 0.8237\n",
      "Epoch 461/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0197 - pos_accuracy: 0.9794 - val_loss: 0.1909 - val_pos_accuracy: 0.8237\n",
      "Epoch 462/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0197 - pos_accuracy: 0.9794 - val_loss: 0.1908 - val_pos_accuracy: 0.8214\n",
      "Epoch 463/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0196 - pos_accuracy: 0.9794 - val_loss: 0.1908 - val_pos_accuracy: 0.8214\n",
      "Epoch 464/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0194 - pos_accuracy: 0.9787 - val_loss: 0.1902 - val_pos_accuracy: 0.8237\n",
      "Epoch 465/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0194 - pos_accuracy: 0.9794 - val_loss: 0.1900 - val_pos_accuracy: 0.8237\n",
      "Epoch 466/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0193 - pos_accuracy: 0.9800 - val_loss: 0.1907 - val_pos_accuracy: 0.8237\n",
      "Epoch 467/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0192 - pos_accuracy: 0.9787 - val_loss: 0.1903 - val_pos_accuracy: 0.8237\n",
      "Epoch 468/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0191 - pos_accuracy: 0.9794 - val_loss: 0.1904 - val_pos_accuracy: 0.8237\n",
      "Epoch 469/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0190 - pos_accuracy: 0.9794 - val_loss: 0.1904 - val_pos_accuracy: 0.8214\n",
      "Epoch 470/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0190 - pos_accuracy: 0.9794 - val_loss: 0.1895 - val_pos_accuracy: 0.8237\n",
      "Epoch 471/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0189 - pos_accuracy: 0.9794 - val_loss: 0.1903 - val_pos_accuracy: 0.8214\n",
      "Epoch 472/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0187 - pos_accuracy: 0.9800 - val_loss: 0.1894 - val_pos_accuracy: 0.8237\n",
      "Epoch 473/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0188 - pos_accuracy: 0.9800 - val_loss: 0.1892 - val_pos_accuracy: 0.8237\n",
      "Epoch 474/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0187 - pos_accuracy: 0.9794 - val_loss: 0.1897 - val_pos_accuracy: 0.8237\n",
      "Epoch 475/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0186 - pos_accuracy: 0.9800 - val_loss: 0.1890 - val_pos_accuracy: 0.8237\n",
      "Epoch 476/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0184 - pos_accuracy: 0.9800 - val_loss: 0.1886 - val_pos_accuracy: 0.8237\n",
      "Epoch 477/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0184 - pos_accuracy: 0.9800 - val_loss: 0.1890 - val_pos_accuracy: 0.8237\n",
      "Epoch 478/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0183 - pos_accuracy: 0.9800 - val_loss: 0.1887 - val_pos_accuracy: 0.8237\n",
      "Epoch 479/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0182 - pos_accuracy: 0.9800 - val_loss: 0.1887 - val_pos_accuracy: 0.8237\n",
      "Epoch 480/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0182 - pos_accuracy: 0.9806 - val_loss: 0.1885 - val_pos_accuracy: 0.8237\n",
      "Epoch 481/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0180 - pos_accuracy: 0.9800 - val_loss: 0.1884 - val_pos_accuracy: 0.8237\n",
      "Epoch 482/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0180 - pos_accuracy: 0.9806 - val_loss: 0.1888 - val_pos_accuracy: 0.8259\n",
      "Epoch 483/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0179 - pos_accuracy: 0.9800 - val_loss: 0.1880 - val_pos_accuracy: 0.8237\n",
      "Epoch 484/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0179 - pos_accuracy: 0.9800 - val_loss: 0.1888 - val_pos_accuracy: 0.8259\n",
      "Epoch 485/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0178 - pos_accuracy: 0.9806 - val_loss: 0.1878 - val_pos_accuracy: 0.8237\n",
      "Epoch 486/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0177 - pos_accuracy: 0.9812 - val_loss: 0.1877 - val_pos_accuracy: 0.8237\n",
      "Epoch 487/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0177 - pos_accuracy: 0.9812 - val_loss: 0.1873 - val_pos_accuracy: 0.8237\n",
      "Epoch 488/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0176 - pos_accuracy: 0.9812 - val_loss: 0.1876 - val_pos_accuracy: 0.8237\n",
      "Epoch 489/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0175 - pos_accuracy: 0.9812 - val_loss: 0.1880 - val_pos_accuracy: 0.8237\n",
      "Epoch 490/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0174 - pos_accuracy: 0.9812 - val_loss: 0.1873 - val_pos_accuracy: 0.8237\n",
      "Epoch 491/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0173 - pos_accuracy: 0.9812 - val_loss: 0.1876 - val_pos_accuracy: 0.8237\n",
      "Epoch 492/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0173 - pos_accuracy: 0.9812 - val_loss: 0.1872 - val_pos_accuracy: 0.8259\n",
      "Epoch 493/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0171 - pos_accuracy: 0.9825 - val_loss: 0.1877 - val_pos_accuracy: 0.8237\n",
      "Epoch 494/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0172 - pos_accuracy: 0.9812 - val_loss: 0.1872 - val_pos_accuracy: 0.8237\n",
      "Epoch 495/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0170 - pos_accuracy: 0.9819 - val_loss: 0.1864 - val_pos_accuracy: 0.8237\n",
      "Epoch 496/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0169 - pos_accuracy: 0.9812 - val_loss: 0.1868 - val_pos_accuracy: 0.8237\n",
      "Epoch 497/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0170 - pos_accuracy: 0.9819 - val_loss: 0.1864 - val_pos_accuracy: 0.8237\n",
      "Epoch 498/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0168 - pos_accuracy: 0.9819 - val_loss: 0.1865 - val_pos_accuracy: 0.8237\n",
      "Epoch 499/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0167 - pos_accuracy: 0.9831 - val_loss: 0.1864 - val_pos_accuracy: 0.8259\n",
      "Epoch 500/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0167 - pos_accuracy: 0.9819 - val_loss: 0.1861 - val_pos_accuracy: 0.8259\n",
      "Epoch 501/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0166 - pos_accuracy: 0.9819 - val_loss: 0.1865 - val_pos_accuracy: 0.8259\n",
      "Epoch 502/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0165 - pos_accuracy: 0.9825 - val_loss: 0.1864 - val_pos_accuracy: 0.8281\n",
      "Epoch 503/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0165 - pos_accuracy: 0.9819 - val_loss: 0.1860 - val_pos_accuracy: 0.8237\n",
      "Epoch 504/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0164 - pos_accuracy: 0.9812 - val_loss: 0.1858 - val_pos_accuracy: 0.8259\n",
      "Epoch 505/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0164 - pos_accuracy: 0.9812 - val_loss: 0.1858 - val_pos_accuracy: 0.8281\n",
      "Epoch 506/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0163 - pos_accuracy: 0.9819 - val_loss: 0.1860 - val_pos_accuracy: 0.8259\n",
      "Epoch 507/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0162 - pos_accuracy: 0.9825 - val_loss: 0.1854 - val_pos_accuracy: 0.8237\n",
      "Epoch 508/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0161 - pos_accuracy: 0.9819 - val_loss: 0.1851 - val_pos_accuracy: 0.8259\n",
      "Epoch 509/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0160 - pos_accuracy: 0.9819 - val_loss: 0.1851 - val_pos_accuracy: 0.8259\n",
      "Epoch 510/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0160 - pos_accuracy: 0.9819 - val_loss: 0.1854 - val_pos_accuracy: 0.8259\n",
      "Epoch 511/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0159 - pos_accuracy: 0.9819 - val_loss: 0.1851 - val_pos_accuracy: 0.8281\n",
      "Epoch 512/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0159 - pos_accuracy: 0.9819 - val_loss: 0.1845 - val_pos_accuracy: 0.8259\n",
      "Epoch 513/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0158 - pos_accuracy: 0.9831 - val_loss: 0.1848 - val_pos_accuracy: 0.8281\n",
      "Epoch 514/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0157 - pos_accuracy: 0.9825 - val_loss: 0.1846 - val_pos_accuracy: 0.8281\n",
      "Epoch 515/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0157 - pos_accuracy: 0.9825 - val_loss: 0.1849 - val_pos_accuracy: 0.8281\n",
      "Epoch 516/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0156 - pos_accuracy: 0.9831 - val_loss: 0.1846 - val_pos_accuracy: 0.8259\n",
      "Epoch 517/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0155 - pos_accuracy: 0.9825 - val_loss: 0.1840 - val_pos_accuracy: 0.8237\n",
      "Epoch 518/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0155 - pos_accuracy: 0.9831 - val_loss: 0.1843 - val_pos_accuracy: 0.8281\n",
      "Epoch 519/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0153 - pos_accuracy: 0.9831 - val_loss: 0.1843 - val_pos_accuracy: 0.8281\n",
      "Epoch 520/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0153 - pos_accuracy: 0.9825 - val_loss: 0.1838 - val_pos_accuracy: 0.8281\n",
      "Epoch 521/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0153 - pos_accuracy: 0.9837 - val_loss: 0.1835 - val_pos_accuracy: 0.8259\n",
      "Epoch 522/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0152 - pos_accuracy: 0.9819 - val_loss: 0.1841 - val_pos_accuracy: 0.8281\n",
      "Epoch 523/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0151 - pos_accuracy: 0.9837 - val_loss: 0.1837 - val_pos_accuracy: 0.8281\n",
      "Epoch 524/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0151 - pos_accuracy: 0.9831 - val_loss: 0.1833 - val_pos_accuracy: 0.8281\n",
      "Epoch 525/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0150 - pos_accuracy: 0.9837 - val_loss: 0.1835 - val_pos_accuracy: 0.8281\n",
      "Epoch 526/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0149 - pos_accuracy: 0.9837 - val_loss: 0.1834 - val_pos_accuracy: 0.8281\n",
      "Epoch 527/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0149 - pos_accuracy: 0.9831 - val_loss: 0.1833 - val_pos_accuracy: 0.8281\n",
      "Epoch 528/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0148 - pos_accuracy: 0.9837 - val_loss: 0.1826 - val_pos_accuracy: 0.8281\n",
      "Epoch 529/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0148 - pos_accuracy: 0.9837 - val_loss: 0.1835 - val_pos_accuracy: 0.8281\n",
      "Epoch 530/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0147 - pos_accuracy: 0.9837 - val_loss: 0.1826 - val_pos_accuracy: 0.8281\n",
      "Epoch 531/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0146 - pos_accuracy: 0.9831 - val_loss: 0.1827 - val_pos_accuracy: 0.8326\n",
      "Epoch 532/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0145 - pos_accuracy: 0.9850 - val_loss: 0.1825 - val_pos_accuracy: 0.8326\n",
      "Epoch 533/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0145 - pos_accuracy: 0.9837 - val_loss: 0.1828 - val_pos_accuracy: 0.8326\n",
      "Epoch 534/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9844 - val_loss: 0.1824 - val_pos_accuracy: 0.8281\n",
      "Epoch 535/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9844 - val_loss: 0.1824 - val_pos_accuracy: 0.8281\n",
      "Epoch 536/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0144 - pos_accuracy: 0.9844 - val_loss: 0.1828 - val_pos_accuracy: 0.8326\n",
      "Epoch 537/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0143 - pos_accuracy: 0.9844 - val_loss: 0.1825 - val_pos_accuracy: 0.8326\n",
      "Epoch 538/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0142 - pos_accuracy: 0.9844 - val_loss: 0.1822 - val_pos_accuracy: 0.8281\n",
      "Epoch 539/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0141 - pos_accuracy: 0.9850 - val_loss: 0.1819 - val_pos_accuracy: 0.8281\n",
      "Epoch 540/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0141 - pos_accuracy: 0.9850 - val_loss: 0.1817 - val_pos_accuracy: 0.8281\n",
      "Epoch 541/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0140 - pos_accuracy: 0.9850 - val_loss: 0.1819 - val_pos_accuracy: 0.8281\n",
      "Epoch 542/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0140 - pos_accuracy: 0.9850 - val_loss: 0.1821 - val_pos_accuracy: 0.8326\n",
      "Epoch 543/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0139 - pos_accuracy: 0.9844 - val_loss: 0.1812 - val_pos_accuracy: 0.8326\n",
      "Epoch 544/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0139 - pos_accuracy: 0.9850 - val_loss: 0.1812 - val_pos_accuracy: 0.8281\n",
      "Epoch 545/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0138 - pos_accuracy: 0.9862 - val_loss: 0.1813 - val_pos_accuracy: 0.8281\n",
      "Epoch 546/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0138 - pos_accuracy: 0.9856 - val_loss: 0.1813 - val_pos_accuracy: 0.8281\n",
      "Epoch 547/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0137 - pos_accuracy: 0.9856 - val_loss: 0.1815 - val_pos_accuracy: 0.8326\n",
      "Epoch 548/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0136 - pos_accuracy: 0.9850 - val_loss: 0.1812 - val_pos_accuracy: 0.8326\n",
      "Epoch 549/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0136 - pos_accuracy: 0.9844 - val_loss: 0.1808 - val_pos_accuracy: 0.8326\n",
      "Epoch 550/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0135 - pos_accuracy: 0.9850 - val_loss: 0.1810 - val_pos_accuracy: 0.8326\n",
      "Epoch 551/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0135 - pos_accuracy: 0.9856 - val_loss: 0.1804 - val_pos_accuracy: 0.8326\n",
      "Epoch 552/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0134 - pos_accuracy: 0.9856 - val_loss: 0.1810 - val_pos_accuracy: 0.8326\n",
      "Epoch 553/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0133 - pos_accuracy: 0.9856 - val_loss: 0.1803 - val_pos_accuracy: 0.8326\n",
      "Epoch 554/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0133 - pos_accuracy: 0.9856 - val_loss: 0.1806 - val_pos_accuracy: 0.8326\n",
      "Epoch 555/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0132 - pos_accuracy: 0.9856 - val_loss: 0.1806 - val_pos_accuracy: 0.8326\n",
      "Epoch 556/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0132 - pos_accuracy: 0.9862 - val_loss: 0.1809 - val_pos_accuracy: 0.8326\n",
      "Epoch 557/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0131 - pos_accuracy: 0.9856 - val_loss: 0.1808 - val_pos_accuracy: 0.8326\n",
      "Epoch 558/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0131 - pos_accuracy: 0.9856 - val_loss: 0.1804 - val_pos_accuracy: 0.8326\n",
      "Epoch 559/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0130 - pos_accuracy: 0.9856 - val_loss: 0.1802 - val_pos_accuracy: 0.8326\n",
      "Epoch 560/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0130 - pos_accuracy: 0.9869 - val_loss: 0.1798 - val_pos_accuracy: 0.8281\n",
      "Epoch 561/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0130 - pos_accuracy: 0.9862 - val_loss: 0.1799 - val_pos_accuracy: 0.8326\n",
      "Epoch 562/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0129 - pos_accuracy: 0.9856 - val_loss: 0.1801 - val_pos_accuracy: 0.8326\n",
      "Epoch 563/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0128 - pos_accuracy: 0.9862 - val_loss: 0.1799 - val_pos_accuracy: 0.8326\n",
      "Epoch 564/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0128 - pos_accuracy: 0.9862 - val_loss: 0.1794 - val_pos_accuracy: 0.8326\n",
      "Epoch 565/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0127 - pos_accuracy: 0.9862 - val_loss: 0.1793 - val_pos_accuracy: 0.8326\n",
      "Epoch 566/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0126 - pos_accuracy: 0.9869 - val_loss: 0.1790 - val_pos_accuracy: 0.8326\n",
      "Epoch 567/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0126 - pos_accuracy: 0.9869 - val_loss: 0.1796 - val_pos_accuracy: 0.8326\n",
      "Epoch 568/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0126 - pos_accuracy: 0.9862 - val_loss: 0.1790 - val_pos_accuracy: 0.8326\n",
      "Epoch 569/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0126 - pos_accuracy: 0.9856 - val_loss: 0.1794 - val_pos_accuracy: 0.8326\n",
      "Epoch 570/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0125 - pos_accuracy: 0.9862 - val_loss: 0.1790 - val_pos_accuracy: 0.8326\n",
      "Epoch 571/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0124 - pos_accuracy: 0.9856 - val_loss: 0.1788 - val_pos_accuracy: 0.8326\n",
      "Epoch 572/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0124 - pos_accuracy: 0.9856 - val_loss: 0.1790 - val_pos_accuracy: 0.8326\n",
      "Epoch 573/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0123 - pos_accuracy: 0.9856 - val_loss: 0.1785 - val_pos_accuracy: 0.8326\n",
      "Epoch 574/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0123 - pos_accuracy: 0.9862 - val_loss: 0.1788 - val_pos_accuracy: 0.8326\n",
      "Epoch 575/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0122 - pos_accuracy: 0.9869 - val_loss: 0.1784 - val_pos_accuracy: 0.8326\n",
      "Epoch 576/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0122 - pos_accuracy: 0.9862 - val_loss: 0.1783 - val_pos_accuracy: 0.8326\n",
      "Epoch 577/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0121 - pos_accuracy: 0.9869 - val_loss: 0.1781 - val_pos_accuracy: 0.8326\n",
      "Epoch 578/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0121 - pos_accuracy: 0.9862 - val_loss: 0.1779 - val_pos_accuracy: 0.8326\n",
      "Epoch 579/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0120 - pos_accuracy: 0.9862 - val_loss: 0.1782 - val_pos_accuracy: 0.8326\n",
      "Epoch 580/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0120 - pos_accuracy: 0.9869 - val_loss: 0.1780 - val_pos_accuracy: 0.8326\n",
      "Epoch 581/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0120 - pos_accuracy: 0.9862 - val_loss: 0.1779 - val_pos_accuracy: 0.8326\n",
      "Epoch 582/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0119 - pos_accuracy: 0.9862 - val_loss: 0.1782 - val_pos_accuracy: 0.8326\n",
      "Epoch 583/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0118 - pos_accuracy: 0.9862 - val_loss: 0.1779 - val_pos_accuracy: 0.8326\n",
      "Epoch 584/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0118 - pos_accuracy: 0.9856 - val_loss: 0.1778 - val_pos_accuracy: 0.8326\n",
      "Epoch 585/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0117 - pos_accuracy: 0.9856 - val_loss: 0.1777 - val_pos_accuracy: 0.8326\n",
      "Epoch 586/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0117 - pos_accuracy: 0.9856 - val_loss: 0.1773 - val_pos_accuracy: 0.8326\n",
      "Epoch 587/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0116 - pos_accuracy: 0.9862 - val_loss: 0.1774 - val_pos_accuracy: 0.8326\n",
      "Epoch 588/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0116 - pos_accuracy: 0.9856 - val_loss: 0.1772 - val_pos_accuracy: 0.8326\n",
      "Epoch 589/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0116 - pos_accuracy: 0.9856 - val_loss: 0.1768 - val_pos_accuracy: 0.8326\n",
      "Epoch 590/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0115 - pos_accuracy: 0.9869 - val_loss: 0.1770 - val_pos_accuracy: 0.8326\n",
      "Epoch 591/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0115 - pos_accuracy: 0.9856 - val_loss: 0.1769 - val_pos_accuracy: 0.8326\n",
      "Epoch 592/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0115 - pos_accuracy: 0.9856 - val_loss: 0.1773 - val_pos_accuracy: 0.8326\n",
      "Epoch 593/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0114 - pos_accuracy: 0.9856 - val_loss: 0.1770 - val_pos_accuracy: 0.8326\n",
      "Epoch 594/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0114 - pos_accuracy: 0.9856 - val_loss: 0.1766 - val_pos_accuracy: 0.8326\n",
      "Epoch 595/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0112 - pos_accuracy: 0.9856 - val_loss: 0.1774 - val_pos_accuracy: 0.8348\n",
      "Epoch 596/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0113 - pos_accuracy: 0.9856 - val_loss: 0.1766 - val_pos_accuracy: 0.8326\n",
      "Epoch 597/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0112 - pos_accuracy: 0.9856 - val_loss: 0.1765 - val_pos_accuracy: 0.8326\n",
      "Epoch 598/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0112 - pos_accuracy: 0.9856 - val_loss: 0.1763 - val_pos_accuracy: 0.8326\n",
      "Epoch 599/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0111 - pos_accuracy: 0.9856 - val_loss: 0.1765 - val_pos_accuracy: 0.8326\n",
      "Epoch 600/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0111 - pos_accuracy: 0.9856 - val_loss: 0.1762 - val_pos_accuracy: 0.8326\n",
      "Epoch 601/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0110 - pos_accuracy: 0.9856 - val_loss: 0.1761 - val_pos_accuracy: 0.8326\n",
      "Epoch 602/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0110 - pos_accuracy: 0.9856 - val_loss: 0.1763 - val_pos_accuracy: 0.8326\n",
      "Epoch 603/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0110 - pos_accuracy: 0.9856 - val_loss: 0.1760 - val_pos_accuracy: 0.8326\n",
      "Epoch 604/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0109 - pos_accuracy: 0.9856 - val_loss: 0.1761 - val_pos_accuracy: 0.8326\n",
      "Epoch 605/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0109 - pos_accuracy: 0.9856 - val_loss: 0.1759 - val_pos_accuracy: 0.8326\n",
      "Epoch 606/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0108 - pos_accuracy: 0.9856 - val_loss: 0.1758 - val_pos_accuracy: 0.8348\n",
      "Epoch 607/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0108 - pos_accuracy: 0.9856 - val_loss: 0.1756 - val_pos_accuracy: 0.8326\n",
      "Epoch 608/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0108 - pos_accuracy: 0.9856 - val_loss: 0.1759 - val_pos_accuracy: 0.8348\n",
      "Epoch 609/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0107 - pos_accuracy: 0.9856 - val_loss: 0.1753 - val_pos_accuracy: 0.8348\n",
      "Epoch 610/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0107 - pos_accuracy: 0.9856 - val_loss: 0.1752 - val_pos_accuracy: 0.8326\n",
      "Epoch 611/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0107 - pos_accuracy: 0.9856 - val_loss: 0.1752 - val_pos_accuracy: 0.8326\n",
      "Epoch 612/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0106 - pos_accuracy: 0.9856 - val_loss: 0.1751 - val_pos_accuracy: 0.8326\n",
      "Epoch 613/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0105 - pos_accuracy: 0.9856 - val_loss: 0.1752 - val_pos_accuracy: 0.8326\n",
      "Epoch 614/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0105 - pos_accuracy: 0.9856 - val_loss: 0.1757 - val_pos_accuracy: 0.8348\n",
      "Epoch 615/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0105 - pos_accuracy: 0.9856 - val_loss: 0.1750 - val_pos_accuracy: 0.8326\n",
      "Epoch 616/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0105 - pos_accuracy: 0.9856 - val_loss: 0.1752 - val_pos_accuracy: 0.8326\n",
      "Epoch 617/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0104 - pos_accuracy: 0.9856 - val_loss: 0.1749 - val_pos_accuracy: 0.8326\n",
      "Epoch 618/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0103 - pos_accuracy: 0.9856 - val_loss: 0.1748 - val_pos_accuracy: 0.8348\n",
      "Epoch 619/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0103 - pos_accuracy: 0.9862 - val_loss: 0.1748 - val_pos_accuracy: 0.8348\n",
      "Epoch 620/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0103 - pos_accuracy: 0.9862 - val_loss: 0.1743 - val_pos_accuracy: 0.8326\n",
      "Epoch 621/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0102 - pos_accuracy: 0.9862 - val_loss: 0.1747 - val_pos_accuracy: 0.8348\n",
      "Epoch 622/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0102 - pos_accuracy: 0.9856 - val_loss: 0.1745 - val_pos_accuracy: 0.8326\n",
      "Epoch 623/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0101 - pos_accuracy: 0.9862 - val_loss: 0.1739 - val_pos_accuracy: 0.8326\n",
      "Epoch 624/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0102 - pos_accuracy: 0.9869 - val_loss: 0.1741 - val_pos_accuracy: 0.8348\n",
      "Epoch 625/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0101 - pos_accuracy: 0.9869 - val_loss: 0.1744 - val_pos_accuracy: 0.8326\n",
      "Epoch 626/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0101 - pos_accuracy: 0.9869 - val_loss: 0.1743 - val_pos_accuracy: 0.8326\n",
      "Epoch 627/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0100 - pos_accuracy: 0.9862 - val_loss: 0.1741 - val_pos_accuracy: 0.8348\n",
      "Epoch 628/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0100 - pos_accuracy: 0.9856 - val_loss: 0.1743 - val_pos_accuracy: 0.8348\n",
      "Epoch 629/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0099 - pos_accuracy: 0.9856 - val_loss: 0.1743 - val_pos_accuracy: 0.8348\n",
      "Epoch 630/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0099 - pos_accuracy: 0.9856 - val_loss: 0.1735 - val_pos_accuracy: 0.8348\n",
      "Epoch 631/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0098 - pos_accuracy: 0.9862 - val_loss: 0.1740 - val_pos_accuracy: 0.8348\n",
      "Epoch 632/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0098 - pos_accuracy: 0.9869 - val_loss: 0.1740 - val_pos_accuracy: 0.8348\n",
      "Epoch 633/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0098 - pos_accuracy: 0.9869 - val_loss: 0.1734 - val_pos_accuracy: 0.8371\n",
      "Epoch 634/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0098 - pos_accuracy: 0.9875 - val_loss: 0.1734 - val_pos_accuracy: 0.8348\n",
      "Epoch 635/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0097 - pos_accuracy: 0.9862 - val_loss: 0.1734 - val_pos_accuracy: 0.8326\n",
      "Epoch 636/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0097 - pos_accuracy: 0.9862 - val_loss: 0.1735 - val_pos_accuracy: 0.8348\n",
      "Epoch 637/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0096 - pos_accuracy: 0.9869 - val_loss: 0.1731 - val_pos_accuracy: 0.8348\n",
      "Epoch 638/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0096 - pos_accuracy: 0.9881 - val_loss: 0.1732 - val_pos_accuracy: 0.8371\n",
      "Epoch 639/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0096 - pos_accuracy: 0.9869 - val_loss: 0.1732 - val_pos_accuracy: 0.8348\n",
      "Epoch 640/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9875 - val_loss: 0.1732 - val_pos_accuracy: 0.8438\n",
      "Epoch 641/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9869 - val_loss: 0.1731 - val_pos_accuracy: 0.8460\n",
      "Epoch 642/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0094 - pos_accuracy: 0.9881 - val_loss: 0.1727 - val_pos_accuracy: 0.8348\n",
      "Epoch 643/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0095 - pos_accuracy: 0.9881 - val_loss: 0.1726 - val_pos_accuracy: 0.8438\n",
      "Epoch 644/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0094 - pos_accuracy: 0.9875 - val_loss: 0.1729 - val_pos_accuracy: 0.8438\n",
      "Epoch 645/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0094 - pos_accuracy: 0.9875 - val_loss: 0.1728 - val_pos_accuracy: 0.8460\n",
      "Epoch 646/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0094 - pos_accuracy: 0.9887 - val_loss: 0.1726 - val_pos_accuracy: 0.8460\n",
      "Epoch 647/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0093 - pos_accuracy: 0.9875 - val_loss: 0.1724 - val_pos_accuracy: 0.8482\n",
      "Epoch 648/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0093 - pos_accuracy: 0.9887 - val_loss: 0.1726 - val_pos_accuracy: 0.8438\n",
      "Epoch 649/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0092 - pos_accuracy: 0.9887 - val_loss: 0.1723 - val_pos_accuracy: 0.8482\n",
      "Epoch 650/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0092 - pos_accuracy: 0.9887 - val_loss: 0.1721 - val_pos_accuracy: 0.8438\n",
      "Epoch 651/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0092 - pos_accuracy: 0.9887 - val_loss: 0.1722 - val_pos_accuracy: 0.8460\n",
      "Epoch 652/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0091 - pos_accuracy: 0.9887 - val_loss: 0.1720 - val_pos_accuracy: 0.8460\n",
      "Epoch 653/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0091 - pos_accuracy: 0.9887 - val_loss: 0.1720 - val_pos_accuracy: 0.8482\n",
      "Epoch 654/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0091 - pos_accuracy: 0.9887 - val_loss: 0.1723 - val_pos_accuracy: 0.8438\n",
      "Epoch 655/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0090 - pos_accuracy: 0.9900 - val_loss: 0.1718 - val_pos_accuracy: 0.8460\n",
      "Epoch 656/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0090 - pos_accuracy: 0.9894 - val_loss: 0.1721 - val_pos_accuracy: 0.8438\n",
      "Epoch 657/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0090 - pos_accuracy: 0.9887 - val_loss: 0.1719 - val_pos_accuracy: 0.8482\n",
      "Epoch 658/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0089 - pos_accuracy: 0.9900 - val_loss: 0.1718 - val_pos_accuracy: 0.8460\n",
      "Epoch 659/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0089 - pos_accuracy: 0.9906 - val_loss: 0.1716 - val_pos_accuracy: 0.8482\n",
      "Epoch 660/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0089 - pos_accuracy: 0.9900 - val_loss: 0.1717 - val_pos_accuracy: 0.8460\n",
      "Epoch 661/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0088 - pos_accuracy: 0.9912 - val_loss: 0.1715 - val_pos_accuracy: 0.8460\n",
      "Epoch 662/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0088 - pos_accuracy: 0.9912 - val_loss: 0.1714 - val_pos_accuracy: 0.8482\n",
      "Epoch 663/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0088 - pos_accuracy: 0.9906 - val_loss: 0.1715 - val_pos_accuracy: 0.8460\n",
      "Epoch 664/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0088 - pos_accuracy: 0.9912 - val_loss: 0.1717 - val_pos_accuracy: 0.8460\n",
      "Epoch 665/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9906 - val_loss: 0.1717 - val_pos_accuracy: 0.8482\n",
      "Epoch 666/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9912 - val_loss: 0.1713 - val_pos_accuracy: 0.8482\n",
      "Epoch 667/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0087 - pos_accuracy: 0.9912 - val_loss: 0.1713 - val_pos_accuracy: 0.8482\n",
      "Epoch 668/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0086 - pos_accuracy: 0.9912 - val_loss: 0.1709 - val_pos_accuracy: 0.8482\n",
      "Epoch 669/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0086 - pos_accuracy: 0.9912 - val_loss: 0.1713 - val_pos_accuracy: 0.8460\n",
      "Epoch 670/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0086 - pos_accuracy: 0.9906 - val_loss: 0.1713 - val_pos_accuracy: 0.8460\n",
      "Epoch 671/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0085 - pos_accuracy: 0.9912 - val_loss: 0.1707 - val_pos_accuracy: 0.8482\n",
      "Epoch 672/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0085 - pos_accuracy: 0.9912 - val_loss: 0.1708 - val_pos_accuracy: 0.8482\n",
      "Epoch 673/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0085 - pos_accuracy: 0.9912 - val_loss: 0.1706 - val_pos_accuracy: 0.8482\n",
      "Epoch 674/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0084 - pos_accuracy: 0.9912 - val_loss: 0.1705 - val_pos_accuracy: 0.8482\n",
      "Epoch 675/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0084 - pos_accuracy: 0.9912 - val_loss: 0.1708 - val_pos_accuracy: 0.8482\n",
      "Epoch 676/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0084 - pos_accuracy: 0.9912 - val_loss: 0.1709 - val_pos_accuracy: 0.8482\n",
      "Epoch 677/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0084 - pos_accuracy: 0.9912 - val_loss: 0.1703 - val_pos_accuracy: 0.8482\n",
      "Epoch 678/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9912 - val_loss: 0.1704 - val_pos_accuracy: 0.8482\n",
      "Epoch 679/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9912 - val_loss: 0.1707 - val_pos_accuracy: 0.8482\n",
      "Epoch 680/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0083 - pos_accuracy: 0.9912 - val_loss: 0.1703 - val_pos_accuracy: 0.8482\n",
      "Epoch 681/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0082 - pos_accuracy: 0.9912 - val_loss: 0.1701 - val_pos_accuracy: 0.8482\n",
      "Epoch 682/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0082 - pos_accuracy: 0.9912 - val_loss: 0.1702 - val_pos_accuracy: 0.8482\n",
      "Epoch 683/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0082 - pos_accuracy: 0.9912 - val_loss: 0.1701 - val_pos_accuracy: 0.8482\n",
      "Epoch 684/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0082 - pos_accuracy: 0.9912 - val_loss: 0.1698 - val_pos_accuracy: 0.8482\n",
      "Epoch 685/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0081 - pos_accuracy: 0.9912 - val_loss: 0.1698 - val_pos_accuracy: 0.8482\n",
      "Epoch 686/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0081 - pos_accuracy: 0.9912 - val_loss: 0.1703 - val_pos_accuracy: 0.8482\n",
      "Epoch 687/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0081 - pos_accuracy: 0.9912 - val_loss: 0.1699 - val_pos_accuracy: 0.8482\n",
      "Epoch 688/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0081 - pos_accuracy: 0.9912 - val_loss: 0.1698 - val_pos_accuracy: 0.8482\n",
      "Epoch 689/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0080 - pos_accuracy: 0.9912 - val_loss: 0.1701 - val_pos_accuracy: 0.8482\n",
      "Epoch 690/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0080 - pos_accuracy: 0.9912 - val_loss: 0.1697 - val_pos_accuracy: 0.8482\n",
      "Epoch 691/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0080 - pos_accuracy: 0.9912 - val_loss: 0.1696 - val_pos_accuracy: 0.8482\n",
      "Epoch 692/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9912 - val_loss: 0.1700 - val_pos_accuracy: 0.8482\n",
      "Epoch 693/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0079 - pos_accuracy: 0.9912 - val_loss: 0.1699 - val_pos_accuracy: 0.8482\n",
      "Epoch 694/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0079 - pos_accuracy: 0.9912 - val_loss: 0.1692 - val_pos_accuracy: 0.8482\n",
      "Epoch 695/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0078 - pos_accuracy: 0.9912 - val_loss: 0.1694 - val_pos_accuracy: 0.8482\n",
      "Epoch 696/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0078 - pos_accuracy: 0.9912 - val_loss: 0.1692 - val_pos_accuracy: 0.8482\n",
      "Epoch 697/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0078 - pos_accuracy: 0.9912 - val_loss: 0.1692 - val_pos_accuracy: 0.8482\n",
      "Epoch 698/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0078 - pos_accuracy: 0.9912 - val_loss: 0.1690 - val_pos_accuracy: 0.8482\n",
      "Epoch 699/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0077 - pos_accuracy: 0.9912 - val_loss: 0.1692 - val_pos_accuracy: 0.8482\n",
      "Epoch 700/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0077 - pos_accuracy: 0.9912 - val_loss: 0.1692 - val_pos_accuracy: 0.8482\n",
      "Epoch 701/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0077 - pos_accuracy: 0.9912 - val_loss: 0.1688 - val_pos_accuracy: 0.8482\n",
      "Epoch 702/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0077 - pos_accuracy: 0.9912 - val_loss: 0.1689 - val_pos_accuracy: 0.8527\n",
      "Epoch 703/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0076 - pos_accuracy: 0.9919 - val_loss: 0.1689 - val_pos_accuracy: 0.8482\n",
      "Epoch 704/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0076 - pos_accuracy: 0.9919 - val_loss: 0.1687 - val_pos_accuracy: 0.8482\n",
      "Epoch 705/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0076 - pos_accuracy: 0.9912 - val_loss: 0.1689 - val_pos_accuracy: 0.8482\n",
      "Epoch 706/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0076 - pos_accuracy: 0.9912 - val_loss: 0.1687 - val_pos_accuracy: 0.8527\n",
      "Epoch 707/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0075 - pos_accuracy: 0.9912 - val_loss: 0.1686 - val_pos_accuracy: 0.8527\n",
      "Epoch 708/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0075 - pos_accuracy: 0.9912 - val_loss: 0.1685 - val_pos_accuracy: 0.8527\n",
      "Epoch 709/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0075 - pos_accuracy: 0.9912 - val_loss: 0.1685 - val_pos_accuracy: 0.8527\n",
      "Epoch 710/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0075 - pos_accuracy: 0.9912 - val_loss: 0.1688 - val_pos_accuracy: 0.8527\n",
      "Epoch 711/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0074 - pos_accuracy: 0.9912 - val_loss: 0.1683 - val_pos_accuracy: 0.8527\n",
      "Epoch 712/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0074 - pos_accuracy: 0.9919 - val_loss: 0.1682 - val_pos_accuracy: 0.8482\n",
      "Epoch 713/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0074 - pos_accuracy: 0.9912 - val_loss: 0.1684 - val_pos_accuracy: 0.8482\n",
      "Epoch 714/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0074 - pos_accuracy: 0.9912 - val_loss: 0.1684 - val_pos_accuracy: 0.8482\n",
      "Epoch 715/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0073 - pos_accuracy: 0.9912 - val_loss: 0.1681 - val_pos_accuracy: 0.8527\n",
      "Epoch 716/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0073 - pos_accuracy: 0.9912 - val_loss: 0.1682 - val_pos_accuracy: 0.8527\n",
      "Epoch 717/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0073 - pos_accuracy: 0.9919 - val_loss: 0.1681 - val_pos_accuracy: 0.8527\n",
      "Epoch 718/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0073 - pos_accuracy: 0.9912 - val_loss: 0.1682 - val_pos_accuracy: 0.8527\n",
      "Epoch 719/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0072 - pos_accuracy: 0.9919 - val_loss: 0.1678 - val_pos_accuracy: 0.8527\n",
      "Epoch 720/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0072 - pos_accuracy: 0.9919 - val_loss: 0.1679 - val_pos_accuracy: 0.8527\n",
      "Epoch 721/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0072 - pos_accuracy: 0.9919 - val_loss: 0.1678 - val_pos_accuracy: 0.8527\n",
      "Epoch 722/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0072 - pos_accuracy: 0.9912 - val_loss: 0.1678 - val_pos_accuracy: 0.8527\n",
      "Epoch 723/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0071 - pos_accuracy: 0.9919 - val_loss: 0.1676 - val_pos_accuracy: 0.8527\n",
      "Epoch 724/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0071 - pos_accuracy: 0.9919 - val_loss: 0.1678 - val_pos_accuracy: 0.8527\n",
      "Epoch 725/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0071 - pos_accuracy: 0.9919 - val_loss: 0.1677 - val_pos_accuracy: 0.8527\n",
      "Epoch 726/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0071 - pos_accuracy: 0.9919 - val_loss: 0.1675 - val_pos_accuracy: 0.8527\n",
      "Epoch 727/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9919 - val_loss: 0.1676 - val_pos_accuracy: 0.8527\n",
      "Epoch 728/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9919 - val_loss: 0.1674 - val_pos_accuracy: 0.8527\n",
      "Epoch 729/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0070 - pos_accuracy: 0.9919 - val_loss: 0.1672 - val_pos_accuracy: 0.8527\n",
      "Epoch 730/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9919 - val_loss: 0.1671 - val_pos_accuracy: 0.8527\n",
      "Epoch 731/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9919 - val_loss: 0.1674 - val_pos_accuracy: 0.8527\n",
      "Epoch 732/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9919 - val_loss: 0.1674 - val_pos_accuracy: 0.8527\n",
      "Epoch 733/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9919 - val_loss: 0.1677 - val_pos_accuracy: 0.8527\n",
      "Epoch 734/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9912 - val_loss: 0.1672 - val_pos_accuracy: 0.8527\n",
      "Epoch 735/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0069 - pos_accuracy: 0.9919 - val_loss: 0.1671 - val_pos_accuracy: 0.8527\n",
      "Epoch 736/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0068 - pos_accuracy: 0.9919 - val_loss: 0.1672 - val_pos_accuracy: 0.8527\n",
      "Epoch 737/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0068 - pos_accuracy: 0.9919 - val_loss: 0.1672 - val_pos_accuracy: 0.8527\n",
      "Epoch 738/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0068 - pos_accuracy: 0.9919 - val_loss: 0.1672 - val_pos_accuracy: 0.8527\n",
      "Epoch 739/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0068 - pos_accuracy: 0.9919 - val_loss: 0.1670 - val_pos_accuracy: 0.8527\n",
      "Epoch 740/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0067 - pos_accuracy: 0.9919 - val_loss: 0.1667 - val_pos_accuracy: 0.8527\n",
      "Epoch 741/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9919 - val_loss: 0.1671 - val_pos_accuracy: 0.8527\n",
      "Epoch 742/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9919 - val_loss: 0.1665 - val_pos_accuracy: 0.8527\n",
      "Epoch 743/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0067 - pos_accuracy: 0.9919 - val_loss: 0.1668 - val_pos_accuracy: 0.8527\n",
      "Epoch 744/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0066 - pos_accuracy: 0.9919 - val_loss: 0.1663 - val_pos_accuracy: 0.8527\n",
      "Epoch 745/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0066 - pos_accuracy: 0.9919 - val_loss: 0.1665 - val_pos_accuracy: 0.8527\n",
      "Epoch 746/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0066 - pos_accuracy: 0.9919 - val_loss: 0.1665 - val_pos_accuracy: 0.8527\n",
      "Epoch 747/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0066 - pos_accuracy: 0.9919 - val_loss: 0.1667 - val_pos_accuracy: 0.8527\n",
      "Epoch 748/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0066 - pos_accuracy: 0.9919 - val_loss: 0.1661 - val_pos_accuracy: 0.8527\n",
      "Epoch 749/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9919 - val_loss: 0.1662 - val_pos_accuracy: 0.8527\n",
      "Epoch 750/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9919 - val_loss: 0.1663 - val_pos_accuracy: 0.8527\n",
      "Epoch 751/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9919 - val_loss: 0.1663 - val_pos_accuracy: 0.8527\n",
      "Epoch 752/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9919 - val_loss: 0.1664 - val_pos_accuracy: 0.8527\n",
      "Epoch 753/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0065 - pos_accuracy: 0.9919 - val_loss: 0.1661 - val_pos_accuracy: 0.8527\n",
      "Epoch 754/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9919 - val_loss: 0.1657 - val_pos_accuracy: 0.8527\n",
      "Epoch 755/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9919 - val_loss: 0.1660 - val_pos_accuracy: 0.8527\n",
      "Epoch 756/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0064 - pos_accuracy: 0.9919 - val_loss: 0.1657 - val_pos_accuracy: 0.8527\n",
      "Epoch 757/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9919 - val_loss: 0.1656 - val_pos_accuracy: 0.8527\n",
      "Epoch 758/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9919 - val_loss: 0.1657 - val_pos_accuracy: 0.8527\n",
      "Epoch 759/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0064 - pos_accuracy: 0.9919 - val_loss: 0.1657 - val_pos_accuracy: 0.8527\n",
      "Epoch 760/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0063 - pos_accuracy: 0.9919 - val_loss: 0.1658 - val_pos_accuracy: 0.8527\n",
      "Epoch 761/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0063 - pos_accuracy: 0.9919 - val_loss: 0.1658 - val_pos_accuracy: 0.8527\n",
      "Epoch 762/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0063 - pos_accuracy: 0.9919 - val_loss: 0.1655 - val_pos_accuracy: 0.8527\n",
      "Epoch 763/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0063 - pos_accuracy: 0.9925 - val_loss: 0.1658 - val_pos_accuracy: 0.8527\n",
      "Epoch 764/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0062 - pos_accuracy: 0.9919 - val_loss: 0.1653 - val_pos_accuracy: 0.8527\n",
      "Epoch 765/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0062 - pos_accuracy: 0.9925 - val_loss: 0.1654 - val_pos_accuracy: 0.8527\n",
      "Epoch 766/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0062 - pos_accuracy: 0.9919 - val_loss: 0.1653 - val_pos_accuracy: 0.8527\n",
      "Epoch 767/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9925 - val_loss: 0.1657 - val_pos_accuracy: 0.8527\n",
      "Epoch 768/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0062 - pos_accuracy: 0.9919 - val_loss: 0.1652 - val_pos_accuracy: 0.8527\n",
      "Epoch 769/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9925 - val_loss: 0.1653 - val_pos_accuracy: 0.8527\n",
      "Epoch 770/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9919 - val_loss: 0.1651 - val_pos_accuracy: 0.8527\n",
      "Epoch 771/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0061 - pos_accuracy: 0.9919 - val_loss: 0.1650 - val_pos_accuracy: 0.8527\n",
      "Epoch 772/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0061 - pos_accuracy: 0.9919 - val_loss: 0.1652 - val_pos_accuracy: 0.8527\n",
      "Epoch 773/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0061 - pos_accuracy: 0.9925 - val_loss: 0.1651 - val_pos_accuracy: 0.8527\n",
      "Epoch 774/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9925 - val_loss: 0.1652 - val_pos_accuracy: 0.8527\n",
      "Epoch 775/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0060 - pos_accuracy: 0.9925 - val_loss: 0.1650 - val_pos_accuracy: 0.8527\n",
      "Epoch 776/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9925 - val_loss: 0.1649 - val_pos_accuracy: 0.8527\n",
      "Epoch 777/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9919 - val_loss: 0.1651 - val_pos_accuracy: 0.8527\n",
      "Epoch 778/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0060 - pos_accuracy: 0.9925 - val_loss: 0.1648 - val_pos_accuracy: 0.8527\n",
      "Epoch 779/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0059 - pos_accuracy: 0.9925 - val_loss: 0.1652 - val_pos_accuracy: 0.8527\n",
      "Epoch 780/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0059 - pos_accuracy: 0.9925 - val_loss: 0.1648 - val_pos_accuracy: 0.8527\n",
      "Epoch 781/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0059 - pos_accuracy: 0.9925 - val_loss: 0.1648 - val_pos_accuracy: 0.8527\n",
      "Epoch 782/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0059 - pos_accuracy: 0.9925 - val_loss: 0.1647 - val_pos_accuracy: 0.8527\n",
      "Epoch 783/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0059 - pos_accuracy: 0.9919 - val_loss: 0.1647 - val_pos_accuracy: 0.8527\n",
      "Epoch 784/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9925 - val_loss: 0.1646 - val_pos_accuracy: 0.8527\n",
      "Epoch 785/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9925 - val_loss: 0.1644 - val_pos_accuracy: 0.8527\n",
      "Epoch 786/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9925 - val_loss: 0.1644 - val_pos_accuracy: 0.8527\n",
      "Epoch 787/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9925 - val_loss: 0.1644 - val_pos_accuracy: 0.8527\n",
      "Epoch 788/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0058 - pos_accuracy: 0.9925 - val_loss: 0.1645 - val_pos_accuracy: 0.8527\n",
      "Epoch 789/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0058 - pos_accuracy: 0.9925 - val_loss: 0.1643 - val_pos_accuracy: 0.8527\n",
      "Epoch 790/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0058 - pos_accuracy: 0.9925 - val_loss: 0.1641 - val_pos_accuracy: 0.8527\n",
      "Epoch 791/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9925 - val_loss: 0.1643 - val_pos_accuracy: 0.8527\n",
      "Epoch 792/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9925 - val_loss: 0.1642 - val_pos_accuracy: 0.8527\n",
      "Epoch 793/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9925 - val_loss: 0.1644 - val_pos_accuracy: 0.8527\n",
      "Epoch 794/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0057 - pos_accuracy: 0.9925 - val_loss: 0.1643 - val_pos_accuracy: 0.8527\n",
      "Epoch 795/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0057 - pos_accuracy: 0.9925 - val_loss: 0.1640 - val_pos_accuracy: 0.8527\n",
      "Epoch 796/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9925 - val_loss: 0.1639 - val_pos_accuracy: 0.8527\n",
      "Epoch 797/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9925 - val_loss: 0.1644 - val_pos_accuracy: 0.8527\n",
      "Epoch 798/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9925 - val_loss: 0.1642 - val_pos_accuracy: 0.8527\n",
      "Epoch 799/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0056 - pos_accuracy: 0.9931 - val_loss: 0.1639 - val_pos_accuracy: 0.8527\n",
      "Epoch 800/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0056 - pos_accuracy: 0.9925 - val_loss: 0.1636 - val_pos_accuracy: 0.8527\n",
      "Epoch 801/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0055 - pos_accuracy: 0.9925 - val_loss: 0.1638 - val_pos_accuracy: 0.8527\n",
      "Epoch 802/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0055 - pos_accuracy: 0.9931 - val_loss: 0.1637 - val_pos_accuracy: 0.8527\n",
      "Epoch 803/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9944 - val_loss: 0.1637 - val_pos_accuracy: 0.8527\n",
      "Epoch 804/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0055 - pos_accuracy: 0.9944 - val_loss: 0.1635 - val_pos_accuracy: 0.8527\n",
      "Epoch 805/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0055 - pos_accuracy: 0.9944 - val_loss: 0.1635 - val_pos_accuracy: 0.8527\n",
      "Epoch 806/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0055 - pos_accuracy: 0.9944 - val_loss: 0.1636 - val_pos_accuracy: 0.8527\n",
      "Epoch 807/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9944 - val_loss: 0.1634 - val_pos_accuracy: 0.8527\n",
      "Epoch 808/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9944 - val_loss: 0.1634 - val_pos_accuracy: 0.8527\n",
      "Epoch 809/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0054 - pos_accuracy: 0.9944 - val_loss: 0.1635 - val_pos_accuracy: 0.8527\n",
      "Epoch 810/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9944 - val_loss: 0.1632 - val_pos_accuracy: 0.8527\n",
      "Epoch 811/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9944 - val_loss: 0.1634 - val_pos_accuracy: 0.8527\n",
      "Epoch 812/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9944 - val_loss: 0.1631 - val_pos_accuracy: 0.8527\n",
      "Epoch 813/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0054 - pos_accuracy: 0.9944 - val_loss: 0.1632 - val_pos_accuracy: 0.8527\n",
      "Epoch 814/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9944 - val_loss: 0.1632 - val_pos_accuracy: 0.8527\n",
      "Epoch 815/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9944 - val_loss: 0.1632 - val_pos_accuracy: 0.8527\n",
      "Epoch 816/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9944 - val_loss: 0.1632 - val_pos_accuracy: 0.8527\n",
      "Epoch 817/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9944 - val_loss: 0.1631 - val_pos_accuracy: 0.8527\n",
      "Epoch 818/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0053 - pos_accuracy: 0.9944 - val_loss: 0.1630 - val_pos_accuracy: 0.8527\n",
      "Epoch 819/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0053 - pos_accuracy: 0.9944 - val_loss: 0.1629 - val_pos_accuracy: 0.8527\n",
      "Epoch 820/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0052 - pos_accuracy: 0.9944 - val_loss: 0.1628 - val_pos_accuracy: 0.8527\n",
      "Epoch 821/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9944 - val_loss: 0.1628 - val_pos_accuracy: 0.8527\n",
      "Epoch 822/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9944 - val_loss: 0.1630 - val_pos_accuracy: 0.8527\n",
      "Epoch 823/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9944 - val_loss: 0.1627 - val_pos_accuracy: 0.8527\n",
      "Epoch 824/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0052 - pos_accuracy: 0.9944 - val_loss: 0.1626 - val_pos_accuracy: 0.8527\n",
      "Epoch 825/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0052 - pos_accuracy: 0.9944 - val_loss: 0.1627 - val_pos_accuracy: 0.8527\n",
      "Epoch 826/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9944 - val_loss: 0.1626 - val_pos_accuracy: 0.8527\n",
      "Epoch 827/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0051 - pos_accuracy: 0.9944 - val_loss: 0.1624 - val_pos_accuracy: 0.8616\n",
      "Epoch 828/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0051 - pos_accuracy: 0.9944 - val_loss: 0.1627 - val_pos_accuracy: 0.8527\n",
      "Epoch 829/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9944 - val_loss: 0.1625 - val_pos_accuracy: 0.8527\n",
      "Epoch 830/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9944 - val_loss: 0.1625 - val_pos_accuracy: 0.8527\n",
      "Epoch 831/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0051 - pos_accuracy: 0.9944 - val_loss: 0.1625 - val_pos_accuracy: 0.8527\n",
      "Epoch 832/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9944 - val_loss: 0.1625 - val_pos_accuracy: 0.8527\n",
      "Epoch 833/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9944 - val_loss: 0.1622 - val_pos_accuracy: 0.8527\n",
      "Epoch 834/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9944 - val_loss: 0.1625 - val_pos_accuracy: 0.8527\n",
      "Epoch 835/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9944 - val_loss: 0.1625 - val_pos_accuracy: 0.8527\n",
      "Epoch 836/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9944 - val_loss: 0.1622 - val_pos_accuracy: 0.8527\n",
      "Epoch 837/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0050 - pos_accuracy: 0.9944 - val_loss: 0.1623 - val_pos_accuracy: 0.8527\n",
      "Epoch 838/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9944 - val_loss: 0.1620 - val_pos_accuracy: 0.8527\n",
      "Epoch 839/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9944 - val_loss: 0.1623 - val_pos_accuracy: 0.8527\n",
      "Epoch 840/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0049 - pos_accuracy: 0.9944 - val_loss: 0.1622 - val_pos_accuracy: 0.8616\n",
      "Epoch 841/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0049 - pos_accuracy: 0.9950 - val_loss: 0.1618 - val_pos_accuracy: 0.8638\n",
      "Epoch 842/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9944 - val_loss: 0.1618 - val_pos_accuracy: 0.8616\n",
      "Epoch 843/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9944 - val_loss: 0.1619 - val_pos_accuracy: 0.8616\n",
      "Epoch 844/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0049 - pos_accuracy: 0.9944 - val_loss: 0.1619 - val_pos_accuracy: 0.8616\n",
      "Epoch 845/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9950 - val_loss: 0.1618 - val_pos_accuracy: 0.8527\n",
      "Epoch 846/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9944 - val_loss: 0.1618 - val_pos_accuracy: 0.8527\n",
      "Epoch 847/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9944 - val_loss: 0.1617 - val_pos_accuracy: 0.8616\n",
      "Epoch 848/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0048 - pos_accuracy: 0.9944 - val_loss: 0.1616 - val_pos_accuracy: 0.8616\n",
      "Epoch 849/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9944 - val_loss: 0.1618 - val_pos_accuracy: 0.8527\n",
      "Epoch 850/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0048 - pos_accuracy: 0.9944 - val_loss: 0.1617 - val_pos_accuracy: 0.8616\n",
      "Epoch 851/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0048 - pos_accuracy: 0.9950 - val_loss: 0.1615 - val_pos_accuracy: 0.8638\n",
      "Epoch 852/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0048 - pos_accuracy: 0.9944 - val_loss: 0.1618 - val_pos_accuracy: 0.8527\n",
      "Epoch 853/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9950 - val_loss: 0.1614 - val_pos_accuracy: 0.8616\n",
      "Epoch 854/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0047 - pos_accuracy: 0.9944 - val_loss: 0.1616 - val_pos_accuracy: 0.8527\n",
      "Epoch 855/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9950 - val_loss: 0.1616 - val_pos_accuracy: 0.8616\n",
      "Epoch 856/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0047 - pos_accuracy: 0.9944 - val_loss: 0.1613 - val_pos_accuracy: 0.8638\n",
      "Epoch 857/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0047 - pos_accuracy: 0.9950 - val_loss: 0.1616 - val_pos_accuracy: 0.8527\n",
      "Epoch 858/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0046 - pos_accuracy: 0.9956 - val_loss: 0.1615 - val_pos_accuracy: 0.8616\n",
      "Epoch 859/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9950 - val_loss: 0.1615 - val_pos_accuracy: 0.8616\n",
      "Epoch 860/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9950 - val_loss: 0.1611 - val_pos_accuracy: 0.8616\n",
      "Epoch 861/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0046 - pos_accuracy: 0.9950 - val_loss: 0.1612 - val_pos_accuracy: 0.8616\n",
      "Epoch 862/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0046 - pos_accuracy: 0.9944 - val_loss: 0.1611 - val_pos_accuracy: 0.8616\n",
      "Epoch 863/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9950 - val_loss: 0.1612 - val_pos_accuracy: 0.8616\n",
      "Epoch 864/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9956 - val_loss: 0.1610 - val_pos_accuracy: 0.8616\n",
      "Epoch 865/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9956 - val_loss: 0.1610 - val_pos_accuracy: 0.8616\n",
      "Epoch 866/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0046 - pos_accuracy: 0.9956 - val_loss: 0.1609 - val_pos_accuracy: 0.8616\n",
      "Epoch 867/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0045 - pos_accuracy: 0.9956 - val_loss: 0.1610 - val_pos_accuracy: 0.8616\n",
      "Epoch 868/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9956 - val_loss: 0.1609 - val_pos_accuracy: 0.8638\n",
      "Epoch 869/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9950 - val_loss: 0.1608 - val_pos_accuracy: 0.8638\n",
      "Epoch 870/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9962 - val_loss: 0.1608 - val_pos_accuracy: 0.8616\n",
      "Epoch 871/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9956 - val_loss: 0.1609 - val_pos_accuracy: 0.8638\n",
      "Epoch 872/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0045 - pos_accuracy: 0.9956 - val_loss: 0.1607 - val_pos_accuracy: 0.8638\n",
      "Epoch 873/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9956 - val_loss: 0.1608 - val_pos_accuracy: 0.8638\n",
      "Epoch 874/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0044 - pos_accuracy: 0.9962 - val_loss: 0.1606 - val_pos_accuracy: 0.8638\n",
      "Epoch 875/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0044 - pos_accuracy: 0.9962 - val_loss: 0.1607 - val_pos_accuracy: 0.8638\n",
      "Epoch 876/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9956 - val_loss: 0.1608 - val_pos_accuracy: 0.8638\n",
      "Epoch 877/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0044 - pos_accuracy: 0.9956 - val_loss: 0.1609 - val_pos_accuracy: 0.8638\n",
      "Epoch 878/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9962 - val_loss: 0.1607 - val_pos_accuracy: 0.8616\n",
      "Epoch 879/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9956 - val_loss: 0.1605 - val_pos_accuracy: 0.8638\n",
      "Epoch 880/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9962 - val_loss: 0.1605 - val_pos_accuracy: 0.8638\n",
      "Epoch 881/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0044 - pos_accuracy: 0.9962 - val_loss: 0.1604 - val_pos_accuracy: 0.8638\n",
      "Epoch 882/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9956 - val_loss: 0.1604 - val_pos_accuracy: 0.8638\n",
      "Epoch 883/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9962 - val_loss: 0.1606 - val_pos_accuracy: 0.8638\n",
      "Epoch 884/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9962 - val_loss: 0.1606 - val_pos_accuracy: 0.8616\n",
      "Epoch 885/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0043 - pos_accuracy: 0.9956 - val_loss: 0.1603 - val_pos_accuracy: 0.8638\n",
      "Epoch 886/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9962 - val_loss: 0.1604 - val_pos_accuracy: 0.8638\n",
      "Epoch 887/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9962 - val_loss: 0.1603 - val_pos_accuracy: 0.8638\n",
      "Epoch 888/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0043 - pos_accuracy: 0.9962 - val_loss: 0.1602 - val_pos_accuracy: 0.8638\n",
      "Epoch 889/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0043 - pos_accuracy: 0.9962 - val_loss: 0.1603 - val_pos_accuracy: 0.8638\n",
      "Epoch 890/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9962 - val_loss: 0.1601 - val_pos_accuracy: 0.8638\n",
      "Epoch 891/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9962 - val_loss: 0.1601 - val_pos_accuracy: 0.8638\n",
      "Epoch 892/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9962 - val_loss: 0.1600 - val_pos_accuracy: 0.8638\n",
      "Epoch 893/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9962 - val_loss: 0.1602 - val_pos_accuracy: 0.8638\n",
      "Epoch 894/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9962 - val_loss: 0.1601 - val_pos_accuracy: 0.8638\n",
      "Epoch 895/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0042 - pos_accuracy: 0.9962 - val_loss: 0.1600 - val_pos_accuracy: 0.8638\n",
      "Epoch 896/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0042 - pos_accuracy: 0.9962 - val_loss: 0.1601 - val_pos_accuracy: 0.8638\n",
      "Epoch 897/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0042 - pos_accuracy: 0.9962 - val_loss: 0.1600 - val_pos_accuracy: 0.8638\n",
      "Epoch 898/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1601 - val_pos_accuracy: 0.8638\n",
      "Epoch 899/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1600 - val_pos_accuracy: 0.8638\n",
      "Epoch 900/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1600 - val_pos_accuracy: 0.8638\n",
      "Epoch 901/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1597 - val_pos_accuracy: 0.8638\n",
      "Epoch 902/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1597 - val_pos_accuracy: 0.8638\n",
      "Epoch 903/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1597 - val_pos_accuracy: 0.8638\n",
      "Epoch 904/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1596 - val_pos_accuracy: 0.8638\n",
      "Epoch 905/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1597 - val_pos_accuracy: 0.8638\n",
      "Epoch 906/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1597 - val_pos_accuracy: 0.8638\n",
      "Epoch 907/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9962 - val_loss: 0.1595 - val_pos_accuracy: 0.8638\n",
      "Epoch 908/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0041 - pos_accuracy: 0.9962 - val_loss: 0.1595 - val_pos_accuracy: 0.8638\n",
      "Epoch 909/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0040 - pos_accuracy: 0.9962 - val_loss: 0.1594 - val_pos_accuracy: 0.8638\n",
      "Epoch 910/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9962 - val_loss: 0.1593 - val_pos_accuracy: 0.8638\n",
      "Epoch 911/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9962 - val_loss: 0.1593 - val_pos_accuracy: 0.8638\n",
      "Epoch 912/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9962 - val_loss: 0.1593 - val_pos_accuracy: 0.8638\n",
      "Epoch 913/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9962 - val_loss: 0.1593 - val_pos_accuracy: 0.8638\n",
      "Epoch 914/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9962 - val_loss: 0.1595 - val_pos_accuracy: 0.8638\n",
      "Epoch 915/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.1592 - val_pos_accuracy: 0.8638\n",
      "Epoch 916/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0040 - pos_accuracy: 0.9962 - val_loss: 0.1592 - val_pos_accuracy: 0.8638\n",
      "Epoch 917/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.1592 - val_pos_accuracy: 0.8638\n",
      "Epoch 918/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.1591 - val_pos_accuracy: 0.8638\n",
      "Epoch 919/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.1591 - val_pos_accuracy: 0.8638\n",
      "Epoch 920/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.1592 - val_pos_accuracy: 0.8638\n",
      "Epoch 921/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.1591 - val_pos_accuracy: 0.8638\n",
      "Epoch 922/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.1591 - val_pos_accuracy: 0.8638\n",
      "Epoch 923/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.1590 - val_pos_accuracy: 0.8638\n",
      "Epoch 924/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0039 - pos_accuracy: 0.9962 - val_loss: 0.1588 - val_pos_accuracy: 0.8638\n",
      "Epoch 925/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0038 - pos_accuracy: 0.9962 - val_loss: 0.1589 - val_pos_accuracy: 0.8638\n",
      "Epoch 926/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9969 - val_loss: 0.1590 - val_pos_accuracy: 0.8638\n",
      "Epoch 927/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9962 - val_loss: 0.1590 - val_pos_accuracy: 0.8638\n",
      "Epoch 928/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9962 - val_loss: 0.1589 - val_pos_accuracy: 0.8638\n",
      "Epoch 929/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0038 - pos_accuracy: 0.9969 - val_loss: 0.1589 - val_pos_accuracy: 0.8638\n",
      "Epoch 930/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9969 - val_loss: 0.1588 - val_pos_accuracy: 0.8638\n",
      "Epoch 931/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0038 - pos_accuracy: 0.9975 - val_loss: 0.1588 - val_pos_accuracy: 0.8638\n",
      "Epoch 932/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9962 - val_loss: 0.1591 - val_pos_accuracy: 0.8638\n",
      "Epoch 933/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9962 - val_loss: 0.1588 - val_pos_accuracy: 0.8638\n",
      "Epoch 934/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9975 - val_loss: 0.1586 - val_pos_accuracy: 0.8638\n",
      "Epoch 935/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0038 - pos_accuracy: 0.9969 - val_loss: 0.1587 - val_pos_accuracy: 0.8638\n",
      "Epoch 936/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9981 - val_loss: 0.1586 - val_pos_accuracy: 0.8638\n",
      "Epoch 937/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9975 - val_loss: 0.1587 - val_pos_accuracy: 0.8638\n",
      "Epoch 938/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9981 - val_loss: 0.1585 - val_pos_accuracy: 0.8638\n",
      "Epoch 939/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9981 - val_loss: 0.1585 - val_pos_accuracy: 0.8638\n",
      "Epoch 940/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0037 - pos_accuracy: 0.9981 - val_loss: 0.1585 - val_pos_accuracy: 0.8638\n",
      "Epoch 941/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9981 - val_loss: 0.1585 - val_pos_accuracy: 0.8638\n",
      "Epoch 942/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9981 - val_loss: 0.1584 - val_pos_accuracy: 0.8638\n",
      "Epoch 943/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9981 - val_loss: 0.1583 - val_pos_accuracy: 0.8638\n",
      "Epoch 944/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0037 - pos_accuracy: 0.9975 - val_loss: 0.1585 - val_pos_accuracy: 0.8638\n",
      "Epoch 945/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1582 - val_pos_accuracy: 0.8638\n",
      "Epoch 946/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1580 - val_pos_accuracy: 0.8638\n",
      "Epoch 947/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1585 - val_pos_accuracy: 0.8638\n",
      "Epoch 948/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1583 - val_pos_accuracy: 0.8638\n",
      "Epoch 949/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1582 - val_pos_accuracy: 0.8638\n",
      "Epoch 950/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1583 - val_pos_accuracy: 0.8638\n",
      "Epoch 951/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1583 - val_pos_accuracy: 0.8638\n",
      "Epoch 952/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1580 - val_pos_accuracy: 0.8638\n",
      "Epoch 953/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1580 - val_pos_accuracy: 0.8638\n",
      "Epoch 954/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.1580 - val_pos_accuracy: 0.8638\n",
      "Epoch 955/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1582 - val_pos_accuracy: 0.8638\n",
      "Epoch 956/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1581 - val_pos_accuracy: 0.8638\n",
      "Epoch 957/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1583 - val_pos_accuracy: 0.8638\n",
      "Epoch 958/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1579 - val_pos_accuracy: 0.8638\n",
      "Epoch 959/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1580 - val_pos_accuracy: 0.8638\n",
      "Epoch 960/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1580 - val_pos_accuracy: 0.8638\n",
      "Epoch 961/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1577 - val_pos_accuracy: 0.8638\n",
      "Epoch 962/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1579 - val_pos_accuracy: 0.8638\n",
      "Epoch 963/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1577 - val_pos_accuracy: 0.8638\n",
      "Epoch 964/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1578 - val_pos_accuracy: 0.8638\n",
      "Epoch 965/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1577 - val_pos_accuracy: 0.8638\n",
      "Epoch 966/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0035 - pos_accuracy: 0.9981 - val_loss: 0.1579 - val_pos_accuracy: 0.8638\n",
      "Epoch 967/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1578 - val_pos_accuracy: 0.8638\n",
      "Epoch 968/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1576 - val_pos_accuracy: 0.8638\n",
      "Epoch 969/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1577 - val_pos_accuracy: 0.8638\n",
      "Epoch 970/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1577 - val_pos_accuracy: 0.8638\n",
      "Epoch 971/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1575 - val_pos_accuracy: 0.8638\n",
      "Epoch 972/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1577 - val_pos_accuracy: 0.8638\n",
      "Epoch 973/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1576 - val_pos_accuracy: 0.8638\n",
      "Epoch 974/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1575 - val_pos_accuracy: 0.8638\n",
      "Epoch 975/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1576 - val_pos_accuracy: 0.8638\n",
      "Epoch 976/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0034 - pos_accuracy: 0.9981 - val_loss: 0.1577 - val_pos_accuracy: 0.8638\n",
      "Epoch 977/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1574 - val_pos_accuracy: 0.8638\n",
      "Epoch 978/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1573 - val_pos_accuracy: 0.8638\n",
      "Epoch 979/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1574 - val_pos_accuracy: 0.8638\n",
      "Epoch 980/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1574 - val_pos_accuracy: 0.8638\n",
      "Epoch 981/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1573 - val_pos_accuracy: 0.8638\n",
      "Epoch 982/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1575 - val_pos_accuracy: 0.8638\n",
      "Epoch 983/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1572 - val_pos_accuracy: 0.8638\n",
      "Epoch 984/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1573 - val_pos_accuracy: 0.8638\n",
      "Epoch 985/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1574 - val_pos_accuracy: 0.8638\n",
      "Epoch 986/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1574 - val_pos_accuracy: 0.8638\n",
      "Epoch 987/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1573 - val_pos_accuracy: 0.8638\n",
      "Epoch 988/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0033 - pos_accuracy: 0.9981 - val_loss: 0.1573 - val_pos_accuracy: 0.8638\n",
      "Epoch 989/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1573 - val_pos_accuracy: 0.8638\n",
      "Epoch 990/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1572 - val_pos_accuracy: 0.8638\n",
      "Epoch 991/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1572 - val_pos_accuracy: 0.8638\n",
      "Epoch 992/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1570 - val_pos_accuracy: 0.8638\n",
      "Epoch 993/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1569 - val_pos_accuracy: 0.8638\n",
      "Epoch 994/1000\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1570 - val_pos_accuracy: 0.8638\n",
      "Epoch 995/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1570 - val_pos_accuracy: 0.8638\n",
      "Epoch 996/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1571 - val_pos_accuracy: 0.8638\n",
      "Epoch 997/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1570 - val_pos_accuracy: 0.8638\n",
      "Epoch 998/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1569 - val_pos_accuracy: 0.8638\n",
      "Epoch 999/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0032 - pos_accuracy: 0.9981 - val_loss: 0.1569 - val_pos_accuracy: 0.8638\n",
      "Epoch 1000/1000\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0031 - pos_accuracy: 0.9981 - val_loss: 0.1568 - val_pos_accuracy: 0.8638\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.06.png\n",
      "40960/35478 [==================================] - 0s 0us/step\n",
      "49152/35478 [=========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "2022-10-14 11:19:19.098525: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200\n",
      "<Figure size 720x288 with 1 Axes>\n",
      "9 pixles average = 0.7952069716775599\n",
      "convolution result = 0.795207\n",
      "Downloading data from http://sipi.usc.edu/database/preview/misc/4.1.07.png\n",
      "32768/28517 [==================================] - 0s 0us/step\n",
      "40960/28517 [===========================================] - 0s 0us/step\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "inputs.shape = (None, 80, 80, 3)\n",
      "outputs.shape = (None, 80, 80, 3)\n",
      "<Figure size 720x288 with 2 Axes>\n",
      "9 pixles average = 0.6013071895424837\n",
      "convolution result = 0.6013072\n",
      "답안의 형식 확인이 완료되었습니다.\n",
      "/home/kotech/workspace/webstudy/deep/exam_2022/do_calc.py:1730: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if (tmp == ans05.astype('float32')).all():\n",
      "{\n",
      "    \"res\": [\n",
      "        true,\n",
      "        true,\n",
      "        false,\n",
      "        false,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\nnp.concatenate의 역할과\\naxis=1의 의미에 대해 간단히 적어봅시다.\\n두 행렬을 이어붙이는 역할. \\n만약 axis = 0 이면 행방향으로 이어붙이고, 1인경우 열방향으로 붙인다. \\n그 이상인 경우에도 선택된 축에 대하여 이어붙일줄 알았는데 에러가 난다.\\n차원을 늘려서 다시 해보니 2 이상의 숫자도 문제 없이 동작한다.\\n\",\n",
      "        \"\\nsubplot에서 121, 122의 의미에 대해 간단히 적어봅시다.\\n각 숫자는 row column index 를 의미한다.\\n\",\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        true,\n",
      "        \"\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 간단히 요약해 봅시다.\\nlabel_true와 label_pred는 각각 (배치사이즈, 2)형태로 좌표가 나열되어있는데 \\naxis=1 로 설정하면 두 배열의 열방향 으로 각각의 좌표를 비교하여 (배치사이즈, ) 형태의 bool 배열을 반환한다.\\n\",\n",
      "        \"\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다. \\n0 또는 1로 변환된 (배치사이즈, ) 크기의 bool 1차원 배열의 평균을 구해서 0~1 사이의 값으로 스코어화해서 반환한다.\\n\",\n",
      "        \"\\n최종 출력층의 activation=None인 이유\\n모델에 의한 연산결과값을 비선형적인 가공 없이 그대로 최종 신호로 출력하기 위해서\\n\",\n",
      "        \"\\nrandom seed의 사용 이유 \\n같은 랜덤 데이터로 각종 파라메터를 변경시켜가며 모델을 평가할 수 있다.\\n\",\n",
      "        true,\n",
      "        \"\\n여기에 적어주세요. \\n\"\n",
      "    ],\n",
      "    \"score\": 86.66666666666666,\n",
      "    \"accuracy\": 0.8705\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cp report1_fix/*.ipynb report1\n",
    "#py_list = glob('report1/*.ipynb')\n",
    "for py in py_list:\n",
    "    name = os.path.splitext(py)[0]\n",
    "    py_full = '\"%s.py\"'%name\n",
    "    ipynb_full = '\"%s.ipynb\"'%name\n",
    "    json_full = '\"%s.json\"'%name\n",
    "    if os.path.isfile('%s.json'%name):\n",
    "        continue\n",
    "    print(ipynb_full)\n",
    "    !jupyter nbconvert --to python $ipynb_full \n",
    "    !cat $py_full calc.py > do_calc.py\n",
    "    !rm 'result.json'\n",
    "    !rm ~/.keras/datasets/*.png\n",
    "    !ipython do_calc.py\n",
    "    !mv 'result.json' $json_full\n",
    "    !cat $json_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2e31c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for py in py_list:\n",
    "    name = os.path.splitext(py)[0]\n",
    "    py_full = '\"%s.py\"'%name\n",
    "    ipynb_full = '\"%s.ipynb\"'%name\n",
    "    json_full = '\"%s.json\"'%name\n",
    "    if os.path.isfile('%s.json'%name):\n",
    "        continue\n",
    "    print(ipynb_full)\n",
    "    #!cp $ipynb_full report1_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "727a3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c239e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['filename', 'score', 'check', 'comment', 'accuracy', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n"
     ]
    }
   ],
   "source": [
    "# create df\n",
    "\n",
    "column_names = ['filename', 'score', 'check', 'comment', 'accuracy']\n",
    "for i in range(23):\n",
    "    column_names.append('%i'%(i+1))\n",
    "print(column_names)\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "#df.to_excel('report1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f72e6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUIGUANGZHI_46005\n",
      "정일영_45999\n",
      "김동현_46027\n",
      "안수빈_45998\n",
      "권대천_46052\n",
      "이형우_46056\n",
      "홍준의_46087\n",
      "함현석_46003\n",
      "김지은_46012\n",
      "백찬영_46013\n",
      "윤울암_46060\n",
      "황수연_46036\n",
      "김현빈_45995\n",
      "김원경_46078\n",
      "박준하_46046\n",
      "이범윤_46045\n",
      "윤현기_46095\n",
      "최치헌_46097\n",
      "안다솜_46061\n",
      "김기찬_46058\n",
      "송영석_46064\n",
      "서정민_46091\n",
      "박찬우_46075\n",
      "이정휘_46029\n",
      "우형욱_46053\n",
      "남정표_46051\n",
      "공선호_46039\n",
      "이수정_46098\n",
      "최요한_46089\n",
      "박사일_46055\n",
      "윤경일_46035\n",
      "김미현_46026\n",
      "허인회_46067\n",
      "이민기_46093\n",
      "소재훈_46082\n",
      "장경찬_46049\n",
      "권범수_46040\n",
      "이재인_46024\n",
      "문예지_46065\n",
      "이강희_46077\n",
      "이우근_46090\n",
      "이동주_46069\n",
      "김홍섭_46007\n",
      "이루오_46050\n",
      "윤두환_46057\n",
      "황주훈_46062\n",
      "양해영_46086\n",
      "이지수_46068\n",
      "조민지_46076\n",
      "김선형_46009\n",
      "박해찬_46070\n",
      "유승진_46021\n",
      "박시현_46031\n",
      "고동일_46028\n",
      "석병득_46022\n",
      "이민기_46048\n",
      "김창훈_46030\n",
      "장전인_46037\n",
      "강은혜_46073\n",
      "강길모_46096\n",
      "서정훈_46066\n",
      "황창현_46085\n",
      "이창묵_46042\n",
      "정영욱_46059\n",
      "한성우_46032\n",
      "윤찬혁_46088\n",
      "조형래_46016\n",
      "정다훈_46072\n",
      "김남영_46063\n",
      "이순광_46084\n",
      "최원석_46047\n",
      "김진미_46023\n",
      "구태홍_45996\n",
      "이동훈_46044\n",
      "조성백_46071\n",
      "천동암_46080\n",
      "김선호_46079\n",
      "박현준_46018\n",
      "유하영_46019\n",
      "강연실_46034\n",
      "장철_45997\n",
      "김태권_46033\n",
      "백근화_46025\n",
      "전수인_46015\n"
     ]
    }
   ],
   "source": [
    "json_list = glob('report1/*.ipynb')\n",
    "for j in json_list:\n",
    "    filename = os.path.splitext(os.path.basename(j))[0]\n",
    "    print(filename)\n",
    "    with open('report1/%s.json'%filename) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    #print(json_data)\n",
    "    if len(df[df['filename'] == filename]) == 0:\n",
    "        df = df.append({'filename': filename, 'check': '', 'comment': ''}, ignore_index=True)\n",
    "    row = df[df['filename'] == filename].index.tolist()[0]\n",
    "    df.iloc[row, 1] = round(json_data['score']*10)/10.\n",
    "    df.iloc[row]['accuracy'] = json_data['accuracy']\n",
    "    for i in range(23):\n",
    "        df.iloc[row, i+5] = json_data['res'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4039ef25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>score</th>\n",
       "      <th>check</th>\n",
       "      <th>comment</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUIGUANGZHI_46005</td>\n",
       "      <td>87.9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...</td>\n",
       "      <td>\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...</td>\n",
       "      <td>\\n최종 출력층의 activation=None인 이유 \\n\\n정답 : 이번 학습 모...</td>\n",
       "      <td>\\nrandom seed의 사용 이유 \\n\\n정답 : 동일한 세트의 난수를 생성하기...</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n여기에 적어주세요. \\n강의내용은 소프트웨어 공학과 학생으로써 인공지능 트랙 기...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>정일영_45999</td>\n",
       "      <td>62.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...</td>\n",
       "      <td>\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...</td>\n",
       "      <td>False</td>\n",
       "      <td>\\nrandom seed의 사용 이유 \\n테스트 데이터의 과적합을 테스트 하기 위해서\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n여기에 적어주세요. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김동현_46027</td>\n",
       "      <td>80.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.8822</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...</td>\n",
       "      <td>\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...</td>\n",
       "      <td>\\n최종 출력층의 activation=None인 이유 \\n\\n신경망의 출력 값을 비...</td>\n",
       "      <td>\\nrandom seed의 사용 이유\\n\\n실행될 알고리즘을 개발할 때 random...</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>안수빈_45998</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.9663</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\ntensorflow의 reduce 함수는 차원을 축소하기 위해 사용되는데, re...</td>\n",
       "      <td>\\nreduce_mean 함수가 평균을 통해 차원을 축소시켜 결과값(정확도)를 도출...</td>\n",
       "      <td>\\n결과값(정확도)가 custom metric을 통해 reduce함수를 사용해서 축...</td>\n",
       "      <td>\\n랜덤으로 생성시킨 난수가 계속해서 변경되면 실행을 시킬때마다 다른 결과가 나올 ...</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n어려웠지만 직접 레이어나 파라미터를 추가하고 변경하면서 진행하니\\n이해가 더 잘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>권대천_46052</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.945</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...</td>\n",
       "      <td>\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...</td>\n",
       "      <td>\\n최종 출력층의 activation=None인 이유 \\n\\n해당 문제는 분류를 요...</td>\n",
       "      <td>\\nrandom seed의 사용 이유 \\n\\nrandom 은 실행할 때마다 새로운 ...</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n여기에 적어주세요. \\n\\n다음은 수업에 대한 감상입니다. \\n\\n마치 현장 실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>강연실_46034</td>\n",
       "      <td>100.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.8571</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\ntf.reduce_all의 역할 : tf.reduce는 연산 함수이다. tf.r...</td>\n",
       "      <td>\\ntf.reduce_mean의 역할 : 원소들의 평균을 연산한다.\\n</td>\n",
       "      <td>\\n최종 출력층의 activation=None인 이유 : 최종 출력층의 활성화 함수...</td>\n",
       "      <td>\\nrandom seed의 사용 이유 : 최초 생성된 난수가 유지되게 하기 위함이다...</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n항상 잘 준비된, 심도깊은 강의에 감사드립니다.\\n딥러닝 과목을 2번째 수강하고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>장철_45997</td>\n",
       "      <td>90.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.9447</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...</td>\n",
       "      <td>\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...</td>\n",
       "      <td>\\n최종 출력층의 activation=None인 이유 \\n분류의 결과가 연속적이기 ...</td>\n",
       "      <td>\\nrandom seed의 사용 이유\\n디버깅 등을 위해서 동일한 난수를 발생시키기...</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n여기에 적어주세요. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>김태권_46033</td>\n",
       "      <td>90.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.985</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\\naxis 0 =&gt; 열(세로 기준) AND 연산 진행\\naxis 1 =&gt; 행(...</td>\n",
       "      <td>\\nnumpy 배열의 평균값을 의미한다\\n\\n</td>\n",
       "      <td>\\n최종 출력층의 activation=None인 이유 \\n\\n결과가 그래프처럼 나타...</td>\n",
       "      <td>\\nrandom seed의 사용 이유 \\n\\nrandom seed는 무작위 처럼 보...</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n\\n제가 공부가 미흡해서 강의 내용을 따라오는것만해도 너무 벅찬거같습니다\\n기왕...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>백근화_46025</td>\n",
       "      <td>78.2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.965</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>\\ntf.reduce_all : 지정한 축 방향의 각 요소의 논리와 (and 연산)...</td>\n",
       "      <td>\\ntf.reduce_mean: 정확도 계산한다\\n</td>\n",
       "      <td>\\n최종 출력층의 activation=None인 이유 \\n: 딥러닝 모델이 알아서 ...</td>\n",
       "      <td>\\nrandom seed의 사용 이유\\n매번 다르게 나오는 난수를 프로그램 시행마다...</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n배우고있는 딥러닝이 흥미롭고 더 자세히 알아가고싶습니다.\\n아직은 코드 딥러닝 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>전수인_46015</td>\n",
       "      <td>70.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.925</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\ntf.reduce_all의 역할은 무엇인지, \\n    차원을 AND연산으로 축...</td>\n",
       "      <td>\\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...</td>\n",
       "      <td>\\n최종 출력층의 activation=None인 이유 \\n    활성함수가 없을 경...</td>\n",
       "      <td>\\nrandom seed의 사용 이유 \\n    여러번 실행하더라도 동일한 결과 값...</td>\n",
       "      <td>False</td>\n",
       "      <td>\\n    여기에 적어주세요. \\n    어렵지만 강의 재밌게 듣고 있습니다. 이해...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename  score check comment accuracy     1      2     3     4  \\\n",
       "0   CUIGUANGZHI_46005   87.9                    0.0  True   True  True  True   \n",
       "1           정일영_45999   62.5                    0.0  True   True  True  True   \n",
       "2           김동현_46027   80.0                 0.8822  True   True  True  True   \n",
       "3           안수빈_45998  100.0                 0.9663  True   True  True  True   \n",
       "4           권대천_46052  100.0                  0.945  True   True  True  True   \n",
       "..                ...    ...   ...     ...      ...   ...    ...   ...   ...   \n",
       "79          강연실_46034  100.0                 0.8571  True   True  True  True   \n",
       "80           장철_45997   90.0                 0.9447  True  False  True  True   \n",
       "81          김태권_46033   90.0                  0.985  True  False  True  True   \n",
       "82          백근화_46025   78.2                  0.965  True   True  True  True   \n",
       "83          전수인_46015   70.0                  0.925  True  False  True  True   \n",
       "\n",
       "        5  ...    14    15     16     17  \\\n",
       "0   False  ...  True  True   True  False   \n",
       "1    True  ...  True  True   True  False   \n",
       "2    True  ...  True  True   True   True   \n",
       "3    True  ...  True  True   True   True   \n",
       "4    True  ...  True  True   True   True   \n",
       "..    ...  ...   ...   ...    ...    ...   \n",
       "79   True  ...  True  True   True   True   \n",
       "80   True  ...  True  True   True   True   \n",
       "81   True  ...  True  True   True   True   \n",
       "82   True  ...  True  True  False   True   \n",
       "83   True  ...  True  True   True   True   \n",
       "\n",
       "                                                   18  \\\n",
       "0   \\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...   \n",
       "1   \\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...   \n",
       "2   \\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...   \n",
       "3   \\ntensorflow의 reduce 함수는 차원을 축소하기 위해 사용되는데, re...   \n",
       "4   \\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...   \n",
       "..                                                ...   \n",
       "79  \\ntf.reduce_all의 역할 : tf.reduce는 연산 함수이다. tf.r...   \n",
       "80  \\ntf.reduce_all의 역할은 무엇인지, axis=1은 무엇을 의미하는 지 ...   \n",
       "81  \\n\\naxis 0 => 열(세로 기준) AND 연산 진행\\naxis 1 => 행(...   \n",
       "82  \\ntf.reduce_all : 지정한 축 방향의 각 요소의 논리와 (and 연산)...   \n",
       "83  \\ntf.reduce_all의 역할은 무엇인지, \\n    차원을 AND연산으로 축...   \n",
       "\n",
       "                                                   19  \\\n",
       "0   \\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...   \n",
       "1   \\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...   \n",
       "2   \\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...   \n",
       "3   \\nreduce_mean 함수가 평균을 통해 차원을 축소시켜 결과값(정확도)를 도출...   \n",
       "4   \\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...   \n",
       "..                                                ...   \n",
       "79            \\ntf.reduce_mean의 역할 : 원소들의 평균을 연산한다.\\n   \n",
       "80  \\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...   \n",
       "81                          \\nnumpy 배열의 평균값을 의미한다\\n\\n   \n",
       "82                       \\ntf.reduce_mean: 정확도 계산한다\\n   \n",
       "83  \\ntf.reduce_mean은 코드에서 어떤 역할을 하는지, 간단히 요약해 봅시다...   \n",
       "\n",
       "                                                   20  \\\n",
       "0   \\n최종 출력층의 activation=None인 이유 \\n\\n정답 : 이번 학습 모...   \n",
       "1                                               False   \n",
       "2   \\n최종 출력층의 activation=None인 이유 \\n\\n신경망의 출력 값을 비...   \n",
       "3   \\n결과값(정확도)가 custom metric을 통해 reduce함수를 사용해서 축...   \n",
       "4   \\n최종 출력층의 activation=None인 이유 \\n\\n해당 문제는 분류를 요...   \n",
       "..                                                ...   \n",
       "79  \\n최종 출력층의 activation=None인 이유 : 최종 출력층의 활성화 함수...   \n",
       "80  \\n최종 출력층의 activation=None인 이유 \\n분류의 결과가 연속적이기 ...   \n",
       "81  \\n최종 출력층의 activation=None인 이유 \\n\\n결과가 그래프처럼 나타...   \n",
       "82  \\n최종 출력층의 activation=None인 이유 \\n: 딥러닝 모델이 알아서 ...   \n",
       "83  \\n최종 출력층의 activation=None인 이유 \\n    활성함수가 없을 경...   \n",
       "\n",
       "                                                   21     22  \\\n",
       "0   \\nrandom seed의 사용 이유 \\n\\n정답 : 동일한 세트의 난수를 생성하기...   True   \n",
       "1   \\nrandom seed의 사용 이유 \\n테스트 데이터의 과적합을 테스트 하기 위해서\\n  False   \n",
       "2   \\nrandom seed의 사용 이유\\n\\n실행될 알고리즘을 개발할 때 random...  False   \n",
       "3   \\n랜덤으로 생성시킨 난수가 계속해서 변경되면 실행을 시킬때마다 다른 결과가 나올 ...   True   \n",
       "4   \\nrandom seed의 사용 이유 \\n\\nrandom 은 실행할 때마다 새로운 ...   True   \n",
       "..                                                ...    ...   \n",
       "79  \\nrandom seed의 사용 이유 : 최초 생성된 난수가 유지되게 하기 위함이다...   True   \n",
       "80  \\nrandom seed의 사용 이유\\n디버깅 등을 위해서 동일한 난수를 발생시키기...   True   \n",
       "81  \\nrandom seed의 사용 이유 \\n\\nrandom seed는 무작위 처럼 보...   True   \n",
       "82  \\nrandom seed의 사용 이유\\n매번 다르게 나오는 난수를 프로그램 시행마다...  False   \n",
       "83  \\nrandom seed의 사용 이유 \\n    여러번 실행하더라도 동일한 결과 값...  False   \n",
       "\n",
       "                                                   23  \n",
       "0   \\n여기에 적어주세요. \\n강의내용은 소프트웨어 공학과 학생으로써 인공지능 트랙 기...  \n",
       "1                                     \\n여기에 적어주세요. \\n  \n",
       "2                                                      \n",
       "3   \\n어려웠지만 직접 레이어나 파라미터를 추가하고 변경하면서 진행하니\\n이해가 더 잘...  \n",
       "4   \\n여기에 적어주세요. \\n\\n다음은 수업에 대한 감상입니다. \\n\\n마치 현장 실...  \n",
       "..                                                ...  \n",
       "79  \\n항상 잘 준비된, 심도깊은 강의에 감사드립니다.\\n딥러닝 과목을 2번째 수강하고...  \n",
       "80                                    \\n여기에 적어주세요. \\n  \n",
       "81  \\n\\n제가 공부가 미흡해서 강의 내용을 따라오는것만해도 너무 벅찬거같습니다\\n기왕...  \n",
       "82  \\n배우고있는 딥러닝이 흥미롭고 더 자세히 알아가고싶습니다.\\n아직은 코드 딥러닝 ...  \n",
       "83  \\n    여기에 적어주세요. \\n    어렵지만 강의 재밌게 듣고 있습니다. 이해...  \n",
       "\n",
       "[84 rows x 28 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4023875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('report1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ec258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad9d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
