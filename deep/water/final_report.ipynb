{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dot, Add, ReLU, Activation\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "fm.get_fontconfig_fonts()\n",
    "font_location = '/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf'\n",
    "#font_location = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'\n",
    "# font_location = 'C:/Windows/Fonts/NanumGothic.ttf' # For Windows\n",
    "fprop = fm.FontProperties(fname=font_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'han'\n",
    "gan_files = [ '가평_gan.xlsx', '의암호_gan.xlsx', '서상_gan.xlsx']\n",
    "ori_files = [ '가평_org.xlsx', '의암호_org.xlsx', '서상_org.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'nagdong2'\n",
    "gan_files = [ '도개_gan.xlsx', '신암_gan.xlsx', '회상_gan.xlsx']\n",
    "ori_files = [ '도개_org.xlsx', '신암_org.xlsx', '회상_org.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'nagdong3'\n",
    "gan_files = [ '도개3_gan.xlsx', '신암3_gan.xlsx']\n",
    "ori_files = [ '도개3_org.xlsx', '신암3_org.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'nagdong4'\n",
    "gan_files = [ '도개4_gan.xlsx', '신암4_gan.xlsx']\n",
    "ori_files = [ '도개4_org.xlsx', '신암4_org.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'geum'\n",
    "gan_files = [ '현도_gan.xlsx', '대청호_gan.xlsx', '장계_gan.xlsx']\n",
    "ori_files = [ '현도_ori.xlsx', '대청호_ori.xlsx', '장계_ori.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'geum2'\n",
    "gan_files = [ '현도2_gan.xlsx', '대청호2_gan.xlsx', '장계2_gan.xlsx']\n",
    "ori_files = [ '현도2_ori.xlsx', '대청호2_ori.xlsx', '장계2_ori.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'yeong'\n",
    "gan_files = [ '나주_gan.xlsx', '용봉_gan.xlsx', '우치_gan.xlsx']\n",
    "ori_files = [ '나주_org.xlsx', '용봉_org.xlsx', '우치_gan.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data'\n",
    "def save_gan_data(gan_dfs, ori_dfs, gan_files, ori_files, train_mean, train_std, prefix):\n",
    "    for i in range(len(gan_dfs)):\n",
    "        path = os.path.join(folder, gan_files[i])\n",
    "        gan_dfs[i].to_excel(path, index=False)\n",
    "        print(path)\n",
    "        path = os.path.join(folder, ori_files[i])\n",
    "        ori_dfs[i].to_excel(path, index=False)\n",
    "    path = os.path.join(folder, prefix + '_std.xlsx')\n",
    "    train_std.to_frame(name='std').to_excel(path)\n",
    "    path = os.path.join(folder, prefix + '_mean.xlsx')\n",
    "    train_mean.to_frame(name='mean').to_excel(path)\n",
    "\n",
    "def load_gan_data(gan_files, ori_files, prefix):\n",
    "    gan_dfs = []\n",
    "    ori_dfs = []\n",
    "    for i in range(len(gan_files)):\n",
    "        path = os.path.join(folder, gan_files[i])\n",
    "        gan_dfs.append(pd.read_excel(path))\n",
    "        print(path)\n",
    "        path = os.path.join(folder, ori_files[i])\n",
    "        ori_dfs.append(pd.read_excel(path))\n",
    "    path = os.path.join(folder, prefix + '_std.xlsx')\n",
    "    train_std = pd.read_excel(path, index_col=0).loc[:, 'std']\n",
    "    path = os.path.join(folder, prefix + '_mean.xlsx')\n",
    "    train_mean = pd.read_excel(path, index_col=0).loc[:, 'mean']\n",
    "    return gan_dfs, ori_dfs, train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/도개4_gan.xlsx\n",
      "data/신암4_gan.xlsx\n"
     ]
    }
   ],
   "source": [
    "gan_dfs, ori_dfs, train_mean, train_std = load_gan_data(gan_files, ori_files, prefix=prefix)\n",
    "for i in range(len(gan_dfs)):\n",
    "    gan_dfs[i].columns = gan_dfs[i].columns+'%d'%(i+1)\n",
    "    ori_dfs[i].columns = ori_dfs[i].columns+'%d'%(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmpr_value3</th>\n",
       "      <th>ph_value3</th>\n",
       "      <th>do_value3</th>\n",
       "      <th>ec_value3</th>\n",
       "      <th>toc_value3</th>\n",
       "      <th>총질소_값3</th>\n",
       "      <th>총인_값3</th>\n",
       "      <th>클로로필-a_값3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.096902</td>\n",
       "      <td>0.758452</td>\n",
       "      <td>0.822259</td>\n",
       "      <td>-0.075157</td>\n",
       "      <td>-0.344853</td>\n",
       "      <td>0.275620</td>\n",
       "      <td>0.062420</td>\n",
       "      <td>-0.152076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.139439</td>\n",
       "      <td>0.526553</td>\n",
       "      <td>0.766793</td>\n",
       "      <td>-0.099507</td>\n",
       "      <td>-0.389877</td>\n",
       "      <td>0.283430</td>\n",
       "      <td>-0.027359</td>\n",
       "      <td>-0.220411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.324523</td>\n",
       "      <td>0.522525</td>\n",
       "      <td>0.704634</td>\n",
       "      <td>-0.202995</td>\n",
       "      <td>-0.528889</td>\n",
       "      <td>0.262529</td>\n",
       "      <td>-0.068776</td>\n",
       "      <td>0.082653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.295462</td>\n",
       "      <td>0.524686</td>\n",
       "      <td>0.830331</td>\n",
       "      <td>0.100130</td>\n",
       "      <td>-0.376141</td>\n",
       "      <td>0.190097</td>\n",
       "      <td>-0.022085</td>\n",
       "      <td>-0.000649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.213552</td>\n",
       "      <td>0.770288</td>\n",
       "      <td>0.937213</td>\n",
       "      <td>-0.199829</td>\n",
       "      <td>-0.230602</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.061129</td>\n",
       "      <td>-0.096691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68803</th>\n",
       "      <td>-0.570429</td>\n",
       "      <td>-0.086093</td>\n",
       "      <td>0.581647</td>\n",
       "      <td>-0.034047</td>\n",
       "      <td>-0.546348</td>\n",
       "      <td>-0.619780</td>\n",
       "      <td>-1.060085</td>\n",
       "      <td>-0.646295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68804</th>\n",
       "      <td>-0.576506</td>\n",
       "      <td>-0.086093</td>\n",
       "      <td>0.641735</td>\n",
       "      <td>-0.209620</td>\n",
       "      <td>-0.639205</td>\n",
       "      <td>-0.633176</td>\n",
       "      <td>-1.053605</td>\n",
       "      <td>-0.626142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68805</th>\n",
       "      <td>-0.582583</td>\n",
       "      <td>-0.086093</td>\n",
       "      <td>0.689536</td>\n",
       "      <td>-0.313739</td>\n",
       "      <td>-0.770294</td>\n",
       "      <td>-0.678631</td>\n",
       "      <td>-1.040449</td>\n",
       "      <td>-0.608782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68806</th>\n",
       "      <td>-0.588659</td>\n",
       "      <td>-0.086093</td>\n",
       "      <td>0.688196</td>\n",
       "      <td>-0.312086</td>\n",
       "      <td>-0.747662</td>\n",
       "      <td>-0.725383</td>\n",
       "      <td>-1.046929</td>\n",
       "      <td>-0.590625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68807</th>\n",
       "      <td>-0.588659</td>\n",
       "      <td>-0.086093</td>\n",
       "      <td>0.686856</td>\n",
       "      <td>-0.301295</td>\n",
       "      <td>-0.897478</td>\n",
       "      <td>-0.728689</td>\n",
       "      <td>-1.060085</td>\n",
       "      <td>-0.574617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68808 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tmpr_value3  ph_value3  do_value3  ec_value3  toc_value3    총질소_값3  \\\n",
       "0        -1.096902   0.758452   0.822259  -0.075157   -0.344853  0.275620   \n",
       "1        -1.139439   0.526553   0.766793  -0.099507   -0.389877  0.283430   \n",
       "2        -1.324523   0.522525   0.704634  -0.202995   -0.528889  0.262529   \n",
       "3        -1.295462   0.524686   0.830331   0.100130   -0.376141  0.190097   \n",
       "4        -1.213552   0.770288   0.937213  -0.199829   -0.230602  0.410256   \n",
       "...            ...        ...        ...        ...         ...       ...   \n",
       "68803    -0.570429  -0.086093   0.581647  -0.034047   -0.546348 -0.619780   \n",
       "68804    -0.576506  -0.086093   0.641735  -0.209620   -0.639205 -0.633176   \n",
       "68805    -0.582583  -0.086093   0.689536  -0.313739   -0.770294 -0.678631   \n",
       "68806    -0.588659  -0.086093   0.688196  -0.312086   -0.747662 -0.725383   \n",
       "68807    -0.588659  -0.086093   0.686856  -0.301295   -0.897478 -0.728689   \n",
       "\n",
       "          총인_값3  클로로필-a_값3  \n",
       "0      0.062420  -0.152076  \n",
       "1     -0.027359  -0.220411  \n",
       "2     -0.068776   0.082653  \n",
       "3     -0.022085  -0.000649  \n",
       "4      0.061129  -0.096691  \n",
       "...         ...        ...  \n",
       "68803 -1.060085  -0.646295  \n",
       "68804 -1.053605  -0.626142  \n",
       "68805 -1.040449  -0.608782  \n",
       "68806 -1.046929  -0.590625  \n",
       "68807 -1.060085  -0.574617  \n",
       "\n",
       "[68808 rows x 8 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan_dfs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13732.800000000001"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "68664*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = -3000 # 낙동강\n",
    "#cutoff = -3720 # 영산강\n",
    "gan_dfs[0] = gan_dfs[0][:cutoff]\n",
    "gan_dfs[1] = gan_dfs[1][:cutoff]\n",
    "#gan_dfs[2] = gan_dfs[2][:cutoff]\n",
    "ori_dfs[0] = ori_dfs[0][:cutoff]\n",
    "ori_dfs[1] = ori_dfs[1][:cutoff]\n",
    "#ori_dfs[2] = ori_dfs[2][:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65664, 6566)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_len = len(gan_dfs[0])\n",
    "n_val_len = n_len//10\n",
    "n_len, n_val_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46304"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49304-3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46504"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gan_dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/보_구미보_2016.xlsx\n",
      "data/보_구미보_2017.xlsx\n",
      "data/보_구미보_2018.xlsx\n",
      "data/보_구미보_2019.xlsx\n"
     ]
    }
   ],
   "source": [
    "file_names = []\n",
    "file_names = ['보_구미보_2016.xlsx', '보_구미보_2017.xlsx', '보_구미보_2018.xlsx', '보_구미보_2019.xlsx']\n",
    "folder = 'data'\n",
    "bo_dfs = []\n",
    "for file_name in file_names:\n",
    "    path = os.path.join(folder, file_name)\n",
    "    print(path)\n",
    "    bo_dfs.append(pd.read_excel(path).iloc[:, 2:4])\n",
    "bo_df = pd.concat(bo_dfs, axis=0)\n",
    "\n",
    "bo_mean = bo_df.mean()\n",
    "bo_std = bo_df.std()\n",
    "bo_df = (bo_df - bo_mean)/bo_std\n",
    "\n",
    "bo_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/AWS_옥산_2016.xlsx\n",
      "data/AWS_옥산_2017.xlsx\n",
      "data/AWS_옥산_2018.xlsx\n",
      "data/AWS_옥산_2019.xlsx\n"
     ]
    }
   ],
   "source": [
    "file_names = []\n",
    "file_names = ['AWS_옥산_2016.xlsx', 'AWS_옥산_2017.xlsx', 'AWS_옥산_2018.xlsx', 'AWS_옥산_2019.xlsx']\n",
    "folder = 'data'\n",
    "aws_dfs = []\n",
    "for file_name in file_names:\n",
    "    path = os.path.join(folder, file_name)\n",
    "    print(path)\n",
    "    aws_dfs.append(pd.read_excel(path).iloc[:, -4:])\n",
    "aws_df = pd.concat(aws_dfs, axis=0)\n",
    "\n",
    "aws_mean = aws_df.mean()\n",
    "aws_std = aws_df.std()\n",
    "aws_df = (aws_df - aws_mean)/aws_std\n",
    "aws_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data'\n",
    "path = os.path.join(folder, 'date-15-20.xlsx')\n",
    "date_df = pd.read_excel(path, index_col='ymdh')\n",
    "#date_df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ymdh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01 00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 01:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 02:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 03:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01 04:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-15 19:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-15 20:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-15 21:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-15 22:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-15 23:00</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51504 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [2015-01-01 00:00, 2015-01-01 01:00, 2015-01-01 02:00, 2015-01-01 03:00, 2015-01-01 04:00, 2015-01-01 05:00, 2015-01-01 06:00, 2015-01-01 07:00, 2015-01-01 08:00, 2015-01-01 09:00, 2015-01-01 10:00, 2015-01-01 11:00, 2015-01-01 12:00, 2015-01-01 13:00, 2015-01-01 14:00, 2015-01-01 15:00, 2015-01-01 16:00, 2015-01-01 17:00, 2015-01-01 18:00, 2015-01-01 19:00, 2015-01-01 20:00, 2015-01-01 21:00, 2015-01-01 22:00, 2015-01-01 23:00, 2015-01-02 00:00, 2015-01-02 01:00, 2015-01-02 02:00, 2015-01-02 03:00, 2015-01-02 04:00, 2015-01-02 05:00, 2015-01-02 06:00, 2015-01-02 07:00, 2015-01-02 08:00, 2015-01-02 09:00, 2015-01-02 10:00, 2015-01-02 11:00, 2015-01-02 12:00, 2015-01-02 13:00, 2015-01-02 14:00, 2015-01-02 15:00, 2015-01-02 16:00, 2015-01-02 17:00, 2015-01-02 18:00, 2015-01-02 19:00, 2015-01-02 20:00, 2015-01-02 21:00, 2015-01-02 22:00, 2015-01-02 23:00, 2015-01-03 00:00, 2015-01-03 01:00, 2015-01-03 02:00, 2015-01-03 03:00, 2015-01-03 04:00, 2015-01-03 05:00, 2015-01-03 06:00, 2015-01-03 07:00, 2015-01-03 08:00, 2015-01-03 09:00, 2015-01-03 10:00, 2015-01-03 11:00, 2015-01-03 12:00, 2015-01-03 13:00, 2015-01-03 14:00, 2015-01-03 15:00, 2015-01-03 16:00, 2015-01-03 17:00, 2015-01-03 18:00, 2015-01-03 19:00, 2015-01-03 20:00, 2015-01-03 21:00, 2015-01-03 22:00, 2015-01-03 23:00, 2015-01-04 00:00, 2015-01-04 01:00, 2015-01-04 02:00, 2015-01-04 03:00, 2015-01-04 04:00, 2015-01-04 05:00, 2015-01-04 06:00, 2015-01-04 07:00, 2015-01-04 08:00, 2015-01-04 09:00, 2015-01-04 10:00, 2015-01-04 11:00, 2015-01-04 12:00, 2015-01-04 13:00, 2015-01-04 14:00, 2015-01-04 15:00, 2015-01-04 16:00, 2015-01-04 17:00, 2015-01-04 18:00, 2015-01-04 19:00, 2015-01-04 20:00, 2015-01-04 21:00, 2015-01-04 22:00, 2015-01-04 23:00, 2015-01-05 00:00, 2015-01-05 01:00, 2015-01-05 02:00, 2015-01-05 03:00, ...]\n",
       "\n",
       "[51504 rows x 0 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "              ['말지천_2015.xlsx',\n",
    "               '말지천_2016.xlsx',\n",
    "               '말지천_2017.xlsx',\n",
    "               '말지천_2018.xlsx',\n",
    "               '말지천_2019.xlsx',\n",
    "               '말지천_2020.xlsx'],\n",
    "              ['병성천-1_2015.xlsx',\n",
    "               '병성천-1_2016.xlsx',\n",
    "               '병성천-1_2017.xlsx',\n",
    "               '병성천-1_2018.xlsx',\n",
    "               '병성천-1_2019.xlsx',\n",
    "               '병성천-1_2020.xlsx'],\n",
    "              ['상주3_2015.xlsx',\n",
    "               '상주3_2016.xlsx',\n",
    "               '상주3_2017.xlsx',\n",
    "               '상주3_2018.xlsx',\n",
    "               '상주3_2019.xlsx',\n",
    "               '상주3_2020.xlsx'],\n",
    "              ['위천6_2015.xlsx',\n",
    "               '위천6_2016.xlsx',\n",
    "               '위천6_2017.xlsx',\n",
    "               '위천6_2018.xlsx',\n",
    "               '위천6_2019.xlsx',\n",
    "               '위천6_2020.xlsx'],\n",
    "             ]\n",
    "folder = 'water_data/nak/총량'\n",
    "\n",
    "# what's nh3n, no3n, ss, po4p, dtn, dtp?\n",
    "#qm_cols = ['nh3n', 'no3n', 'ph', 'cod', 'ec', 'bod', 'ss', 'do', 'tn', 'tp', 'chlorophylla', 'toc']\n",
    "tm_cols = ['iem_flux', 'iem_wtrtp',\n",
    "       'iem_ph', 'iem_doc', 'iem_bod', 'iem_cod', 'iem_ss', 'iem_ec', 'iem_tn',\n",
    "       'iem_tp', \n",
    "       'iem_chla', 'iem_toc']\n",
    "#tm_cols = [ 'iem_ph',  'iem_chla']\n",
    "#tm_cols = ['iem_chla']\n",
    "#df = df.loc[:, qm_cols]\n",
    "\n",
    "tm_dfs = []\n",
    "tm_inter_dfs = []\n",
    "for j in range(len(file_names)):\n",
    "    dfs = []\n",
    "    for i in range(len(file_names[j])):\n",
    "        path = os.path.join(folder, file_names[j][i])\n",
    "        dfs.append(pd.read_excel(path, index_col='dt'))\n",
    "        dfs[i] = dfs[i].loc[:, tm_cols]\n",
    "    tm_dfs.append(pd.concat(dfs))\n",
    "    tm_dfs[j] = pd.concat([date_df, tm_dfs[j]], axis=1)\n",
    "    tm_dfs[j].reset_index(drop=True, inplace=True)\n",
    "    first_series = tm_dfs[j].apply(pd.Series.first_valid_index)\n",
    "    last_series = tm_dfs[j].apply(pd.Series.first_valid_index)\n",
    "    for col in first_series.index:\n",
    "        tm_dfs[j].loc[0, col] = tm_dfs[j].loc[first_series[col], col]\n",
    "        tm_dfs[j].loc[len(tm_dfs[j])-1, col] = tm_dfs[j].loc[last_series[col], col]\n",
    "    \n",
    "    tm_dfs[j] = tm_dfs[j].interpolate(method='pchip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_mean = []\n",
    "tm_std = []\n",
    "\n",
    "for i in range(len(tm_dfs)):\n",
    "    tm_mean.append(tm_dfs[i].mean())\n",
    "    tm_std.append(tm_dfs[i].std())\n",
    "    tm_dfs[i] = (tm_dfs[i]-tm_mean[i])/tm_std[i]\n",
    "    tm_dfs[i].columns=tm_dfs[i].columns+'%d'%(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "              ['병성천-1_2015.xlsx',\n",
    "               '병성천-1_2016.xlsx',\n",
    "               '병성천-1_2017.xlsx',\n",
    "               '병성천-1_2018.xlsx',\n",
    "               '병성천-1_2019.xlsx',\n",
    "               '병성천-1_2020.xlsx'],\n",
    "              ['상주2_2015.xlsx',\n",
    "               '상주2_2016.xlsx',\n",
    "               '상주2_2017.xlsx',\n",
    "               '상주2_2018.xlsx',\n",
    "               '상주2_2019.xlsx',\n",
    "               '상주2_2020.xlsx'],\n",
    "              ['상주3_2015.xlsx',\n",
    "               '상주3_2016.xlsx',\n",
    "               '상주3_2017.xlsx',\n",
    "               '상주3_2018.xlsx',\n",
    "               '상주3_2019.xlsx',\n",
    "               '상주3_2020.xlsx'],\n",
    "             ]\n",
    "folder = 'water_data/nak/수질'\n",
    "\n",
    "# what's nh3n, no3n, ss, po4p, dtn, dtp?\n",
    "#qm_cols = ['nh3n', 'no3n', 'ph', 'cod', 'ec', 'bod', 'ss', 'do', 'tn', 'tp', 'chlorophylla', 'toc']\n",
    "#qm_cols = ['ss', 'chlorophylla']\n",
    "qm_cols = ['chlorophylla']\n",
    "#df = df.loc[:, qm_cols]\n",
    "\n",
    "qm_dfs = []\n",
    "qm_inter_dfs = []\n",
    "for j in range(len(file_names)):\n",
    "    dfs = []\n",
    "    for i in range(len(file_names[j])):\n",
    "        path = os.path.join(folder, file_names[j][i])\n",
    "        dfs.append(pd.read_excel(path, index_col='dt'))\n",
    "        dfs[i] = dfs[i].loc[:, qm_cols]\n",
    "    qm_dfs.append(pd.concat(dfs))\n",
    "    qm_dfs[j] = pd.concat([date_df, qm_dfs[j]], axis=1)\n",
    "    qm_dfs[j].reset_index(drop=True, inplace=True)\n",
    "    first_series = qm_dfs[j].apply(pd.Series.first_valid_index)\n",
    "    last_series = qm_dfs[j].apply(pd.Series.first_valid_index)\n",
    "    for col in first_series.index:\n",
    "        qm_dfs[j].loc[0, col] = qm_dfs[j].loc[first_series[col], col]\n",
    "        qm_dfs[j].loc[len(qm_dfs[j])-1, col] = qm_dfs[j].loc[last_series[col], col]\n",
    "    \n",
    "    qm_dfs[j] = qm_dfs[j].interpolate(method='pchip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm_mean = []\n",
    "qm_std = []\n",
    "\n",
    "for i in range(len(qm_dfs)):\n",
    "    qm_dfs[i].columns=qm_dfs[i].columns+'%d'%(i+1)\n",
    "    qm_mean.append(qm_dfs[i].mean())\n",
    "    qm_std.append(qm_dfs[i].std())\n",
    "    qm_dfs[i] = (qm_dfs[i]-qm_mean[i])/qm_std[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bo_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-d7c9cd97751d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbo_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maws_mean\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mqm_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbo_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maws_std\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mqm_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bo_mean' is not defined"
     ]
    }
   ],
   "source": [
    "mean = pd.concat([train_mean, bo_mean, aws_mean]+qm_mean)\n",
    "std = pd.concat([train_std, bo_std, aws_std]+qm_std)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['tmpr_value2', 'ph_value2', 'do_value2', 'ec_value2', 'toc_value2',\\n       '총질소_값2', '총인_값2', '클로로필-a_값2'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-280-2cafaa8031e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgan_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['tmpr_value2', 'ph_value2', 'do_value2', 'ec_value2', 'toc_value2',\\n       '총질소_값2', '총인_값2', '클로로필-a_값2'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "gan_dfs[0][gan_dfs[1].columns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_df = pd.concat(ori_dfs, axis=1)\n",
    "gan_df = pd.concat(gan_dfs, axis=1)\n",
    "#ori_df = pd.concat(ori_dfs+[bo_df, aws_df]+qm_dfs, axis=1)\n",
    "#gan_df = pd.concat(gan_dfs+[bo_df, aws_df]+qm_dfs, axis=1)\n",
    "#ori_df = pd.concat(ori_dfs+qm_dfs+[tm_dfs[2]], axis=1)\n",
    "#gan_df = pd.concat(gan_dfs+qm_dfs+[tm_dfs[2]], axis=1)\n",
    "#ori_df = pd.concat([ori_dfs[0]]+qm_dfs+[tm_dfs[2]], axis=1)\n",
    "#gan_df = pd.concat([gan_dfs[0]]+qm_dfs+[tm_dfs[2]], axis=1)\n",
    "#ori_df = pd.concat([ori_dfs[0]], axis=1)\n",
    "#gan_df = pd.concat([gan_dfs[0]], axis=1)\n",
    "#ori_df = pd.concat(ori_dfs+[bo_df, aws_df], axis=1)\n",
    "#gan_df = pd.concat(gan_dfs+[bo_df, aws_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ori_df = pd.concat(ori_dfs, axis=1)\n",
    "#gan_df = pd.concat(gan_dfs, axis=1)\n",
    "\n",
    "#ori_df = pd.concat(ori_dfs+[bo_df, aws_df]+qm_dfs, axis=1)\n",
    "#gan_df = pd.concat(gan_dfs+[bo_df, aws_df]+qm_dfs, axis=1)\n",
    "\n",
    "total_no = ori_df.shape[0]\n",
    "train_no = int(total_no*0.7)\n",
    "val_no = int(total_no*0.1)\n",
    "\n",
    "train_slice = slice(0, train_no)\n",
    "#train_slice = slice(val_no, val_no+train_no)\n",
    "#val_slice = slice(train_no, train_no+val_no)\n",
    "#val_slice = slice(train_no, None)\n",
    "test_slice = slice(train_no, train_no+val_no)\n",
    "#test_slice = slice(train_no+val_no, None)\n",
    "#test_slice = slice(train_no+val_no, None)\n",
    "#val_slice = slice(train_no+val_no, None)\n",
    "#val_slice = slice(val_no+train_no, None)\n",
    "val_slice = slice(train_no+val_no, None)\n",
    "\n",
    "train_df = pd.DataFrame(gan_df[train_slice])\n",
    "val_df = pd.DataFrame(gan_df[val_slice])\n",
    "test_df = pd.DataFrame(gan_df[test_slice])\n",
    "\n",
    "train_ori_df = pd.DataFrame(ori_df[train_slice])\n",
    "val_ori_df = pd.DataFrame(ori_df[val_slice])\n",
    "test_ori_df = pd.DataFrame(ori_df[test_slice])\n",
    "\n",
    "num_features = train_df.shape[1]\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46216"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_hr_df = gan_df.groupby(gan_df.index //24).mean()\n",
    "total_no = gan_hr_df.shape[0]\n",
    "train_no = int(total_no*0.7)\n",
    "\n",
    "train_slice = slice(0, train_no)\n",
    "val_slice = slice(train_no, None)\n",
    "test_slice = slice(0, None)\n",
    "\n",
    "train_day_df = gan_hr_df[train_slice]\n",
    "val_day_df = gan_hr_df[val_slice]\n",
    "test_day_df = gan_hr_df[test_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tmpr_value1', 'ph_value1', 'do_value1', 'ec_value1', 'toc_value1',\n",
       "       '총질소_값1', '총인_값1', '클로로필-a_값1', 'Day sin1', 'Day cos1', 'Year sin1',\n",
       "       'Year cos1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUT FEATURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2: DO 4: TOC, 5: TN, 6: TP, 7: Chl-a,\n",
    "out_features = [7]\n",
    "out_num_features=len(out_features)\n",
    "input_days = 14 \n",
    "out_feature_name = train_df.columns[out_features[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "            #train_df=None, val_df=None, test_df=None,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]', fontproperties=fprop)\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df, train=True)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for water'\n",
    "    def __init__(self,\n",
    "                 imputed_data,\n",
    "                 ori_data = None,\n",
    "                 batch_size=32,\n",
    "                 input_width=24*7,\n",
    "                 label_width=24*3,\n",
    "                 shift=24*3,\n",
    "                 skip_time = None,\n",
    "                 shuffle = True,\n",
    "                 out_features = None,\n",
    "                 out_num_features = None,\n",
    "                ):\n",
    "        'Initialization'\n",
    "        self.window_size = input_width+shift\n",
    "        self.total_no = imputed_data.shape[0]\n",
    "        self.data = imputed_data\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = (batch_size, input_width, self.data.shape[1])\n",
    "        self.out_num_features = out_num_features\n",
    "        if out_features:\n",
    "            self.out_features = out_features\n",
    "        else:\n",
    "            self.out_features = [i for i in range(out_num_features)]\n",
    "        self.label_shape = (batch_size, label_width, self.out_num_features)\n",
    "        if (skip_time):\n",
    "            # TO-DO\n",
    "            self.no = self.total_no - self.window_size\n",
    "            self.data_idx = np.arange(0, self.no)\n",
    "        else:\n",
    "            self.no = self.total_no - self.window_size\n",
    "            self.data_idx = np.arange(0, self.no)\n",
    "            \n",
    "        if shuffle:\n",
    "            self.batch_idx = np.random.permutation(self.no)\n",
    "        else:\n",
    "            self.batch_idx = np.arange(0, self.no)\n",
    "        self.batch_id = 0\n",
    "        #print('')\n",
    "        #print('self.total_no =', self.total_no)\n",
    "        #print('')\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        #return int(128/self.batch_size)\n",
    "        #return 2\n",
    "        return self.no//self.batch_size\n",
    "        #return 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        #print('index =', index)\n",
    "        #print('self.no =', self.no)\n",
    "        #print('self.total_no =', self.total_no)\n",
    "        #print('')\n",
    "        #print('self.no =', self.no, 'self.batch_id =', self.batch_id)\n",
    "        # Sample batch\n",
    "        label_width = self.label_width\n",
    "        batch_idx = self.batch_idx\n",
    "        \n",
    "        x = np.empty((0, self.input_width, self.data.shape[1]))\n",
    "        y = np.empty((0, self.label_width, self.out_num_features))\n",
    "        for cnt in range(0, self.batch_size):\n",
    "            i = self.batch_id\n",
    "            self.batch_id += 1\n",
    "            idx1 = self.data_idx[batch_idx[i]]\n",
    "            idx2 = idx1 + self.input_width\n",
    "            \n",
    "            X = self.data[idx1:idx2]\n",
    "            \n",
    "            idx1 = self.data_idx[batch_idx[i]] + self.window_size - label_width\n",
    "            idx2 = idx1 + label_width\n",
    "            \n",
    "            #Y = self.data[idx1:idx2,:,:out_num_features]\n",
    "            Y = self.data.iloc[idx1:idx2, self.out_features]\n",
    "            #print('Y.shape = ', Y.shape)\n",
    "            #Y = Y.iloc[:,:out_num_features]\n",
    "            \n",
    "            self.batch_id %= self.no\n",
    "            \n",
    "            x = np.append(x, [X], axis = 0)\n",
    "            y = np.append(y, [Y], axis = 0)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_water(self, data, train=False):\n",
    "  dg = WaterDataGenerator(\n",
    "      data,\n",
    "      batch_size = 128,\n",
    "      #batch_size = 128 if train else data.shape[0]-(self.input_width+self.label_width),\n",
    "      input_width = self.input_width,\n",
    "      label_width = self.label_width,\n",
    "      shift = self.shift,\n",
    "      out_features = out_features,\n",
    "      out_num_features = out_num_features,\n",
    "  )\n",
    "  #self.dg = dg\n",
    "  ds = tf.data.Dataset.from_generator(\n",
    "      lambda: dg,\n",
    "      output_types=(tf.float32, tf.float32),\n",
    "      output_shapes=(\n",
    "        dg.input_shape,\n",
    "        dg.label_shape\n",
    "        #[batch_size, train_generator.dim],\n",
    "        #[batch_size, train_generator.dim],\n",
    "      )\n",
    "  )\n",
    "  #return ds.batch(batch_size=128)\n",
    "  if train:\n",
    "      #return ds.repeat(10).prefetch(3)\n",
    "      return ds.repeat(-1).prefetch(5)\n",
    "  else:\n",
    "      return ds.prefetch(5)\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2(self, model=None, plot_col=0, max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  #plot_out_col_index = self.column_indices[plot_out_col]\n",
    "  out_features_np = np.array(out_features)\n",
    "  plot_out_col_index = np.argwhere(out_features_np == plot_col_index)[0][0]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]', fontproperties=fprop)\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "      label_out_col_index = self.label_columns_indices.get(plot_out_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "      label_out_col_index = plot_out_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.plot(self.label_indices, labels[n, :, label_out_col_index],\n",
    "                label='Labels', c='#2ca02c')\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.plot(self.label_indices, predictions[n, :, label_out_col_index],\n",
    "                  marker=None, label='Predictions',\n",
    "                  c='#ff7f0e')\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot2 = plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tmpr_value1', 'ph_value1', 'do_value1', 'ec_value1', 'toc_value1',\n",
       "       '총질소_값1', '총인_값1', '클로로필-a_값1', 'Day sin1', 'Day cos1', 'Year sin1',\n",
       "       'Year cos1', 'tmpr_value2', 'ph_value2', 'do_value2', 'ec_value2',\n",
       "       'toc_value2', '총질소_값2', '총인_값2', '클로로필-a_값2', 'tmpr_value3',\n",
       "       'ph_value3', 'do_value3', 'ec_value3', 'toc_value3', '총질소_값3', '총인_값3',\n",
       "       '클로로필-a_값3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329d801cc9494433b814d3bce0920bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(gan_dfs[0].iloc[:, 6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e732a2a50104b89a1a640148e12b5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(gan_dfs[0].iloc[:, 7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8699bbab9894dfc8a6f039052a2d0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Total window size: 672\n",
       "Input indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
       "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
       "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
       "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
       "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
       "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
       " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
       " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
       " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
       " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
       " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
       " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
       " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
       " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
       " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
       " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
       " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
       " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
       " 324 325 326 327 328 329 330 331 332 333 334 335]\n",
       "Label indices: [336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353\n",
       " 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371\n",
       " 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389\n",
       " 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407\n",
       " 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425\n",
       " 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443\n",
       " 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461\n",
       " 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479\n",
       " 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497\n",
       " 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515\n",
       " 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533\n",
       " 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551\n",
       " 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569\n",
       " 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587\n",
       " 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605\n",
       " 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623\n",
       " 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641\n",
       " 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659\n",
       " 660 661 662 663 664 665 666 667 668 669 670 671]\n",
       "Label column name(s): None"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUT_STEPS = 24*5\n",
    "OUT_STEPS = 24*14\n",
    "SHIFT = 24*14\n",
    "multi_window = WindowGenerator(input_width=24*input_days,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=SHIFT,\n",
    "                               train_df=train_df,\n",
    "                               val_df=val_df,\n",
    "                               test_df=test_df,\n",
    "                               )\n",
    "\n",
    "#multi_window.plot2(plot_col=out_features[0])\n",
    "#multi_window.plot2(plot_col='클로로필-a_값1', plot_out_col='클로로필-a_값1')\n",
    "#multi_window.plot2(plot_col='클로로필-a_값1')\n",
    "#multi_window.plot2(plot_col='총인_값1')\n",
    "multi_window.plot2(plot_col=out_feature_name)\n",
    "#multi_window.plot2(plot_col='toc_value1')\n",
    "multi_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOUT_STEPS = 5\\nmulti_window = WindowGenerator(input_width=input_days,\\n                               label_width=OUT_STEPS,\\n                               shift=OUT_STEPS,\\n                               train_df=train_day_df,\\n                               val_df=val_day_df,\\n                               test_df=test_day_df,\\n                               )\\n\\n#multi_window.plot2(plot_col=out_features[0])\\n#multi_window.plot2(plot_col='클로로필-a_값1', plot_out_col='클로로필-a_값1')\\nmulti_window.plot2(plot_col='클로로필-a_값1')\\nmulti_window\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "OUT_STEPS = 5\n",
    "multi_window = WindowGenerator(input_width=input_days,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS,\n",
    "                               train_df=train_day_df,\n",
    "                               val_df=val_day_df,\n",
    "                               test_df=test_day_df,\n",
    "                               )\n",
    "\n",
    "#multi_window.plot2(plot_col=out_features[0])\n",
    "#multi_window.plot2(plot_col='클로로필-a_값1', plot_out_col='클로로필-a_값1')\n",
    "multi_window.plot2(plot_col='클로로필-a_값1')\n",
    "multi_window\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_val_performance = {}\n",
    "multi_performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 4s 113ms/step - loss: 0.2475 - mean_absolute_error: 0.3420\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.4109 - mean_absolute_error: 0.4645\n",
      "val performance = [0.2490123212337494, 0.34261825680732727]\n",
      "test performance =  [0.4108981490135193, 0.4644666612148285]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6570fb961574b73a91178e819ed6adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    #print(inputs[:, -1:, 0:1])\n",
    "    #return tf.tile(inputs[:, -1:, :out_num_features], [1, OUT_STEPS, 1])\n",
    "    return tf.tile(inputs[:, -1:, (out_features[0]):(out_features[0]+1)], [1, OUT_STEPS, 1])\n",
    "    #return tf.tile(inputs[:, -1:, out_features[0]:(out_features[1]+1)], [1, OUT_STEPS, 1])\n",
    "\n",
    "last_baseline = MultiStepLastBaseline()\n",
    "last_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "#multi_val_performance = {}\n",
    "#multi_performance = {}\n",
    "\n",
    "multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Last'] = last_baseline.evaluate(multi_window.test)\n",
    "print('val performance =', multi_val_performance['Last'])\n",
    "print('test performance = ', multi_performance['Last'])\n",
    "multi_window.plot2(last_baseline, plot_col=train_df.columns[out_features[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_EPOCHS = 400\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def nse(y_true, y_pred):\n",
    "    mean = tf.reduce_mean(y_true)\n",
    "    return 1. - tf.reduce_sum(tf.square(y_true-y_pred))/tf.reduce_sum(tf.square(y_true-mean))\n",
    "\n",
    "def compile_and_fit(model, window, patience=1000):\n",
    "  checkpoint = keras.callbacks.ModelCheckpoint('save'+'/best_model.h5', monitor='val_loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError(), nse])\n",
    "  #model.compile(loss=GAIN.RMSE_loss)\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS, steps_per_epoch=10,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping, checkpoint])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(history.history['loss'], label='loss')\n",
    "    ax.plot(history.history['mean_absolute_error'], label='mae')\n",
    "    ax.plot(history.history['val_loss'], label='val_loss')\n",
    "    ax.plot(history.history['val_mean_absolute_error'], label='val_mae')\n",
    "    #plt.legend(history.history.keys(), loc='upper right')\n",
    "    #ax.legend(loc='upper center')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"epochs\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "multi_linear_model = tf.keras.Sequential([\n",
    "    # Take the last time-step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*out_num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, out_num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_linear_model, multi_window)\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "#multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val.repeat(-1), steps=100)\n",
    "#multi_performance['Linear'] = multi_linear_model.evaluate(multi_window.test.repeat(-1), verbose=1, steps=100)\n",
    "multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\n",
    "multi_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=1)\n",
    "#multi_window.plot(multi_linear_model, plot_col=0)\n",
    "print('val performance =', multi_val_performance['Linear'])\n",
    "print('test performance = ', multi_performance['Linear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)//128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot2(multi_linear_model, plot_col=out_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "58*24*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10240/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 40 \n",
    "multi_dense_model = tf.keras.Sequential([\n",
    "    # Take the last time step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Shape => [batch, 1, dense_units]\n",
    "    #tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(8192, activation='relu'),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*out_num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, out_num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_dense_model, multi_window)\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)\n",
    "multi_performance['Dense'] = multi_dense_model.evaluate(multi_window.test, verbose=1)\n",
    "print('val performance =', multi_val_performance['Dense'])\n",
    "print('test performance = ', multi_performance['Dense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot2(multi_dense_model, plot_col=out_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 5 \n",
    "#CONV_LAYER_NO = 3\n",
    "CONV_WIDTH = 3\n",
    "multi_conv_model = tf.keras.Sequential([\n",
    "    keras.layers.Conv1D(256, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(256, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPooling1D(padding='same'),\n",
    "    keras.layers.Conv1D(512, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(512, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(512, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPooling1D(padding='same'),\n",
    "    keras.layers.Conv1D(1024, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(1024, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(1024, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPooling1D(padding='same'),\n",
    "    keras.layers.Conv1D(2048, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(2048, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(2048, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    #keras.layers.MaxPooling1D(padding='same'),\n",
    "    \n",
    "    #keras.layers.Conv1D(4096, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    #keras.layers.Activation('relu'),\n",
    "    #keras.layers.Conv1D(4096, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    #keras.layers.Activation('relu'),\n",
    "    #keras.layers.MaxPooling1D(padding='same'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #keras.layers.MaxPooling1D(pool_size=2),\n",
    "    #keras.layers.Conv1D(1024, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.MaxPooling1D(pool_size=2),\n",
    "    #keras.layers.Conv1D(1024, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.MaxPooling1D(pool_size=2),\n",
    "    #keras.layers.Conv1D(1024, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Shape => [batch, 1,  out_steps*features]\n",
    "    #tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          #kernel_initializer=tf.initializers.zeros),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_fit(multi_conv_model, multi_window)\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)\n",
    "multi_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, verbose=1)\n",
    "print('val performance =', multi_val_performance['Conv'])\n",
    "print('test performance = ', multi_performance['Conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU model**  \n",
    "best: all feature, 10 days, 256,256\n",
    "```\n",
    "val performance = [0.46297532320022583, 0.4968714416027069]\n",
    "test performance =  [0.5573816895484924, 0.5664846301078796]\n",
    "```\n",
    "\n",
    "best: aws,bo feature, 5 days, 256,256 (dropout 0.1, 0.5) dense dropout 0.1\n",
    "10%, 80%, 10%(val)\n",
    "```\n",
    "val performance = [0.5091502666473389, 0.5460899472236633, 0.5668533444404602]\n",
    "test performance =  [0.27155840396881104, 0.3736426532268524, 0.59783536195755]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_63 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_64 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 16s 1s/step - loss: 0.9166 - mean_absolute_error: 0.7243 - nse: -0.0221 - val_loss: 0.9048 - val_mean_absolute_error: 0.7026 - val_nse: 0.2076\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90484, saving model to save/best_model.h5\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.7604 - mean_absolute_error: 0.6207 - nse: 0.2252 - val_loss: 0.8031 - val_mean_absolute_error: 0.6556 - val_nse: 0.2967\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.90484 to 0.80310, saving model to save/best_model.h5\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.7476 - mean_absolute_error: 0.5932 - nse: 0.3145 - val_loss: 0.8101 - val_mean_absolute_error: 0.6561 - val_nse: 0.2894\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.80310\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6512 - mean_absolute_error: 0.5611 - nse: 0.3012 - val_loss: 0.7608 - val_mean_absolute_error: 0.6371 - val_nse: 0.3335\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80310 to 0.76084, saving model to save/best_model.h5\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6623 - mean_absolute_error: 0.5691 - nse: 0.3408 - val_loss: 0.7840 - val_mean_absolute_error: 0.6461 - val_nse: 0.3133\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.76084\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6691 - mean_absolute_error: 0.5652 - nse: 0.3058 - val_loss: 0.8857 - val_mean_absolute_error: 0.7071 - val_nse: 0.2237\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.76084\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.6248 - mean_absolute_error: 0.5564 - nse: 0.3145 - val_loss: 0.8871 - val_mean_absolute_error: 0.7001 - val_nse: 0.2198\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.76084\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.6764 - mean_absolute_error: 0.5712 - nse: 0.3549 - val_loss: 0.9441 - val_mean_absolute_error: 0.7107 - val_nse: 0.1720\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.76084\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.5651 - mean_absolute_error: 0.5247 - nse: 0.3788 - val_loss: 0.8115 - val_mean_absolute_error: 0.6517 - val_nse: 0.2878\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.76084\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.5994 - mean_absolute_error: 0.5409 - nse: 0.3544 - val_loss: 0.9917 - val_mean_absolute_error: 0.7293 - val_nse: 0.1303\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.76084\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 14s 1s/step - loss: 0.5459 - mean_absolute_error: 0.5117 - nse: 0.4539 - val_loss: 0.9480 - val_mean_absolute_error: 0.7230 - val_nse: 0.1688\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.76084\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 13s 1s/step - loss: 0.4436 - mean_absolute_error: 0.4888 - nse: 0.4705 - val_loss: 0.9781 - val_mean_absolute_error: 0.7142 - val_nse: 0.1451\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.76084\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5848 - mean_absolute_error: 0.5191 - nse: 0.4430"
     ]
    }
   ],
   "source": [
    "#MAX_EPOCHS = 150\n",
    "MAX_EPOCHS = 20 \n",
    "multi_gru_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    #tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    #tf.keras.layers.GRU(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    #tf.keras.layers.GRU(256, return_sequences=False, dropout=0.1, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.GRU(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.GRU(256, return_sequences=False, dropout=0.1, recurrent_dropout=0.5),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    #tf.keras.layers.Dense(OUT_STEPS*out_num_features*2, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*out_num_features),\n",
    "    #tf.keras.layers.Dense(OUT_STEPS*out_num_features,\n",
    "                          #kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, out_num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_gru_model, multi_window)\n",
    "#multi_gru_model.load_weights('nagdong_cl.h5')\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['GRU'] = multi_gru_model.evaluate(multi_window.val)\n",
    "multi_performance['GRU'] = multi_gru_model.evaluate(multi_window.test, verbose=1)\n",
    "print('val performance =', multi_val_performance['GRU'])\n",
    "print('test performance = ', multi_performance['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 12s 126ms/step - loss: 0.3773 - mean_absolute_error: 0.4370 - nse: 0.3745\n",
      "48/48 [==============================] - 6s 127ms/step - loss: 0.3964 - mean_absolute_error: 0.4475 - nse: 0.4719\n",
      "val performance = [0.3772953152656555, 0.43701237440109253, 0.3745199739933014]\n",
      "test performance =  [0.39642369747161865, 0.44746726751327515, 0.4718872606754303]\n"
     ]
    }
   ],
   "source": [
    "multi_gru_model.load_weights('save/best_model.h5')\n",
    "multi_val_performance['GRU'] = multi_gru_model.evaluate(multi_window.val)\n",
    "multi_performance['GRU'] = multi_gru_model.evaluate(multi_window.test, verbose=1)\n",
    "print('val performance =', multi_val_performance['GRU'])\n",
    "print('test performance = ', multi_performance['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222e7d3b3bb64a4db0dd21e5456672e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65544f6f04be466f949ac83cefe3674c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_window.plot2(multi_gru_model, plot_col=out_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gru_model.save_weights('nagdong_cl.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_48 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_49 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_50 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/15\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.4384 - mean_absolute_error: 0.5326 - nse: -0.2803 - val_loss: 0.1698 - val_mean_absolute_error: 0.3251 - val_nse: -0.2159\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16979, saving model to save/best_model.h5\n",
      "Epoch 2/15\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.3043 - mean_absolute_error: 0.4247 - nse: 0.1351 - val_loss: 0.1125 - val_mean_absolute_error: 0.2611 - val_nse: 0.1954\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16979 to 0.11251, saving model to save/best_model.h5\n",
      "Epoch 3/15\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.2198 - mean_absolute_error: 0.3560 - nse: 0.3406 - val_loss: 0.0828 - val_mean_absolute_error: 0.2206 - val_nse: 0.4075\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11251 to 0.08282, saving model to save/best_model.h5\n",
      "Epoch 4/15\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.1756 - mean_absolute_error: 0.3163 - nse: 0.4706 - val_loss: 0.0984 - val_mean_absolute_error: 0.2447 - val_nse: 0.2951\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.08282\n",
      "Epoch 5/15\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.1541 - mean_absolute_error: 0.2987 - nse: 0.5263 - val_loss: 0.0743 - val_mean_absolute_error: 0.2114 - val_nse: 0.4689\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08282 to 0.07427, saving model to save/best_model.h5\n",
      "Epoch 6/15\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.1593 - mean_absolute_error: 0.2979 - nse: 0.5412 - val_loss: 0.1005 - val_mean_absolute_error: 0.2410 - val_nse: 0.2819\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07427\n",
      "Epoch 7/15\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.1433 - mean_absolute_error: 0.2822 - nse: 0.6225 - val_loss: 0.0773 - val_mean_absolute_error: 0.2126 - val_nse: 0.4460\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07427\n",
      "Epoch 8/15\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.1314 - mean_absolute_error: 0.2712 - nse: 0.6354 - val_loss: 0.0914 - val_mean_absolute_error: 0.2365 - val_nse: 0.3477\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07427\n",
      "Epoch 9/15\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.1358 - mean_absolute_error: 0.2776 - nse: 0.6173 - val_loss: 0.0892 - val_mean_absolute_error: 0.2281 - val_nse: 0.3607\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07427\n",
      "Epoch 10/15\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.1230 - mean_absolute_error: 0.2644 - nse: 0.6545 - val_loss: 0.0849 - val_mean_absolute_error: 0.2236 - val_nse: 0.3927\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07427\n",
      "Epoch 11/15\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.1178 - mean_absolute_error: 0.2589 - nse: 0.6524 - val_loss: 0.0809 - val_mean_absolute_error: 0.2166 - val_nse: 0.4215\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07427\n",
      "Epoch 12/15\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.1201 - mean_absolute_error: 0.2614 - nse: 0.6581 - val_loss: 0.0935 - val_mean_absolute_error: 0.2342 - val_nse: 0.3329\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07427\n",
      "Epoch 13/15\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.1183 - mean_absolute_error: 0.2617 - nse: 0.6720 - val_loss: 0.0915 - val_mean_absolute_error: 0.2308 - val_nse: 0.3459\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07427\n",
      "Epoch 14/15\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.1120 - mean_absolute_error: 0.2508 - nse: 0.6870 - val_loss: 0.0865 - val_mean_absolute_error: 0.2260 - val_nse: 0.3812\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07427\n",
      "Epoch 15/15\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.1145 - mean_absolute_error: 0.2588 - nse: 0.6580 - val_loss: 0.0723 - val_mean_absolute_error: 0.2038 - val_nse: 0.4843\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07427 to 0.07227, saving model to save/best_model.h5\n",
      "93/93 [==============================] - 13s 135ms/step - loss: 0.0723 - mean_absolute_error: 0.2038 - nse: 0.4828\n",
      "45/45 [==============================] - 6s 135ms/step - loss: 0.1664 - mean_absolute_error: 0.3142 - nse: 0.4352\n",
      "val performance = [0.07226569205522537, 0.20380105078220367, 0.48276129364967346]\n",
      "test performance =  [0.1664344072341919, 0.31417927145957947, 0.4352313280105591]\n"
     ]
    }
   ],
   "source": [
    "#MAX_EPOCHS = 150\n",
    "MAX_EPOCHS = 15\n",
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    #tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    #tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    #tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    #tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.GRU(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.GRU(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.5),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    #tf.keras.layers.Dense(OUT_STEPS*out_num_features,\n",
    "                          #kernel_initializer=tf.initializers.zeros),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*out_num_features),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, out_num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_lstm_model, multi_window)\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\n",
    "multi_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test)\n",
    "print('val performance =', multi_val_performance['LSTM'])\n",
    "print('test performance = ', multi_performance['LSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e5d63054284f8eae7063ff037165ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106950d1821b4792bce2287c65f09e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_window.plot2(multi_lstm_model, plot_col=out_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(multi_performance))\n",
    "width = 0.3\n",
    "\n",
    "\n",
    "metric_name = 'mean_absolute_error'\n",
    "metric_index = multi_gru_model.metrics_names.index('mean_absolute_error')\n",
    "val_mae = [v[metric_index] for v in multi_val_performance.values()]\n",
    "test_mae = [v[metric_index] for v in multi_performance.values()]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "plt.bar(x + 0.17, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=multi_performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel(f'MAE (average over all times and outputs)')\n",
    "_ = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daily Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class DaysPrediction(object):\n",
    "    def __init__(self,\n",
    "                 df=test_df,\n",
    "                 in_steps=24*input_days,\n",
    "                 out_steps=OUT_STEPS,\n",
    "                 shift=SHIFT,\n",
    "                ):\n",
    "        self.df = df\n",
    "        self.in_steps = in_steps\n",
    "        self.out_steps = out_steps\n",
    "        # fix해야함\n",
    "        #len_tt = df.shape[0]//in_steps*in_steps\n",
    "        #self.n_days = (len_tt - in_steps)//24\n",
    "        self.n_days = df.shape[0]//24\n",
    "        self.pred_days = out_steps//24\n",
    "        self.start_day = (SHIFT-OUT_STEPS)//24\n",
    "        self.in_days = in_steps//24\n",
    "        self.gt = np.full((self.n_days), np.nan)\n",
    "        for i in range(self.n_days):\n",
    "            x = self.df[i*24:(i*24+24)].to_numpy()\n",
    "            #x = val_df[i*24:(i*24+24)].to_numpy()\n",
    "            self.gt[i] = np.average(x[0:24, out_features[0]])\n",
    "        \n",
    "    def predict(self, model):\n",
    "        preds = np.full((self.start_day+self.pred_days, self.n_days), np.nan)\n",
    "        x_batch = np.empty(((self.n_days-self.in_days)*24, self.in_steps, self.df.shape[1]))\n",
    "        for i in range(self.n_days-self.in_days):\n",
    "            x = self.df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            #x = val_df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            x = x.reshape((-1, self.in_steps, self.df.shape[1]))\n",
    "            x_batch[i*24:(i+1)*24] = x\n",
    "            \n",
    "        #print('x_batch.shape =', x_batch.shape)\n",
    "        ds = tf.data.Dataset.from_tensor_slices(x_batch).batch(256)\n",
    "        y_batch = model.predict(ds, verbose=1)\n",
    "        #print('y_batch.shape =', y_batch.shape)\n",
    "        for i in range(self.n_days-self.in_days):\n",
    "            y = y_batch[i*24:(i+1)*24]\n",
    "            for j in range(self.pred_days):\n",
    "                if i+self.in_days+j+self.start_day >= self.n_days:\n",
    "                    break\n",
    "                preds[j+self.start_day][i+self.in_days+j+self.start_day] = np.average(y[:, j*24:j*24+24, :])\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 144)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHIFT, OUT_STEPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(object):\n",
    "    def __init__(self,\n",
    "                 df=test_df,\n",
    "                 in_steps=input_days,\n",
    "                 out_steps=OUT_STEPS,\n",
    "                ):\n",
    "        self.df = df\n",
    "        self.in_steps = in_steps\n",
    "        self.out_steps = out_steps\n",
    "        len_tt = df.shape[0]\n",
    "        #self.n_days = len_tt - in_steps\n",
    "        self.n_days = len_tt\n",
    "        self.pred_days = out_steps\n",
    "        self.in_days = in_steps\n",
    "        self.gt = self.df.iloc[:, out_features[0]].to_numpy()\n",
    "        \n",
    "    def predict(self, model):\n",
    "        preds = np.full((self.pred_days, self.n_days), np.nan)\n",
    "        for i in tqdm(range(self.n_days-self.in_days)):\n",
    "        #for i in tqdm(range(1)):\n",
    "            x = self.df[i:(i+self.in_steps)].to_numpy()\n",
    "            #x = val_df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            x = x.reshape((-1, self.in_steps, self.df.shape[1]))\n",
    "            y = model.predict(x)\n",
    "            #print(y.shape)\n",
    "            for j in range(self.pred_days):\n",
    "                if i+self.in_days+j >= self.n_days:\n",
    "                    break\n",
    "                preds[j][i+self.in_days+j] = y[0, j, 0]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoursPrediction(object):\n",
    "    def __init__(self,\n",
    "                 df=test_df,\n",
    "                 in_steps=24*input_days,\n",
    "                 out_steps=OUT_STEPS,\n",
    "                ):\n",
    "        self.df = df\n",
    "        self.in_steps = in_steps\n",
    "        self.out_steps = out_steps\n",
    "        # fix해야함\n",
    "        #len_tt = df.shape[0]//in_steps*in_steps\n",
    "        #self.n_days = (len_tt - in_steps)//24\n",
    "        self.n_days = df.shape[0]//24\n",
    "        self.pred_days = out_steps//24\n",
    "        self.in_days = in_steps//24\n",
    "        self.gt = self.df.iloc[:self.n_days*24, out_features[0]].to_numpy()\n",
    "        #print('gt.shape =', self.gt.shape)\n",
    "        #print('n_days =', self.n_days)\n",
    "        #print('n_days*24 =', self.n_days*24)\n",
    "        \n",
    "    def predict(self, model):\n",
    "        preds = np.full((self.pred_days, self.n_days*24), np.nan)\n",
    "        #x_batch = np.empty((0, self.in_steps, self.df.shape[1]))\n",
    "        for i in tqdm(range(self.n_days-self.in_days)):\n",
    "            x = self.df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            #x = val_df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            x = x.reshape((-1, self.in_steps, self.df.shape[1]))\n",
    "            y = model.predict(x)\n",
    "            for j in range(self.pred_days):\n",
    "                if i+self.in_days+j >= self.n_days:\n",
    "                    break\n",
    "                preds[j][(i+self.in_days+j)*24:(i+self.in_days+j+1)*24] = y[0, j*24:j*24+24, 0]\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(data):\n",
    "    return data*train_std[out_features[0]]+train_mean[out_features[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = {}\n",
    "preds_val = {}\n",
    "preds_test = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daily avaraged prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_prediction_train = DaysPrediction(df=train_df)\n",
    "days_prediction_val = DaysPrediction(df=val_df)\n",
    "days_prediction_test = DaysPrediction(df=test_df)\n",
    "daily = True\n",
    "\n",
    "gt_train = denormalize(days_prediction_train.gt)\n",
    "gt_val = denormalize(days_prediction_val.gt)\n",
    "gt_test = denormalize(days_prediction_test.gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hourly prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_prediction_train = HoursPrediction(df=train_df)\n",
    "days_prediction_val = HoursPrediction(df=val_df)\n",
    "days_prediction_test = HoursPrediction(df=test_df)\n",
    "daily = False\n",
    "\n",
    "gt_train = denormalize(days_prediction_train.gt)\n",
    "gt_val = denormalize(days_prediction_val.gt)\n",
    "gt_test = denormalize(days_prediction_test.gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daily avaraged Model(warning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_prediction_train = Prediction(df=train_day_df)\n",
    "days_prediction_val = Prediction(df=val_day_df)\n",
    "days_prediction_test = Prediction(df=test_day_df)\n",
    "daily = True\n",
    "\n",
    "gt_train = denormalize(days_prediction_train.gt)\n",
    "gt_val = denormalize(days_prediction_val.gt)\n",
    "gt_test = denormalize(days_prediction_test.gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 18s 93ms/step\n",
      "50/50 [==============================] - 5s 91ms/step\n",
      "25/25 [==============================] - 2s 92ms/step\n"
     ]
    }
   ],
   "source": [
    "#preds_dict = {}\n",
    "#days_prediction = DaysPrediction()\n",
    "preds_train['GRU'] = denormalize(days_prediction_train.predict(multi_gru_model))\n",
    "preds_val['GRU'] = denormalize(days_prediction_val.predict(multi_gru_model))\n",
    "preds_test['GRU'] = denormalize(days_prediction_test.predict(multi_gru_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan, 8.64702736,\n",
       "       9.10766748, 9.31373713, 9.51382185, 9.65105033, 8.93027386,\n",
       "       8.78962696, 8.73516727, 8.87070062, 9.16555979, 8.54188793,\n",
       "       8.46553372, 8.69801248, 8.49381289, 8.45470907, 8.47526382])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_val['GRU'][4][0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 23s 127ms/step\n",
      "48/48 [==============================] - 6s 126ms/step\n",
      "24/24 [==============================] - 3s 128ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_train['LSTM'] = denormalize(days_prediction_train.predict(multi_lstm_model))\n",
    "preds_val['LSTM'] = denormalize(days_prediction_val.predict(multi_lstm_model))\n",
    "preds_test['LSTM'] = denormalize(days_prediction_test.predict(multi_lstm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train['Dense'] = denormalize(days_prediction_train.predict(multi_dense_model))\n",
    "preds_val['Dense'] = denormalize(days_prediction_val.predict(multi_dense_model))\n",
    "preds_test['Dense'] = denormalize(days_prediction_test.predict(multi_dense_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7564b78ceb434f879a285b661857dc85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e79af00c3f94937a0c56cf3ef459856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfafe8a53afb4afc8c4f3b87c2eae813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(gt_train)\n",
    "plt.plot(preds_train['GRU'][pred_day])\n",
    "#plt.plot(preds_train['LSTM'][pred_day])\n",
    "#plt.plot(preds_train['Dense'][pred_day])\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(gt_val)\n",
    "plt.plot(preds_val['GRU'][pred_day])\n",
    "#plt.plot(preds_val['LSTM'][pred_day])\n",
    "#plt.plot(preds_val['Dense'][pred_day])\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(gt_test)\n",
    "plt.plot(preds_test['GRU'][pred_day])\n",
    "#plt.plot(preds_test['LSTM'][pred_day])\n",
    "#plt.plot(preds_test['Dense'][pred_day])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_performance_metric(gt, pred):\n",
    "    mae = np.mean(np.absolute(gt - pred))\n",
    "    mape = np.mean(np.absolute(gt - pred)/gt)\n",
    "    mean = np.mean(gt)\n",
    "    nse = 1.-np.sum((gt - pred)**2) /  np.sum((gt - mean)**2)\n",
    "    return { 'mae': mae, 'nse': nse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_train_performance = {}\n",
    "daily_val_performance = {}\n",
    "daily_test_performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37564839568161756\n",
      "0.30499512990033206\n",
      "0.16201144808644163\n"
     ]
    }
   ],
   "source": [
    "#model_names = ['GRU', 'LSTM', 'Dense']\n",
    "model_names = ['GRU']\n",
    "#model_names = ['GRU', 'LSTM']\n",
    "prefix = 'awsbo_'\n",
    "\n",
    "if daily == True:\n",
    "    pred_slice = slice(days_prediction_val.in_days+pred_day, None)\n",
    "else:\n",
    "    pred_slice = slice((days_prediction_val.in_days+pred_day)*24, None)\n",
    "\n",
    "for model_name in model_names:\n",
    "    daily_train_performance[prefix+model_name] = daily_performance_metric(gt_train[pred_slice], preds_train[model_name][pred_day][pred_slice])\n",
    "    daily_val_performance[prefix+model_name] = daily_performance_metric(gt_val[pred_slice], preds_val[model_name][pred_day][pred_slice])\n",
    "    daily_test_performance[prefix+model_name] = daily_performance_metric(gt_test[pred_slice], preds_test[model_name][pred_day][pred_slice])\n",
    "print(daily_train_performance[prefix+'GRU']['nse'])\n",
    "print(daily_test_performance[prefix+'GRU']['nse'])\n",
    "print(daily_val_performance[prefix+'GRU']['nse'])\n",
    "#print(daily_train_performance[prefix+'LSTM']['nse'])\n",
    "#print(daily_test_performance[prefix+'LSTM']['nse'])\n",
    "#print(daily_val_performance[prefix+'LSTM']['nse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30499512990033206"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_test_performance[prefix+'GRU']['nse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16201144808644163"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_val_performance[prefix+'GRU']['nse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7428a405ea3d438f874c38cfae26c165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eedc430d0d943f6b7bd04d2455c3a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(daily_test_performance))\n",
    "width = 0.3\n",
    "\n",
    "\n",
    "#metric_name = 'mean_absolute_error'\n",
    "#metric_index = multi_linear_model.metrics_names.index('mean_absolute_error')\n",
    "metric_index = 'mae'\n",
    "#metric_index = 'mae'\n",
    "train_mae = [v[metric_index] for v in daily_train_performance.values()]\n",
    "val_mae = [v[metric_index] for v in daily_val_performance.values()]\n",
    "test_mae = [v[metric_index] for v in daily_test_performance.values()]\n",
    "#test_mae = [v[metric_index] for v in multi_performance.values()]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x - 0.32, train_mae, width, label='Train')\n",
    "plt.bar(x, val_mae, width, label='Validation')\n",
    "plt.bar(x + 0.32, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=daily_test_performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel(f'MAE (average over all times and outputs)')\n",
    "_ = plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#metric_name = 'mean_absolute_error'\n",
    "#metric_index = multi_linear_model.metrics_names.index('mean_absolute_error')\n",
    "metric_index = 'nse'\n",
    "#metric_index = 'mae'\n",
    "train_nse = [v[metric_index] for v in daily_train_performance.values()]\n",
    "val_nse = [v[metric_index] for v in daily_val_performance.values()]\n",
    "test_nse = [v[metric_index] for v in daily_test_performance.values()]\n",
    "#test_mae = [v[metric_index] for v in multi_performance.values()]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x - 0.32, train_nse, width, label='Train')\n",
    "plt.bar(x, val_nse, width, label='Validation')\n",
    "plt.bar(x + 0.32, test_nse, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=daily_test_performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel(f'NSE (average over all times and outputs)')\n",
    "_ = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 4\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 4\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['GRU'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['LSTM'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 0\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['GRU'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['LSTM'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 0\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['GRU'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['LSTM'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 0\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae = np.mean(np.absolute(gt[pred_slice] - (denorm_preds['GRU'][pred_day])[pred_slice]))\n",
    "print(mae)\n",
    "mape = np.mean(np.absolute((gt[pred_slice] - denorm_preds['GRU'][pred_day][pred_slice])/gt[pred_slice]))\n",
    "print(mape)\n",
    "mean = np.mean(gt[pred_slice])\n",
    "nse = 1.-np.sum((gt[pred_slice] - denorm_preds['GRU'][pred_day][pred_slice])**2) /  np.sum((gt[pred_slice] - mean)**2)\n",
    "print('nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 0\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae = np.mean(np.absolute(gt[pred_slice] - (denorm_preds['LSTM'][pred_day])[pred_slice]))\n",
    "print(mae)\n",
    "mape = np.mean(np.absolute((gt[pred_slice] - denorm_preds['LSTM'][pred_day][pred_slice])/gt[pred_slice]))\n",
    "print(mape)\n",
    "mean = np.mean(gt[pred_slice])\n",
    "nse = 1.-np.sum((gt[pred_slice] - denorm_preds['LSTM'][pred_day][pred_slice])**2) /  np.sum((gt[pred_slice] - mean)**2)\n",
    "print('nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=24*10\n",
    "#width=3\n",
    "channels=8\n",
    "kernel_size=3\n",
    "in_data = keras.layers.Input(shape=(width, channels))\n",
    "x = in_data\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "#out_data = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "out_data = x\n",
    "\n",
    "model = keras.models.Model(inputs=in_data, outputs=out_data)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8-7+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128*150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 장기예측 (14일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
