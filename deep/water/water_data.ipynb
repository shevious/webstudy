{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dot, Add, ReLU, Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data'\n",
    "file_names = ['가평_2019.xlsx', '의암호_2019.xlsx']\n",
    "\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df_full = []\n",
    "df = []\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    path = os.path.join(folder, file_names[i])\n",
    "\n",
    "    df_full.append(pd.read_excel(path))\n",
    "    df.append(df_full[i].iloc[:, 2:11])\n",
    "    date_time = pd.to_datetime(df_full[i].iloc[:, 0], format='%Y.%m.%d %H:%M')\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    df[i]['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    df[i]['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    df[i]['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    df[i]['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>수온</th>\n",
       "      <th>수소이온농도</th>\n",
       "      <th>전기전도도</th>\n",
       "      <th>용존산소</th>\n",
       "      <th>탁도</th>\n",
       "      <th>총유기탄소</th>\n",
       "      <th>총질소</th>\n",
       "      <th>총인</th>\n",
       "      <th>클로로필-a</th>\n",
       "      <th>Day sin</th>\n",
       "      <th>Day cos</th>\n",
       "      <th>Year sin</th>\n",
       "      <th>Year cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>91.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.469</td>\n",
       "      <td>0.001</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-0.004430</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>91.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.526</td>\n",
       "      <td>0.002</td>\n",
       "      <td>8.1</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-0.003713</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>91.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.623</td>\n",
       "      <td>0.002</td>\n",
       "      <td>8.2</td>\n",
       "      <td>-9.659258e-01</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>-0.002996</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.1</td>\n",
       "      <td>7.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.637</td>\n",
       "      <td>0.003</td>\n",
       "      <td>8.4</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-2.466750e-12</td>\n",
       "      <td>-0.002279</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.1</td>\n",
       "      <td>7.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.563</td>\n",
       "      <td>0.002</td>\n",
       "      <td>8.5</td>\n",
       "      <td>-9.659258e-01</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>-0.001563</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>4.8</td>\n",
       "      <td>7.7</td>\n",
       "      <td>99.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.430</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>-0.012185</td>\n",
       "      <td>0.999926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>4.8</td>\n",
       "      <td>7.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.421</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>-9.659258e-01</td>\n",
       "      <td>-0.011468</td>\n",
       "      <td>0.999934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>4.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.457</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.543654e-12</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.010752</td>\n",
       "      <td>0.999942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>4.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>101.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.447</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>-9.659258e-01</td>\n",
       "      <td>-0.010035</td>\n",
       "      <td>0.999950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>4.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>101.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.449</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>-0.009318</td>\n",
       "      <td>0.999957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       수온  수소이온농도  전기전도도  용존산소   탁도  총유기탄소    총질소     총인  클로로필-a  \\\n",
       "0     3.0     7.2   91.0  12.2  0.7    1.3  1.469  0.001     8.0   \n",
       "1     3.0     7.2   91.0  12.1  0.8    1.3  1.526  0.002     8.1   \n",
       "2     3.0     7.2   91.0  12.1  0.8    1.3  1.623  0.002     8.2   \n",
       "3     3.1     7.2   90.0  12.1  0.8    1.3  1.637  0.003     8.4   \n",
       "4     3.1     7.2   90.0  12.1  0.8    1.3  1.563  0.002     8.5   \n",
       "...   ...     ...    ...   ...  ...    ...    ...    ...     ...   \n",
       "8755  4.8     7.7   99.0  12.3  0.7    1.5  1.430  0.003     NaN   \n",
       "8756  4.8     7.7  100.0  12.4  0.7    1.5  1.421  0.003     NaN   \n",
       "8757  4.7     7.7  100.0  12.4  0.7    1.5  1.457  0.002     NaN   \n",
       "8758  4.7     7.7  101.0  12.4  0.7    1.5  1.447  0.003     NaN   \n",
       "8759  4.6     7.7  101.0  12.4  0.7    1.5  1.449  0.002     NaN   \n",
       "\n",
       "           Day sin       Day cos  Year sin  Year cos  \n",
       "0    -7.071068e-01 -7.071068e-01 -0.004430  0.999990  \n",
       "1    -8.660254e-01 -5.000000e-01 -0.003713  0.999993  \n",
       "2    -9.659258e-01 -2.588190e-01 -0.002996  0.999996  \n",
       "3    -1.000000e+00 -2.466750e-12 -0.002279  0.999997  \n",
       "4    -9.659258e-01  2.588190e-01 -0.001563  0.999999  \n",
       "...            ...           ...       ...       ...  \n",
       "8755  5.000000e-01 -8.660254e-01 -0.012185  0.999926  \n",
       "8756  2.588190e-01 -9.659258e-01 -0.011468  0.999934  \n",
       "8757  2.543654e-12 -1.000000e+00 -0.010752  0.999942  \n",
       "8758 -2.588190e-01 -9.659258e-01 -0.010035  0.999950  \n",
       "8759 -5.000000e-01 -8.660254e-01 -0.009318  0.999957  \n",
       "\n",
       "[8760 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "df_all = pd.concat(df)\n",
    "df_all\n",
    "\n",
    "train_mean = df_all.mean()\n",
    "train_std = df_all.std()\n",
    "for i in range(len(file_names)):\n",
    "    df[i] = (df[i]-train_mean)/train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>수온</th>\n",
       "      <th>수소이온농도</th>\n",
       "      <th>전기전도도</th>\n",
       "      <th>용존산소</th>\n",
       "      <th>탁도</th>\n",
       "      <th>총유기탄소</th>\n",
       "      <th>총질소</th>\n",
       "      <th>총인</th>\n",
       "      <th>클로로필-a</th>\n",
       "      <th>Day sin</th>\n",
       "      <th>Day cos</th>\n",
       "      <th>Year sin</th>\n",
       "      <th>Year cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.496853</td>\n",
       "      <td>-0.945645</td>\n",
       "      <td>-1.065617</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>-0.669890</td>\n",
       "      <td>-0.875013</td>\n",
       "      <td>0.872656</td>\n",
       "      <td>-0.515611</td>\n",
       "      <td>-1.026082</td>\n",
       "      <td>-9.999715e-01</td>\n",
       "      <td>-9.999715e-01</td>\n",
       "      <td>-0.006269</td>\n",
       "      <td>1.415570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.496853</td>\n",
       "      <td>-0.945645</td>\n",
       "      <td>-1.065617</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>-0.591824</td>\n",
       "      <td>-0.875013</td>\n",
       "      <td>1.020773</td>\n",
       "      <td>-0.249286</td>\n",
       "      <td>-0.988738</td>\n",
       "      <td>-1.224710e+00</td>\n",
       "      <td>-7.070866e-01</td>\n",
       "      <td>-0.005255</td>\n",
       "      <td>1.415574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.496853</td>\n",
       "      <td>-0.945645</td>\n",
       "      <td>-1.065617</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>-0.747956</td>\n",
       "      <td>-0.875013</td>\n",
       "      <td>0.850980</td>\n",
       "      <td>-0.781935</td>\n",
       "      <td>-1.026082</td>\n",
       "      <td>-1.365986e+00</td>\n",
       "      <td>-3.660150e-01</td>\n",
       "      <td>-0.004242</td>\n",
       "      <td>1.415577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.510410</td>\n",
       "      <td>-0.945645</td>\n",
       "      <td>-1.065617</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>-0.669890</td>\n",
       "      <td>-0.875013</td>\n",
       "      <td>0.887106</td>\n",
       "      <td>-0.249286</td>\n",
       "      <td>-1.013634</td>\n",
       "      <td>-1.414173e+00</td>\n",
       "      <td>-3.519073e-12</td>\n",
       "      <td>-0.003229</td>\n",
       "      <td>1.415580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.496853</td>\n",
       "      <td>-0.945645</td>\n",
       "      <td>-1.065617</td>\n",
       "      <td>0.947057</td>\n",
       "      <td>-0.669890</td>\n",
       "      <td>-0.875013</td>\n",
       "      <td>0.883494</td>\n",
       "      <td>-0.515611</td>\n",
       "      <td>-1.013634</td>\n",
       "      <td>-1.365986e+00</td>\n",
       "      <td>3.660150e-01</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>1.415582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>-1.388397</td>\n",
       "      <td>-0.289409</td>\n",
       "      <td>0.698099</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>-0.201494</td>\n",
       "      <td>0.704985</td>\n",
       "      <td>0.681187</td>\n",
       "      <td>-1.048260</td>\n",
       "      <td>1.326626</td>\n",
       "      <td>7.070866e-01</td>\n",
       "      <td>-1.224710e+00</td>\n",
       "      <td>-0.017232</td>\n",
       "      <td>1.415479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>-1.401954</td>\n",
       "      <td>-0.289409</td>\n",
       "      <td>0.698099</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>-0.201494</td>\n",
       "      <td>0.309986</td>\n",
       "      <td>0.710088</td>\n",
       "      <td>-1.314584</td>\n",
       "      <td>1.227040</td>\n",
       "      <td>3.660150e-01</td>\n",
       "      <td>-1.365986e+00</td>\n",
       "      <td>-0.016219</td>\n",
       "      <td>1.415491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>-1.415511</td>\n",
       "      <td>-0.289409</td>\n",
       "      <td>0.698099</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>-0.201494</td>\n",
       "      <td>0.309986</td>\n",
       "      <td>0.753440</td>\n",
       "      <td>-1.048260</td>\n",
       "      <td>1.152351</td>\n",
       "      <td>3.576374e-12</td>\n",
       "      <td>-1.414173e+00</td>\n",
       "      <td>-0.015206</td>\n",
       "      <td>1.415502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>-1.415511</td>\n",
       "      <td>-0.289409</td>\n",
       "      <td>0.698099</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>-0.201494</td>\n",
       "      <td>0.309986</td>\n",
       "      <td>0.699250</td>\n",
       "      <td>-1.048260</td>\n",
       "      <td>1.177248</td>\n",
       "      <td>-3.660150e-01</td>\n",
       "      <td>-1.365986e+00</td>\n",
       "      <td>-0.014193</td>\n",
       "      <td>1.415512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>-1.429068</td>\n",
       "      <td>-0.289409</td>\n",
       "      <td>0.698099</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>-0.279560</td>\n",
       "      <td>0.309986</td>\n",
       "      <td>0.634223</td>\n",
       "      <td>-1.048260</td>\n",
       "      <td>1.102559</td>\n",
       "      <td>-7.070866e-01</td>\n",
       "      <td>-1.224710e+00</td>\n",
       "      <td>-0.013179</td>\n",
       "      <td>1.415522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            수온    수소이온농도     전기전도도      용존산소        탁도     총유기탄소       총질소  \\\n",
       "0    -1.496853 -0.945645 -1.065617  0.998777 -0.669890 -0.875013  0.872656   \n",
       "1    -1.496853 -0.945645 -1.065617  0.998777 -0.591824 -0.875013  1.020773   \n",
       "2    -1.496853 -0.945645 -1.065617  0.998777 -0.747956 -0.875013  0.850980   \n",
       "3    -1.510410 -0.945645 -1.065617  0.998777 -0.669890 -0.875013  0.887106   \n",
       "4    -1.496853 -0.945645 -1.065617  0.947057 -0.669890 -0.875013  0.883494   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8755 -1.388397 -0.289409  0.698099  0.998777 -0.201494  0.704985  0.681187   \n",
       "8756 -1.401954 -0.289409  0.698099  0.998777 -0.201494  0.309986  0.710088   \n",
       "8757 -1.415511 -0.289409  0.698099  0.998777 -0.201494  0.309986  0.753440   \n",
       "8758 -1.415511 -0.289409  0.698099  0.998777 -0.201494  0.309986  0.699250   \n",
       "8759 -1.429068 -0.289409  0.698099  0.998777 -0.279560  0.309986  0.634223   \n",
       "\n",
       "            총인    클로로필-a       Day sin       Day cos  Year sin  Year cos  \n",
       "0    -0.515611 -1.026082 -9.999715e-01 -9.999715e-01 -0.006269  1.415570  \n",
       "1    -0.249286 -0.988738 -1.224710e+00 -7.070866e-01 -0.005255  1.415574  \n",
       "2    -0.781935 -1.026082 -1.365986e+00 -3.660150e-01 -0.004242  1.415577  \n",
       "3    -0.249286 -1.013634 -1.414173e+00 -3.519073e-12 -0.003229  1.415580  \n",
       "4    -0.515611 -1.013634 -1.365986e+00  3.660150e-01 -0.002215  1.415582  \n",
       "...        ...       ...           ...           ...       ...       ...  \n",
       "8755 -1.048260  1.326626  7.070866e-01 -1.224710e+00 -0.017232  1.415479  \n",
       "8756 -1.314584  1.227040  3.660150e-01 -1.365986e+00 -0.016219  1.415491  \n",
       "8757 -1.048260  1.152351  3.576374e-12 -1.414173e+00 -0.015206  1.415502  \n",
       "8758 -1.048260  1.177248 -3.660150e-01 -1.365986e+00 -0.014193  1.415512  \n",
       "8759 -1.048260  1.102559 -7.070866e-01 -1.224710e+00 -0.013179  1.415522  \n",
       "\n",
       "[8760 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[0]\n",
    "val_df = df[0]\n",
    "test_df = df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "            #train_df=None, val_df=None, test_df=None,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "fm.get_fontconfig_fonts()\n",
    "font_location = '/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf'\n",
    "#font_location = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'\n",
    "# font_location = 'C:/Windows/Fonts/NanumGothic.ttf' # For Windows\n",
    "fprop = fm.FontProperties(fname=font_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]', fontproperties=fprop)\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used\n",
    "# original make_dataset code\n",
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=True,\n",
    "      batch_size=32,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "#WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 7\n",
       "Input indices: [0 1 2 3 4 5]\n",
       "Label indices: [6]\n",
       "Label column name(s): None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = WindowGenerator(input_width=6, label_width=1, shift=1,\n",
    "                     label_columns=None)\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All shapes are: (batch, time, features)\n",
      "Window shape: (3, 7, 13)\n",
      "Inputs shape: (3, 6, 13)\n",
      "labels shape: (3, 1, 13)\n"
     ]
    }
   ],
   "source": [
    "# Stack three slices, the length of the total window:\n",
    "example_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n",
    "                           np.array(train_df[100:100+w2.total_window_size]),\n",
    "                           np.array(train_df[200:200+w2.total_window_size])])\n",
    "\n",
    "\n",
    "example_inputs, example_labels = w2.split_window(example_window)\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'labels shape: {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.example = example_inputs, example_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fe90962f374993af66dd09595e17f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w2.plot(plot_col='수온')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch_index(total, batch_size):\n",
    "    '''Sample index of the mini-batch.\n",
    "\n",
    "    Args:\n",
    "        - total: total number of samples\n",
    "        - batch_size: batch size\n",
    "\n",
    "    Returns:\n",
    "        - batch_idx: batch index\n",
    "    '''\n",
    "    total_idx = np.random.permutation(total)\n",
    "    batch_idx = total_idx[:batch_size]\n",
    "    return batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_sampler(p, shape):\n",
    "  '''Sample binary random variables.\n",
    "  \n",
    "  Args:\n",
    "    - p: probability of 1\n",
    "    - shape: matrix shape\n",
    "    \n",
    "  Returns:\n",
    "    - binary_random_matrix: generated binary random matrix.\n",
    "  '''\n",
    "  unif_random_matrix = np.random.uniform(0., 1., size = shape)\n",
    "  binary_random_matrix = 1*(unif_random_matrix < p)\n",
    "  return binary_random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_sampler(low, high, shape):\n",
    "  '''Sample uniform random variables.\n",
    "  \n",
    "  Args:\n",
    "    - low: low limit\n",
    "    - high: high limit\n",
    "    - rows: the number of rows\n",
    "    - cols: the number of columns\n",
    "    \n",
    "  Returns:\n",
    "    - uniform_random_matrix: generated uniform random matrix.\n",
    "  '''\n",
    "  return np.random.uniform(low, high, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization (data, parameters=None):\n",
    "  '''Normalize data in [0, 1] range.\n",
    "  \n",
    "  Args:\n",
    "    - data: original data\n",
    "  \n",
    "  Returns:\n",
    "    - norm_data: normalized data\n",
    "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
    "  '''\n",
    "\n",
    "  # Parameters\n",
    "  _, dim = data.shape\n",
    "  norm_data = data.copy()\n",
    "\n",
    "  if parameters is None:\n",
    "\n",
    "    # MixMax normalization\n",
    "    min_val = np.zeros(dim)\n",
    "    max_val = np.zeros(dim)\n",
    "   \n",
    "    # For each dimension\n",
    "    for i in range(dim):\n",
    "      min_val[i] = np.nanmin(norm_data[:,i])\n",
    "      norm_data[:,i] = norm_data[:,i] - np.nanmin(norm_data[:,i])\n",
    "      max_val[i] = np.nanmax(norm_data[:,i])\n",
    "      norm_data[:,i] = norm_data[:,i] / (np.nanmax(norm_data[:,i]) + 1e-6)\n",
    "\n",
    "    # Return norm_parameters for renormalization\n",
    "    norm_parameters = {'min_val': min_val,\n",
    "                       'max_val': max_val}\n",
    "  else:\n",
    "    min_val = parameters['min_val']\n",
    "    max_val = parameters['max_val']\n",
    "\n",
    "    # For each dimension\n",
    "    for i in range(dim):\n",
    "      norm_data[:,i] = norm_data[:,i] - min_val[i]\n",
    "      norm_data[:,i] = norm_data[:,i] / (max_val[i] + 1e-6)\n",
    "\n",
    "    norm_parameters = parameters\n",
    "\n",
    "  return norm_data, norm_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissData(object):\n",
    "    def __init__(self, load_dir=None):\n",
    "        if load_dir:\n",
    "            self.missarr = np.load(os.path.join(load_dir, 'miss.npy'))\n",
    "            self.idxarr = np.load(os.path.join(load_dir, 'idx.npy'))\n",
    "            \n",
    "    def make_missdata(self, data_x, missrate=0.2):\n",
    "        data = data_x.copy()\n",
    "        rows, cols = data_x.shape\n",
    "        total_no = rows*cols\n",
    "        total_miss_no = np.round(total_no*missrate).astype(int)\n",
    "        total_idx = self.idxarr.shape[0]\n",
    "        idxarr = self.idxarr\n",
    "        missarr = self.missarr\n",
    "        #print(total_miss_no)\n",
    "        miss_no = 0\n",
    "        cum_no = self.idxarr[:,3:4]\n",
    "        cum_no = cum_no.reshape((total_idx))\n",
    "        cum_sum = np.max(cum_no)\n",
    "        #print(cum_no)\n",
    "        #print(total_idx)\n",
    "        while True:\n",
    "            loc_count = np.around(np.random.random()*cum_sum)\n",
    "            #print('loc_count =', loc_count)\n",
    "            idx = len(cum_no[cum_no <= loc_count])-1\n",
    "            #print(cum_no[cum_no <= loc_count])\n",
    "            #print('idx =', idx)\n",
    "            startnan = idxarr[idx][0]\n",
    "            nanlen = idxarr[idx][2]\n",
    "            loc = np.around(np.random.random()*(rows-nanlen)).astype(int)\n",
    "            #print('loc =', loc)\n",
    "            #print(loc_count, idx)\n",
    "            #print(idxarr[idx])\n",
    "            #data_copy = data[loc:loc+nanlen].copy()\n",
    "            data_copy = data[loc:loc+nanlen]\n",
    "            #print('startnan=', startnan)\n",
    "            #isnan = missarr[startnan:startnan+nanlen].copy()\n",
    "            isnan = missarr[startnan:startnan+nanlen]\n",
    "            #print('isnan =',isnan)\n",
    "            miss_no += idxarr[idx][1]\n",
    "            if (miss_no > total_miss_no):\n",
    "                break\n",
    "            data_copy[isnan==1] = np.nan\n",
    "            data[loc:loc+nanlen] = data_copy\n",
    "        #print('miss_data =', data)\n",
    "        return data\n",
    "    \n",
    "    def save(data, max_tseq, save_dir='save'):\n",
    "        no, dim = data.shape\n",
    "        #print((no, dim))\n",
    "        isnan = np.isnan(data).astype(int)\n",
    "        isany = np.any(isnan, axis=1).astype(int)\n",
    "        shifted = np.roll(isany, 1)\n",
    "        shifted[0] = 1\n",
    "        #print(isnan)\n",
    "        #print(isany.astype(int))\n",
    "        #print(shifted)\n",
    "        startnan = ((isany == 1) & (shifted ==0)).astype(int)\n",
    "        #print(startnan)\n",
    "        group = startnan.cumsum()\n",
    "        group = group*isany\n",
    "        #print(group)\n",
    "        n = np.max(group)\n",
    "        #print(n)\n",
    "        missarr = None\n",
    "        cum_no = 0\n",
    "        rowidx = 0\n",
    "        for i in range(1, n+1):\n",
    "            g = (group == i).astype(int)\n",
    "            i = np.argmax(g)\n",
    "            rows = g.sum()\n",
    "            #print(len)\n",
    "            #print(i)\n",
    "            #print(type(missarr))\n",
    "            if rows <= max_tseq:\n",
    "                nanseq = isnan[i:i+rows, :]\n",
    "                no = np.sum(nanseq)\n",
    "                #print(no)\n",
    "                if missarr is None:\n",
    "                    missarr = nanseq\n",
    "                    idxarr = np.array([[rowidx, no, rows, cum_no]])\n",
    "                else:\n",
    "                    missarr = np.concatenate((missarr, nanseq))\n",
    "                    idxarr = np.concatenate((idxarr, [[rowidx, no, rows, cum_no]]), axis=0)\n",
    "                cum_no += no\n",
    "                rowidx += rows\n",
    "\n",
    "        #print(idxarr)\n",
    "        miss_npy_file = os.path.join(save_dir, 'miss.npy')\n",
    "        idx_npy_file = os.path.join(save_dir, 'idx.npy')\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        np.save(miss_npy_file, missarr)\n",
    "        np.save(idx_npy_file, idxarr)\n",
    "        print('miss_data file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_data file saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_df = pd.concat(df,axis=0)\n",
    "n_data = norm_df.to_numpy()\n",
    "MissData.save(n_data, max_tseq=10)\n",
    "n_data\n",
    "n_data = n_data[0:100]\n",
    "isnan = np.isnan(n_data).astype(int)\n",
    "isnan[50:100]\n",
    "miss = MissData(load_dir='save')\n",
    "tt = miss.make_missdata(n_data)\n",
    "tt = np.isnan(tt).astype(int)\n",
    "tt[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**miss data 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss_data file saved\n"
     ]
    }
   ],
   "source": [
    "norm_df = pd.concat(df,axis=0)\n",
    "norm_data = norm_df.to_numpy()\n",
    "MissData.save(norm_data, max_tseq = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(np_data, max_gap=3):\n",
    "    #n = np_data.shape[1]\n",
    "    data = pd.DataFrame(np_data)\n",
    "    #data[0][0] = np.nan\n",
    "    #data[0][1] = np.nan\n",
    "    #data[0][2] = np.nan\n",
    "    #data[data.columns[0]][0] = np.nan\n",
    "    #data[data.columns[0]][1] = np.nan\n",
    "    #data[data.columns[0]][2] = np.nan\n",
    "    \n",
    "    # create mask\n",
    "    mask = data.copy()\n",
    "    grp = ((mask.notnull() != mask.shift().notnull()).cumsum())\n",
    "    grp['ones'] = 1\n",
    "    for i in data.columns:\n",
    "        mask[i] = (grp.groupby(i)['ones'].transform('count') < max_gap) | data[i].notnull()\n",
    "    data = data.interpolate(method='polynomial', order=5, limit=max_gap, axis=0).bfill()[mask]\n",
    "    return data\n",
    "    \n",
    "#filled_data = interpolate(norm_data, max_gap=3)\n",
    "#np.arange(0, 5, dtype=int)\n",
    "#['%d'%val for val in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class GainDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for GAIN'\n",
    "    def __init__(self,\n",
    "                 data_list,\n",
    "                 batch_size=32,\n",
    "                 input_width=24*3,\n",
    "                 label_width=24*3,\n",
    "                 shift=0,\n",
    "                 fill_no=4,\n",
    "                 miss_rate=0.2,\n",
    "                 hint_rate=0.9,\n",
    "                 normalize=True,\n",
    "                 miss_pattern=None,\n",
    "                 alpha=100.):\n",
    "        'Initialization'\n",
    "        window_size = input_width\n",
    "        \n",
    "        # interpollation\n",
    "        filled_data = []\n",
    "        for data in data_list:\n",
    "            data = interpolate(data, max_gap=fill_no)\n",
    "            filled_data.append(data)\n",
    "            \n",
    "        data_list = filled_data\n",
    "        \n",
    "        # whole data\n",
    "        self.data = np.concatenate(data_list)\n",
    "\n",
    "        # TO-DO\n",
    "        \n",
    "        # pre calculation for  sequence data\n",
    "        last_cum = 0\n",
    "        cums = []\n",
    "        for data in data_list:\n",
    "            isnan = np.isnan(data)\n",
    "            isany = np.any(isnan, axis=1)\n",
    "            shifted = np.roll(isany, 1)\n",
    "            shifted[0] = True # set to nan\n",
    "            start_seq = ((isany == False) & (shifted == True)).astype(int)\n",
    "            cum = start_seq.cumsum()\n",
    "            cum += last_cum\n",
    "            last_cum = np.max(cum)\n",
    "            cum[isany == 1] = np.nan\n",
    "            cums.append(cum)\n",
    "            \n",
    "        \n",
    "        # normlize for spam\n",
    "        if normalize:\n",
    "            self.data, norm_param = normalization(self.data)\n",
    "        #print(norm_param)\n",
    "        \n",
    "        # Define mask matrix\n",
    "        if miss_pattern is None:\n",
    "            self.data_m = binary_sampler(1-miss_rate, self.data.shape)\n",
    "        else:\n",
    "            #MissData.save(self.data, max_tseq = 12)\n",
    "            self.miss = MissData(load_dir='save')\n",
    "            self.miss_rate = miss_rate\n",
    "            miss_data = self.miss.make_missdata(self.data, self.miss_rate)\n",
    "            self.data_m = 1. - np.isnan(miss_data).astype(float)\n",
    "        \n",
    "        # sequence data\n",
    "        self.ids = np.concatenate(cums)\n",
    "        data_idx = np.empty((0), dtype=int)\n",
    "        for i in range(1, last_cum+1):\n",
    "            seq_len = (self.ids == i).sum()\n",
    "            start_id = np.argmax(self.ids == i)\n",
    "            time_len = seq_len - window_size + 1\n",
    "            start_ids = np.arange(start_id, start_id+time_len)\n",
    "            data_idx = np.append(data_idx, start_ids)\n",
    "            \n",
    "        # start index set for sequence data\n",
    "        self.data_idx = data_idx\n",
    "        self.input_width = input_width\n",
    "        self.no = len(data_idx)\n",
    "        \n",
    "        #print('self.no = ', self.no)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # random shuffling  index\n",
    "        self.batch_idx = sample_batch_index(self.no, self.no)\n",
    "        self.batch_id = 0\n",
    "        self.shape = (batch_size,self.input_width)+self.data.shape[1:]\n",
    "        #self.hint_rate = hint_rate\n",
    "            \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        #return int(128/self.batch_size)\n",
    "        #return 2\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        #print('index =', index)\n",
    "        # Sample batch\n",
    "        x = np.empty((0, self.input_width, self.data.shape[1]))\n",
    "        #m = np.empty((0, self.input_width, self.data.shape[1]))\n",
    "        #h = np.empty((0, self.input_width, self.data.shape[1]))\n",
    "        y = np.empty((0, self.input_width, self.data.shape[1]))\n",
    "        #print(x.shape)\n",
    "        #print(self.data.shape)\n",
    "        #print(self.input_width)\n",
    "        #self.batch_idx = sample_batch_index(self.no, self.batch_size)\n",
    "        for cnt in range(0, self.batch_size):\n",
    "            i = self.batch_idx[self.batch_id]\n",
    "            self.batch_id += 1\n",
    "            #self.batch_id %= self.batch_size\n",
    "            self.batch_id %= self.no\n",
    "            if (self.batch_id == 0):\n",
    "                self.batch_idx = sample_batch_index(self.no, self.no)\n",
    "                #miss_data = self.miss.make_missdata(self.data, self.miss_rate)\n",
    "                #self.data_m = 1. - np.isnan(miss_data).astype(float)\n",
    "            idx1 = self.data_idx[i]\n",
    "            idx2 = self.data_idx[i]+self.input_width\n",
    "            #print(idx1, idx2)\n",
    "        \n",
    "            Y_mb = self.data[idx1:idx2]\n",
    "            X_mb = Y_mb.copy()\n",
    "            M_mb = self.data_m[idx1:idx2]\n",
    "            Z_mb = uniform_sampler(0, 0.01, shape=X_mb.shape)\n",
    "            X_mb = M_mb*X_mb + (1-M_mb)*Z_mb\n",
    "            #H_mb_temp = binary_sampler(self.hint_rate, shape=X_mb.shape)\n",
    "            #H_mb = M_mb * H_mb_temp\n",
    "            X_mb[M_mb == 0] = np.nan\n",
    "            x = np.append(x, [X_mb], axis=0)\n",
    "            #m = np.append(m, [M_mb], axis=0)\n",
    "            #h = np.append(h, [H_mb], axis=0)\n",
    "            y = np.append(y, [Y_mb], axis=0)\n",
    "            \n",
    "        #return [x, m, h], y\n",
    "        return x, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        return\n",
    "\n",
    "dgen = GainDataGenerator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 72, 13)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAIN(keras.Model):\n",
    "    def __init__(self, shape, alpha=100., load=False, hint_rate=0.9, gen_sigmoid=True, **kwargs):\n",
    "        super(GAIN, self).__init__(**kwargs)\n",
    "        self.shape = shape\n",
    "        self.dim = np.prod(shape).astype(int)\n",
    "        self.h_dim = self.dim\n",
    "        self.gen_sigmoid = gen_sigmoid\n",
    "        self.build_generator()\n",
    "        self.build_discriminator()\n",
    "        self.hint_rate = hint_rate\n",
    "        self.alpha = alpha\n",
    "        self.generator_optimizer = Adam()\n",
    "        self.discriminator_optimizer = Adam()\n",
    "\n",
    "    ## GAIN models\n",
    "    def build_generator(self):\n",
    "        last_activation = 'sigmoid' if self.gen_sigmoid else None\n",
    "        xavier_initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        x = Input(shape=(self.dim,), name='generator_input_x')\n",
    "        m = Input(shape=(self.dim,), name='generator_input_m')\n",
    "\n",
    "        a = Concatenate()([x, m])\n",
    "\n",
    "        a = Dense(self.h_dim, activation='relu', kernel_initializer=xavier_initializer)(a)\n",
    "        #a = keras.layers.BatchNormalization()(a)\n",
    "        a = Dense(self.h_dim, activation='relu', kernel_initializer=xavier_initializer)(a)\n",
    "        #a = keras.layers.BatchNormalization()(a)\n",
    "        G_prob = Dense(self.dim, activation=last_activation, kernel_initializer=xavier_initializer)(a)\n",
    "        self.generator = keras.models.Model([x, m], G_prob, name='generator')\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        xavier_initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "        x = Input(shape=(self.dim,), name='discriminator_input_x')\n",
    "        h = Input(shape=(self.dim,), name='discriminator_input_h')\n",
    "\n",
    "        a = Concatenate()([x, h])\n",
    "\n",
    "        a = Dense(self.h_dim, activation='relu', kernel_initializer=xavier_initializer)(a)\n",
    "        a = Dense(self.h_dim, activation='relu', kernel_initializer=xavier_initializer)(a)\n",
    "        D_prob = Dense(self.dim, activation='sigmoid', kernel_initializer=xavier_initializer)(a)\n",
    "        self.discriminator = keras.models.Model([x, h], D_prob, name='discriminator')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if isinstance(inputs, tuple):\n",
    "            inputs = inputs[0]\n",
    "        shape = inputs.shape\n",
    "        dims = np.prod(shape[1:])\n",
    "        input_width = shape[1]\n",
    "        #print('inputs.shape=',inputs.shape)\n",
    "        x = inputs\n",
    "        #x = x.reshape((n, -1))\n",
    "        #print('dims=',dims)\n",
    "        x = keras.layers.Reshape((dims,))(x)\n",
    "        #x = keras.layers.Reshape(tf.TensorShape((self.dim,)))(x)\n",
    "        #print('x =', x)\n",
    "        #print('x.shape = ', x.shape)\n",
    "        #x = keras.layers.Reshape(tf.TensorShape([57]))(x)\n",
    "        \n",
    "        isnan = tf.math.is_nan(x)\n",
    "        #m = 1.- keras.backend.cast(isnan, dtype=tf.float32)\n",
    "        m = tf.where(isnan, 0., 1.)\n",
    "        z = keras.backend.random_uniform(shape=tf.shape(x), minval=0.0, maxval=0.01)\n",
    "        x = tf.where(isnan, z, x)\n",
    "        #z = uniform_sampler(0, 0.01, shape=x.shape)\n",
    "        #z = tf.keras.backend.random_uniform(shape=x.shape, minval=0.0, maxval=0.01)\n",
    "        imputed_data = self.generator([x, m], training=False)\n",
    "        #imputed_data = m*x + (1-m)*imputed_data\n",
    "        imputed_data = tf.where(isnan, imputed_data, np.nan)\n",
    "        imputed_data = keras.layers.Reshape(shape[1:])(imputed_data)\n",
    "        #print('imputed_data.shape = ', imputed_data.shape)\n",
    "        \n",
    "        return imputed_data\n",
    "    \n",
    "    def D_loss(M, D_prob):\n",
    "        ## GAIN loss\n",
    "        return -tf.reduce_mean(M * tf.keras.backend.log(D_prob + 1e-8) \\\n",
    "                         + (1-M) * tf.keras.backend.log(1. - D_prob + 1e-8))\n",
    "    \n",
    "    def G_loss(self, M, D_prob, X, G_sample):\n",
    "        G_loss_temp = -tf.reduce_mean((1-M) * tf.keras.backend.log(D_prob + 1e-8))\n",
    "        MSE_loss = tf.reduce_mean((M * X - M * G_sample)**2) / (tf.reduce_mean(M) + 1e-8)\n",
    "        #G_loss_temp = GAIN.G_loss_bincross(M, D_prob)\n",
    "        #MSE_loss = GAIN.MSE_loss(M, X, G_sample)\n",
    "        G_loss = G_loss_temp + self.alpha * MSE_loss\n",
    "        return G_loss\n",
    "        \n",
    "    def RMSE_loss(y_true, y_pred):\n",
    "        isnan = tf.math.is_nan(y_pred)\n",
    "        M = tf.where(isnan, 1., 0.)\n",
    "        return tf.sqrt(tf.reduce_sum(tf.where(isnan, 0., y_pred-y_true)**2)/tf.reduce_sum(1-M))\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        #[x, m, h], y = data\n",
    "        x, y = data\n",
    "        #X = keras.layers.Reshape((self.dim,), input_shape=self.shape)(x)\n",
    "        #Y = keras.layers.Reshape((self.dim,), input_shape=self.shape)(y)\n",
    "        X = keras.layers.Flatten()(x)\n",
    "        Y = keras.layers.Flatten()(y)\n",
    "        #X = tf.reshape(x, shape=(x.shape[0], -1))\n",
    "        #Y = tf.reshape(y, shape=(x.shape[0], -1))\n",
    "        isnan = tf.math.is_nan(X)\n",
    "        #M = 1 - keras.backend.cast(isnan, dtype=tf.float32)\n",
    "        M = tf.where(isnan, 0., 1.)\n",
    "        Z = keras.backend.random_uniform(shape=tf.shape(X), minval=0.0, maxval=0.01)\n",
    "        #H_temp = binary_sampler(self.hint_rate, shape=X.shape)\n",
    "        H_rand = keras.backend.random_uniform(shape=tf.shape(X), minval=0.0, maxval=1.)\n",
    "        #H_temp = 1*keras.backend.cast((H_rand < self.hint_rate), dtype=tf.float32)\n",
    "        H_temp = tf.where(H_rand < self.hint_rate, 1., 0.)\n",
    "        \n",
    "        H = M * H_temp\n",
    "        #X = M * X + (1-M) * Z\n",
    "        X = tf.where(isnan, Z, X)\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            G_sample = self.generator([X, M], training=True)\n",
    "\n",
    "            # Combine with observed data\n",
    "            #Hat_X = tf.where(isnan, G_sample, X)\n",
    "            Hat_X = X * M + G_sample * (1-M)\n",
    "            D_prob = self.discriminator([Hat_X, H], training=True)\n",
    "            gen_loss = self.G_loss(M, D_prob, X, G_sample)\n",
    "            disc_loss = tf.keras.backend.mean(tf.keras.losses.binary_crossentropy(M, D_prob))\n",
    "            #disc_loss = GAIN.D_loss(M, D_prob)\n",
    "            #disc_loss = GAIN.D_loss(M, D_prob)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "        \n",
    "        rmse = tf.sqrt(tf.reduce_sum(tf.where(isnan, G_sample - Y, 0.)**2)/tf.reduce_sum(1-M))\n",
    "        return {\n",
    "                 'gen_loss':     gen_loss,\n",
    "                 'disc_loss':    disc_loss,\n",
    "                 'rmse':         rmse,\n",
    "               }\n",
    "    \n",
    "    def save(self, save_dir='savedta'):\n",
    "        if not os.path.exists(save_dir):\n",
    "          os.makedirs(save_dir)\n",
    "        disc_savefile = os.path.join(save_dir, 'discriminator.h5')\n",
    "        gen_savefile = os.path.join(save_dir, 'generator.h5')\n",
    "        self.discriminator.save_weights(disc_savefile)\n",
    "        self.generator.save_weights(gen_savefile)\n",
    "\n",
    "    def load(self, save_dir='savedata'):\n",
    "        disc_savefile = os.path.join(save_dir, 'discriminator.h5')\n",
    "        gen_savefile = os.path.join(save_dir, 'generator.h5')\n",
    "        try:\n",
    "          self.discriminator.load_weights(disc_savefile)\n",
    "          self.generator.load_weights(gen_savefile)\n",
    "          print('model weights loaded')\n",
    "        except:\n",
    "          print('model loadinng error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spam data gain 학습 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 57)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((128, 1, 57), (128, 1, 57))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam = pd.read_csv('data/spam.csv')\n",
    "dg_spam = GainDataGenerator([df_spam], batch_size=128, input_width=1, label_width=1)\n",
    "it = iter(dg_spam)\n",
    "x,y = next(it)\n",
    "print(dg_spam.shape)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAIN(shape=dg_spam.shape[1:])\n",
    "model.compile(loss=GAIN.RMSE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 426ms/step - gen_loss: 23.9984 - disc_loss: 0.7359 - rmse: 0.4920\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 26ms/step - gen_loss: 23.4976 - disc_loss: 0.7257 - rmse: 0.4858\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 25ms/step - gen_loss: 22.8372 - disc_loss: 0.7121 - rmse: 0.4836\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - gen_loss: 22.3302 - disc_loss: 0.7024 - rmse: 0.4768\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 24ms/step - gen_loss: 21.9141 - disc_loss: 0.6939 - rmse: 0.4669\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 30ms/step - gen_loss: 21.3730 - disc_loss: 0.6860 - rmse: 0.4657\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 31ms/step - gen_loss: 20.6089 - disc_loss: 0.6735 - rmse: 0.4591\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - gen_loss: 19.9901 - disc_loss: 0.6682 - rmse: 0.4510\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 25ms/step - gen_loss: 19.2561 - disc_loss: 0.6587 - rmse: 0.4419\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 27ms/step - gen_loss: 18.6030 - disc_loss: 0.6519 - rmse: 0.4319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f770683ebd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dg_spam, batch_size=128, epochs=10)\n",
    "#model.fit(x, y, batch_size=128)\n",
    "#model.fit(dg_spam, batch_size=4601, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 1, 57)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dg_spam.data.copy()\n",
    "y = dg_spam.data\n",
    "m = dg_spam.data_m\n",
    "x[m == 0] = np.nan\n",
    "x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "y = y.reshape(y.shape[0], 1, y.shape[1])\n",
    "x.shape\n",
    "\n",
    "\n",
    "#model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spam data rmse 측정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 1, 57)\n",
      "144/144 [==============================] - 0s 2ms/step - loss: 0.4239\n",
      "0.4257941246032715\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "ret = model.evaluate(x, y)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse = 0.42585734478225756\n"
     ]
    }
   ],
   "source": [
    "x_input = x[0:4601]\n",
    "y_true = y[0:4601]\n",
    "y_pred = model.predict(x_input)\n",
    "#print(x_input)\n",
    "#print(y_true)\n",
    "#print(y_pred)\n",
    "isnan = np.isnan(y_pred)\n",
    "diff = y_pred - y_true\n",
    "diff[isnan] = 0.\n",
    "#print(diff)\n",
    "m = isnan.astype(int)\n",
    "n = np.sum(1-m)\n",
    "rmse = np.sqrt(np.sum(diff**2)/float(n))\n",
    "print('rmse =', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gain_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator (Functional)       (None, 57)                13167     \n",
      "_________________________________________________________________\n",
      "discriminator (Functional)   (None, 57)                13167     \n",
      "=================================================================\n",
      "Total params: 26,334\n",
      "Trainable params: 26,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spam data dataset으로 학습하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "  lambda: dg_spam,\n",
    "  output_types=(tf.float32, tf.float32),\n",
    "  output_shapes=(\n",
    "    dg_spam.shape,\n",
    "    dg_spam.shape\n",
    "    #[batch_size, train_generator.dim],\n",
    "    #[batch_size, train_generator.dim],\n",
    "  )\n",
    ").repeat(-1).prefetch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 1, 57]), TensorShape([128, 1, 57]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(ds)\n",
    "x,y = next(it)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 0s 15ms/step - gen_loss: 13.5773 - disc_loss: 0.6204 - rmse: 0.3697\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 15ms/step - gen_loss: 5.5987 - disc_loss: 0.5648 - rmse: 0.2394\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 1.5007 - disc_loss: 0.5311 - rmse: 0.1207\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4939 - disc_loss: 0.5059 - rmse: 0.0649\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4284 - disc_loss: 0.4949 - rmse: 0.0623\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.3959 - disc_loss: 0.4921 - rmse: 0.0567\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.3914 - disc_loss: 0.4892 - rmse: 0.0568\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.3887 - disc_loss: 0.4827 - rmse: 0.0566\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4174 - disc_loss: 0.4844 - rmse: 0.0587\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.3779 - disc_loss: 0.4779 - rmse: 0.0561\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4528 - disc_loss: 0.4744 - rmse: 0.0612\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4019 - disc_loss: 0.4695 - rmse: 0.0562\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.3994 - disc_loss: 0.4654 - rmse: 0.0592\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4001 - disc_loss: 0.4592 - rmse: 0.0611\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4037 - disc_loss: 0.4580 - rmse: 0.0558\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4129 - disc_loss: 0.4528 - rmse: 0.0553\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4013 - disc_loss: 0.4445 - rmse: 0.0557\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4430 - disc_loss: 0.4376 - rmse: 0.0628\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.3869 - disc_loss: 0.4352 - rmse: 0.0552\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 13ms/step - gen_loss: 0.4286 - disc_loss: 0.4287 - rmse: 0.0555\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4234 - disc_loss: 0.4232 - rmse: 0.0578\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4068 - disc_loss: 0.4146 - rmse: 0.0589\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4209 - disc_loss: 0.4108 - rmse: 0.0643\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4462 - disc_loss: 0.4054 - rmse: 0.0549\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 13ms/step - gen_loss: 0.4140 - disc_loss: 0.4011 - rmse: 0.0563\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4130 - disc_loss: 0.3957 - rmse: 0.0551\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4151 - disc_loss: 0.3877 - rmse: 0.0574\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4021 - disc_loss: 0.3860 - rmse: 0.0521\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4211 - disc_loss: 0.3813 - rmse: 0.0599\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4385 - disc_loss: 0.3782 - rmse: 0.0559\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4613 - disc_loss: 0.3719 - rmse: 0.0579\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4095 - disc_loss: 0.3681 - rmse: 0.0538\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4445 - disc_loss: 0.3656 - rmse: 0.0554\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4202 - disc_loss: 0.3612 - rmse: 0.0564\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 13ms/step - gen_loss: 0.4360 - disc_loss: 0.3588 - rmse: 0.0537\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4427 - disc_loss: 0.3500 - rmse: 0.0579\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4663 - disc_loss: 0.3496 - rmse: 0.0570\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4547 - disc_loss: 0.3443 - rmse: 0.0560\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4737 - disc_loss: 0.3423 - rmse: 0.0538\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4255 - disc_loss: 0.3410 - rmse: 0.0573\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4477 - disc_loss: 0.3358 - rmse: 0.0595\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4520 - disc_loss: 0.3321 - rmse: 0.0539\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4795 - disc_loss: 0.3296 - rmse: 0.0590\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4335 - disc_loss: 0.3280 - rmse: 0.0512\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4686 - disc_loss: 0.3261 - rmse: 0.0590\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4549 - disc_loss: 0.3230 - rmse: 0.0548\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4663 - disc_loss: 0.3186 - rmse: 0.0539\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4576 - disc_loss: 0.3199 - rmse: 0.0522\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4496 - disc_loss: 0.3153 - rmse: 0.0566\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 14ms/step - gen_loss: 0.4822 - disc_loss: 0.3133 - rmse: 0.0573\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds, steps_per_epoch=10, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습성능 측정(rsme)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05563908442854881"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습 그래프**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e057be747cde4b03a092a8afffa15ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(history.history['gen_loss'], label='gen_loss')\n",
    "ax.plot(history.history['disc_loss'], label='disc_loss')\n",
    "ax2.plot(history.history['rmse'], label='rmse', color='green')\n",
    "#ax2.plot(history.history['val_loss'], label='val_loss', color='red')\n",
    "#plt.legend(history.history.keys(), loc='upper right')\n",
    "#ax.legend(loc='upper center')\n",
    "ax.legend(loc='upper center')\n",
    "ax2.legend(loc='upper right')\n",
    "ax.set_xlabel(\"epochs\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax2.set_ylabel(\"rmse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수질 GAIN 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_gain(self, data):\n",
    "  dg = GainDataGenerator(\n",
    "      df,\n",
    "      input_width = self.input_width,\n",
    "      label_width = self.label_width,\n",
    "      batch_size = 128,\n",
    "      normalize = False,\n",
    "      miss_pattern = True,\n",
    "      miss_rate = 0.2,\n",
    "      fill_no = 2,\n",
    "  )\n",
    "  self.dg = dg\n",
    "  ds = tf.data.Dataset.from_generator(\n",
    "      lambda: dg,\n",
    "      output_types=(tf.float32, tf.float32),\n",
    "      output_shapes=(\n",
    "        dg.shape,\n",
    "        dg.shape\n",
    "        #[batch_size, train_generator.dim],\n",
    "        #[batch_size, train_generator.dim],\n",
    "      )\n",
    "  )\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_all\n",
    "val_df = df_all\n",
    "test_df = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 72\n",
       "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
       " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71]\n",
       "Label indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
       " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71]\n",
       "Label column name(s): None"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_window = WindowGenerator(\n",
    "    input_width=24*3, label_width=24*3, shift=0,\n",
    "    #label_columns=['T (degC)']\n",
    ")\n",
    "\n",
    "wide_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2(self, model=None, plot_col='T (degC)', max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(9, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]')\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "\n",
    "#WindowGenerator.plot = plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>수온</th>\n",
       "      <th>수소이온농도</th>\n",
       "      <th>전기전도도</th>\n",
       "      <th>용존산소</th>\n",
       "      <th>탁도</th>\n",
       "      <th>총유기탄소</th>\n",
       "      <th>총질소</th>\n",
       "      <th>총인</th>\n",
       "      <th>클로로필-a</th>\n",
       "      <th>Day sin</th>\n",
       "      <th>Day cos</th>\n",
       "      <th>Year sin</th>\n",
       "      <th>Year cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.469739</td>\n",
       "      <td>-0.070664</td>\n",
       "      <td>-1.653523</td>\n",
       "      <td>0.791895</td>\n",
       "      <td>-0.513758</td>\n",
       "      <td>-1.270012</td>\n",
       "      <td>-0.218354</td>\n",
       "      <td>-1.314584</td>\n",
       "      <td>-0.503258</td>\n",
       "      <td>-9.999715e-01</td>\n",
       "      <td>-9.999715e-01</td>\n",
       "      <td>-0.006269</td>\n",
       "      <td>1.415570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.469739</td>\n",
       "      <td>-0.070664</td>\n",
       "      <td>-1.653523</td>\n",
       "      <td>0.740174</td>\n",
       "      <td>-0.435692</td>\n",
       "      <td>-1.270012</td>\n",
       "      <td>-0.012435</td>\n",
       "      <td>-1.048260</td>\n",
       "      <td>-0.490810</td>\n",
       "      <td>-1.224710e+00</td>\n",
       "      <td>-7.070866e-01</td>\n",
       "      <td>-0.005255</td>\n",
       "      <td>1.415574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.469739</td>\n",
       "      <td>-0.070664</td>\n",
       "      <td>-1.653523</td>\n",
       "      <td>0.740174</td>\n",
       "      <td>-0.435692</td>\n",
       "      <td>-1.270012</td>\n",
       "      <td>0.337989</td>\n",
       "      <td>-1.048260</td>\n",
       "      <td>-0.478362</td>\n",
       "      <td>-1.365986e+00</td>\n",
       "      <td>-3.660150e-01</td>\n",
       "      <td>-0.004242</td>\n",
       "      <td>1.415577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.456182</td>\n",
       "      <td>-0.070664</td>\n",
       "      <td>-1.751507</td>\n",
       "      <td>0.740174</td>\n",
       "      <td>-0.435692</td>\n",
       "      <td>-1.270012</td>\n",
       "      <td>0.388566</td>\n",
       "      <td>-0.781935</td>\n",
       "      <td>-0.453465</td>\n",
       "      <td>-1.414173e+00</td>\n",
       "      <td>-3.519073e-12</td>\n",
       "      <td>-0.003229</td>\n",
       "      <td>1.415580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.456182</td>\n",
       "      <td>-0.070664</td>\n",
       "      <td>-1.751507</td>\n",
       "      <td>0.740174</td>\n",
       "      <td>-0.435692</td>\n",
       "      <td>-1.270012</td>\n",
       "      <td>0.121232</td>\n",
       "      <td>-1.048260</td>\n",
       "      <td>-0.441017</td>\n",
       "      <td>-1.365986e+00</td>\n",
       "      <td>3.660150e-01</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>1.415582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>-1.225713</td>\n",
       "      <td>1.023062</td>\n",
       "      <td>-0.869649</td>\n",
       "      <td>0.843615</td>\n",
       "      <td>-0.513758</td>\n",
       "      <td>-0.480013</td>\n",
       "      <td>-0.359246</td>\n",
       "      <td>-0.781935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.070866e-01</td>\n",
       "      <td>-1.224710e+00</td>\n",
       "      <td>-0.017232</td>\n",
       "      <td>1.415479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>-1.225713</td>\n",
       "      <td>1.023062</td>\n",
       "      <td>-0.771665</td>\n",
       "      <td>0.895336</td>\n",
       "      <td>-0.513758</td>\n",
       "      <td>-0.480013</td>\n",
       "      <td>-0.391759</td>\n",
       "      <td>-0.781935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.660150e-01</td>\n",
       "      <td>-1.365986e+00</td>\n",
       "      <td>-0.016219</td>\n",
       "      <td>1.415491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>-1.239270</td>\n",
       "      <td>1.023062</td>\n",
       "      <td>-0.771665</td>\n",
       "      <td>0.895336</td>\n",
       "      <td>-0.513758</td>\n",
       "      <td>-0.480013</td>\n",
       "      <td>-0.261705</td>\n",
       "      <td>-1.048260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.576374e-12</td>\n",
       "      <td>-1.414173e+00</td>\n",
       "      <td>-0.015206</td>\n",
       "      <td>1.415502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>-1.239270</td>\n",
       "      <td>1.023062</td>\n",
       "      <td>-0.673680</td>\n",
       "      <td>0.895336</td>\n",
       "      <td>-0.513758</td>\n",
       "      <td>-0.480013</td>\n",
       "      <td>-0.297831</td>\n",
       "      <td>-0.781935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.660150e-01</td>\n",
       "      <td>-1.365986e+00</td>\n",
       "      <td>-0.014193</td>\n",
       "      <td>1.415512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>-1.252827</td>\n",
       "      <td>1.023062</td>\n",
       "      <td>-0.673680</td>\n",
       "      <td>0.895336</td>\n",
       "      <td>-0.513758</td>\n",
       "      <td>-0.480013</td>\n",
       "      <td>-0.290606</td>\n",
       "      <td>-1.048260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.070866e-01</td>\n",
       "      <td>-1.224710e+00</td>\n",
       "      <td>-0.013179</td>\n",
       "      <td>1.415522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            수온    수소이온농도     전기전도도      용존산소        탁도     총유기탄소       총질소  \\\n",
       "0    -1.469739 -0.070664 -1.653523  0.791895 -0.513758 -1.270012 -0.218354   \n",
       "1    -1.469739 -0.070664 -1.653523  0.740174 -0.435692 -1.270012 -0.012435   \n",
       "2    -1.469739 -0.070664 -1.653523  0.740174 -0.435692 -1.270012  0.337989   \n",
       "3    -1.456182 -0.070664 -1.751507  0.740174 -0.435692 -1.270012  0.388566   \n",
       "4    -1.456182 -0.070664 -1.751507  0.740174 -0.435692 -1.270012  0.121232   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8755 -1.225713  1.023062 -0.869649  0.843615 -0.513758 -0.480013 -0.359246   \n",
       "8756 -1.225713  1.023062 -0.771665  0.895336 -0.513758 -0.480013 -0.391759   \n",
       "8757 -1.239270  1.023062 -0.771665  0.895336 -0.513758 -0.480013 -0.261705   \n",
       "8758 -1.239270  1.023062 -0.673680  0.895336 -0.513758 -0.480013 -0.297831   \n",
       "8759 -1.252827  1.023062 -0.673680  0.895336 -0.513758 -0.480013 -0.290606   \n",
       "\n",
       "            총인    클로로필-a       Day sin       Day cos  Year sin  Year cos  \n",
       "0    -1.314584 -0.503258 -9.999715e-01 -9.999715e-01 -0.006269  1.415570  \n",
       "1    -1.048260 -0.490810 -1.224710e+00 -7.070866e-01 -0.005255  1.415574  \n",
       "2    -1.048260 -0.478362 -1.365986e+00 -3.660150e-01 -0.004242  1.415577  \n",
       "3    -0.781935 -0.453465 -1.414173e+00 -3.519073e-12 -0.003229  1.415580  \n",
       "4    -1.048260 -0.441017 -1.365986e+00  3.660150e-01 -0.002215  1.415582  \n",
       "...        ...       ...           ...           ...       ...       ...  \n",
       "8755 -0.781935       NaN  7.070866e-01 -1.224710e+00 -0.017232  1.415479  \n",
       "8756 -0.781935       NaN  3.660150e-01 -1.365986e+00 -0.016219  1.415491  \n",
       "8757 -1.048260       NaN  3.576374e-12 -1.414173e+00 -0.015206  1.415502  \n",
       "8758 -0.781935       NaN -3.660150e-01 -1.365986e+00 -0.014193  1.415512  \n",
       "8759 -1.048260       NaN -7.070866e-01 -1.224710e+00 -0.013179  1.415522  \n",
       "\n",
       "[8760 rows x 13 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92f6b1cdff24d0393acc1907e2d46c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wide_window.plot(plot_col='총질소')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e7ad73580b462d92b4305798bd9e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,10))\n",
    "isnan = np.isnan(norm_data).astype(int)\n",
    "data = isnan\n",
    "n = data.shape[0]\n",
    "seq_len = n//8\n",
    "for i in range(8):\n",
    "    plt.subplot(181+i)\n",
    "    plt.imshow(data[i*seq_len:(i+1)*seq_len, 0:7], aspect='auto')\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a620c21c86a74c659273a476ed0bb7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,10))\n",
    "n = wide_window.dg.data_m.shape[0]\n",
    "train = n//8\n",
    "for i in range(8):\n",
    "    plt.subplot(181+i)\n",
    "    plt.imshow(wide_window.dg.data_m[i*train:(i+1)*train, 0:7], aspect='auto')\n",
    "    plt.yticks([])\n",
    "#plt.imshow(wide_window.dg.data[0:100])\n",
    "#plt.imshow(wide_window.dg.data_m[800:900], aspect='auto')\n",
    "#print(wide_window.dg.data[0:50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 컴파일 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance = {}\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = GAIN(shape=wide_window.dg.shape[1:], gen_sigmoid=False)\n",
    "gain.compile(loss=GAIN.RMSE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 300\n",
    "\n",
    "def compile_and_fit(model, window, patience=10):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  #model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                #optimizer=tf.optimizers.Adam(),\n",
    "                #metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "  model.compile(loss=GAIN.RMSE_loss)\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 1s 636ms/step - gen_loss: 121.5713 - disc_loss: 0.7209 - rmse: 1.0753 - val_loss: 1.0491\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 90ms/step - gen_loss: 95.6069 - disc_loss: 0.5924 - rmse: 1.0371 - val_loss: 0.9607\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 113ms/step - gen_loss: 90.6982 - disc_loss: 0.4909 - rmse: 1.0027 - val_loss: 0.8746\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 126ms/step - gen_loss: 76.4847 - disc_loss: 0.5005 - rmse: 0.9370 - val_loss: 0.8436\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 96ms/step - gen_loss: 68.7315 - disc_loss: 0.4501 - rmse: 0.9120 - val_loss: 0.7885\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 113ms/step - gen_loss: 56.7884 - disc_loss: 0.4094 - rmse: 0.8564 - val_loss: 0.7358\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 117ms/step - gen_loss: 54.8340 - disc_loss: 0.4070 - rmse: 0.8403 - val_loss: 0.7147\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 118ms/step - gen_loss: 40.6020 - disc_loss: 0.4153 - rmse: 0.6945 - val_loss: 0.6542\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 115ms/step - gen_loss: 37.3864 - disc_loss: 0.4104 - rmse: 0.6770 - val_loss: 0.6868\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 131ms/step - gen_loss: 37.9178 - disc_loss: 0.3908 - rmse: 0.7526 - val_loss: 0.6598\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 112ms/step - gen_loss: 29.5409 - disc_loss: 0.3971 - rmse: 0.6167 - val_loss: 0.6584\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 141ms/step - gen_loss: 28.2068 - disc_loss: 0.4071 - rmse: 0.6180 - val_loss: 0.5954\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 122ms/step - gen_loss: 27.0812 - disc_loss: 0.3834 - rmse: 0.6025 - val_loss: 0.5479\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 133ms/step - gen_loss: 24.1044 - disc_loss: 0.3779 - rmse: 0.6303 - val_loss: 0.5865\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 20.2192 - disc_loss: 0.3919 - rmse: 0.5140 - val_loss: 0.5621\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 127ms/step - gen_loss: 21.0772 - disc_loss: 0.3863 - rmse: 0.6031 - val_loss: 0.5030\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 114ms/step - gen_loss: 18.8091 - disc_loss: 0.3914 - rmse: 0.5361 - val_loss: 0.5302\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 103ms/step - gen_loss: 17.6202 - disc_loss: 0.3741 - rmse: 0.5333 - val_loss: 0.4994\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 102ms/step - gen_loss: 16.6282 - disc_loss: 0.3814 - rmse: 0.5689 - val_loss: 0.5172\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 116ms/step - gen_loss: 14.8009 - disc_loss: 0.3768 - rmse: 0.5016 - val_loss: 0.5041\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 111ms/step - gen_loss: 15.0887 - disc_loss: 0.3764 - rmse: 0.5818 - val_loss: 0.5193\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 139ms/step - gen_loss: 14.4299 - disc_loss: 0.3828 - rmse: 0.5754 - val_loss: 0.4953\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 106ms/step - gen_loss: 12.6448 - disc_loss: 0.3783 - rmse: 0.4341 - val_loss: 0.4063\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 11.3952 - disc_loss: 0.3691 - rmse: 0.4796 - val_loss: 0.4816\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 105ms/step - gen_loss: 10.0529 - disc_loss: 0.3837 - rmse: 0.4011 - val_loss: 0.4361\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 100ms/step - gen_loss: 11.1047 - disc_loss: 0.3740 - rmse: 0.4985 - val_loss: 0.4299\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 111ms/step - gen_loss: 10.8600 - disc_loss: 0.3800 - rmse: 0.4666 - val_loss: 0.4678\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 9.9150 - disc_loss: 0.3773 - rmse: 0.4254 - val_loss: 0.4223\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 139ms/step - gen_loss: 9.9604 - disc_loss: 0.3739 - rmse: 0.4692 - val_loss: 0.4339\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 133ms/step - gen_loss: 10.4791 - disc_loss: 0.3597 - rmse: 0.4852 - val_loss: 0.4442\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 148ms/step - gen_loss: 10.1504 - disc_loss: 0.3568 - rmse: 0.5558 - val_loss: 0.4264\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 136ms/step - gen_loss: 10.2474 - disc_loss: 0.3658 - rmse: 0.4865 - val_loss: 0.3983\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 112ms/step - gen_loss: 7.9135 - disc_loss: 0.3701 - rmse: 0.3708 - val_loss: 0.4409\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 91ms/step - gen_loss: 8.7021 - disc_loss: 0.3605 - rmse: 0.5109 - val_loss: 0.3223\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 120ms/step - gen_loss: 8.0574 - disc_loss: 0.3582 - rmse: 0.4780 - val_loss: 0.4059\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 92ms/step - gen_loss: 7.8122 - disc_loss: 0.3359 - rmse: 0.4464 - val_loss: 0.4245\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 7.4908 - disc_loss: 0.3481 - rmse: 0.3821 - val_loss: 0.3921\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 126ms/step - gen_loss: 7.3546 - disc_loss: 0.3523 - rmse: 0.4500 - val_loss: 0.4299\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 120ms/step - gen_loss: 8.1415 - disc_loss: 0.3475 - rmse: 0.4769 - val_loss: 0.3636\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 115ms/step - gen_loss: 6.8449 - disc_loss: 0.3494 - rmse: 0.3805 - val_loss: 0.3854\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 100ms/step - gen_loss: 6.9134 - disc_loss: 0.3492 - rmse: 0.4201 - val_loss: 0.4641\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 89ms/step - gen_loss: 6.0104 - disc_loss: 0.3278 - rmse: 0.3399 - val_loss: 0.4350\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 109ms/step - gen_loss: 7.1094 - disc_loss: 0.3249 - rmse: 0.3630 - val_loss: 0.3663\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 101ms/step - gen_loss: 7.4610 - disc_loss: 0.3265 - rmse: 0.5190 - val_loss: 0.3717\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 128ms/step - gen_loss: 6.5377 - disc_loss: 0.3371 - rmse: 0.3730 - val_loss: 0.3347\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 141ms/step - gen_loss: 6.4557 - disc_loss: 0.3252 - rmse: 0.3671 - val_loss: 0.2748\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 111ms/step - gen_loss: 6.4103 - disc_loss: 0.3153 - rmse: 0.4209 - val_loss: 0.4102\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 137ms/step - gen_loss: 5.6649 - disc_loss: 0.3251 - rmse: 0.3480 - val_loss: 0.3917\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 143ms/step - gen_loss: 6.0235 - disc_loss: 0.3138 - rmse: 0.3626 - val_loss: 0.3512\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 93ms/step - gen_loss: 5.7905 - disc_loss: 0.3069 - rmse: 0.4403 - val_loss: 0.4042\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 73ms/step - gen_loss: 5.9664 - disc_loss: 0.2959 - rmse: 0.4370 - val_loss: 0.3821\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 109ms/step - gen_loss: 5.3341 - disc_loss: 0.2980 - rmse: 0.3324 - val_loss: 0.3302\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 133ms/step - gen_loss: 6.4605 - disc_loss: 0.2859 - rmse: 0.4525 - val_loss: 0.3121\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 100ms/step - gen_loss: 5.9302 - disc_loss: 0.2811 - rmse: 0.3969 - val_loss: 0.3126\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 117ms/step - gen_loss: 6.5175 - disc_loss: 0.2911 - rmse: 0.4844 - val_loss: 0.3581\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 101ms/step - gen_loss: 5.4103 - disc_loss: 0.2757 - rmse: 0.3525 - val_loss: 0.3523\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 75ms/step - gen_loss: 5.0696 - disc_loss: 0.2854 - rmse: 0.3694 - val_loss: 0.3241\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 117ms/step - gen_loss: 5.6548 - disc_loss: 0.2883 - rmse: 0.4097 - val_loss: 0.3972\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 134ms/step - gen_loss: 5.1258 - disc_loss: 0.2773 - rmse: 0.4050 - val_loss: 0.4182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 133ms/step - gen_loss: 5.4125 - disc_loss: 0.2735 - rmse: 0.4391 - val_loss: 0.3470\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 101ms/step - gen_loss: 4.5460 - disc_loss: 0.2818 - rmse: 0.3040 - val_loss: 0.4180\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 116ms/step - gen_loss: 4.1093 - disc_loss: 0.2691 - rmse: 0.3233 - val_loss: 0.3222\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 120ms/step - gen_loss: 4.9185 - disc_loss: 0.2685 - rmse: 0.3591 - val_loss: 0.3394\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 109ms/step - gen_loss: 4.3909 - disc_loss: 0.2622 - rmse: 0.2953 - val_loss: 0.3813\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 105ms/step - gen_loss: 4.9722 - disc_loss: 0.2654 - rmse: 0.3989 - val_loss: 0.3180\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 118ms/step - gen_loss: 5.2180 - disc_loss: 0.2478 - rmse: 0.4289 - val_loss: 0.3412\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 90ms/step - gen_loss: 4.5609 - disc_loss: 0.2523 - rmse: 0.3767 - val_loss: 0.3536\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 119ms/step - gen_loss: 4.7878 - disc_loss: 0.2498 - rmse: 0.3848 - val_loss: 0.2460\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 86ms/step - gen_loss: 5.0235 - disc_loss: 0.2464 - rmse: 0.4587 - val_loss: 0.3139\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 119ms/step - gen_loss: 4.1612 - disc_loss: 0.2563 - rmse: 0.2946 - val_loss: 0.3655\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 85ms/step - gen_loss: 3.9636 - disc_loss: 0.2517 - rmse: 0.2814 - val_loss: 0.3970\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 127ms/step - gen_loss: 4.1900 - disc_loss: 0.2470 - rmse: 0.3965 - val_loss: 0.3557\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 139ms/step - gen_loss: 3.9118 - disc_loss: 0.2409 - rmse: 0.2462 - val_loss: 0.2935\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 149ms/step - gen_loss: 3.8942 - disc_loss: 0.2359 - rmse: 0.2962 - val_loss: 0.3176\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 124ms/step - gen_loss: 4.1158 - disc_loss: 0.2362 - rmse: 0.2877 - val_loss: 0.3758\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 134ms/step - gen_loss: 4.5976 - disc_loss: 0.2277 - rmse: 0.3924 - val_loss: 0.3561\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 126ms/step - gen_loss: 4.1500 - disc_loss: 0.2268 - rmse: 0.3516 - val_loss: 0.3285\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 98ms/step - gen_loss: 4.5907 - disc_loss: 0.2292 - rmse: 0.4324 - val_loss: 0.3457\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 108ms/step - gen_loss: 3.7401 - disc_loss: 0.2321 - rmse: 0.2749 - val_loss: 0.3175\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 118ms/step - gen_loss: 4.4438 - disc_loss: 0.2300 - rmse: 0.3303 - val_loss: 0.3368\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 4.1131 - disc_loss: 0.2178 - rmse: 0.4091 - val_loss: 0.3561\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 130ms/step - gen_loss: 4.0931 - disc_loss: 0.2159 - rmse: 0.4047 - val_loss: 0.3074\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 135ms/step - gen_loss: 3.9234 - disc_loss: 0.2116 - rmse: 0.3929 - val_loss: 0.3564\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 122ms/step - gen_loss: 4.1408 - disc_loss: 0.2242 - rmse: 0.3677 - val_loss: 0.2907\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 147ms/step - gen_loss: 3.4083 - disc_loss: 0.2169 - rmse: 0.2682 - val_loss: 0.3386\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 151ms/step - gen_loss: 3.7978 - disc_loss: 0.2141 - rmse: 0.3268 - val_loss: 0.3614\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 158ms/step - gen_loss: 3.7923 - disc_loss: 0.2146 - rmse: 0.2834 - val_loss: 0.2738\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 113ms/step - gen_loss: 3.8831 - disc_loss: 0.2082 - rmse: 0.3398 - val_loss: 0.2903\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 95ms/step - gen_loss: 4.1137 - disc_loss: 0.2134 - rmse: 0.4101 - val_loss: 0.3503\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 147ms/step - gen_loss: 3.6002 - disc_loss: 0.2116 - rmse: 0.3238 - val_loss: 0.3208\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 109ms/step - gen_loss: 3.6630 - disc_loss: 0.2123 - rmse: 0.3456 - val_loss: 0.3400\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 137ms/step - gen_loss: 3.7044 - disc_loss: 0.2051 - rmse: 0.3321 - val_loss: 0.4017\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 144ms/step - gen_loss: 3.9801 - disc_loss: 0.2124 - rmse: 0.3115 - val_loss: 0.3027\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 132ms/step - gen_loss: 3.5804 - disc_loss: 0.2020 - rmse: 0.3034 - val_loss: 0.3173\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 120ms/step - gen_loss: 3.9605 - disc_loss: 0.2067 - rmse: 0.3623 - val_loss: 0.3337\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 96ms/step - gen_loss: 3.9208 - disc_loss: 0.2000 - rmse: 0.3815 - val_loss: 0.3061\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 99ms/step - gen_loss: 3.4505 - disc_loss: 0.2030 - rmse: 0.2922 - val_loss: 0.3228\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 135ms/step - gen_loss: 3.6325 - disc_loss: 0.1884 - rmse: 0.3437 - val_loss: 0.2964\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 109ms/step - gen_loss: 3.6717 - disc_loss: 0.1952 - rmse: 0.3730 - val_loss: 0.3480\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 120ms/step - gen_loss: 3.1900 - disc_loss: 0.2004 - rmse: 0.2958 - val_loss: 0.3119\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 106ms/step - gen_loss: 2.9968 - disc_loss: 0.1937 - rmse: 0.2225 - val_loss: 0.2323\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 124ms/step - gen_loss: 3.6016 - disc_loss: 0.1973 - rmse: 0.3697 - val_loss: 0.3168\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 132ms/step - gen_loss: 3.8200 - disc_loss: 0.1942 - rmse: 0.4032 - val_loss: 0.4453\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 124ms/step - gen_loss: 3.2915 - disc_loss: 0.1860 - rmse: 0.3649 - val_loss: 0.2900\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 125ms/step - gen_loss: 3.7074 - disc_loss: 0.1898 - rmse: 0.3381 - val_loss: 0.2817\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 139ms/step - gen_loss: 3.5993 - disc_loss: 0.1941 - rmse: 0.3248 - val_loss: 0.3524\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 3.4706 - disc_loss: 0.1898 - rmse: 0.3952 - val_loss: 0.3069\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 101ms/step - gen_loss: 3.1184 - disc_loss: 0.1922 - rmse: 0.2852 - val_loss: 0.3603\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 132ms/step - gen_loss: 3.5047 - disc_loss: 0.1889 - rmse: 0.2981 - val_loss: 0.3409\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 137ms/step - gen_loss: 3.1440 - disc_loss: 0.1827 - rmse: 0.2866 - val_loss: 0.2689\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 133ms/step - gen_loss: 3.2456 - disc_loss: 0.1825 - rmse: 0.2263 - val_loss: 0.2693\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 125ms/step - gen_loss: 3.5057 - disc_loss: 0.1799 - rmse: 0.3900 - val_loss: 0.2690\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 127ms/step - gen_loss: 3.2183 - disc_loss: 0.1883 - rmse: 0.3129 - val_loss: 0.3245\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 3.4215 - disc_loss: 0.1837 - rmse: 0.3040 - val_loss: 0.3114\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 101ms/step - gen_loss: 3.1239 - disc_loss: 0.1704 - rmse: 0.2934 - val_loss: 0.2821\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 84ms/step - gen_loss: 3.3848 - disc_loss: 0.1806 - rmse: 0.3731 - val_loss: 0.3092\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 111ms/step - gen_loss: 3.0973 - disc_loss: 0.1875 - rmse: 0.3135 - val_loss: 0.3443\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 98ms/step - gen_loss: 3.2592 - disc_loss: 0.1815 - rmse: 0.4006 - val_loss: 0.3388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 123ms/step - gen_loss: 3.3446 - disc_loss: 0.1744 - rmse: 0.3954 - val_loss: 0.3238\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 133ms/step - gen_loss: 3.1078 - disc_loss: 0.1695 - rmse: 0.3348 - val_loss: 0.3083\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 112ms/step - gen_loss: 3.1116 - disc_loss: 0.1774 - rmse: 0.2886 - val_loss: 0.3078\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 120ms/step - gen_loss: 3.0683 - disc_loss: 0.1741 - rmse: 0.3199 - val_loss: 0.3387\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 100ms/step - gen_loss: 2.9129 - disc_loss: 0.1776 - rmse: 0.2531 - val_loss: 0.2657\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 82ms/step - gen_loss: 3.0203 - disc_loss: 0.1710 - rmse: 0.2628 - val_loss: 0.3192\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 106ms/step - gen_loss: 3.0731 - disc_loss: 0.1793 - rmse: 0.3349 - val_loss: 0.3030\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 101ms/step - gen_loss: 2.6881 - disc_loss: 0.1649 - rmse: 0.2769 - val_loss: 0.3225\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 102ms/step - gen_loss: 3.5110 - disc_loss: 0.1779 - rmse: 0.3664 - val_loss: 0.2317\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 128ms/step - gen_loss: 2.8631 - disc_loss: 0.1787 - rmse: 0.2760 - val_loss: 0.3241\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 111ms/step - gen_loss: 2.9189 - disc_loss: 0.1703 - rmse: 0.3106 - val_loss: 0.3043\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 85ms/step - gen_loss: 2.9438 - disc_loss: 0.1677 - rmse: 0.3128 - val_loss: 0.3301\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 129ms/step - gen_loss: 2.8248 - disc_loss: 0.1655 - rmse: 0.3290 - val_loss: 0.2688\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 87ms/step - gen_loss: 3.0368 - disc_loss: 0.1711 - rmse: 0.3544 - val_loss: 0.2970\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 100ms/step - gen_loss: 2.9530 - disc_loss: 0.1663 - rmse: 0.2616 - val_loss: 0.2460\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 100ms/step - gen_loss: 2.7686 - disc_loss: 0.1628 - rmse: 0.3908 - val_loss: 0.3735\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 72ms/step - gen_loss: 2.8681 - disc_loss: 0.1630 - rmse: 0.3335 - val_loss: 0.2614\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 100ms/step - gen_loss: 2.7214 - disc_loss: 0.1645 - rmse: 0.3051 - val_loss: 0.2918\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 116ms/step - gen_loss: 3.2065 - disc_loss: 0.1649 - rmse: 0.3873 - val_loss: 0.3336\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 108ms/step - gen_loss: 3.0927 - disc_loss: 0.1624 - rmse: 0.3638 - val_loss: 0.3334\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 131ms/step - gen_loss: 2.8701 - disc_loss: 0.1544 - rmse: 0.3284 - val_loss: 0.2896\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 2.9231 - disc_loss: 0.1644 - rmse: 0.2545 - val_loss: 0.3497\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 117ms/step - gen_loss: 2.9394 - disc_loss: 0.1715 - rmse: 0.3121 - val_loss: 0.3486\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 110ms/step - gen_loss: 2.8734 - disc_loss: 0.1697 - rmse: 0.2491 - val_loss: 0.2891\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 113ms/step - gen_loss: 2.9385 - disc_loss: 0.1702 - rmse: 0.3156 - val_loss: 0.2831\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 110ms/step - gen_loss: 2.6953 - disc_loss: 0.1630 - rmse: 0.2715 - val_loss: 0.2327\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 108ms/step - gen_loss: 2.6405 - disc_loss: 0.1561 - rmse: 0.2120 - val_loss: 0.2950\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 135ms/step - gen_loss: 2.8556 - disc_loss: 0.1675 - rmse: 0.3267 - val_loss: 0.3367\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 126ms/step - gen_loss: 2.9055 - disc_loss: 0.1607 - rmse: 0.3661 - val_loss: 0.3780\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 126ms/step - gen_loss: 2.4962 - disc_loss: 0.1573 - rmse: 0.2703 - val_loss: 0.2932\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 119ms/step - gen_loss: 2.9228 - disc_loss: 0.1602 - rmse: 0.3543 - val_loss: 0.2603\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 140ms/step - gen_loss: 2.5528 - disc_loss: 0.1603 - rmse: 0.2618 - val_loss: 0.3076\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 127ms/step - gen_loss: 2.7501 - disc_loss: 0.1639 - rmse: 0.2979 - val_loss: 0.2769\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 117ms/step - gen_loss: 2.9113 - disc_loss: 0.1501 - rmse: 0.3463 - val_loss: 0.2977\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 112ms/step - gen_loss: 2.6972 - disc_loss: 0.1542 - rmse: 0.2840 - val_loss: 0.3012\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 121ms/step - gen_loss: 2.7322 - disc_loss: 0.1595 - rmse: 0.3026 - val_loss: 0.3038\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 115ms/step - gen_loss: 2.7196 - disc_loss: 0.1538 - rmse: 0.3022 - val_loss: 0.2952\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 123ms/step - gen_loss: 2.6102 - disc_loss: 0.1608 - rmse: 0.3253 - val_loss: 0.2254\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 116ms/step - gen_loss: 2.8191 - disc_loss: 0.1603 - rmse: 0.3030 - val_loss: 0.2578\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 125ms/step - gen_loss: 2.8389 - disc_loss: 0.1557 - rmse: 0.3005 - val_loss: 0.2461\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 116ms/step - gen_loss: 2.8334 - disc_loss: 0.1508 - rmse: 0.3129 - val_loss: 0.3562\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 158ms/step - gen_loss: 2.5660 - disc_loss: 0.1580 - rmse: 0.3018 - val_loss: 0.3388\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 125ms/step - gen_loss: 2.7756 - disc_loss: 0.1533 - rmse: 0.3190 - val_loss: 0.3066\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 126ms/step - gen_loss: 2.6113 - disc_loss: 0.1589 - rmse: 0.2641 - val_loss: 0.3233\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 99ms/step - gen_loss: 2.5623 - disc_loss: 0.1585 - rmse: 0.2457 - val_loss: 0.2906\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 120ms/step - gen_loss: 2.7196 - disc_loss: 0.1487 - rmse: 0.3737 - val_loss: 0.2599\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 145ms/step - gen_loss: 2.5126 - disc_loss: 0.1516 - rmse: 0.2409 - val_loss: 0.2474\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 133ms/step - gen_loss: 2.5435 - disc_loss: 0.1470 - rmse: 0.3061 - val_loss: 0.2611\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 106ms/step - gen_loss: 2.5075 - disc_loss: 0.1505 - rmse: 0.2997 - val_loss: 0.3546\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 134ms/step - gen_loss: 2.5801 - disc_loss: 0.1519 - rmse: 0.3486 - val_loss: 0.2883\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 123ms/step - gen_loss: 2.6702 - disc_loss: 0.1503 - rmse: 0.3528 - val_loss: 0.3295\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 113ms/step - gen_loss: 2.5939 - disc_loss: 0.1482 - rmse: 0.3170 - val_loss: 0.3030\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 99ms/step - gen_loss: 2.7174 - disc_loss: 0.1477 - rmse: 0.2427 - val_loss: 0.2612\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 102ms/step - gen_loss: 2.5735 - disc_loss: 0.1536 - rmse: 0.3697 - val_loss: 0.3100\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 121ms/step - gen_loss: 2.3957 - disc_loss: 0.1508 - rmse: 0.2627 - val_loss: 0.3105\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 110ms/step - gen_loss: 2.6667 - disc_loss: 0.1553 - rmse: 0.3071 - val_loss: 0.2979\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 85ms/step - gen_loss: 2.5205 - disc_loss: 0.1512 - rmse: 0.2402 - val_loss: 0.2998\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 114ms/step - gen_loss: 2.4747 - disc_loss: 0.1513 - rmse: 0.3129 - val_loss: 0.3584\n",
      "Epoch 177/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 103ms/step - gen_loss: 2.5495 - disc_loss: 0.1476 - rmse: 0.2707 - val_loss: 0.2938\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 122ms/step - gen_loss: 2.9000 - disc_loss: 0.1562 - rmse: 0.3675 - val_loss: 0.2640\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 122ms/step - gen_loss: 2.3821 - disc_loss: 0.1490 - rmse: 0.3206 - val_loss: 0.2986\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 97ms/step - gen_loss: 2.4293 - disc_loss: 0.1404 - rmse: 0.2974 - val_loss: 0.3060\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 118ms/step - gen_loss: 2.4026 - disc_loss: 0.1439 - rmse: 0.3131 - val_loss: 0.3053\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 121ms/step - gen_loss: 2.3838 - disc_loss: 0.1424 - rmse: 0.2337 - val_loss: 0.2764\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 97ms/step - gen_loss: 2.5839 - disc_loss: 0.1510 - rmse: 0.2995 - val_loss: 0.3239\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 108ms/step - gen_loss: 2.4714 - disc_loss: 0.1431 - rmse: 0.2100 - val_loss: 0.2569\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 100ms/step - gen_loss: 2.5407 - disc_loss: 0.1498 - rmse: 0.2748 - val_loss: 0.2544\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 100ms/step - gen_loss: 2.8130 - disc_loss: 0.1420 - rmse: 0.4148 - val_loss: 0.2719\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 127ms/step - gen_loss: 2.6572 - disc_loss: 0.1416 - rmse: 0.3590 - val_loss: 0.2964\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 141ms/step - gen_loss: 2.7201 - disc_loss: 0.1411 - rmse: 0.4105 - val_loss: 0.2764\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 119ms/step - gen_loss: 2.3541 - disc_loss: 0.1472 - rmse: 0.2260 - val_loss: 0.2820\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 124ms/step - gen_loss: 2.1741 - disc_loss: 0.1443 - rmse: 0.1903 - val_loss: 0.3555\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 130ms/step - gen_loss: 2.5719 - disc_loss: 0.1441 - rmse: 0.3255 - val_loss: 0.3064\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 140ms/step - gen_loss: 2.5064 - disc_loss: 0.1438 - rmse: 0.3143 - val_loss: 0.2460\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 118ms/step - gen_loss: 2.5682 - disc_loss: 0.1406 - rmse: 0.2753 - val_loss: 0.3271\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 108ms/step - gen_loss: 2.5458 - disc_loss: 0.1466 - rmse: 0.2780 - val_loss: 0.3218\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 119ms/step - gen_loss: 2.5820 - disc_loss: 0.1403 - rmse: 0.3266 - val_loss: 0.3269\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 2.2900 - disc_loss: 0.1435 - rmse: 0.2532 - val_loss: 0.2554\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 126ms/step - gen_loss: 2.4225 - disc_loss: 0.1376 - rmse: 0.3639 - val_loss: 0.3312\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 118ms/step - gen_loss: 2.4927 - disc_loss: 0.1442 - rmse: 0.2560 - val_loss: 0.3072\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 137ms/step - gen_loss: 2.3527 - disc_loss: 0.1442 - rmse: 0.2847 - val_loss: 0.2418\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 115ms/step - gen_loss: 2.7304 - disc_loss: 0.1329 - rmse: 0.3783 - val_loss: 0.3194\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 106ms/step - gen_loss: 2.6609 - disc_loss: 0.1331 - rmse: 0.2838 - val_loss: 0.2970\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 77ms/step - gen_loss: 2.4128 - disc_loss: 0.1413 - rmse: 0.2638 - val_loss: 0.2860\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 106ms/step - gen_loss: 2.4021 - disc_loss: 0.1375 - rmse: 0.3179 - val_loss: 0.2964\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 96ms/step - gen_loss: 2.3235 - disc_loss: 0.1388 - rmse: 0.2831 - val_loss: 0.3102\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 118ms/step - gen_loss: 2.3408 - disc_loss: 0.1389 - rmse: 0.2750 - val_loss: 0.2976\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 102ms/step - gen_loss: 2.4964 - disc_loss: 0.1441 - rmse: 0.3714 - val_loss: 0.3000\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 124ms/step - gen_loss: 2.3752 - disc_loss: 0.1400 - rmse: 0.2604 - val_loss: 0.2546\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 121ms/step - gen_loss: 2.4080 - disc_loss: 0.1375 - rmse: 0.2433 - val_loss: 0.2937\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 134ms/step - gen_loss: 2.2610 - disc_loss: 0.1379 - rmse: 0.3090 - val_loss: 0.2790\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 134ms/step - gen_loss: 2.2705 - disc_loss: 0.1296 - rmse: 0.2992 - val_loss: 0.2681\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 129ms/step - gen_loss: 2.3171 - disc_loss: 0.1384 - rmse: 0.2632 - val_loss: 0.2928\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 124ms/step - gen_loss: 2.3902 - disc_loss: 0.1357 - rmse: 0.2603 - val_loss: 0.2720\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 99ms/step - gen_loss: 2.2733 - disc_loss: 0.1335 - rmse: 0.2702 - val_loss: 0.3448\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 119ms/step - gen_loss: 2.4146 - disc_loss: 0.1328 - rmse: 0.3423 - val_loss: 0.3187\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 126ms/step - gen_loss: 2.3276 - disc_loss: 0.1354 - rmse: 0.3198 - val_loss: 0.3007\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 138ms/step - gen_loss: 2.4095 - disc_loss: 0.1356 - rmse: 0.2897 - val_loss: 0.2870\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2379\n"
     ]
    }
   ],
   "source": [
    "history = compile_and_fit(gain, wide_window, patience=MAX_EPOCHS//5)\n",
    "\n",
    "\n",
    "val_performance['Gain'] = gain.evaluate(wide_window.val)\n",
    "performance['Gain'] = gain.evaluate(wide_window.test, verbose=0)\n",
    "\n",
    "\n",
    "#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#                                                    patience=2,\n",
    "#                                                    mode='min')\n",
    "#gain.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습 loss history 출력**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24b2feccf9a43f1820f61760dfa6d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(history.history['gen_loss'], label='gen_loss')\n",
    "ax.plot(history.history['disc_loss'], label='disc_loss')\n",
    "ax2.plot(history.history['rmse'], label='rmse', color='green')\n",
    "ax2.plot(history.history['val_loss'], label='val_loss', color='red')\n",
    "#plt.legend(history.history.keys(), loc='upper right')\n",
    "#ax.legend(loc='upper center')\n",
    "ax.legend(loc='upper center')\n",
    "ax2.legend(loc='upper right')\n",
    "ax.set_xlabel(\"epochs\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax2.set_ylabel(\"rmse\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 22ms/step - loss: 0.2198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21983663737773895"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain.evaluate(wide_window.test.repeat(), steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플 prediction 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84856cf82b44a02b86e14aaa0ffd9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wide_window.plot(gain, plot_col='클로로필-a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17520\n",
      "(72, 13)\n",
      "936\n",
      "16848\n",
      "x.shape = (16848, 13)\n",
      "x.shape = (234, 72, 13)\n"
     ]
    }
   ],
   "source": [
    "total_n = wide_window.dg.data.shape[0]\n",
    "print(total_n)\n",
    "unit_shape = wide_window.dg.shape[1:]\n",
    "print(unit_shape)\n",
    "dim = np.prod(wide_window.dg.shape[1:]).astype(int)\n",
    "print(dim)\n",
    "n = (total_n//dim)*dim\n",
    "print(n)\n",
    "x = wide_window.dg.data[0:n].copy()\n",
    "y = wide_window.dg.data[0:n].copy()\n",
    "m = wide_window.dg.data_m[0:n]\n",
    "x[m == 0] = np.nan\n",
    "print('x.shape =', x.shape)\n",
    "x = x.reshape((-1,)+unit_shape)\n",
    "y_true = y.reshape((-1,)+unit_shape)\n",
    "print('x.shape =', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gain.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape((n, 13))\n",
    "x = x.reshape((n, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16848, 13)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8bdfd0bffc45ff938ba26c25fe0f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(x[:, 8])\n",
    "plt.plot(y_pred[:, 8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본 데이터 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.concat(df,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17520\n",
      "(72, 13)\n",
      "936\n",
      "x.shape = (16848, 13)\n",
      "x_reshape.shape = (234, 72, 13)\n"
     ]
    }
   ],
   "source": [
    "data = norm_df.to_numpy()\n",
    "x = data[0:n].copy()\n",
    "y_true = data[0:n].copy()\n",
    "isnan = np.isnan(x)\n",
    "x[isnan] = np.nan\n",
    "\n",
    "total_n = wide_window.dg.data.shape[0]\n",
    "print(total_n)\n",
    "unit_shape = wide_window.dg.shape[1:]\n",
    "print(unit_shape)\n",
    "dim = np.prod(wide_window.dg.shape[1:]).astype(int)\n",
    "print(dim)\n",
    "n = (total_n//dim)*dim\n",
    "\n",
    "print('x.shape =', x.shape)\n",
    "x_reshape = x.reshape((-1,)+unit_shape)\n",
    "print('x_reshape.shape =', x_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gain.predict(x_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16848, 13)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_pred.reshape(y_true.shape)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c89f6bec1fd4998b252c7fe0fa405bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 8\n",
    "plt.figure(figsize=(9,20))\n",
    "for i in range(n):\n",
    "    #plt.subplot('%d1%d'%(n,i))\n",
    "    plt.subplot(811+i)\n",
    "    plt.plot(x[:, i])\n",
    "    plt.plot(y_pred[:, i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습 섹션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(wide_window.val)\n",
    "x,y = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 72, 13]), TensorShape([128, 72, 13]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 0s 125ms/step - gen_loss: 4.0399 - disc_loss: 0.1777 - rmse: 0.2527 - val_loss: 0.2436\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 121ms/step - gen_loss: 5.3992 - disc_loss: 0.1813 - rmse: 0.2878 - val_loss: 0.2451\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 115ms/step - gen_loss: 4.3048 - disc_loss: 0.1678 - rmse: 0.2535 - val_loss: 0.2482\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 119ms/step - gen_loss: 4.2210 - disc_loss: 0.1749 - rmse: 0.2513 - val_loss: 0.2404\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 104ms/step - gen_loss: 4.8031 - disc_loss: 0.1710 - rmse: 0.2734 - val_loss: 0.2435\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 103ms/step - gen_loss: 4.4161 - disc_loss: 0.1702 - rmse: 0.2548 - val_loss: 0.2301\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 115ms/step - gen_loss: 4.1049 - disc_loss: 0.1782 - rmse: 0.2475 - val_loss: 0.2213\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 85ms/step - gen_loss: 4.6900 - disc_loss: 0.1662 - rmse: 0.2618 - val_loss: 0.2282\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 110ms/step - gen_loss: 3.7765 - disc_loss: 0.1689 - rmse: 0.2408 - val_loss: 0.2359\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 87ms/step - gen_loss: 3.8911 - disc_loss: 0.1689 - rmse: 0.2339 - val_loss: 0.2294\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 99ms/step - gen_loss: 4.0691 - disc_loss: 0.1690 - rmse: 0.2460 - val_loss: 0.2295\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 108ms/step - gen_loss: 3.3060 - disc_loss: 0.1656 - rmse: 0.2277 - val_loss: 0.2378\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 126ms/step - gen_loss: 3.9541 - disc_loss: 0.1624 - rmse: 0.2405 - val_loss: 0.2141\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 117ms/step - gen_loss: 4.2166 - disc_loss: 0.1634 - rmse: 0.2553 - val_loss: 0.2378\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 113ms/step - gen_loss: 3.6849 - disc_loss: 0.1698 - rmse: 0.2367 - val_loss: 0.2078\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 107ms/step - gen_loss: 4.1318 - disc_loss: 0.1712 - rmse: 0.2549 - val_loss: 0.2199\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 102ms/step - gen_loss: 4.2363 - disc_loss: 0.1575 - rmse: 0.2601 - val_loss: 0.2267\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 127ms/step - gen_loss: 3.9436 - disc_loss: 0.1551 - rmse: 0.2509 - val_loss: 0.2297\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 109ms/step - gen_loss: 4.3237 - disc_loss: 0.1668 - rmse: 0.2527 - val_loss: 0.2293\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 119ms/step - gen_loss: 3.7475 - disc_loss: 0.1548 - rmse: 0.2356 - val_loss: 0.2254\n"
     ]
    }
   ],
   "source": [
    "history = gain.fit(wide_window.train, epochs=20,\n",
    "                      validation_data=wide_window.val,\n",
    "                      callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 72, 13]), TensorShape([128, 72, 13]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(wide_window.val)\n",
    "x,y = next(it)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan, -0.70629   , ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan, -0.5684247 , ...,         nan,\n",
       "                 nan,         nan]],\n",
       "\n",
       "       [[-0.47959065, -1.3098589 ,  0.50830597, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [-0.59616673, -1.345786  ,  0.6714608 , ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [-0.46581385, -1.5985745 ,  0.6721713 , ...,         nan,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan]],\n",
       "\n",
       "       [[        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [-0.57537687, -1.5107378 ,  0.52730054, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan]],\n",
       "\n",
       "       [[        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan]],\n",
       "\n",
       "       [[-1.3936929 ,  0.5227711 , -0.15087639, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [-1.5083269 ,  0.5675902 ,  0.01117315, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [-1.4755071 ,  0.34875906, -0.01961976, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [-1.4049993 ,  0.48896185, -0.03740352, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan]]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "수온           577\n",
       "수소이온농도       575\n",
       "전기전도도        580\n",
       "용존산소         603\n",
       "탁도           860\n",
       "총유기탄소        745\n",
       "총질소          756\n",
       "총인          1831\n",
       "클로로필-a       507\n",
       "Day sin        0\n",
       "Day cos        0\n",
       "Year sin       0\n",
       "Year cos       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].isna().astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time1 = pd.to_datetime(df_full[0].iloc[:, 0], format='%Y.%m.%d %H:%M')\n",
    "date_time2 = pd.to_datetime(df_full[0].iloc[:, 0], format='%Y.%m.%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_s1 = date_time1.map(datetime.datetime.timestamp)\n",
    "timestamp_s2 = date_time2.map(datetime.datetime.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "df[0]['Day sin'] = np.sin(timestamp_s1 * (2 * np.pi / day))\n",
    "df[0]['Day cos'] = np.cos(timestamp_s1 * (2 * np.pi / day))\n",
    "df[0]['Year sin'] = np.sin(timestamp_s1 * (2 * np.pi / year))\n",
    "df[0]['Year cos'] = np.cos(timestamp_s1 * (2 * np.pi / year))\n",
    "\n",
    "df[1]['Day sin'] = np.sin(timestamp_s2 * (2 * np.pi / day))\n",
    "df[1]['Day cos'] = np.cos(timestamp_s2 * (2 * np.pi / day))\n",
    "df[1]['Year sin'] = np.sin(timestamp_s2 * (2 * np.pi / year))\n",
    "df[1]['Year cos'] = np.cos(timestamp_s2 * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        print(data[0].shape)\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "(None, 32)\n",
      "(None, 32)\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1982 - mae: 0.9713\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4911 - mae: 0.5818\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2830 - mae: 0.4266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7674278750>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.fit(x, y, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(32,), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(1,), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 32), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.batch(5)\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32)\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2308 - mae: 0.3886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f76e40d9a50>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with data generator\n",
    "\n",
    "https://towardsdatascience.com/keras-custom-data-generators-example-with-mnist-dataset-2a7a2d2b0360\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    " \n",
    "    def __init__(self, X_data , y_data, batch_size, dim, n_classes,\n",
    "                 to_fit, shuffle = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.X_data = X_data\n",
    "        self.labels = y_data\n",
    "        self.y_data = y_data\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.n = 0\n",
    "        self.list_IDs = np.arange(len(self.X_data))\n",
    "        self.on_epoch_end()\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "        \n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "        \n",
    "        return data\n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes)/self.batch_size)\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:\n",
    "            (index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "        \n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.indexes = np.arange(len(self.X_data))\n",
    "        \n",
    "        if self.shuffle: \n",
    "            np.random.shuffle(self.indexes)\n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "               \n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            \n",
    "            X[i,] = self.X_data[ID]\n",
    "            \n",
    "            # Normalize data\n",
    "            X = (X/255).astype('float32')\n",
    "            \n",
    "        return X[:,:,:, np.newaxis]\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        \n",
    "        y = np.empty(self.batch_size)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            \n",
    "            y[i] = self.y_data[ID]\n",
    "            \n",
    "        return keras.utils.to_categorical(\n",
    "                y,num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "input_shape = (28, 28)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28 , 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(x_train, y_train, batch_size = 64,\n",
    "                                dim = input_shape,\n",
    "                                n_classes=10, \n",
    "                                to_fit=True, shuffle=True)\n",
    "val_generator =  DataGenerator(x_test, y_test, batch_size=64, \n",
    "                               dim = input_shape, \n",
    "                               n_classes= n_classes, \n",
    "                               to_fit=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/ipykernel_launcher.py:60: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 9s 7ms/step - loss: 2.3026 - accuracy: 0.1095 - val_loss: 2.3025 - val_accuracy: 0.1124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7674178c90>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=1,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 28, 28, 1)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "input_data = keras.layers.Input(shape=input_shape)\n",
    "x = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu')(input_data)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_data = Dense(n_classes, activation='softmax')(x)\n",
    "model = CustomModel(input_data, output_data)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, None)\n",
      "(None, None, None, None)\n",
      "938/938 [==============================] - 7s 7ms/step - loss: 2.3028 - accuracy: 0.1065 - val_loss: 2.3025 - val_accuracy: 0.1130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f767410a6d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=1,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결론: DataGenerator 만으로는 train_step에 input data의 shape에 None으로 들어간다.\n",
    "\n",
    "```py\n",
    "        X = keras.layers.Reshape((tf.reduce_sum(x.shape[1:]),))(x)\n",
    "        Y = keras.layers.Reshape((tf.reduce_sum(x.shape[1:]),))(y)\n",
    "        X = tf.reshape(x, shape=(x.shape[0], -1))\n",
    "        Y = tf.reshape(y, shape=(x.shape[0], -1)\n",
    "```\n",
    "\n",
    "이런 함수들을 train_step 내에 사용할 수 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 폰트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/cmsy10.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Bold.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Bold.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymReg.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymReg.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizThreeSymBol.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneral.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBol.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymBol.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-BoldOblique.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFourSymBol.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Bold.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-Italic.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-BoldOblique.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUni.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralItalic.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/cmex10.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizOneSymReg.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizFiveSymReg.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans-Oblique.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/cmmi10.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/cmss10.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBol.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXGeneralBolIta.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymReg.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono-Oblique.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansDisplay.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/cmr10.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerif-BoldItalic.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniIta.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSansMono.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/cmb10.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXSizTwoSymBol.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/STIXNonUniBolIta.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSerifDisplay.ttf',\n",
       " '/home/kotech/workspace/venv-tensor2n-gpu/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-gujr-extra/padmaa.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationSans-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypewriter-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationSerif-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstFarsi.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-beng-extra/ani.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-gujarati/Lohit-Gujarati.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/Ubuntu-RI.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSans-ExtraLight.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/Ubuntu-C.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationMono-Regular.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSansCJK-Thin.ttc',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationMono-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/Navilu/Navilu.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Sawasdee-Oblique.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSansCJK-Medium.ttc',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypist-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-punjabi/Lohit-Gurmukhi.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSerif-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/Ubuntu-BI.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-telugu/Lohit-Telugu.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeSans.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-devanagari/Lohit-Devanagari.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Waree-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Garuda-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-telu-extra/Pothana2000.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Norasi-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Purisa-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Kinnari-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-assamese/Lohit-Assamese.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/Ubuntu-LI.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypist-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Kinnari-Italic.ttf',\n",
       " '/usr/share/fonts/opentype/malayalam/Manjari-Regular.otf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypist.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Norasi-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-beng-extra/MuktiNarrowBold.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeSerif.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-kannada/Lohit-Kannada.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSerif-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstPoster.ttf',\n",
       " '/usr/share/fonts/truetype/Nakula/nakula.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationSans-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-tamil-classical/Lohit-Tamil-Classical.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSansMono-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-bengali/Lohit-Bengali.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Garuda.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeMonoBold.ttf',\n",
       " '/usr/share/fonts/truetype/openoffice/opens___.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationMono-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Umpush.ttf',\n",
       " '/usr/share/fonts/truetype/Gargi/Gargi.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/mry_KacstQurn.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSans-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSerif-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypo.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-deva-extra/chandas1-2.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeMonoOblique.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSerifCJK-Bold.ttc',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationMono-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationSerif-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgMono-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Kinnari-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgMono.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstQurn.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeSansBoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/samyak/Samyak-Devanagari.ttf',\n",
       " '/usr/share/fonts/truetype/tibetan-machine/TibetanMachineUni.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Umpush-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/Gubbi/Gubbi.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Purisa-Oblique.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSansCJK-DemiLight.ttc',\n",
       " '/usr/share/fonts/truetype/fonts-gujr-extra/aakar-medium.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypewriter.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Umpush-Light.ttf',\n",
       " '/usr/share/fonts/truetype/noto/NotoMono-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypo-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Laksaman.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypist-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Kinnari-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/Rachana-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Purisa.ttf',\n",
       " '/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Sawasdee-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/Sarai/Sarai.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Umpush-LightOblique.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/UbuntuMono-RI.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstLetter.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-beng-extra/JamrulNormal.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-kalapi/Kalapi.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/AnjaliOldLipi.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/Rachana-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Laksaman-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/Suruma.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationSans-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Kinnari.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeSansBold.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-deva-extra/samanata.ttf',\n",
       " '/usr/share/fonts/opentype/malayalam/Manjari-Thin.otf',\n",
       " '/usr/share/fonts/truetype/lohit-malayalam/Lohit-Malayalam.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Waree-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypo-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-gujr-extra/padmaa-Bold.1.1.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-beng-extra/mitra.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Kinnari-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/Ubuntu-B.ttf',\n",
       " '/usr/share/fonts/truetype/pagul/Pagul.ttf',\n",
       " '/usr/share/fonts/truetype/sinhala/lklug.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Laksaman-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/Ubuntu-M.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/UbuntuMono-R.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-guru-extra/Saab.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/Ubuntu-R.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSans-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Norasi.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstScreen.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstDecorative.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Umpush-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationSans-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/Ubuntu-MI.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-telu-extra/vemana2000.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationMono-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/Chilanka-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-orya-extra/utkal.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstTitleL.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Norasi-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstOffice.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Waree-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Loma-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/padauk/Padauk-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-oriya/Lohit-Odia.ttf',\n",
       " '/usr/share/fonts/truetype/kacst-one/KacstOne-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Loma-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationSerif-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/lao/Phetsarath_OT.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypewriter-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-gujr-extra/padmaa-Medium-0.5.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypo-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf',\n",
       " '/usr/share/fonts/truetype/abyssinica/AbyssinicaSIL-R.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Waree.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-beng-extra/LikhanNormal.ttf',\n",
       " '/usr/share/fonts/truetype/Sahadeva/sahadeva.ttf',\n",
       " '/usr/share/fonts/truetype/ttf-khmeros-core/KhmerOSsys.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Purisa-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/RaghuMalayalamSans-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed.ttf',\n",
       " '/usr/share/fonts/truetype/ttf-khmeros-core/KhmerOS.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Laksaman-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Loma.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/padauk/PadaukBook-Regular.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/UbuntuMono-B.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstPen.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeSerifBold.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeSerifBoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Umpush-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Garuda-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeMonoBoldOblique.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSerifCJK-Light.ttc',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgMono-BoldOblique.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSansCJK-Light.ttc',\n",
       " '/usr/share/fonts/truetype/tlwg/Norasi-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/Dyuthi.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Sawasdee.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSerifCJK-SemiBold.ttc',\n",
       " '/usr/share/fonts/truetype/tlwg/Sawasdee-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuMathTeXGyre.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Loma-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/Uroob.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstNaskh.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeSerifItalic.ttf',\n",
       " '/usr/share/fonts/opentype/malayalam/Manjari-Bold.otf',\n",
       " '/usr/share/fonts/truetype/tlwg/Norasi-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgMono-Oblique.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSerifCJK-ExtraLight.ttc',\n",
       " '/usr/share/fonts/truetype/freefont/FreeSansOblique.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc',\n",
       " '/usr/share/fonts/truetype/padauk/PadaukBook-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/UbuntuMono-BI.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/TlwgTypewriter-Oblique.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstTitle.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/Keraleeyam.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-beng-extra/MuktiNarrow.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSerifCJK-Medium.ttc',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-gujr-extra/Rekha.ttf',\n",
       " '/usr/share/fonts/truetype/lohit-tamil/Lohit-Tamil.ttf',\n",
       " '/usr/share/fonts/truetype/samyak-fonts/Samyak-Tamil.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/Meera.ttf',\n",
       " '/usr/share/fonts/truetype/fonts-deva-extra/kalimati.ttf',\n",
       " '/usr/share/fonts/truetype/liberation2/LiberationSerif-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstBook.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstArt.ttf',\n",
       " '/usr/share/fonts/truetype/malayalam/Karumbi.ttf',\n",
       " '/usr/share/fonts/truetype/kacst/KacstDigital.ttf',\n",
       " '/usr/share/fonts/truetype/kacst-one/KacstOne.ttf',\n",
       " '/usr/share/fonts/truetype/tlwg/Garuda-BoldOblique.ttf',\n",
       " '/usr/share/fonts/truetype/freefont/FreeMono.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',\n",
       " '/usr/share/fonts/opentype/noto/NotoSerifCJK-Black.ttc',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf',\n",
       " '/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/samyak-fonts/Samyak-Gujarati.ttf',\n",
       " '/usr/share/fonts/truetype/padauk/Padauk-Bold.ttf',\n",
       " '/usr/share/fonts/truetype/dejavu/DejaVuSerif-Italic.ttf',\n",
       " '/usr/share/fonts/opentype/noto/NotoSansCJK-Black.ttc',\n",
       " '/usr/share/fonts/truetype/samyak-fonts/Samyak-Malayalam.ttf',\n",
       " '/usr/share/fonts/truetype/ubuntu/Ubuntu-L.ttf']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "\n",
    "[f.fname for f in matplotlib.font_manager.fontManager.ttflist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/share/fonts/truetype/nanum/NanumSquareRoundB.ttf: 나눔스퀘어라운드,NanumSquareRound,NanumSquareRound Bold,나눔스퀘어라운드 Bold:style=Bold,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Bold.ttc: Noto Serif CJK SC:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Bold.ttc: Noto Serif CJK TC:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Black.ttc: Noto Sans CJK HK,Noto Sans CJK HK Black:style=Black,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Bold.ttc: Noto Serif CJK JP:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Bold.ttc: Noto Serif CJK KR:style=Bold\r\n",
      "/usr/share/fonts/truetype/nanum/NanumSquareRoundR.ttf: 나눔스퀘어라운드,NanumSquareRound,NanumSquareRound Regular,나눔스퀘어라운드 Regular:style=Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumSquareB.ttf: 나눔스퀘어,NanumSquare,NanumSquare Bold,나눔스퀘어 Bold:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans CJK JP:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans CJK HK:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans CJK KR:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Black.ttc: Noto Sans CJK TC,Noto Sans CJK TC Black:style=Black,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Medium.ttc: Noto Serif CJK KR,Noto Serif CJK KR Medium:style=Medium,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Black.ttc: Noto Sans CJK KR,Noto Sans CJK KR Black:style=Black,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumGothicLight.ttf: 나눔고딕,NanumGothic,NanumGothic Light,나눔고딕 Light:style=Light,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumBrush.ttf: 나눔손글씨 붓,Nanum Brush Script:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans CJK SC:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-SemiBold.ttc: Noto Serif CJK SC,Noto Serif CJK SC SemiBold:style=SemiBold,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans CJK TC:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Medium.ttc: Noto Sans CJK JP,Noto Sans CJK JP Medium:style=Medium,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf: 나눔바른고딕,NanumBarunGothic:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Black.ttc: Noto Serif CJK JP,Noto Serif CJK JP Black:style=Black,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumBarunGothicUltraLight.ttf: 나눔바른고딕,NanumBarunGothic,NanumBarunGothic UltraLight,나눔바른고딕 UltraLight:style=UltraLight\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Light.ttc: Noto Sans CJK KR,Noto Sans CJK KR Light:style=Light,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Light.ttc: Noto Sans CJK HK,Noto Sans CJK HK Light:style=Light,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Black.ttc: Noto Sans CJK SC,Noto Sans CJK SC Black:style=Black,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumGothicEcoExtraBold.ttf: 나눔고딕 에코,NanumGothic Eco,NanumGothic Eco ExtraBold,나눔고딕 에코 ExtraBold:style=ExtraBold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc: Noto Serif CJK SC:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc: Noto Serif CJK TC:style=Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumGothic.ttf: 나눔고딕,NanumGothic:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Light.ttc: Noto Sans CJK SC,Noto Sans CJK SC Light:style=Light,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Light.ttc: Noto Serif CJK JP,Noto Serif CJK JP Light:style=Light,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc: Noto Serif CJK JP:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc: Noto Serif CJK KR:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-ExtraLight.ttc: Noto Serif CJK SC,Noto Serif CJK SC ExtraLight:style=ExtraLight,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumGothicEcoBold.ttf: 나눔고딕 에코,NanumGothic Eco:style=Bold\r\n",
      "/usr/share/fonts/truetype/nanum/NanumBarunpenB.ttf: 나눔바른펜,NanumBarunpen,NanumBarunpen Bold:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Medium.ttc: Noto Sans CJK KR,Noto Sans CJK KR Medium:style=Medium,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumGothicEco.ttf: 나눔고딕 에코,NanumGothic Eco:style=Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf: 나눔고딕코딩,NanumGothicCoding:style=Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumBarunGothicBold.ttf: 나눔바른고딕,NanumBarunGothic:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-DemiLight.ttc: Noto Sans CJK JP,Noto Sans CJK JP DemiLight:style=DemiLight,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Thin.ttc: Noto Sans CJK JP,Noto Sans CJK JP Thin:style=Thin,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumPen.ttf: 나눔손글씨 펜,Nanum Pen Script:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Light.ttc: Noto Sans CJK JP,Noto Sans CJK JP Light:style=Light,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Light.ttc: Noto Serif CJK SC,Noto Serif CJK SC Light:style=Light,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumBarunGothicLight.ttf: 나눔바른고딕,NanumBarunGothic,NanumBarunGothic Light,나눔바른고딕 Light:style=Light\r\n",
      "/usr/share/fonts/truetype/nanum/NanumSquareRoundL.ttf: 나눔스퀘어라운드,NanumSquareRound,NanumSquareRound Light,나눔스퀘어라운드 Light:style=Light,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-ExtraLight.ttc: Noto Serif CJK TC,Noto Serif CJK TC ExtraLight:style=ExtraLight,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Thin.ttc: Noto Sans CJK KR,Noto Sans CJK KR Thin:style=Thin,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-ExtraLight.ttc: Noto Serif CJK KR,Noto Serif CJK KR ExtraLight:style=ExtraLight,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Thin.ttc: Noto Sans CJK HK,Noto Sans CJK HK Thin:style=Thin,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Thin.ttc: Noto Sans CJK SC,Noto Sans CJK SC Thin:style=Thin,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-SemiBold.ttc: Noto Serif CJK JP,Noto Serif CJK JP SemiBold:style=SemiBold,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Black.ttc: Noto Serif CJK SC,Noto Serif CJK SC Black:style=Black,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-DemiLight.ttc: Noto Sans CJK TC,Noto Sans CJK TC DemiLight:style=DemiLight,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Medium.ttc: Noto Serif CJK SC,Noto Serif CJK SC Medium:style=Medium,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf: 나눔고딕,NanumGothic:style=Bold\r\n",
      "/usr/share/fonts/truetype/nanum/NanumGothicCoding-Bold.ttf: 나눔고딕코딩,NanumGothicCoding:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-DemiLight.ttc: Noto Sans CJK SC,Noto Sans CJK SC DemiLight:style=DemiLight,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumMyeongjoEcoExtraBold.ttf: 나눔명조 에코,NanumMyeongjo Eco,NanumMyeongjo Eco ExtraBold,나눔명조 에코 ExtraBold:style=ExtraBold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Medium.ttc: Noto Sans CJK TC,Noto Sans CJK TC Medium:style=Medium,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Black.ttc: Noto Sans CJK JP,Noto Sans CJK JP Black:style=Black,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Light.ttc: Noto Serif CJK KR,Noto Serif CJK KR Light:style=Light,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-SemiBold.ttc: Noto Serif CJK KR,Noto Serif CJK KR SemiBold:style=SemiBold,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Medium.ttc: Noto Sans CJK SC,Noto Sans CJK SC Medium:style=Medium,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Black.ttc: Noto Serif CJK TC,Noto Serif CJK TC Black:style=Black,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans Mono CJK TC:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-DemiLight.ttc: Noto Sans CJK KR,Noto Sans CJK KR DemiLight:style=DemiLight,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans Mono CJK SC:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans Mono CJK KR:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans Mono CJK HK:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans Mono CJK JP:style=Bold\r\n",
      "/usr/share/fonts/truetype/nanum/NanumSquareEB.ttf: 나눔스퀘어,NanumSquare,NanumSquare ExtraBold,나눔스퀘어 ExtraBold:style=ExtraBold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Medium.ttc: Noto Serif CJK TC,Noto Serif CJK TC Medium:style=Medium,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-ExtraLight.ttc: Noto Serif CJK JP,Noto Serif CJK JP ExtraLight:style=ExtraLight,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumMyeongjoEcoBold.ttf: 나눔명조 에코,NanumMyeongjo Eco:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Black.ttc: Noto Serif CJK KR,Noto Serif CJK KR Black:style=Black,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Medium.ttc: Noto Sans CJK HK,Noto Sans CJK HK Medium:style=Medium,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-SemiBold.ttc: Noto Serif CJK TC,Noto Serif CJK TC SemiBold:style=SemiBold,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans Mono CJK SC:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans Mono CJK TC:style=Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumMyeongjoEco.ttf: 나눔명조 에코,NanumMyeongjo Eco:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Light.ttc: Noto Serif CJK TC,Noto Serif CJK TC Light:style=Light,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans Mono CJK HK:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans Mono CJK KR:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc: Noto Sans Mono CJK JP:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Thin.ttc: Noto Sans CJK TC,Noto Sans CJK TC Thin:style=Thin,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-DemiLight.ttc: Noto Sans CJK HK,Noto Sans CJK HK DemiLight:style=DemiLight,Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans CJK JP:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans CJK KR:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans CJK HK:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans CJK TC:style=Bold\r\n",
      "/usr/share/fonts/truetype/nanum/NanumBarunpenR.ttf: 나눔바른펜,NanumBarunpen:style=Regular\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc: Noto Sans CJK SC:style=Bold\r\n",
      "/usr/share/fonts/truetype/nanum/NanumSquareR.ttf: 나눔스퀘어,NanumSquare:style=Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumMyeongjo.ttf: 나눔명조,NanumMyeongjo:style=Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumSquareL.ttf: 나눔스퀘어,NanumSquare,NanumSquare Light,나눔스퀘어 Light:style=Light\r\n",
      "/usr/share/fonts/opentype/noto/NotoSerifCJK-Medium.ttc: Noto Serif CJK JP,Noto Serif CJK JP Medium:style=Medium,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumMyeongjoBold.ttf: 나눔명조,NanumMyeongjo:style=Bold\r\n",
      "/usr/share/fonts/opentype/noto/NotoSansCJK-Light.ttc: Noto Sans CJK TC,Noto Sans CJK TC Light:style=Light,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumMyeongjoExtraBold.ttf: 나눔명조,NanumMyeongjo,NanumMyeongjoExtraBold,나눔명조 ExtraBold:style=ExtraBold\r\n",
      "/usr/share/fonts/truetype/nanum/NanumSquareRoundEB.ttf: 나눔스퀘어라운드,NanumSquareRound,NanumSquareRound ExtraBold,나눔스퀘어라운드 ExtraBold:style=ExtraBold,Regular\r\n",
      "/usr/share/fonts/truetype/nanum/NanumGothicExtraBold.ttf: 나눔고딕,NanumGothic,NanumGothicExtraBold,나눔고딕 ExtraBold:style=ExtraBold\r\n"
     ]
    }
   ],
   "source": [
    "! fc-list :lang=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "fm.get_fontconfig_fonts()\n",
    "font_location = '/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf'\n",
    "#font_location = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'\n",
    "# font_location = 'C:/Windows/Fonts/NanumGothic.ttf' # For Windows\n",
    "fprop = fm.FontProperties(fname=font_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d809976e75541b0a2d83c6f943d9948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()  \n",
    "plt.plot((1,1), label='가-가가')  \n",
    "plt.title('가가가',fontproperties=fprop)  \n",
    "plt.legend(prop=fprop)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
