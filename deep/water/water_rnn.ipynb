{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dot, Add, ReLU, Activation\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "fm.get_fontconfig_fonts()\n",
    "font_location = '/usr/share/fonts/truetype/nanum/NanumGothicCoding.ttf'\n",
    "#font_location = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'\n",
    "# font_location = 'C:/Windows/Fonts/NanumGothic.ttf' # For Windows\n",
    "fprop = fm.FontProperties(fname=font_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_files = [ '도개_gan.xlsx', '신암_gan.xlsx', '회상_gan.xlsx']\n",
    "ori_files = [ '도개_org.xlsx', '신암_org.xlsx', '회상_org.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data'\n",
    "prefix = 'nagdong2'\n",
    "def save_gan_data(gan_dfs, ori_dfs, gan_files, ori_files, train_mean, train_std, prefix):\n",
    "    for i in range(len(gan_dfs)):\n",
    "        path = os.path.join(folder, gan_files[i])\n",
    "        gan_dfs[i].to_excel(path, index=False)\n",
    "        print(path)\n",
    "        path = os.path.join(folder, ori_files[i])\n",
    "        ori_dfs[i].to_excel(path, index=False)\n",
    "    path = os.path.join(folder, prefix + '_std.xlsx')\n",
    "    train_std.to_frame(name='std').to_excel(path)\n",
    "    path = os.path.join(folder, prefix + '_mean.xlsx')\n",
    "    train_mean.to_frame(name='mean').to_excel(path)\n",
    "\n",
    "def load_gan_data(gan_files, ori_files, prefix):\n",
    "    gan_dfs = []\n",
    "    ori_dfs = []\n",
    "    for i in range(len(gan_files)):\n",
    "        path = os.path.join(folder, gan_files[i])\n",
    "        gan_dfs.append(pd.read_excel(path))\n",
    "        print(path)\n",
    "        path = os.path.join(folder, ori_files[i])\n",
    "        ori_dfs.append(pd.read_excel(path))\n",
    "    path = os.path.join(folder, prefix + '_std.xlsx')\n",
    "    train_std = pd.read_excel(path, index_col=0).loc[:, 'std']\n",
    "    path = os.path.join(folder, prefix + '_mean.xlsx')\n",
    "    train_mean = pd.read_excel(path, index_col=0).loc[:, 'mean']\n",
    "    return gan_dfs, ori_dfs, train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/도개_gan.xlsx\n",
      "data/신암_gan.xlsx\n",
      "data/회상_gan.xlsx\n"
     ]
    }
   ],
   "source": [
    "gan_dfs, ori_dfs, train_mean, train_std = load_gan_data(gan_files, ori_files, prefix=prefix)\n",
    "for i in range(len(gan_dfs)):\n",
    "    gan_dfs[i].columns = gan_dfs[i].columns+'%d'%(i+1)\n",
    "    ori_dfs[i].columns = ori_dfs[i].columns+'%d'%(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/보_구미보_2016.xlsx\n",
      "data/보_구미보_2017.xlsx\n",
      "data/보_구미보_2018.xlsx\n",
      "data/보_구미보_2019.xlsx\n"
     ]
    }
   ],
   "source": [
    "file_names = []\n",
    "file_names = ['보_구미보_2016.xlsx', '보_구미보_2017.xlsx', '보_구미보_2018.xlsx', '보_구미보_2019.xlsx']\n",
    "folder = 'data'\n",
    "bo_dfs = []\n",
    "for file_name in file_names:\n",
    "    path = os.path.join(folder, file_name)\n",
    "    print(path)\n",
    "    bo_dfs.append(pd.read_excel(path).iloc[:, 2:4])\n",
    "bo_df = pd.concat(bo_dfs, axis=0)\n",
    "\n",
    "bo_mean = bo_df.mean()\n",
    "bo_std = bo_df.std()\n",
    "bo_df = (bo_df - bo_mean)/bo_std\n",
    "\n",
    "bo_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/AWS_옥산_2016.xlsx\n",
      "data/AWS_옥산_2017.xlsx\n",
      "data/AWS_옥산_2018.xlsx\n",
      "data/AWS_옥산_2019.xlsx\n"
     ]
    }
   ],
   "source": [
    "file_names = []\n",
    "file_names = ['AWS_옥산_2016.xlsx', 'AWS_옥산_2017.xlsx', 'AWS_옥산_2018.xlsx', 'AWS_옥산_2019.xlsx']\n",
    "folder = 'data'\n",
    "aws_dfs = []\n",
    "for file_name in file_names:\n",
    "    path = os.path.join(folder, file_name)\n",
    "    print(path)\n",
    "    aws_dfs.append(pd.read_excel(path).iloc[:, -4:])\n",
    "aws_df = pd.concat(aws_dfs, axis=0)\n",
    "\n",
    "aws_mean = aws_df.mean()\n",
    "aws_std = aws_df.std()\n",
    "aws_df = (aws_df - aws_mean)/aws_std\n",
    "aws_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data'\n",
    "path = os.path.join(folder, 'date.xlsx')\n",
    "date_df = pd.read_excel(path, index_col='ymdh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "              ['상주1_2016.xlsx',\n",
    "               '상주1_2017.xlsx',\n",
    "               '상주1_2018.xlsx',\n",
    "               '상주1_2019.xlsx'],\n",
    "              ['영강2-1_2016.xlsx',\n",
    "               '영강2-1_2017.xlsx',\n",
    "               '영강2-1_2018.xlsx',\n",
    "               '영강2-1_2019.xlsx'],\n",
    "             ]\n",
    "folder = 'water_data/낙동강2/수질측정망'\n",
    "\n",
    "filename = '상주1_2016.xlsx'\n",
    "\n",
    "# what's nh3n, no3n, ss, po4p, dtn, dtp?\n",
    "qm_cols = ['nh3n', 'no3n', 'ph', 'cod', 'ec', 'bod', 'ss', 'do', 'tn', 'tp', 'chlorophylla', 'toc']\n",
    "#df = df.loc[:, qm_cols]\n",
    "\n",
    "qm_dfs = []\n",
    "qm_inter_dfs = []\n",
    "for j in range(len(file_names)):\n",
    "    dfs = []\n",
    "    for i in range(len(file_names[j])):\n",
    "        path = os.path.join(folder, file_names[j][i])\n",
    "        dfs.append(pd.read_excel(path, index_col='dt'))\n",
    "        dfs[i] = dfs[i].loc[:, qm_cols]\n",
    "    qm_dfs.append(pd.concat(dfs))\n",
    "    qm_dfs[j] = pd.concat([date_df, qm_dfs[j]], axis=1)\n",
    "    qm_dfs[j].reset_index(drop=True, inplace=True)\n",
    "    first_series = qm_dfs[j].apply(pd.Series.first_valid_index)\n",
    "    last_series = qm_dfs[j].apply(pd.Series.first_valid_index)\n",
    "    for col in first_series.index:\n",
    "        qm_dfs[j].loc[0, col] = qm_dfs[j].loc[first_series[col], col]\n",
    "        qm_dfs[j].loc[len(qm_dfs[j])-1, col] = qm_dfs[j].loc[last_series[col], col]\n",
    "    \n",
    "    qm_dfs[j] = qm_dfs[j].interpolate(method='pchip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm_mean = []\n",
    "qm_std = []\n",
    "\n",
    "for i in range(len(qm_dfs)):\n",
    "    qm_dfs[i].columns=qm_dfs[i].columns+'%d'%(i+1)\n",
    "    qm_mean.append(qm_dfs[i].mean())\n",
    "    qm_std.append(qm_dfs[i].std())\n",
    "    qm_dfs[i] = (qm_dfs[i]-qm_mean[i])/qm_std[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmpr_value       1.619773e+01\n",
       "ph_value         7.957707e+00\n",
       "do_value         1.028830e+01\n",
       "ec_value         2.361925e+02\n",
       "toc_value        2.949259e+00\n",
       "총질소_값            2.511900e+00\n",
       "총인_값             1.923296e-02\n",
       "클로로필-a_값         1.821510e+01\n",
       "Day sin          6.037103e-15\n",
       "Day cos          2.747682e-15\n",
       "Year sin        -5.681266e-08\n",
       "Year cos         2.053380e-05\n",
       "upstrim_wlv      3.246092e+03\n",
       "dwstrm_wlv       2.571552e+03\n",
       "rn60m_value      7.638889e-02\n",
       "ta_value         8.758325e+00\n",
       "hm_value         2.880299e+01\n",
       "ps_value         7.607794e+02\n",
       "nh3n1            7.441017e-02\n",
       "no3n1            1.995208e+00\n",
       "ph1              7.986643e+00\n",
       "cod1             5.426084e+00\n",
       "ec1              2.315260e+02\n",
       "bod1             1.595262e+00\n",
       "ss1              6.677707e+00\n",
       "do1              1.144842e+01\n",
       "tn1              2.423511e+00\n",
       "tp1              2.772457e-02\n",
       "chlorophylla1    1.183009e+01\n",
       "toc1             2.633367e+00\n",
       "nh3n2            2.100124e-01\n",
       "no3n2            1.963437e+00\n",
       "ph2              7.703737e+00\n",
       "cod2             3.973896e+00\n",
       "ec2              3.184885e+02\n",
       "bod2             1.199198e+00\n",
       "ss2              7.029391e+00\n",
       "do2              1.042362e+01\n",
       "tn2              2.676597e+00\n",
       "tp2              2.383757e-02\n",
       "chlorophylla2    5.764591e+00\n",
       "toc2             2.594384e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = pd.concat([train_mean, bo_mean, aws_mean]+qm_mean)\n",
    "std = pd.concat([train_std, bo_std, aws_std]+qm_std)\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ori_df = pd.concat(ori_dfs, axis=1)\n",
    "#gan_df = pd.concat(gan_dfs, axis=1)\n",
    "#ori_df = pd.concat(ori_dfs+[bo_df, aws_df]+qm_dfs, axis=1)\n",
    "#gan_df = pd.concat(gan_dfs+[bo_df, aws_df]+qm_dfs, axis=1)\n",
    "#ori_df = pd.concat(ori_dfs+[qm_dfs[1]], axis=1)\n",
    "#gan_df = pd.concat(gan_dfs+[qm_dfs[1]], axis=1)\n",
    "ori_df = pd.concat(ori_dfs+[bo_df, aws_df], axis=1)\n",
    "gan_df = pd.concat(gan_dfs+[bo_df, aws_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ori_df = pd.concat(ori_dfs, axis=1)\n",
    "#gan_df = pd.concat(gan_dfs, axis=1)\n",
    "\n",
    "#ori_df = pd.concat(ori_dfs+[bo_df, aws_df]+qm_dfs, axis=1)\n",
    "#gan_df = pd.concat(gan_dfs+[bo_df, aws_df]+qm_dfs, axis=1)\n",
    "\n",
    "total_no = ori_df.shape[0]\n",
    "train_no = int(total_no*0.8)\n",
    "val_no = int(total_no*0.1)\n",
    "\n",
    "train_slice = slice(0, train_no)\n",
    "#val_slice = slice(train_no, train_no+val_no)\n",
    "#val_slice = slice(train_no, None)\n",
    "test_slice = slice(train_no, None)\n",
    "#test_slice = slice(train_no+val_no, None)\n",
    "#test_slice = slice(train_no+val_no, None)\n",
    "val_slice = slice(train_no+val_no, None)\n",
    "\n",
    "train_df = pd.DataFrame(gan_df[train_slice])\n",
    "val_df = pd.DataFrame(gan_df[val_slice])\n",
    "test_df = pd.DataFrame(gan_df[test_slice])\n",
    "\n",
    "train_ori_df = pd.DataFrame(ori_df[train_slice])\n",
    "val_ori_df = pd.DataFrame(ori_df[val_slice])\n",
    "test_ori_df = pd.DataFrame(ori_df[test_slice])\n",
    "\n",
    "num_features = train_df.shape[1]\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_hr_df = gan_df.groupby(gan_df.index //24).mean()\n",
    "total_no = gan_hr_df.shape[0]\n",
    "train_no = int(total_no*0.7)\n",
    "\n",
    "train_slice = slice(0, train_no)\n",
    "val_slice = slice(train_no, None)\n",
    "test_slice = slice(0, None)\n",
    "\n",
    "train_day_df = gan_hr_df[train_slice]\n",
    "val_day_df = gan_hr_df[val_slice]\n",
    "test_day_df = gan_hr_df[test_slice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUT FEATURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features = [7]\n",
    "out_num_features=len(out_features)\n",
    "input_days = 10\n",
    "out_feature_name = train_df.columns[out_features[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "            #train_df=None, val_df=None, test_df=None,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]', fontproperties=fprop)\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df, train=True)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for water'\n",
    "    def __init__(self,\n",
    "                 imputed_data,\n",
    "                 ori_data = None,\n",
    "                 batch_size=32,\n",
    "                 input_width=24*7,\n",
    "                 label_width=24*3,\n",
    "                 shift=24*3,\n",
    "                 skip_time = None,\n",
    "                 shuffle = True,\n",
    "                 out_features = None,\n",
    "                 out_num_features = None,\n",
    "                ):\n",
    "        'Initialization'\n",
    "        self.window_size = input_width+shift\n",
    "        self.total_no = imputed_data.shape[0]\n",
    "        self.data = imputed_data\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = (batch_size, input_width, self.data.shape[1])\n",
    "        self.out_num_features = out_num_features\n",
    "        if out_features:\n",
    "            self.out_features = out_features\n",
    "        else:\n",
    "            self.out_features = [i for i in range(out_num_features)]\n",
    "        self.label_shape = (batch_size, label_width, self.out_num_features)\n",
    "        if (skip_time):\n",
    "            # TO-DO\n",
    "            self.no = self.total_no - self.window_size\n",
    "            self.data_idx = np.arange(0, self.no)\n",
    "        else:\n",
    "            self.no = self.total_no - self.window_size\n",
    "            self.data_idx = np.arange(0, self.no)\n",
    "            \n",
    "        if shuffle:\n",
    "            self.batch_idx = np.random.permutation(self.no)\n",
    "        else:\n",
    "            self.batch_idx = np.arange(0, self.no)\n",
    "        self.batch_id = 0\n",
    "        #print('')\n",
    "        #print('self.total_no =', self.total_no)\n",
    "        #print('')\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        #return int(128/self.batch_size)\n",
    "        #return 2\n",
    "        return self.no//self.batch_size\n",
    "        #return 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        #print('index =', index)\n",
    "        #print('self.no =', self.no)\n",
    "        #print('self.total_no =', self.total_no)\n",
    "        #print('')\n",
    "        #print('self.no =', self.no, 'self.batch_id =', self.batch_id)\n",
    "        # Sample batch\n",
    "        label_width = self.label_width\n",
    "        batch_idx = self.batch_idx\n",
    "        \n",
    "        x = np.empty((0, self.input_width, self.data.shape[1]))\n",
    "        y = np.empty((0, self.label_width, self.out_num_features))\n",
    "        for cnt in range(0, self.batch_size):\n",
    "            i = self.batch_id\n",
    "            self.batch_id += 1\n",
    "            idx1 = self.data_idx[batch_idx[i]]\n",
    "            idx2 = idx1 + self.input_width\n",
    "            \n",
    "            X = self.data[idx1:idx2]\n",
    "            \n",
    "            idx1 = self.data_idx[batch_idx[i]] + self.window_size - label_width\n",
    "            idx2 = idx1 + label_width\n",
    "            \n",
    "            #Y = self.data[idx1:idx2,:,:out_num_features]\n",
    "            Y = self.data.iloc[idx1:idx2, self.out_features]\n",
    "            #print('Y.shape = ', Y.shape)\n",
    "            #Y = Y.iloc[:,:out_num_features]\n",
    "            \n",
    "            self.batch_id %= self.no\n",
    "            \n",
    "            x = np.append(x, [X], axis = 0)\n",
    "            y = np.append(y, [Y], axis = 0)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_water(self, data, train=False):\n",
    "  dg = WaterDataGenerator(\n",
    "      data,\n",
    "      batch_size = 128,\n",
    "      #batch_size = 128 if train else data.shape[0]-(self.input_width+self.label_width),\n",
    "      input_width = self.input_width,\n",
    "      label_width = self.label_width,\n",
    "      shift = self.label_width,\n",
    "      out_features = out_features,\n",
    "      out_num_features = out_num_features,\n",
    "  )\n",
    "  #self.dg = dg\n",
    "  ds = tf.data.Dataset.from_generator(\n",
    "      lambda: dg,\n",
    "      output_types=(tf.float32, tf.float32),\n",
    "      output_shapes=(\n",
    "        dg.input_shape,\n",
    "        dg.label_shape\n",
    "        #[batch_size, train_generator.dim],\n",
    "        #[batch_size, train_generator.dim],\n",
    "      )\n",
    "  )\n",
    "  #return ds.batch(batch_size=128)\n",
    "  if train:\n",
    "      #return ds.repeat(10).prefetch(3)\n",
    "      return ds.repeat(-1).prefetch(5)\n",
    "  else:\n",
    "      return ds.prefetch(5)\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2(self, model=None, plot_col=0, max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  #plot_out_col_index = self.column_indices[plot_out_col]\n",
    "  out_features_np = np.array(out_features)\n",
    "  plot_out_col_index = np.argwhere(out_features_np == plot_col_index)[0][0]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]', fontproperties=fprop)\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "      label_out_col_index = self.label_columns_indices.get(plot_out_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "      label_out_col_index = plot_out_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.plot(self.label_indices, labels[n, :, label_out_col_index],\n",
    "                label='Labels', c='#2ca02c')\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.plot(self.label_indices, predictions[n, :, label_out_col_index],\n",
    "                  marker=None, label='Predictions',\n",
    "                  c='#ff7f0e')\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot2 = plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tmpr_value1', 'ph_value1', 'do_value1', 'ec_value1', 'toc_value1',\n",
       "       '총질소_값1', '총인_값1', '클로로필-a_값1', 'Day sin1', 'Day cos1', 'Year sin1',\n",
       "       'Year cos1', 'tmpr_value2', 'ph_value2', 'do_value2', 'ec_value2',\n",
       "       'toc_value2', '총질소_값2', '총인_값2', '클로로필-a_값2', 'tmpr_value3',\n",
       "       'ph_value3', 'do_value3', 'ec_value3', 'toc_value3', '총질소_값3', '총인_값3',\n",
       "       '클로로필-a_값3', 'upstrim_wlv', 'dwstrm_wlv', 'rn60m_value', 'ta_value',\n",
       "       'hm_value', 'ps_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c010a635a884ecab6636335ba0faad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Total window size: 360\n",
       "Input indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
       "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
       "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
       "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
       "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
       "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
       " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
       " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
       " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
       " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
       " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
       " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
       " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
       " 234 235 236 237 238 239]\n",
       "Label indices: [240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
       " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
       " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
       " 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311\n",
       " 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329\n",
       " 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347\n",
       " 348 349 350 351 352 353 354 355 356 357 358 359]\n",
       "Label column name(s): None"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_STEPS = 24*5\n",
    "multi_window = WindowGenerator(input_width=24*input_days,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS,\n",
    "                               train_df=train_df,\n",
    "                               val_df=val_df,\n",
    "                               test_df=test_df,\n",
    "                               )\n",
    "\n",
    "#multi_window.plot2(plot_col=out_features[0])\n",
    "#multi_window.plot2(plot_col='클로로필-a_값1', plot_out_col='클로로필-a_값1')\n",
    "multi_window.plot2(plot_col='클로로필-a_값1')\n",
    "multi_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OUT_STEPS = 5\n",
    "multi_window = WindowGenerator(input_width=input_days,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS,\n",
    "                               train_df=train_day_df,\n",
    "                               val_df=val_day_df,\n",
    "                               test_df=test_day_df,\n",
    "                               )\n",
    "\n",
    "#multi_window.plot2(plot_col=out_features[0])\n",
    "#multi_window.plot2(plot_col='클로로필-a_값1', plot_out_col='클로로필-a_값1')\n",
    "multi_window.plot2(plot_col='클로로필-a_값1')\n",
    "multi_window\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_val_performance = {}\n",
    "multi_performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    #print(inputs[:, -1:, 0:1])\n",
    "    #return tf.tile(inputs[:, -1:, :out_num_features], [1, OUT_STEPS, 1])\n",
    "    return tf.tile(inputs[:, -1:, (out_features[0]):(out_features[0]+1)], [1, OUT_STEPS, 1])\n",
    "    #return tf.tile(inputs[:, -1:, out_features[0]:(out_features[1]+1)], [1, OUT_STEPS, 1])\n",
    "\n",
    "last_baseline = MultiStepLastBaseline()\n",
    "last_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "#multi_val_performance = {}\n",
    "#multi_performance = {}\n",
    "\n",
    "multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Last'] = last_baseline.evaluate(multi_window.test)\n",
    "print('val performance =', multi_val_performance['Last'])\n",
    "print('test performance = ', multi_performance['Last'])\n",
    "multi_window.plot2(last_baseline, plot_col=train_df.columns[out_features[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_EPOCHS = 400\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def nse(y_true, y_pred):\n",
    "    mean = tf.reduce_mean(y_true)\n",
    "    return 1. - tf.reduce_sum(tf.square(y_true-y_pred))/tf.reduce_sum(tf.square(y_true-mean))\n",
    "\n",
    "def compile_and_fit(model, window, patience=1000):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError(), nse])\n",
    "  #model.compile(loss=GAIN.RMSE_loss)\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS, steps_per_epoch=10,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(history.history['loss'], label='loss')\n",
    "    ax.plot(history.history['mean_absolute_error'], label='mae')\n",
    "    ax.plot(history.history['val_loss'], label='val_loss')\n",
    "    ax.plot(history.history['val_mean_absolute_error'], label='val_mae')\n",
    "    #plt.legend(history.history.keys(), loc='upper right')\n",
    "    #ax.legend(loc='upper center')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"epochs\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "multi_linear_model = tf.keras.Sequential([\n",
    "    # Take the last time-step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*out_num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, out_num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_linear_model, multi_window)\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "#multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val.repeat(-1), steps=100)\n",
    "#multi_performance['Linear'] = multi_linear_model.evaluate(multi_window.test.repeat(-1), verbose=1, steps=100)\n",
    "multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\n",
    "multi_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=1)\n",
    "#multi_window.plot(multi_linear_model, plot_col=0)\n",
    "print('val performance =', multi_val_performance['Linear'])\n",
    "print('test performance = ', multi_performance['Linear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)//128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot2(multi_linear_model, plot_col=out_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "58*24*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10240/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 40 \n",
    "multi_dense_model = tf.keras.Sequential([\n",
    "    # Take the last time step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Shape => [batch, 1, dense_units]\n",
    "    #tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(8192, activation='relu'),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*out_num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, out_num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_dense_model, multi_window)\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)\n",
    "multi_performance['Dense'] = multi_dense_model.evaluate(multi_window.test, verbose=1)\n",
    "print('val performance =', multi_val_performance['Dense'])\n",
    "print('test performance = ', multi_performance['Dense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot2(multi_dense_model, plot_col=out_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 5 \n",
    "#CONV_LAYER_NO = 3\n",
    "CONV_WIDTH = 3\n",
    "multi_conv_model = tf.keras.Sequential([\n",
    "    keras.layers.Conv1D(256, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(256, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPooling1D(padding='same'),\n",
    "    keras.layers.Conv1D(512, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(512, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(512, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPooling1D(padding='same'),\n",
    "    keras.layers.Conv1D(1024, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(1024, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(1024, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPooling1D(padding='same'),\n",
    "    keras.layers.Conv1D(2048, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(2048, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv1D(2048, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    #keras.layers.MaxPooling1D(padding='same'),\n",
    "    \n",
    "    #keras.layers.Conv1D(4096, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    #keras.layers.Activation('relu'),\n",
    "    #keras.layers.Conv1D(4096, activation='linear', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    #keras.layers.Activation('relu'),\n",
    "    #keras.layers.MaxPooling1D(padding='same'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #keras.layers.MaxPooling1D(pool_size=2),\n",
    "    #keras.layers.Conv1D(1024, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.MaxPooling1D(pool_size=2),\n",
    "    #keras.layers.Conv1D(1024, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    #keras.layers.MaxPooling1D(pool_size=2),\n",
    "    #keras.layers.Conv1D(1024, activation='relu', kernel_size=(CONV_WIDTH)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Shape => [batch, 1,  out_steps*features]\n",
    "    #tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          #kernel_initializer=tf.initializers.zeros),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_fit(multi_conv_model, multi_window)\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)\n",
    "multi_performance['Conv'] = multi_conv_model.evaluate(multi_window.test, verbose=1)\n",
    "print('val performance =', multi_val_performance['Conv'])\n",
    "print('test performance = ', multi_performance['Conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU model**  \n",
    "best: all feature, 10 days, 256,256\n",
    "```\n",
    "val performance = [0.46297532320022583, 0.4968714416027069]\n",
    "test performance =  [0.5573816895484924, 0.5664846301078796]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "10/10 [==============================] - 11s 918ms/step - loss: 0.8369 - mean_absolute_error: 0.6639 - nse: 0.0840 - val_loss: 0.9455 - val_mean_absolute_error: 0.7501 - val_nse: 0.2052\n",
      "24/24 [==============================] - 4s 157ms/step - loss: 0.4865 - mean_absolute_error: 0.5208 - nse: 0.5891\n",
      "51/51 [==============================] - 8s 155ms/step - loss: 0.4113 - mean_absolute_error: 0.4467 - nse: 0.5090\n",
      "val performance = [0.486502081155777, 0.5207767486572266, 0.5891494750976562]\n",
      "test performance =  [0.41129836440086365, 0.4467115104198456, 0.5090007781982422]\n"
     ]
    }
   ],
   "source": [
    "#MAX_EPOCHS = 150\n",
    "MAX_EPOCHS = 1 \n",
    "multi_gru_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    #tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    tf.keras.layers.GRU(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.GRU(256, return_sequences=False, dropout=0.1, recurrent_dropout=0.5),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*out_num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, out_num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_gru_model, multi_window)\n",
    "#multi_gru_model.load_weights('nagdong_cl.h5')\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['GRU'] = multi_gru_model.evaluate(multi_window.val)\n",
    "multi_performance['GRU'] = multi_gru_model.evaluate(multi_window.test, verbose=1)\n",
    "print('val performance =', multi_val_performance['GRU'])\n",
    "print('test performance = ', multi_performance['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_val_performance['GRU'] = multi_gru_model.evaluate(multi_window.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 4s 157ms/step - loss: 0.4907 - mean_absolute_error: 0.5226 - nse: 0.5851\n",
      "51/51 [==============================] - 8s 153ms/step - loss: 0.4123 - mean_absolute_error: 0.4473 - nse: 0.5066\n",
      "val performance = [0.4906741678714752, 0.5226204991340637, 0.5850632786750793]\n",
      "test performance =  [0.4123188555240631, 0.4473099112510681, 0.5066457390785217]\n"
     ]
    }
   ],
   "source": [
    "multi_val_performance['GRU'] = multi_gru_model.evaluate(multi_window.val)\n",
    "multi_performance['GRU'] = multi_gru_model.evaluate(multi_window.test, verbose=1)\n",
    "print('val performance =', multi_val_performance['GRU'])\n",
    "print('test performance = ', multi_performance['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot2(multi_gru_model, plot_col=out_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gru_model.save_weights('nagdong_cl.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_EPOCHS = 150\n",
    "MAX_EPOCHS = 5\n",
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    #tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    #tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    #tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=False, dropout=0.1, recurrent_dropout=0.5),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(OUT_STEPS*out_num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, out_num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_lstm_model, multi_window)\n",
    "\n",
    "#IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\n",
    "multi_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test)\n",
    "print('val performance =', multi_val_performance['LSTM'])\n",
    "print('test performance = ', multi_performance['LSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot2(multi_lstm_model, plot_col=out_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(multi_performance))\n",
    "width = 0.3\n",
    "\n",
    "\n",
    "metric_name = 'mean_absolute_error'\n",
    "metric_index = multi_gru_model.metrics_names.index('mean_absolute_error')\n",
    "val_mae = [v[metric_index] for v in multi_val_performance.values()]\n",
    "test_mae = [v[metric_index] for v in multi_performance.values()]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "plt.bar(x + 0.17, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=multi_performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel(f'MAE (average over all times and outputs)')\n",
    "_ = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class DaysPrediction(object):\n",
    "    def __init__(self,\n",
    "                 df=test_df,\n",
    "                 in_steps=24*input_days,\n",
    "                 out_steps=OUT_STEPS,\n",
    "                ):\n",
    "        self.df = df\n",
    "        self.in_steps = in_steps\n",
    "        self.out_steps = out_steps\n",
    "        # fix해야함\n",
    "        #len_tt = df.shape[0]//in_steps*in_steps\n",
    "        #self.n_days = (len_tt - in_steps)//24\n",
    "        self.n_days = df.shape[0]//24\n",
    "        self.pred_days = out_steps//24\n",
    "        self.in_days = in_steps//24\n",
    "        self.gt = np.full((self.n_days), np.nan)\n",
    "        for i in range(self.n_days):\n",
    "            x = self.df[i*24:(i*24+24)].to_numpy()\n",
    "            #x = val_df[i*24:(i*24+24)].to_numpy()\n",
    "            self.gt[i] = np.average(x[0:24, out_features[0]])\n",
    "        \n",
    "    def predict(self, model):\n",
    "        preds = np.full((self.pred_days, self.n_days), np.nan)\n",
    "        for i in tqdm(range(self.n_days-self.in_days)):\n",
    "            x = self.df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            #x = val_df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            x = x.reshape((-1, self.in_steps, self.df.shape[1]))\n",
    "            y = model.predict(x)\n",
    "            for j in range(self.pred_days):\n",
    "                if i+self.in_days+j >= self.n_days:\n",
    "                    break\n",
    "                preds[j][i+self.in_days+j] = np.average(y[:, j*24:j*24+24, :])\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(object):\n",
    "    def __init__(self,\n",
    "                 df=test_df,\n",
    "                 in_steps=input_days,\n",
    "                 out_steps=OUT_STEPS,\n",
    "                ):\n",
    "        self.df = df\n",
    "        self.in_steps = in_steps\n",
    "        self.out_steps = out_steps\n",
    "        len_tt = df.shape[0]\n",
    "        #self.n_days = len_tt - in_steps\n",
    "        self.n_days = len_tt\n",
    "        self.pred_days = out_steps\n",
    "        self.in_days = in_steps\n",
    "        self.gt = self.df.iloc[:, out_features[0]].to_numpy()\n",
    "        \n",
    "    def predict(self, model):\n",
    "        preds = np.full((self.pred_days, self.n_days), np.nan)\n",
    "        for i in tqdm(range(self.n_days-self.in_days)):\n",
    "        #for i in tqdm(range(1)):\n",
    "            x = self.df[i:(i+self.in_steps)].to_numpy()\n",
    "            #x = val_df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            x = x.reshape((-1, self.in_steps, self.df.shape[1]))\n",
    "            y = model.predict(x)\n",
    "            #print(y.shape)\n",
    "            for j in range(self.pred_days):\n",
    "                if i+self.in_days+j >= self.n_days:\n",
    "                    break\n",
    "                preds[j][i+self.in_days+j] = y[0, j, 0]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoursPrediction(object):\n",
    "    def __init__(self,\n",
    "                 df=test_df,\n",
    "                 in_steps=24*input_days,\n",
    "                 out_steps=OUT_STEPS,\n",
    "                ):\n",
    "        self.df = df\n",
    "        self.in_steps = in_steps\n",
    "        self.out_steps = out_steps\n",
    "        # fix해야함\n",
    "        #len_tt = df.shape[0]//in_steps*in_steps\n",
    "        #self.n_days = (len_tt - in_steps)//24\n",
    "        self.n_days = df.shape[0]//24\n",
    "        self.pred_days = out_steps//24\n",
    "        self.in_days = in_steps//24\n",
    "        self.gt = self.df.iloc[:self.n_days*24, out_features[0]].to_numpy()\n",
    "        #print('gt.shape =', self.gt.shape)\n",
    "        #print('n_days =', self.n_days)\n",
    "        #print('n_days*24 =', self.n_days*24)\n",
    "        \n",
    "    def predict(self, model):\n",
    "        preds = np.full((self.pred_days, self.n_days*24), np.nan)\n",
    "        for i in tqdm(range(self.n_days-self.in_days)):\n",
    "            x = self.df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            #x = val_df[i*24:(i*24+self.in_steps)].to_numpy()\n",
    "            x = x.reshape((-1, self.in_steps, self.df.shape[1]))\n",
    "            y = model.predict(x)\n",
    "            for j in range(self.pred_days):\n",
    "                if i+self.in_days+j >= self.n_days:\n",
    "                    break\n",
    "                preds[j][(i+self.in_days+j)*24:(i+self.in_days+j+1)*24] = y[0, j*24:j*24+24, 0]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(data):\n",
    "    return data*train_std[out_features[0]]+train_mean[out_features[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = {}\n",
    "preds_val = {}\n",
    "preds_test = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daily avaraged prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_prediction_train = DaysPrediction(df=train_df)\n",
    "days_prediction_val = DaysPrediction(df=val_df)\n",
    "days_prediction_test = DaysPrediction(df=test_df)\n",
    "daily = True\n",
    "\n",
    "gt_train = denormalize(days_prediction_train.gt)\n",
    "gt_val = denormalize(days_prediction_val.gt)\n",
    "gt_test = denormalize(days_prediction_test.gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hourly prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_prediction_train = HoursPrediction(df=train_df)\n",
    "days_prediction_val = HoursPrediction(df=val_df)\n",
    "days_prediction_test = HoursPrediction(df=test_df)\n",
    "daily = False\n",
    "\n",
    "gt_train = denormalize(days_prediction_train.gt)\n",
    "gt_val = denormalize(days_prediction_val.gt)\n",
    "gt_test = denormalize(days_prediction_test.gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daily avaraged Model(warning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_prediction_train = Prediction(df=train_day_df)\n",
    "days_prediction_val = Prediction(df=val_day_df)\n",
    "days_prediction_test = Prediction(df=test_day_df)\n",
    "daily = True\n",
    "\n",
    "gt_train = denormalize(days_prediction_train.gt)\n",
    "gt_val = denormalize(days_prediction_val.gt)\n",
    "gt_test = denormalize(days_prediction_test.gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1158/1158 [02:38<00:00,  7.31it/s]\n",
      "100%|██████████| 136/136 [00:18<00:00,  7.48it/s]\n",
      "100%|██████████| 282/282 [00:38<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "#preds_dict = {}\n",
    "#days_prediction = DaysPrediction()\n",
    "preds_train['GRU'] = denormalize(days_prediction_train.predict(multi_gru_model))\n",
    "preds_val['GRU'] = denormalize(days_prediction_val.predict(multi_gru_model))\n",
    "preds_test['GRU'] = denormalize(days_prediction_test.predict(multi_gru_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train['LSTM'] = denormalize(days_prediction_train.predict(multi_lstm_model))\n",
    "preds_val['LSTM'] = denormalize(days_prediction_val.predict(multi_lstm_model))\n",
    "preds_test['LSTM'] = denormalize(days_prediction_test.predict(multi_lstm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train['Dense'] = denormalize(days_prediction_train.predict(multi_dense_model))\n",
    "preds_val['Dense'] = denormalize(days_prediction_val.predict(multi_dense_model))\n",
    "preds_test['Dense'] = denormalize(days_prediction_test.predict(multi_dense_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40379750501e481aa49bad2297ef11d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b97ab2fa9741debadd8ec4f5de6fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6560fba49a684e05b5280fc274024361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(gt_train)\n",
    "plt.plot(preds_train['GRU'][pred_day])\n",
    "#plt.plot(preds_train['LSTM'][pred_day])\n",
    "#plt.plot(preds_train['Dense'][pred_day])\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(gt_val)\n",
    "plt.plot(preds_val['GRU'][pred_day])\n",
    "#plt.plot(preds_val['LSTM'][pred_day])\n",
    "#plt.plot(preds_val['Dense'][pred_day])\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(gt_test)\n",
    "plt.plot(preds_test['GRU'][pred_day])\n",
    "#plt.plot(preds_test['LSTM'][pred_day])\n",
    "#plt.plot(preds_test['Dense'][pred_day])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_performance_metric(gt, pred):\n",
    "    mae = np.mean(np.absolute(gt - pred))\n",
    "    mape = np.mean(np.absolute(gt - pred)/gt)\n",
    "    mean = np.mean(gt)\n",
    "    nse = 1.-np.sum((gt - pred)**2) /  np.sum((gt - mean)**2)\n",
    "    return { 'mae': mae, 'nse': nse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_train_performance = {}\n",
    "daily_val_performance = {}\n",
    "daily_test_performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_names = ['GRU', 'LSTM', 'Dense']\n",
    "model_names = ['GRU']\n",
    "prefix = 'awsbo_'\n",
    "\n",
    "if daily == True:\n",
    "    pred_slice = slice(days_prediction_val.in_days+pred_day, None)\n",
    "else:\n",
    "    pred_slice = slice((days_prediction_val.in_days+pred_day)*24, None)\n",
    "\n",
    "for model_name in model_names:\n",
    "    daily_train_performance[prefix+model_name] = daily_performance_metric(gt_train[pred_slice], preds_train[model_name][pred_day][pred_slice])\n",
    "    daily_val_performance[prefix+model_name] = daily_performance_metric(gt_val[pred_slice], preds_val[model_name][pred_day][pred_slice])\n",
    "    daily_test_performance[prefix+model_name] = daily_performance_metric(gt_test[pred_slice], preds_test[model_name][pred_day][pred_slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5060862508056738"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_val_performance[prefix+'GRU']['nse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a3ca868d4f43d8aafa08d2b6d3003d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae57e6e6c444e2ba8202f2edb75ce5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(daily_test_performance))\n",
    "width = 0.3\n",
    "\n",
    "\n",
    "#metric_name = 'mean_absolute_error'\n",
    "#metric_index = multi_linear_model.metrics_names.index('mean_absolute_error')\n",
    "metric_index = 'mae'\n",
    "#metric_index = 'mae'\n",
    "train_mae = [v[metric_index] for v in daily_train_performance.values()]\n",
    "val_mae = [v[metric_index] for v in daily_val_performance.values()]\n",
    "test_mae = [v[metric_index] for v in daily_test_performance.values()]\n",
    "#test_mae = [v[metric_index] for v in multi_performance.values()]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x - 0.32, train_mae, width, label='Train')\n",
    "plt.bar(x, val_mae, width, label='Validation')\n",
    "plt.bar(x + 0.32, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=daily_test_performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel(f'MAE (average over all times and outputs)')\n",
    "_ = plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#metric_name = 'mean_absolute_error'\n",
    "#metric_index = multi_linear_model.metrics_names.index('mean_absolute_error')\n",
    "metric_index = 'nse'\n",
    "#metric_index = 'mae'\n",
    "train_nse = [v[metric_index] for v in daily_train_performance.values()]\n",
    "val_nse = [v[metric_index] for v in daily_val_performance.values()]\n",
    "test_nse = [v[metric_index] for v in daily_test_performance.values()]\n",
    "#test_mae = [v[metric_index] for v in multi_performance.values()]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x - 0.32, train_nse, width, label='Train')\n",
    "plt.bar(x, val_nse, width, label='Validation')\n",
    "plt.bar(x + 0.32, test_nse, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=daily_test_performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel(f'NSE (average over all times and outputs)')\n",
    "_ = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 4\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 4\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['GRU'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['LSTM'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 0\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['GRU'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['LSTM'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 0\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['GRU'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)\n",
    "mae, mape, nse = daily_performance_metric(gt[pred_slice], denorm_preds['LSTM'][pred_day][pred_slice])\n",
    "print('mae =', mae, 'mape =', mape, 'nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 0\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae = np.mean(np.absolute(gt[pred_slice] - (denorm_preds['GRU'][pred_day])[pred_slice]))\n",
    "print(mae)\n",
    "mape = np.mean(np.absolute((gt[pred_slice] - denorm_preds['GRU'][pred_day][pred_slice])/gt[pred_slice]))\n",
    "print(mape)\n",
    "mean = np.mean(gt[pred_slice])\n",
    "nse = 1.-np.sum((gt[pred_slice] - denorm_preds['GRU'][pred_day][pred_slice])**2) /  np.sum((gt[pred_slice] - mean)**2)\n",
    "print('nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_day = 0\n",
    "pred_slice = slice(days_prediction.in_days+pred_day, None)\n",
    "\n",
    "mae = np.mean(np.absolute(gt[pred_slice] - (denorm_preds['LSTM'][pred_day])[pred_slice]))\n",
    "print(mae)\n",
    "mape = np.mean(np.absolute((gt[pred_slice] - denorm_preds['LSTM'][pred_day][pred_slice])/gt[pred_slice]))\n",
    "print(mape)\n",
    "mean = np.mean(gt[pred_slice])\n",
    "nse = 1.-np.sum((gt[pred_slice] - denorm_preds['LSTM'][pred_day][pred_slice])**2) /  np.sum((gt[pred_slice] - mean)**2)\n",
    "print('nse =', nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=24*10\n",
    "#width=3\n",
    "channels=8\n",
    "kernel_size=3\n",
    "in_data = keras.layers.Input(shape=(width, channels))\n",
    "x = in_data\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "x = keras.layers.MaxPooling1D(padding='same')(x)\n",
    "#out_data = keras.layers.Conv1D(32, kernel_size=(kernel_size,), strides=1)(x)\n",
    "out_data = x\n",
    "\n",
    "model = keras.models.Model(inputs=in_data, outputs=out_data)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8-7+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128*150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
