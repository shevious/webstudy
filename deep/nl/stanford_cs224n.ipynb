{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "described-treatment",
   "metadata": {},
   "source": [
    "# Lecture 1\n",
    "\n",
    "[CS224n: Natural Language Processing with Deep Learning(Winter 2019)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tutorial-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kotech/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "female-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: good\n",
      "noun: good, goodness\n",
      "noun: good, goodness\n",
      "noun: commodity, trade_good, good\n",
      "adj: good\n",
      "adj (s): full, good\n",
      "adj: good\n",
      "adj (s): estimable, good, honorable, respectable\n",
      "adj (s): beneficial, good\n",
      "adj (s): good\n",
      "adj (s): good, just, upright\n",
      "adj (s): adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adj (s): good\n",
      "adj (s): dear, good, near\n",
      "adj (s): dependable, good, safe, secure\n",
      "adj (s): good, right, ripe\n",
      "adj (s): good, well\n",
      "adj (s): effective, good, in_effect, in_force\n",
      "adj (s): good\n",
      "adj (s): good, serious\n",
      "adj (s): good, sound\n",
      "adj (s): good, salutary\n",
      "adj (s): good, honest\n",
      "adj (s): good, undecomposed, unspoiled, unspoilt\n",
      "adj (s): good\n",
      "adv: well, good\n",
      "adv: thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses = {'n': 'noun', 'v': 'verb', 's':'adj (s)', 'a': 'adj', 'r': 'adv'}\n",
    "for synset in wn.synsets('good'):\n",
    "    print('{}: {}'.format(poses[synset.pos()], ', '.join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparative-degree",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-asbestos",
   "metadata": {},
   "source": [
    "# Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-pontiac",
   "metadata": {},
   "source": [
    "SVD\n",
    "\n",
    "$X$ Coocurrence matrix  \n",
    "\n",
    "$X = U\\Sigma V^t$  \n",
    "$(V,V) = (V,V)(V,V)(V,V)$  \n",
    "\n",
    "Use only $k$ vectors and eigenvalues.  \n",
    "$(V,k)(k,k)(k,V)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-creativity",
   "metadata": {},
   "source": [
    "**Negative sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-practice",
   "metadata": {},
   "source": [
    "[Word Vectors I: Introduction, SVD and Word2Vec](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes01-wordvecs1.pdf)  \n",
    "\n",
    "[Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) \n",
    "\n",
    "[Word2Vec의 학습 방식](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/)  \n",
    "\n",
    "[wevi: word embedding visual inspector](https://ronxin.github.io/wevi/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-repair",
   "metadata": {},
   "source": [
    "**GloVe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-circumstances",
   "metadata": {},
   "source": [
    "(1) [Word Vectors II: GloVe, Evaluation and Training 2](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes02-wordvecs2.pdf)  \n",
    "\n",
    "(2) [Lecture slide 2](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture02-wordvecs2.pdf)  \n",
    "\n",
    "(3) [딥 러닝을 이용한 자연어 처리 입문 05)글로브(GloVe)](https://wikidocs.net/22885)  \n",
    "\n",
    "(4) [GloVe original paper](https://nlp.stanford.edu/projects/glove/)  \n",
    "\n",
    "(5) [GloVe를 이해해보자!](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/09/glove/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-prediction",
   "metadata": {},
   "source": [
    "(2)에 따르면 다음과 같다.  \n",
    "Q: How can we capture ratios of co-occurrence probabilities as\n",
    "linear meaning components in a word vector space?  \n",
    "\n",
    "A: Log-bilinear model:  \n",
    "$$\n",
    "w_i\\cdot w_j = \\log P(i|j)\n",
    "$$  \n",
    "with vector diffrences\n",
    "$$\n",
    "w_x\\cdot(w_a-w_b) = \\log{P(x|a)\\over P(x|b)}\n",
    "$$\n",
    "\n",
    "(3)의 설명으로 좀더 정확하게 표현해 보면..\n",
    "$$\n",
    "w_i\\cdot \\tilde{w}_k = P_{ik} = P(k|i)\n",
    "$$\n",
    "\n",
    "(4)에 따르면 $\\tilde{w},w$는 word vector이나 두 개의 다른 equivanlent set이다.  \n",
    "$$\n",
    "w_i\\cdot \\tilde{w}_k = \\log{X_{ik}} - \\log{X_i}\\\\\n",
    "w_k\\cdot \\tilde{w}_i = \\log{X_{ki}} - \\log{X_k}\n",
    "$$\n",
    "\n",
    "(5) 두 dot product를 같게 만들기 위해 bios term을 도입\n",
    "$$\n",
    "w_k\\cdot \\tilde{w}_k =  \\log{X_{ik}} - b_i - \\tilde{b}_k\n",
    "$$\n",
    "There is no satisfactory explanation about bios.    \n",
    "From  original paper (4):  \n",
    "> Next, we note that Eqn. (6) would exhibit the exchange symmetry if not for the $\\log(X_i)$ on the\n",
    "right-hand side. However, this term is independent of $k$ so it can be absorbed into a bias $b_i$ for\n",
    "$w_i$. Finally, adding an additional bias $\\tilde b_k$ for $\\tilde{w}_k$ restores the symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-jersey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
