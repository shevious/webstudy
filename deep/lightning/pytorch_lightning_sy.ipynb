{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0e5a61-55de-4e2d-90d9-dfb60063fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf421872-07f0-4ec3-9e43-e86e8b409c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchvision.models import vgg16\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4f8578-bdfa-4c06-838f-57e76f3f093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-lightning==2.2.2\n",
    "#pip install --upgrade pytorch-lightning torchmetrics\n",
    "#pip install torch torchvision pytorch-lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d769074-727d-46a7-bc87-066304e5d628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True가 출력되어야 GPU를 사용할 수 있음\n",
    "print(torch.cuda.device_count())  # 사용 가능한 GPU 개수 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7bb584-f0b6-4699-a4f4-7b3c145e650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53162b4-df9b-4607-81f3-ca203437fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        # VGG16 모델 로드\n",
    "        self.feature_extractor = vgg16(pretrained=True)\n",
    "        # MNIST는 1채널 이미지이므로 VGG16의 첫 번째 Conv 레이어를 수정\n",
    "        self.feature_extractor.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        # 분류를 위한 최종 레이어 수정 (MNIST는 10개 클래스)\n",
    "        self.feature_extractor.classifier[6] = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "        mnist_train = datasets.MNIST(root=\"./\", train=True, download=True, transform=transform)\n",
    "        train_loader = DataLoader(mnist_train, batch_size=32, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "        mnist_val = datasets.MNIST(root=\"./\", train=False, download=True, transform=transform)\n",
    "        val_loader = DataLoader(mnist_val, batch_size=32)\n",
    "        return val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fabb148a-856d-4626-baa7-5e3e47dfab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스화\n",
    "model = MNISTModel()\n",
    "\n",
    "# 학습 단계에서 최적의 모델을 저장하기 위한 체크포인트 콜백 설정\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", dirpath=\"model/\", filename=\"mnist-{epoch:02d}-{val_loss:.2f}\", save_top_k=1)\n",
    "\n",
    "# 트레이너 설정\n",
    "#trainer = Trainer(max_epochs=5, accelerator=\"gpu\", devices=2, callbacks=[checkpoint_callback])\n",
    "trainer = Trainer(max_epochs=5, devices=1, accelerator=\"gpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017325d0-9ee5-4a6b-81a9-fc1807f7632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2024-04-18 09:09:26.917359: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-18 09:09:26.919747: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-18 09:09:26.957651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 09:09:27.708905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name              | Type | Params\n",
      "-------------------------------------------\n",
      "0 | feature_extractor | VGG  | 134 M \n",
      "-------------------------------------------\n",
      "134 M     Trainable params\n",
      "0         Non-trainable params\n",
      "134 M     Total params\n",
      "537.201   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmttr/workspace/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n",
      "/home/tmttr/workspace/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=79` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a2811efda04106a85715e78f685270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmttr/workspace/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46daefde-b930-4615-977c-38afbece6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증 손실에 대한 기록을 가져와서 그래프로 그림\n",
    "train_loss = trainer.callback_metrics[\"train_loss\"]\n",
    "val_loss = trainer.callback_metrics[\"val_loss\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91fc6ee-51b1-48c3-87b4-7dd4383af049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kotech/venv-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/kotech/venv-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "/home/kotech/venv-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | layer_1 | Linear | 100 K \n",
      "1 | layer_2 | Linear | 33.0 K\n",
      "2 | layer_3 | Linear | 2.6 K \n",
      "-----------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n",
      "/home/kotech/venv-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b2ec5bd67145328defe2e2af87cc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotech/venv-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "class MNISTModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = Linear(28 * 28, 128)\n",
    "        self.layer_2 = Linear(128, 256)\n",
    "        self.layer_3 = Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the image\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.layer_3(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n",
    "        MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transforms.ToTensor())\n",
    "        train_loader = DataLoader(mnist_train, batch_size=64)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        mnist_val = MNIST(os.getcwd(), train=False, download=False, transform=transforms.ToTensor())\n",
    "        val_loader = DataLoader(mnist_val, batch_size=64)\n",
    "        return val_loader\n",
    "\n",
    "model = MNISTModel()\n",
    "\n",
    "# 2개의 GPU를 사용하여 학습 준비\n",
    "trainer = Trainer(max_epochs=5, devices=2, accelerator=\"gpu\")\n",
    "trainer.fit(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60fb2c-ec07-40ff-887c-024040c34048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
