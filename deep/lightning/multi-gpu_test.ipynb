{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb424d24-0b17-41b4-84e1-44474eacb80f",
   "metadata": {},
   "source": [
    "**not working**  \n",
    "\n",
    "https://github.com/Lightning-AI/pytorch-lightning/issues/18541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2c6efd-7bba-48eb-bc71-c59da2130dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch in /home/kotech/venv-lightning/lib/python3.8/site-packages (from timm) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /home/kotech/venv-lightning/lib/python3.8/site-packages (from timm) (0.17.2)\n",
      "Requirement already satisfied: pyyaml in /home/kotech/venv-lightning/lib/python3.8/site-packages (from timm) (6.0.1)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /home/kotech/venv-lightning/lib/python3.8/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from huggingface_hub->timm) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from huggingface_hub->timm) (23.2)\n",
      "Requirement already satisfied: requests in /home/kotech/venv-lightning/lib/python3.8/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from huggingface_hub->timm) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from huggingface_hub->timm) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torch->timm) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.3.101)\n",
      "Requirement already satisfied: numpy in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torchvision->timm) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from torchvision->timm) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/kotech/venv-lightning/lib/python3.8/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface_hub, timm\n",
      "Successfully installed huggingface_hub-0.22.2 safetensors-0.4.3 timm-0.9.16\n"
     ]
    }
   ],
   "source": [
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de62afd2-39c6-498b-b84e-9c6a1ef562cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/kotech/venv-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/kotech/venv-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | SwinTransformer  | 86.8 M\n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "86.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "86.8 M    Total params\n",
      "347.014   Total estimated model params size (MB)\n",
      "/home/kotech/venv-lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from timm import create_model\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class ToyLitModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = F.interpolate(x, size=224)\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        train_set = torchvision.datasets.CIFAR10(\n",
    "            root=\"~/data\", train=True, download=True\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "        )\n",
    "\n",
    "        batch_size = 32\n",
    "\n",
    "        train_set = torchvision.datasets.CIFAR10(\n",
    "            root=\"~/data\", train=True, transform=transform\n",
    "        )\n",
    "        # train_set = torch.utils.data.Subset(train_set, list(range(1000)))\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_set, batch_size=batch_size, shuffle=True, num_workers=4\n",
    "        )\n",
    "        return train_loader\n",
    "        \n",
    "    def setup_(self, stage: str) -> None:\n",
    "        print('### setup')\n",
    "        self.model = ToyLitModel(create_model(\"swin_base_patch4_window7_224\", num_classes=10))\n",
    "        #self.model = create_model(\"swin_base_patch4_window7_224\", num_classes=10)\n",
    "\n",
    "\n",
    "def run():\n",
    "    model = create_model(\"swin_base_patch4_window7_224\", num_classes=10)\n",
    "    # model = create_model(\"resnet18\", num_classes=10)\n",
    "    task = ToyLitModel(model)\n",
    "    #trainer = pl.Trainer(max_epochs=1, strategy=\"ddp_fork\", devices=2)\n",
    "    trainer = pl.Trainer(max_epochs=1, strategy=\"ddp_notebook\", devices=1)\n",
    "    trainer.fit(task)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70b965-181d-4e91-8cf0-861c3840027d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b6cc3-23ea-460d-81f5-dd81ee289153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from lightning.pytorch import LightningModule, Trainer\n",
    "\n",
    "\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class BoringModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self(batch).sum()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return your dataloader here\n",
    "        return DataLoader(RandomDataset(32, 64), batch_size=2)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "def run():\n",
    "    model = BoringModel()\n",
    "    trainer = Trainer(max_epochs=1, strategy=\"ddp_notebook\", devices=2)\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ef9fd9-4d85-42ba-82bf-023a358754e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | layer1   | Sequential | 728   \n",
      "1 | layer2   | Sequential | 39.3 K\n",
      "2 | drop_out | Dropout    | 0     \n",
      "3 | fc1      | Linear     | 2.7 M \n",
      "4 | fc2      | Linear     | 10.0 K\n",
      "----------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.180    Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 1 terminated with signal SIGSEGV",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m     devices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-lightning/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-lightning/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/venv-lightning/lib/python3.8/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/venv-lightning/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:140\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exitcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    139\u001b[0m     name \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mSignals(\u001b[38;5;241m-\u001b[39mexitcode)\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[1;32m    142\u001b[0m         error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    143\u001b[0m         error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    144\u001b[0m         exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    145\u001b[0m         signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    150\u001b[0m         error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    151\u001b[0m         error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    152\u001b[0m         exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    153\u001b[0m     )\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with signal SIGSEGV"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from lightning.pytorch import LightningModule, Trainer\n",
    " \n",
    "class LitCNN(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define your model architecture here\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 28, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(28, 56, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 56, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        return self.fc2(out)\n",
    " \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    " \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    " \n",
    "    def train_dataloader(self):\n",
    "        # Transformations applied on each image\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    " \n",
    "        # Download training data from open datasets.\n",
    "        dataset = MNIST(os.getcwd(), download=True, train=True, transform=transform)\n",
    "        # DataLoader\n",
    "        return DataLoader(dataset, batch_size=64, num_workers=4, shuffle=True)\n",
    " \n",
    "\n",
    "def run():\n",
    "    model = LitCNN()\n",
    "    # Setup the trainer\n",
    "    trainer = Trainer(accelerator=\"gpu\",\n",
    "        devices=[0, 1], max_epochs=5)\n",
    "    # Train the model\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df7b5a-fbb0-42ce-b530-ea36bb28b52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
