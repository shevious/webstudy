{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ed814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import load_yaml, save_yaml, get_logger\n",
    "\n",
    "from modules.earlystoppers import EarlyStopper\n",
    "from modules.recorders import Recorder\n",
    "from modules.datasets import QADataset\n",
    "from modules.trainer import Trainer\n",
    "\n",
    "from modules.optimizers import get_optimizer\n",
    "from modules.metrics import get_metric\n",
    "from modules.losses import get_loss\n",
    "from models.utils import get_model\n",
    "\n",
    "from transformers import ElectraTokenizerFast, ElectraTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bd6839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug True\n"
     ]
    }
   ],
   "source": [
    "# Root directory\n",
    "PROJECT_DIR = os.path.dirname('.')\n",
    "\n",
    "# Load config\n",
    "config_path = os.path.join(PROJECT_DIR, 'config', 'train_config.yml')\n",
    "config = load_yaml(config_path)\n",
    "\n",
    "# Train Serial\n",
    "kst = timezone(timedelta(hours=9))\n",
    "train_serial = datetime.now(tz=kst).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Recorder directory\n",
    "DEBUG = config['TRAINER']['debug']\n",
    "print(f'debug {DEBUG}')\n",
    "\n",
    "RECORDER_DIR = os.path.join(PROJECT_DIR, 'results', 'train', train_serial)\n",
    "os.makedirs(RECORDER_DIR, exist_ok=True)\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = config['DIRECTORY']['dataset']\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(config['TRAINER']['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(config['TRAINER']['seed'])\n",
    "random.seed(config['TRAINER']['seed'])\n",
    "\n",
    "# GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(config['TRAINER']['gpu'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecc4204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kotech/workspace/webstudy/deep/chal2022/results/train/20220612_020902\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "00. Set Logger\n",
    "\"\"\"\n",
    "logger = get_logger(name='train', dir_=RECORDER_DIR, stream=False)\n",
    "logger.info(f\"Set Logger {RECORDER_DIR}\")\n",
    "print('/'.join(logger.handlers[0].baseFilename.split('/')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da6b7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraTokenizerFast\n",
      "monologg/koelectra-base-v3-discriminator\n",
      "PreTrainedTokenizerFast(name_or_path='monologg/koelectra-base-v3-discriminator', vocab_size=35000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "01. Load data\n",
    "\"\"\"\n",
    "# Load tokenizer\n",
    "\n",
    "tokenizer_dict = {'ElectraTokenizerFast': ElectraTokenizerFast, 'ElectraTokenizer': ElectraTokenizer }\n",
    "\n",
    "#tokenizer = tokenizer_dict[config['TRAINER']['tokenizer']].from_pretrained(config['TRAINER']['pretrained'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['TRAINER']['pretrained'])\n",
    "print(config['TRAINER']['tokenizer'])\n",
    "print(config['TRAINER']['pretrained'])\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29278fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acea165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f249011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3627877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json('dataset/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb613d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(mode, data, debug=False):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    question_ids = []\n",
    "    answers = []\n",
    "    data = data.copy()\n",
    "\n",
    "    # train - val split\n",
    "    if mode == 'train':\n",
    "        data['data'] = data['data'][:-1*int(len(data['data'])*0.1)]\n",
    "    elif mode == 'val':\n",
    "        data['data'] = data['data'][-1*int(len(data['data'])*0.1):]\n",
    "\n",
    "\n",
    "    till = 100 if debug else len(data['data'])\n",
    "\n",
    "\n",
    "    for group in data['data'][:till]:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                if mode == 'test':\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    question_ids.append(qa['question_id'])\n",
    "                else: # train or val\n",
    "                    for ans in qa['answers']:\n",
    "                        contexts.append(context)\n",
    "                        questions.append(question)\n",
    "\n",
    "                        if qa['is_impossible']:\n",
    "                            answers.append({'text':[''],'answer_start':[-1]})\n",
    "                        else:\n",
    "                            #answers.append([ans])\n",
    "                            answers.append({'text':[ans['text']],'answer_start':[ans['answer_start']]})\n",
    "    # return formatted data lists\n",
    "    return contexts, questions, answers, question_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39818630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "#import pandas as pd\n",
    "contexts, questions, answers, question_ids = read_squad('train', data)\n",
    "ids = []\n",
    "for i in range(len(contexts)):\n",
    "    ids.append(i)\n",
    "train_dataset = Dataset.from_dict({'id':ids, 'context': contexts, 'question':questions,\n",
    "                                   'answers':answers})\n",
    "    \n",
    "#df = pd.DataFrame({'context': contexts, 'question': questions, 'answers': answers})\n",
    "#df.to_csv('train.csv', index=False)\n",
    "contexts, questions, answers, question_ids = read_squad('val', data)\n",
    "ids = []\n",
    "for i in range(len(contexts)):\n",
    "    ids.append(i)\n",
    "val_dataset = Dataset.from_dict({'id':ids, 'context': contexts, 'question':questions,\n",
    "                                   'answers':answers})\n",
    "#df = pd.DataFrame({'context': contexts, 'question': questions, 'answers': answers})\n",
    "#df.to_csv('val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9521d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "datasets = DatasetDict({'train':train_dataset, 'validation':val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ad20b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'context': '이 글에서는 제안한 기술경쟁력 평가 방법의 특징은 두 가지로 요약된다. 첫째, 현재의 경쟁력보다는 미래의 경쟁력을 평가하도록 평가의 대상을 확대해야 한다. 지금까지의 경쟁력 평가가 현위치를 중심으로 한 것이었다면, 앞으로의 대안은 과정과 경로를 중시하는 것이 되어야 한다는 것이다. 사실 지식정보화사회에서 가진 것이란 허망한 것이다. 현재의 기술수준이 높더라고, 이를 끊임없이 개량하고 새로운 기술을 발전시킬 수 있는 능력이 없다면 미래의 전망은 어둡다. 기술이나 정보는 실물 자산보다 진부화율이 매우 높다. 둘째, 기술경쟁력 평가를 순위를 매기는 작업이 아니라 이를 통하여 장점과 단점을 파악하는 과정으로 활용하여야 한다. 순위는 대중적인 흥미는 끌 수 있지만, 그것으로부터 교훈이 도출되는 것은 아니다. 기술경쟁력의 평가를 통해서 외국시스템의 장점을 배우고, 한국 시스템의 단점을 교정하는 대안이 발견될 수 있다. 경쟁이란 남과 비교하는 것을 의미한다. 내가 아무리 잘 하더라도 남이 나보다 더 잘 한다면 경쟁에서 진다. 그래서 경쟁은 각박하지만, 과학기술은 경쟁을 피할 수 있는 분야가 아니다. 경쟁 상대국을 연구하고, 경쟁 상대국을 앞설 수 있는 방안을 강구하기 위해서 기술경쟁력에 관한 연구가 더욱 활발해져야 할 것이다.',\n",
       " 'question': '경쟁 상대국을 연구하고 경쟁 상대국을 앞설 수 있는 방법을 연구하기 위해서 더욱 활발해져야 할 연구는 뭐지',\n",
       " 'answers': {'answer_start': [603], 'text': ['기술경쟁력']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e88a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_length = 384 # The maximum length of a feature (question and context)\n",
    "#doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "max_length = 512 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7aa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    #tokenized_examples['input_ids'] = torch.Tensor(tokenized_examples['input_ids'])\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5a787b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c215097e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62759700048b43f08e16d36516903936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381201edb6f5495f866b4bac658cc9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36a6c135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7541ec4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "704e3b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train'][0]['start_positions'])\n",
    "print(tokenized_datasets['train'][0]['end_positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66f09d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'기술경쟁'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets['train'][0]['input_ids'][347:350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dc011d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 경쟁 상대국을 연구하고 경쟁 상대국을 앞설 수 있는 방법을 연구하기 위해서 더욱 활발해져야 할 연구는 뭐지 [SEP] 이 글에서는 제안한 기술경쟁력 평가 방법의 특징은 두 가지로 요약된다. 첫째, 현재의 경쟁력보다는 미래의 경쟁력을 평가하도록 평가의 대상을 확대해야 한다. 지금까지의 경쟁력 평가가 현위치를 중심으로 한 것이었다면, 앞으로의 대안은 과정과 경로를 중시하는 것이 되어야 한다는 것이다. 사실 지식정보화사회에서 가진 것이란 허망한 것이다. 현재의 기술수준이 높더라고, 이를 끊임없이 개량하고 새로운 기술을 발전시킬 수 있는 능력이 없다면 미래의 전망은 어둡다. 기술이나 정보는 실물 자산보다 진부화율이 매우 높다. 둘째, 기술경쟁력 평가를 순위를 매기는 작업이 아니라 이를 통하여 장점과 단점을 파악하는 과정으로 활용하여야 한다. 순위는 대중적인 흥미는 끌 수 있지만, 그것으로부터 교훈이 도출되는 것은 아니다. 기술경쟁력의 평가를 통해서 외국시스템의 장점을 배우고, 한국 시스템의 단점을 교정하는 대안이 발견될 수 있다. 경쟁이란 남과 비교하는 것을 의미한다. 내가 아무리 잘 하더라도 남이 나보다 더 잘 한다면 경쟁에서 진다. 그래서 경쟁은 각박하지만, 과학기술은 경쟁을 피할 수 있는 분야가 아니다. 경쟁 상대국을 연구하고, 경쟁 상대국을 앞설 수 있는 방안을 강구하기 위해서 기술경쟁력에 관한 연구가 더욱 활발해져야 할 것이다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e36791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b36eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c1c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b638d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "294e5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__ (self, data_dir: str, tokenizer, max_seq_len: int, mode = 'train', debug = False):\n",
    "        self.mode = mode\n",
    "        self.data = load_json(data_dir)\n",
    "\n",
    "        # self.encodings = encodings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.debug = debug\n",
    "        self.pad_on_right = tokenizer.padding_side == \"right\"\n",
    "        if mode == 'test':\n",
    "            self.encodings, self.question_ids = self.preprocess()\n",
    "        else:\n",
    "            self.encodings, self.answers = self.preprocess()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        print('index = ', index)\n",
    "        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def preprocess(self):\n",
    "        contexts, questions, answers, question_ids = self.read_squad()\n",
    "        if self.mode == 'test':\n",
    "            encodings = self.tokenizer(\n",
    "                questions if self.pad_on_right else contexts,\n",
    "                contexts if self.pad_on_right else questions,\n",
    "                truncation=True, max_length = self.max_seq_len, padding=True)\n",
    "            return encodings, question_ids\n",
    "        else: # train or val\n",
    "            self.add_end_idx(answers, contexts)\n",
    "            encodings = self.tokenizer(\n",
    "                questions if self.pad_on_right else contexts,\n",
    "                contexts if self.pad_on_right else questions,\n",
    "                truncation=True, max_length = self.max_seq_len, padding=True)\n",
    "            self.add_token_positions(encodings, answers)\n",
    "\n",
    "            return encodings, answers\n",
    "\n",
    "    def read_squad(self):\n",
    "        contexts = []\n",
    "        questions = []\n",
    "        question_ids = []\n",
    "        answers = []\n",
    "\n",
    "        # train - val split\n",
    "        if self.mode == 'train':\n",
    "            self.data['data'] = self.data['data'][:-1*int(len(self.data['data'])*0.1)]\n",
    "        elif self.mode == 'val':\n",
    "            self.data['data'] = self.data['data'][-1*int(len(self.data['data'])*0.1):]\n",
    "\n",
    "\n",
    "        till = 100 if self.debug else len(self.data['data'])\n",
    "\n",
    "        for group in self.data['data'][:till]:\n",
    "            for passage in group['paragraphs']:\n",
    "                context = passage['context']\n",
    "                for qa in passage['qas']:\n",
    "                    question = qa['question']\n",
    "                    if self.mode == 'test':\n",
    "                        contexts.append(context)\n",
    "                        questions.append(question)\n",
    "                        question_ids.append(qa['question_id'])\n",
    "                    else: # train or val\n",
    "                        for ans in qa['answers']:\n",
    "                            contexts.append(context)\n",
    "                            questions.append(question)\n",
    "\n",
    "                            if qa['is_impossible']:\n",
    "                                answers.append({'text':'','answer_start':-1})\n",
    "                            else:\n",
    "                                answers.append(ans)\n",
    "\n",
    "        # return formatted data lists\n",
    "        return contexts, questions, answers, question_ids\n",
    "\n",
    "    def add_end_idx(self, answers, contexts):\n",
    "        for answer, context in zip(answers, contexts):\n",
    "            gold_text = answer['text']\n",
    "            start_idx = answer['answer_start']\n",
    "            end_idx = start_idx + len(gold_text)\n",
    "\n",
    "            # in case the indices are off 1-2 idxs\n",
    "            if context[start_idx:end_idx] == gold_text:\n",
    "                answer['answer_end'] = end_idx\n",
    "            else:\n",
    "                for n in [1, 2]:\n",
    "                    if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx - n\n",
    "                        answer['answer_end'] = end_idx - n\n",
    "                    elif context[start_idx+n:end_idx+n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx + n\n",
    "                        answer['answer_end'] = end_idx + n\n",
    "\n",
    "    def add_token_positions(self, encodings, answers):\n",
    "        # should use Fast tokenizer\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        for i in range(len(answers)):\n",
    "            if answers[i]['answer_start'] == -1:\n",
    "                # set [CLS] token as answer if is_impossible\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(1)\n",
    "            else:\n",
    "                start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "\n",
    "                assert 'answer_end' in answers[i].keys(), f'no answer_end at {i}'\n",
    "                end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "\n",
    "            # answer passage truncated\n",
    "            if start_positions[-1] is None:\n",
    "                start_positions[-1] = tokenizer.model_max_length\n",
    "            # end position cannot be found, shift until found\n",
    "            shift = 1\n",
    "            while end_positions[-1] is None:\n",
    "                end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
    "                shift += 1\n",
    "        # char-based -> token based\n",
    "        encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e1223348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__ (self, data_dir: str, tokenizer, max_seq_len: int, mode = 'train', debug = False):\n",
    "        self.mode = mode\n",
    "        self.data = load_json(data_dir)\n",
    "        \n",
    "        # self.encodings = encodings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.debug = debug\n",
    "\n",
    "        self.pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "        if mode == 'test':\n",
    "            self.encodings, self.question_ids = self.preprocess()\n",
    "        else:\n",
    "            self.encodings, self.answers = self.preprocess()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "\n",
    "    \n",
    "    def preprocess(self):\n",
    "        pad_on_right = self.pad_on_right\n",
    "        doc_stride = 256\n",
    "        contexts, questions, answers, question_ids = self.read_squad()\n",
    "        self.contexts = contexts\n",
    "        print('len contexts =', len(contexts))\n",
    "        if self.mode == 'test':\n",
    "            encodings = self.tokenizer(\n",
    "                questions if pad_on_right else contexts,\n",
    "                contexts if pad_on_right else questions,\n",
    "                truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "                max_length = self.max_seq_len,\n",
    "                stride=doc_stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\")\n",
    "            return encodings, question_ids\n",
    "        else: # train or val\n",
    "            self.add_end_idx(answers, contexts)\n",
    "            encodings = self.tokenizer(\n",
    "                questions if pad_on_right else contexts,\n",
    "                contexts if pad_on_right else questions,\n",
    "                truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "                max_length = self.max_seq_len,\n",
    "                stride=doc_stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\")\n",
    "            self.add_token_positions(encodings, answers)\n",
    "            for i in range(11):\n",
    "                print(contexts[i][answers[i]['answer_start']:answers[i]['answer_end']])\n",
    "        \n",
    "            return encodings, answers\n",
    "    \n",
    "    def read_squad(self):\n",
    "        contexts = []\n",
    "        questions = []\n",
    "        question_ids = []\n",
    "        answers = []\n",
    "        \n",
    "        # train - val split\n",
    "        if self.mode == 'train':\n",
    "            self.data['data'] = self.data['data'][0:10]\n",
    "            #self.data['data'] = self.data['data'][:-1*int(len(self.data['data'])*0.1)]\n",
    "        elif self.mode == 'val':\n",
    "            self.data['data'] = self.data['data'][-1*int(len(self.data['data'])*0.1):]\n",
    "        print('count=',len(self.data['data']))\n",
    "        \n",
    "        till = 100 if self.debug else len(self.data['data'])\n",
    "        \n",
    "        for group in self.data['data'][:till]:\n",
    "            for passage in group['paragraphs']:\n",
    "                context = passage['context']\n",
    "                for qa in passage['qas']:\n",
    "                    question = qa['question']\n",
    "                    if self.mode == 'test':\n",
    "                        contexts.append(context)\n",
    "                        questions.append(question)\n",
    "                        question_ids.append(qa['question_id'])\n",
    "                    else: # train or val\n",
    "                        for ans in qa['answers']:\n",
    "                            contexts.append(context)\n",
    "                            questions.append(question)\n",
    "\n",
    "                            if qa['is_impossible']:\n",
    "                                answers.append({'text':'','answer_start':-1})\n",
    "                            else:\n",
    "                                answers.append(ans)\n",
    "                \n",
    "        # return formatted data lists\n",
    "        return contexts, questions, answers, question_ids\n",
    "    \n",
    "    def add_end_idx(self, answers, contexts):\n",
    "        for answer, context in zip(answers, contexts):\n",
    "            gold_text = answer['text']\n",
    "            start_idx = answer['answer_start']\n",
    "            end_idx = start_idx + len(gold_text)\n",
    "\n",
    "            # in case the indices are off 1-2 idxs\n",
    "            if context[start_idx:end_idx] == gold_text:\n",
    "                answer['answer_end'] = end_idx\n",
    "            else:\n",
    "                for n in [1, 2]:\n",
    "                    if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx - n\n",
    "                        answer['answer_end'] = end_idx - n\n",
    "                    elif context[start_idx+n:end_idx+n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx + n\n",
    "                        answer['answer_end'] = end_idx + n\n",
    "\n",
    "    def add_token_positions(self, tokenized_examples, answers):\n",
    "        examples = self.data\n",
    "        pad_on_right = self.pad_on_right\n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # Let's label those examples!\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            if (i == 10):\n",
    "                print('i, sample_index =', i, sample_index)\n",
    "                print('offsets =', offsets)\n",
    "                print('contexts[i]=', self.contexts[i])\n",
    "                print('len(offsets)=', len(offsets))\n",
    "                print(tokenizer.convert_ids_to_tokens(tokenized_examples.input_ids[10]))\n",
    "                #for j in range(len(offsets)):\n",
    "                    #print(self.contexts[i][offsets[j][0]:offsets[j][1]])\n",
    "            #answers = examples[\"answers\"][sample_index]\n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            #if len(answers[sample_index][\"answer_start\"]) == 0:\n",
    "                #start_positions.append(cls_index)\n",
    "                #end_positions.append(cls_index)\n",
    "            #else:\n",
    "            if True:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                #start_char = answers[sample_index][\"answer_start\"][0]\n",
    "                #end_char = start_char + len(answers[\"text\"][0])\n",
    "                start_char = answers[sample_index][\"answer_start\"]\n",
    "                end_char = start_char + len(answers[sample_index][\"text\"])\n",
    "                if i == 10: \n",
    "                    print('start_char =', start_char)\n",
    "                    print('end_char =', end_char)\n",
    "                    print(self.contexts[sample_index][start_char:end_char])\n",
    "\n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "                #print(sequence_ids)\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    start_positions.append(cls_index)\n",
    "                    end_positions.append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    start_positions.append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    end_positions.append(token_end_index + 1)\n",
    "\n",
    "        # char-based -> token based\n",
    "        tokenized_examples.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0fc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__ (self, data_dir: str, tokenizer, max_seq_len: int, mode = 'train', debug = False):\n",
    "        self.mode = mode\n",
    "        self.data = load_json(data_dir)\n",
    "        \n",
    "        # self.encodings = encodings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.debug = debug\n",
    "\n",
    "        self.pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "        if mode == 'test':\n",
    "            self.encodings, self.question_ids = self.preprocess()\n",
    "        else:\n",
    "            self.encodings, self.answers = self.preprocess()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "\n",
    "    \n",
    "    def preprocess(self):\n",
    "        pad_on_right = self.pad_on_right\n",
    "        doc_stride = 256\n",
    "        contexts, questions, answers, question_ids = self.read_squad()\n",
    "        if self.mode == 'test':\n",
    "            encodings = self.tokenizer(\n",
    "                questions if pad_on_right else contexts,\n",
    "                contexts if pad_on_right else questions,\n",
    "                truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "                max_length = self.max_seq_len,\n",
    "                stride=doc_stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\")\n",
    "            return encodings, question_ids\n",
    "        else: # train or val\n",
    "            self.add_end_idx(answers, contexts)\n",
    "            encodings = self.tokenizer(\n",
    "                questions if pad_on_right else contexts,\n",
    "                contexts if pad_on_right else questions,\n",
    "                truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "                max_length = self.max_seq_len,\n",
    "                stride=doc_stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\")\n",
    "            self.add_token_positions(encodings, answers)\n",
    "        \n",
    "            return encodings, answers\n",
    "    \n",
    "    def read_squad(self):\n",
    "        contexts = []\n",
    "        questions = []\n",
    "        question_ids = []\n",
    "        answers = []\n",
    "        \n",
    "        # train - val split\n",
    "        if self.mode == 'train':\n",
    "            #self.data['data'] = self.data['data'][0:100]\n",
    "            self.data['data'] = self.data['data'][:-1*int(len(self.data['data'])*0.1)]\n",
    "        elif self.mode == 'val':\n",
    "            self.data['data'] = self.data['data'][-1*int(len(self.data['data'])*0.1):]\n",
    "        \n",
    "        till = 100 if self.debug else len(self.data['data'])\n",
    "        \n",
    "        for group in self.data['data'][:till]:\n",
    "            for passage in group['paragraphs']:\n",
    "                context = passage['context']\n",
    "                for qa in passage['qas']:\n",
    "                    question = qa['question']\n",
    "                    if self.mode == 'test':\n",
    "                        contexts.append(context)\n",
    "                        questions.append(question)\n",
    "                        question_ids.append(qa['question_id'])\n",
    "                    else: # train or val\n",
    "                        for ans in qa['answers']:\n",
    "                            contexts.append(context)\n",
    "                            questions.append(question)\n",
    "\n",
    "                            if qa['is_impossible']:\n",
    "                                answers.append({'text':'','answer_start':-1})\n",
    "                            else:\n",
    "                                answers.append(ans)\n",
    "                \n",
    "        # return formatted data lists\n",
    "        return contexts, questions, answers, question_ids\n",
    "    \n",
    "    def add_end_idx(self, answers, contexts):\n",
    "        for answer, context in zip(answers, contexts):\n",
    "            gold_text = answer['text']\n",
    "            start_idx = answer['answer_start']\n",
    "            end_idx = start_idx + len(gold_text)\n",
    "\n",
    "            # in case the indices are off 1-2 idxs\n",
    "            if context[start_idx:end_idx] == gold_text:\n",
    "                answer['answer_end'] = end_idx\n",
    "            else:\n",
    "                for n in [1, 2]:\n",
    "                    if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx - n\n",
    "                        answer['answer_end'] = end_idx - n\n",
    "                    elif context[start_idx+n:end_idx+n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx + n\n",
    "                        answer['answer_end'] = end_idx + n\n",
    "\n",
    "    def add_token_positions(self, tokenized_examples, answers):\n",
    "        examples = self.data\n",
    "        pad_on_right = self.pad_on_right\n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # Let's label those examples!\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            #answers = examples[\"answers\"][sample_index]\n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            #if len(answers[sample_index][\"answer_start\"]) == 0:\n",
    "                #start_positions.append(cls_index)\n",
    "                #end_positions.append(cls_index)\n",
    "            #else:\n",
    "            if True:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                #start_char = answers[sample_index][\"answer_start\"][0]\n",
    "                #end_char = start_char + len(answers[\"text\"][0])\n",
    "                start_char = answers[sample_index][\"answer_start\"]\n",
    "                end_char = start_char + len(answers[sample_index][\"text\"])\n",
    "\n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "                #print(sequence_ids)\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    start_positions.append(cls_index)\n",
    "                    end_positions.append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    start_positions.append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    end_positions.append(token_end_index + 1)\n",
    "\n",
    "        # char-based -> token based\n",
    "        tokenized_examples.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eca3422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_dataset = QADataset(data_dir=os.path.join(DATA_DIR, 'train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6c1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b2342cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd84958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806d4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7d036d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기술경쟁\n",
      "기술경쟁력\n",
      "평균 32\n",
      "평균 32위\n",
      "종합과학기술력\n",
      "종합과학기술력 평가\n",
      "연공서열\n",
      "연공서열제\n",
      "산출지\n",
      "산출지표\n",
      "기술기획의 분권\n",
      "기술기획의 분권화가\n",
      "동태적\n",
      "동태적 경쟁력\n",
      "과정과\n",
      "과정과 경로\n",
      "국가기술혁신시스템이\n",
      "국가기술혁신시스템이론\n",
      "PPP frame\n",
      "PPP framework\n",
      "\n",
      "학습\n",
      "과도한 순환보\n",
      "과도한 순환보직\n",
      "Port\n",
      "Porter\n",
      "표본이탈\n",
      "표본이탈률\n",
      "표본이탈\n",
      "표본이탈 횟수\n",
      "송헌\n",
      "송헌재\n",
      "심\n",
      "심영상\n",
      "이탈위험 표본관리방\n",
      "이탈위험 표본관리방법\n",
      "60\n",
      "60세\n",
      "표본이\n",
      "표본이탈\n",
      "패널데이\n",
      "패널데이터\n",
      "비례위험 회귀분석\n",
      "비례위험 회귀분석 모형\n",
      "동일표본 반복추적\n",
      "동일표본 반복추적조사\n",
      "해양공간계\n",
      "해양공간계획\n",
      "해양\n",
      "해양공간\n",
      "항거곤\n",
      "항거곤란\n",
      "\n",
      "강간\n",
      "간음 － 남녀성기결합\n",
      "간음 － 남녀성기결합설\n",
      "항거곤\n",
      "항거곤란\n",
      "준강간\n",
      "준강간죄\n",
      "항거불\n",
      "항거불능\n",
      "5년\n",
      "5년 이하\n",
      "성적 자기결정권의\n",
      "성적 자기결정권의 보호\n",
      "항거불\n",
      "항거불능\n",
      "항거곤\n",
      "항거곤란\n",
      "간\n",
      "간음\n",
      "강간\n",
      "강간죄\n",
      "1992\n",
      "1992년\n",
      "물질적 경제적\n",
      "물질적 경제적 자립\n",
      "자립지원\n",
      "자립지원 서비스\n",
      "장기가출\n",
      "장기가출자\n",
      "청소년쉼\n",
      "청소년쉼터\n",
      "일시쉼\n",
      "일시쉼터\n",
      "2012\n",
      "2012년\n",
      "가출청\n",
      "가출청소년\n",
      "##40\n",
      "##40시간\n",
      "\n",
      "미국\n",
      "유럽형사문제\n",
      "유럽형사문제위원회\n",
      "유럽평의회 사이버범죄협\n",
      "유럽평의회 사이버범죄협약\n",
      "디지털\n",
      "디지털 데이터\n",
      "\n",
      "노동\n",
      "1985\n",
      "1985년\n",
      "실질\n",
      "실질 금리\n",
      "소비자물\n",
      "소비자물가\n",
      "Acemoglu and Guerrier\n",
      "Acemoglu and Guerrieri\n",
      "청소 및 경비용역직\n",
      "청소 및 경비용역직 노동자\n",
      "삶과\n",
      "삶과환경\n",
      "2006년 1\n",
      "2006년 1월\n",
      "음식물 폐기물 수거운반 대행\n",
      "음식물 폐기물 수거운반 대행업체\n",
      "59. 7\n",
      "59. 7세\n",
      "음식물 폐기물 수집\n",
      "음식물 폐기물 수집 운반\n",
      "프랑스 지역관리\n",
      "프랑스 지역관리공사\n",
      "최고가치 ( Best Value ) 입찰\n",
      "최고가치 ( Best Value ) 입찰제\n",
      "대행수수료 지급\n",
      "대행수수료 지급방식\n",
      "2012\n",
      "2012년\n",
      "사회적 양극화\n",
      "사회적 양극화 문제\n",
      "개인의 교육수\n",
      "개인의 교육수준\n",
      "인적자본투자이\n",
      "인적자본투자이론\n",
      "일반고등\n",
      "일반고등학교\n",
      "진로성숙\n",
      "진로성숙도\n",
      "최수\n",
      "최수정\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,71):\n",
    "    print(tokenizer.decode(base_train_dataset[i]['input_ids'][base_train_dataset[i]['start_positions']:\n",
    "                                            base_train_dataset[i]['end_positions']]))\n",
    "    print(tokenizer.decode(base_train_dataset[i]['input_ids'][base_train_dataset[i]['start_positions']:\n",
    "                                            base_train_dataset[i]['end_positions']+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7dfec13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2,  6436, 10707,  4292,  7961,  4279,  4034, 16734,  4112,  2702,\n",
       "          4474,     3,    56, 11868,  9013,  4234,  6271,  4158,  4346,  4361,\n",
       "          6392,  4234,  3667,  4112, 17563,  4105,  4239, 12452,  4880,  4176,\n",
       "            18,  6421,  4129, 17563,  4105,  4034,  6379,    12, 13623, 29679,\n",
       "            13,    16,  3794, 15883,    12,    52,  8296, 17372,    13,    16,\n",
       "          2728, 10455,    12, 32526,  4078,    13,  2613,  6455,  7796,    18,\n",
       "          6379,  4007,  4271,  6271,  4007,  3243,  4292,  6736,  4279,  4034,\n",
       "         11529,  4110,  6455,  4279, 18781,  2279,  6274,  4234,  7006,  6671,\n",
       "          4292, 11757,  7796,    18,  6421,  4129,  2279,  6274,  7006,  6671,\n",
       "          4007,  4118,  3760,  4112,  7028,    19,  6839,    12,  7912, 11825,\n",
       "         16384,  4019,  7047,    19, 23234, 10571, 33553,    13,    16, 10707,\n",
       "          4162,    12,    86, 19430, 22304, 28604,  4019,  7047,    13,    16,\n",
       "          7642,    12, 19190,  6515,  4018,  7179,    13,    16,  3265,  4050,\n",
       "          4028,  4047,  7219,    12, 27529,  6378,  4059,  8032, 29573,  4019,\n",
       "          7047,  8583, 29295, 29943,    13,  3232,  6455,  7796,    18,  7028,\n",
       "            19,  6839,  4112,  6655,  4007, 28885,  6411,  4479,  4031,  6311,\n",
       "          8329,  4199,  6671, 24387,    18,  6271,  7150,  8599,  4629,  4234,\n",
       "          7028,  4084,  4142,  6671,  4007,  3258,  9670, 18991,  3755,  4815,\n",
       "            16,  6263,  2446, 25449,  4139,  6505,  6560,  4073,  9823,  4279,\n",
       "          4034, 10777, 15650,  3249, 16868,  6217,    18, 10707,  4162,  4110,\n",
       "          8648,  4044,  6271,  4112,  6829,  4110, 12790,  4176,    18,  3311,\n",
       "          4112, 10707,  4292,  9366,  4114,  4298,  6271,  4047,  6321,  4200,\n",
       "          2684,  4283,  6271,  4629,  4234,  7685,  4112,  3794,  4374,  4283,\n",
       "          6801,  4110,  6567,    18,  7642,  4112,  6436, 10707,  4292,  7961,\n",
       "          4279,  4034, 16734, 24387,    18,  7642,  4112,  6510,  4172,  4007,\n",
       "          6231,  4118,  6655,  8427,  4073, 19339, 15737,    18,  7642,  4112,\n",
       "          6279,  4199,  4007,  4219,  7492,  4199,  4139,  6379,  4292,  8114,\n",
       "          7262, 15737,    18,  3265,  4050,  4028,  4047,  7219,  4112, 25449,\n",
       "          6505,  4007,  9553,  4279,  4325,  6560,  4519,  2468,  6289,  4239,\n",
       "          3755,  4034,  6671, 24387,    18,  6263,  4047,  6344,  4234,  6560,\n",
       "          4073, 17417, 26509,  6655,  4007,  3265,  4050,  4028,  4479,  4219,\n",
       "          6655,  4234,  6630, 19084,  4007,  7219,  4479, 16868,  6217,    18,\n",
       "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'start_positions': tensor(234),\n",
       " 'end_positions': tensor(234)}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7bf1cba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 6516, 33216, 4292, 6303, 4279, 4219, 6516, 33216, 4292, 3092, 4304, 2967, 3249, 4034, 6472, 4292, 6303, 4279, 4031, 6628, 6605, 8986, 4151, 11910, 3758, 6303, 4034, 2702, 4200, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('경쟁 상대국을 연구하고 경쟁 상대국을 앞설 수 있는 방법을 연구하기 위해서 더욱 활발해져야 할 연구는 뭐지')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7418aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 경쟁 상대국을 연구하고 경쟁 상대국을 앞설 수 있는 방법을 연구하기 위해서 더욱 활발해져야 할 연구는 뭐지 [SEP] 이 글에서는 제안한 기술경쟁력 평가 방법의 특징은 두 가지로 요약된다. 첫째, 현재의 경쟁력보다는 미래의 경쟁력을 평가하도록 평가의 대상을 확대해야 한다. 지금까지의 경쟁력 평가가 현위치를 중심으로 한 것이었다면, 앞으로의 대안은 과정과 경로를 중시하는 것이 되어야 한다는 것이다. 사실 지식정보화사회에서 가진 것이란 허망한 것이다. 현재의 기술수준이 높더라고, 이를 끊임없이 개량하고 새로운 기술을 발전시킬 수 있는 능력이 없다면 미래의 전망은 어둡다. 기술이나 정보는 실물 자산보다 진부화율이 매우 높다. 둘째, 기술경쟁력 평가를 순위를 매기는 작업이 아니라 이를 통하여 장점과 단점을 파악하는 과정으로 활용하여야 한다. 순위는 대중적인 흥미는 끌 수 있지만, 그것으로부터 교훈이 도출되는 것은 아니다. 기술경쟁력의 평가를 통해서 외국시스템의 장점을 배우고, 한국 시스템의 단점을 교정하는 대안이 발견될 수 있다. 경쟁이란 남과 비교하는 것을 의미한다. 내가 아무리 잘 하더라도 남이 나보다 더 잘 한다면 경쟁에서 진다. 그래서 경쟁은 각박하지만, 과학기술은 경쟁을 피할 수 있는 분야가 아니다. 경쟁 상대국을 연구하고, 경쟁 상대국을 앞설 수 있는 방안을 강구하기 위해서 기술경쟁력에 관한 연구가 더욱 활발해져야 할 것이다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(base_train_dataset.encodings.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad65ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fb3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3da86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e7462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0861eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83d948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "538cd972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2,  6516, 33216,  4292,  6303,  4279,  4219,  6516, 33216,  4292,\n",
       "          3092,  4304,  2967,  3249,  4034,  6472,  4292,  6303,  4279,  4031,\n",
       "          6628,  6605,  8986,  4151, 11910,  3758,  6303,  4034,  2702,  4200,\n",
       "             3,  3240,  2129,  4073, 28893,  7200,  4283,  6344,  4158,  4346,\n",
       "          4361,  6392,  6472,  4234,  7511,  4112,  2419,  6274,  4239, 12452,\n",
       "          4880,  4176,    18,  9270,    16,  6339,  4234,  7685,  4275,  6913,\n",
       "          6706,  4234,  7685,  4292,  6392,  4279, 26147,  6392,  4234,  6391,\n",
       "          4292,  6626, 18991,  6217,    18,  6292,  4149,  4200,  4234,  7685,\n",
       "          6392,  4070,  3794, 15883,  4110,  6593, 10749,  3757,  2048,  4007,\n",
       "          4480,  7599,    16,  3092, 10749,  4234,  8560,  4112,  6379,  4047,\n",
       "         10455,  4110, 11160,  4279,  4034,  2048,  4007,  2411, 16868,  6403,\n",
       "          2048, 24387,    18,  6278,  7260, 21590,  4162, 32459,  4073,  4129,\n",
       "          6971,  2048,  4007,  4271, 27837,  4283,  2048, 24387,    18,  6339,\n",
       "          4234,  6344,  4141,  4302,  4007,  2304, 26093,  4219,    16,  3240,\n",
       "          4110,  9903, 15208,  4279,  4219,  6436,  6344,  4292,  6574,  4114,\n",
       "          5180,  2967,  3249,  4034,  6633,  4007,  3123,  7599,  6706,  4234,\n",
       "          6585,  4112, 16914,  4176,    18,  6344, 17482,  6360,  4034, 14713,\n",
       "          7284,  4275,  4176, 24495,  4162,  4351,  4007,  6669,  2304,  4176,\n",
       "            18,  8628,    16,  6344,  4158,  4346,  4361,  6392,  4110,  7935,\n",
       "          4110, 19518,  4034,  6775,  4007,  6231,  4118,  3240,  4110,  8648,\n",
       "          4044,  8311,  4047, 10543,  4292,  7299,  4279,  4034,  6379, 10749,\n",
       "          6649, 26509,  4474,  6217,    18,  7935,  4034,  7519,  4199,  4139,\n",
       "          8930,  4034,  2221,  2967,  3249, 17164,    16,  6353, 10749,  6406,\n",
       "         13316,  4007, 13027,  4479,  4034,  2048,  4112,  6231,  4176,    18,\n",
       "          6344,  4158,  4346,  4361,  4234,  6392,  4110,  8114,  6678,  8073,\n",
       "          4996,  4234,  8311,  4292,  6844,  4219,    16,  6244,  6712,  4234,\n",
       "         10543,  4292, 12205,  4279,  4034,  8560,  4007,  6685,  4587,  2967,\n",
       "          3249,  4176,    18,  6516,  4007,  4271, 20162,  6894,  4279,  4034,\n",
       "          2048,  4292,  6455,  7796,    18,  2252,  4070,  8002,  3258,  3755,\n",
       "         26093,  4086, 17014,  2236,  4275,  4176,  2373,  3258,  7161,  6516,\n",
       "          4073,  4129,  8078,    18,  6489,  6516,  4112,  2011,  4335,  4279,\n",
       "         17164,    16,  6703, 19293,  4112,  6516,  4292, 11714,  2967,  3249,\n",
       "          4034,  6690,  4070,  6231,  4176,    18,  6516, 33216,  4292,  6303,\n",
       "          4279,  4219,    16,  6516, 33216,  4292,  3092,  4304,  2967,  3249,\n",
       "          4034,  6888,  4292, 14688,  4279,  4031,  6628,  6344,  4158,  4346,\n",
       "          4361,  4073,  6938,  6303,  4070,  6605,  8986,  4151, 11910,  3758,\n",
       "          2048, 24387,    18,     3,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'start_positions': tensor(347),\n",
       " 'end_positions': tensor(350)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769dfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc69b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54191a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995dd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_dataset = QADataset(data_dir=os.path.join(DATA_DIR, 'train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'train')\n",
    "base_val_dataset = QADataset(data_dir=os.path.join(DATA_DIR, 'train.json'), tokenizer = tokenizer, max_seq_len = 512, mode = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57975601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e075354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5e25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5ab06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115274e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 경쟁 상대국을 연구하고 경쟁 상대국을 앞설 수 있는 방법을 연구하기 위해서 더욱 활발해져야 할 연구는 뭐지 [SEP] 이 글에서는 제안한 기술경쟁력 평가 방법의 특징은 두 가지로 요약된다. 첫째, 현재의 경쟁력보다는 미래의 경쟁력을 평가하도록 평가의 대상을 확대해야 한다. 지금까지의 경쟁력 평가가 현위치를 중심으로 한 것이었다면, 앞으로의 대안은 과정과 경로를 중시하는 것이 되어야 한다는 것이다. 사실 지식정보화사회에서 가진 것이란 허망한 것이다. 현재의 기술수준이 높더라고, 이를 끊임없이 개량하고 새로운 기술을 발전시킬 수 있는 능력이 없다면 미래의 전망은 어둡다. 기술이나 정보는 실물 자산보다 진부화율이 매우 높다. 둘째, 기술경쟁력 평가를 순위를 매기는 작업이 아니라 이를 통하여 장점과 단점을 파악하는 과정으로 활용하여야 한다. 순위는 대중적인 흥미는 끌 수 있지만, 그것으로부터 교훈이 도출되는 것은 아니다. 기술경쟁력의 평가를 통해서 외국시스템의 장점을 배우고, 한국 시스템의 단점을 교정하는 대안이 발견될 수 있다. 경쟁이란 남과 비교하는 것을 의미한다. 내가 아무리 잘 하더라도 남이 나보다 더 잘 한다면 경쟁에서 진다. 그래서 경쟁은 각박하지만, 과학기술은 경쟁을 피할 수 있는 분야가 아니다. 경쟁 상대국을 연구하고, 경쟁 상대국을 앞설 수 있는 방안을 강구하기 위해서 기술경쟁력에 관한 연구가 더욱 활발해져야 할 것이다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(base_train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f91d7",
   "metadata": {},
   "source": [
    "**순서가 context, question임.  why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b25f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "[2, 20967, 4801, 4162, 6940, 2728, 9994, 4028, 6891, 4292, 6260, 6325, 8188, 4007, 7370, 4162, 2411, 4181, 4129, 6570, 4007, 17051, 4279, 4219, 3249, 4025, 3, 6721, 10749, 6377, 12178, 4225, 4142, 6371, 4073, 6985, 6700, 8767, 4110, 7083, 4279, 4737, 4176, 18, 6417, 4283, 6377, 12178, 4225, 4142, 6371, 4112, 6417, 4283, 10368, 4292, 6442, 4114, 5180, 2967, 3249, 18781, 16, 29676, 4034, 3240, 3330, 7002, 4414, 4162, 4239, 7713, 6325, 4147, 4297, 4086, 7235, 16, 2089, 4332, 17450, 4047, 6377, 24105, 6448, 4234, 7601, 4292, 7083, 4279, 4737, 4176, 18, 7233, 4556, 10008, 4234, 7002, 4199, 4195, 4034, 11663, 4234, 28, 18, 29, 9, 2575, 6377, 4120, 4031, 6766, 4139, 21, 18, 22, 9, 3238, 27, 18, 24, 4238, 6502, 4292, 6370, 4219, 3249, 4025, 7605, 7002, 4199, 4195, 4110, 13177, 6995, 4114, 4343, 4226, 4569, 4070, 6700, 10008, 4234, 8767, 4070, 2413, 2048, 10749, 6612, 4880, 4176, 18, 9994, 4028, 6891, 2728, 20967, 4801, 4162, 6940, 4110, 6260, 6325, 8188, 4007, 7370, 4162, 4479, 4181, 4129, 2089, 4332, 17450, 4007, 9451, 4279, 4219, 3249, 18781, 16, 2089, 4332, 17450, 4112, 6263, 4158, 4346, 4292, 10160, 4519, 2967, 3249, 4176, 18, 6380, 6253, 7362, 33472, 4110, 6260, 6832, 4292, 7767, 4418, 10749, 4994, 10056, 4234, 6377, 6408, 4292, 6737, 4279, 4034, 6940, 4034, 6263, 6516, 4005, 11407, 18014, 4587, 4141, 3249, 4176, 18, 6377, 12178, 4225, 4142, 6371, 4073, 6985, 6721, 6700, 8767, 4239, 6377, 12178, 6873, 4028, 4292, 6311, 6530, 4246, 6912, 4292, 7083, 4279, 4737, 4176, 18, 8910, 4199, 4139, 6377, 8892, 14655, 4073, 6720, 4283, 6832, 4177, 4585, 16, 6530, 4246, 8742, 4110, 7484, 7002, 4141, 4218, 6855, 16, 6658, 6377, 4031, 4042, 3092, 7901, 2728, 22725, 6496, 6935, 10749, 6700, 10008, 4073, 4129, 6530, 4246, 6912, 4007, 6818, 4199, 10749, 6866, 4479, 4219, 3249, 4176, 18, 6377, 12178, 6873, 4028, 4292, 6311, 6530, 4246, 6912, 4234, 15192, 32299, 4192, 6700, 4113, 4070, 4234, 6530, 4246, 6912, 4369, 4461, 4292, 6894, 16, 7083, 4279, 4737, 4176, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for key,val in base_val_dataset.encodings.items():\n",
    "    print(key)\n",
    "    print(val[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c64fe11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 25262\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 2762\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214c7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = tokenized_datasets['train']\n",
    "#val_dataset = tokenized_datasets['validation']\n",
    "train_dataset = base_train_dataset\n",
    "val_dataset = base_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ab29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=config['DATALOADER']['batch_size'],\n",
    "                              num_workers=config['DATALOADER']['num_workers'],\n",
    "                              shuffle=config['DATALOADER']['shuffle'],\n",
    "                              pin_memory=config['DATALOADER']['pin_memory'],\n",
    "                              drop_last=config['DATALOADER']['drop_last'])\n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=config['DATALOADER']['batch_size'],\n",
    "                            num_workers=config['DATALOADER']['num_workers'],\n",
    "                            shuffle=False,\n",
    "                            pin_memory=config['DATALOADER']['pin_memory'],\n",
    "                            drop_last=config['DATALOADER']['drop_last'])\n",
    "\n",
    "logger.info(f\"Load data, train:{len(train_dataset)} val:{len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c38543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['TRAINER']['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff27dd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 1 to 5.0000e-05.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "02. Set model\n",
    "\"\"\"\n",
    "# Load model\n",
    "model_name = config['TRAINER']['model']\n",
    "model = get_model(model_name=model_name, pretrained=config['TRAINER']['pretrained']).to(device)\n",
    "\n",
    "checkpoint = torch.load(os.path.join('results/train/20220611_232924', 'model.pt'))\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "#for param in model.model.electra.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "\"\"\"\n",
    "03. Set trainer\n",
    "\"\"\"\n",
    "# Optimizer\n",
    "optimizer = get_optimizer(optimizer_name=config['TRAINER']['optimizer'])\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "weight_decay = 0.1\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "#optimizer = optimizer(params=model.parameters(),lr=config['TRAINER']['learning_rate'])\n",
    "optimizer = optimizer(optimizer_grouped_parameters,lr=config['TRAINER']['learning_rate'])\n",
    "#optimizer = optimizer(params=model.parameters(),lr=5e-4)\n",
    "\n",
    "# lr reducer\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.800, verbose=True)\n",
    "\n",
    "# Loss\n",
    "loss = get_loss(loss_name=config['TRAINER']['loss'])\n",
    "\n",
    "# Metric\n",
    "metrics = {metric_name: get_metric(metric_name) for metric_name in config['TRAINER']['metric']}\n",
    "\n",
    "# Early stoppper\n",
    "early_stopper = EarlyStopper(patience=config['TRAINER']['early_stopping_patience'],\n",
    "                            mode=config['TRAINER']['early_stopping_mode'],\n",
    "                            logger=logger)\n",
    "# AMP\n",
    "if config['TRAINER']['amp'] == True:\n",
    "    from apex import amp\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics,\n",
    "                  device=device,\n",
    "                  logger=logger,\n",
    "                  tokenizer=tokenizer,\n",
    "                  amp=amp if config['TRAINER']['amp'] else None,\n",
    "                  interval=config['LOGGER']['logging_interval'])\n",
    "\"\"\"\n",
    "Logger\n",
    "\"\"\"\n",
    "# Recorder\n",
    "recorder = Recorder(record_dir=RECORDER_DIR,\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=None,\n",
    "                    amp=amp if config['TRAINER']['amp'] else None,\n",
    "                    logger=logger)\n",
    "\n",
    "# !Wandb\n",
    "if config['LOGGER']['wandb'] == True:\n",
    "    wandb_project_serial = 'template'\n",
    "    wandb_username =  ''\n",
    "    wandb.init(project=wandb_project_serial, dir=RECORDER_DIR, entity=wandb_username)\n",
    "    wandb.run.name = train_serial\n",
    "    wandb.config.update(config)\n",
    "    wandb.watch(model)\n",
    "\n",
    "# Save train config\n",
    "save_yaml(os.path.join(RECORDER_DIR, 'train_config.yml'), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f0ae51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.electra.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d72a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2add4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3efeaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  6358,  7353,  ...,     0,     0,     0],\n",
       "         [    2,  7612,  4250,  ...,     0,     0,     0],\n",
       "         [    2,  7644,  7591,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    2,  8869,  8327,  ...,     0,     0,     0],\n",
       "         [    2,  7644,  6244,  ...,     0,     0,     0],\n",
       "         [    2, 21766,  4234,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'start_positions': tensor([116,  37, 244,  33,  55, 146, 162,  44, 117, 200, 142,  26, 275,  52,\n",
       "          23,  42]),\n",
       " 'end_positions': tensor([117,  40, 244,  33,  57, 148, 164,  53, 124, 200, 147,  31, 277,  56,\n",
       "          25,  46])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dce9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask=inputs['attention_mask'].to(device)\n",
    "start_positions=inputs['start_positions'].to(device)\n",
    "end_positions=inputs['end_positions'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d90b4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids,\n",
    "                attention_mask,\n",
    "               start_positions,\n",
    "               end_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb3e4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, start_logits, end_logits = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5acdb5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2061, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ff865fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "461572fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -7.7879, -12.3271, -12.1101,  ..., -12.6438, -12.6048,  -7.7877],\n",
       "        [-10.0337, -11.5036, -12.5624,  ..., -12.5828, -12.5603, -10.0339],\n",
       "        [-10.4607, -12.3261, -12.3840,  ..., -12.6758, -12.6417, -10.4603],\n",
       "        ...,\n",
       "        [-10.7596, -12.2284, -12.5344,  ..., -12.7009, -12.6098, -10.7596],\n",
       "        [-10.9786, -12.0141, -12.0707,  ..., -12.7997, -12.7316, -10.9787],\n",
       "        [ -9.8129, -12.2328, -12.3911,  ..., -12.7641, -12.6870,  -9.8129]],\n",
       "       device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68dba37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = torch.argmax(start_logits, dim=1).cpu().tolist() \n",
    "end_idx = torch.argmax(end_logits, dim=1).cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44c1f6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 37, 244, 33, 55, 146, 162, 44, 117, 196, 142, 26, 275, 31, 23, 42]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70afad71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 40, 244, 33, 57, 147, 164, 53, 124, 200, 147, 31, 278, 35, 25, 46]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d484a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([116,  37, 244,  33,  55, 146, 162,  44, 117, 200, 142,  26, 275,  52,\n",
       "         23,  42], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44b836a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([117,  40, 244,  33,  57, 148, 164,  53, 124, 200, 147,  31, 277,  56,\n",
       "         25,  46], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94f0f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직접고용 비정규직 근로자\n",
      "스토커규제법\n",
      "고령자\n",
      "호주\n",
      "인적자본\n",
      "신재준\n",
      "표준오차\n",
      "Guggemos and Horvath\n",
      "교육부와 시도교육청간 권한 배분\n",
      "해상에 있는 선박\n",
      "확정기간혼합방식\n",
      "공익신고자보호법\n",
      "기타 전공 영역 전공\n",
      "국가통계시스템\n",
      "송헌재\n",
      "장애인 의무고용률\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    print(tokenizer.decode(input_ids[i][start_idx[i]:end_idx[i]+1]))\n",
    "    #print(tokenizer.decode(input_ids[i][start_positions[i]:end_positions[i]+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdf5cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                              | 104/1579 [00:29<07:02,  3.49it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "04. TRAIN\n",
    "\"\"\"\n",
    "# Train\n",
    "n_epochs = config['TRAINER']['n_epochs']\n",
    "for epoch_index in range(n_epochs):\n",
    "\n",
    "    # Set Recorder row\n",
    "    row_dict = dict()\n",
    "    row_dict['epoch_index'] = epoch_index\n",
    "    row_dict['train_serial'] = train_serial\n",
    "\n",
    "    \"\"\"\n",
    "    Train\n",
    "    \"\"\"\n",
    "    print(f\"Train {epoch_index}/{n_epochs}\")\n",
    "    logger.info(f\"--Train {epoch_index}/{n_epochs}\")\n",
    "    trainer.train(dataloader=train_dataloader, epoch_index=epoch_index, tokenizer=tokenizer, mode='train')\n",
    "\n",
    "    row_dict['train_loss'] = trainer.loss_mean\n",
    "    row_dict['train_elapsed_time'] = trainer.elapsed_time\n",
    "\n",
    "    for metric_str, score in trainer.score_dict.items():\n",
    "        row_dict[f\"train_{metric_str}\"] = score\n",
    "    trainer.clear_history()\n",
    "\n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    print(f\"Val {epoch_index}/{n_epochs}\")\n",
    "    logger.info(f\"--Val {epoch_index}/{n_epochs}\")\n",
    "    trainer.train(dataloader=val_dataloader, epoch_index=epoch_index, tokenizer=tokenizer, mode='val')\n",
    "\n",
    "    row_dict['val_loss'] = trainer.loss_mean\n",
    "    row_dict['val_elapsed_time'] = trainer.elapsed_time \n",
    "\n",
    "    for metric_str, score in trainer.score_dict.items():\n",
    "        row_dict[f\"val_{metric_str}\"] = score\n",
    "    trainer.clear_history()\n",
    "\n",
    "    \"\"\"\n",
    "    Record\n",
    "    \"\"\"\n",
    "    recorder.add_row(row_dict)\n",
    "    recorder.save_plot(config['LOGGER']['plot'])\n",
    "\n",
    "    #!WANDB\n",
    "    if config['LOGGER']['wandb'] == True:\n",
    "        wandb.log(row_dict)\n",
    "\n",
    "    \"\"\"\n",
    "    Early stopper\n",
    "    \"\"\"\n",
    "    early_stopping_target = config['TRAINER']['early_stopping_target']\n",
    "    early_stopper.check_early_stopping(loss=row_dict[early_stopping_target])\n",
    "\n",
    "    if early_stopper.patience_counter == 0:\n",
    "        recorder.save_weight(epoch=epoch_index)\n",
    "        best_row_dict = copy.deepcopy(row_dict)\n",
    "\n",
    "    if early_stopper.stop == True:\n",
    "        logger.info(f\"Early stopped, counter {early_stopper.patience_counter}/{config['TRAINER']['early_stopping_patience']}\")\n",
    "\n",
    "        if config['LOGGER']['wandb'] == True:\n",
    "            wandb.log(best_row_dict)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7954c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraModel(\n",
       "  (embeddings): ElectraEmbeddings(\n",
       "    (word_embeddings): Embedding(35000, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (encoder): ElectraEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a572e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.electra.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81f01492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraModel(\n",
       "  (embeddings): ElectraEmbeddings(\n",
       "    (word_embeddings): Embedding(35000, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (encoder): ElectraEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): ElectraLayer(\n",
       "        (attention): ElectraAttention(\n",
       "          (self): ElectraSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): ElectraSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ElectraIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ElectraOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "143fa8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.electra.embeddings.word_embeddings.weight\n",
      "model.electra.embeddings.position_embeddings.weight\n",
      "model.electra.embeddings.token_type_embeddings.weight\n",
      "model.electra.embeddings.LayerNorm.weight\n",
      "model.electra.embeddings.LayerNorm.bias\n",
      "model.electra.encoder.layer.0.attention.self.query.weight\n",
      "model.electra.encoder.layer.0.attention.self.query.bias\n",
      "model.electra.encoder.layer.0.attention.self.key.weight\n",
      "model.electra.encoder.layer.0.attention.self.key.bias\n",
      "model.electra.encoder.layer.0.attention.self.value.weight\n",
      "model.electra.encoder.layer.0.attention.self.value.bias\n",
      "model.electra.encoder.layer.0.attention.output.dense.weight\n",
      "model.electra.encoder.layer.0.attention.output.dense.bias\n",
      "model.electra.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.0.intermediate.dense.weight\n",
      "model.electra.encoder.layer.0.intermediate.dense.bias\n",
      "model.electra.encoder.layer.0.output.dense.weight\n",
      "model.electra.encoder.layer.0.output.dense.bias\n",
      "model.electra.encoder.layer.0.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.0.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.1.attention.self.query.weight\n",
      "model.electra.encoder.layer.1.attention.self.query.bias\n",
      "model.electra.encoder.layer.1.attention.self.key.weight\n",
      "model.electra.encoder.layer.1.attention.self.key.bias\n",
      "model.electra.encoder.layer.1.attention.self.value.weight\n",
      "model.electra.encoder.layer.1.attention.self.value.bias\n",
      "model.electra.encoder.layer.1.attention.output.dense.weight\n",
      "model.electra.encoder.layer.1.attention.output.dense.bias\n",
      "model.electra.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.1.intermediate.dense.weight\n",
      "model.electra.encoder.layer.1.intermediate.dense.bias\n",
      "model.electra.encoder.layer.1.output.dense.weight\n",
      "model.electra.encoder.layer.1.output.dense.bias\n",
      "model.electra.encoder.layer.1.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.1.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.2.attention.self.query.weight\n",
      "model.electra.encoder.layer.2.attention.self.query.bias\n",
      "model.electra.encoder.layer.2.attention.self.key.weight\n",
      "model.electra.encoder.layer.2.attention.self.key.bias\n",
      "model.electra.encoder.layer.2.attention.self.value.weight\n",
      "model.electra.encoder.layer.2.attention.self.value.bias\n",
      "model.electra.encoder.layer.2.attention.output.dense.weight\n",
      "model.electra.encoder.layer.2.attention.output.dense.bias\n",
      "model.electra.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.2.intermediate.dense.weight\n",
      "model.electra.encoder.layer.2.intermediate.dense.bias\n",
      "model.electra.encoder.layer.2.output.dense.weight\n",
      "model.electra.encoder.layer.2.output.dense.bias\n",
      "model.electra.encoder.layer.2.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.2.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.3.attention.self.query.weight\n",
      "model.electra.encoder.layer.3.attention.self.query.bias\n",
      "model.electra.encoder.layer.3.attention.self.key.weight\n",
      "model.electra.encoder.layer.3.attention.self.key.bias\n",
      "model.electra.encoder.layer.3.attention.self.value.weight\n",
      "model.electra.encoder.layer.3.attention.self.value.bias\n",
      "model.electra.encoder.layer.3.attention.output.dense.weight\n",
      "model.electra.encoder.layer.3.attention.output.dense.bias\n",
      "model.electra.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.3.intermediate.dense.weight\n",
      "model.electra.encoder.layer.3.intermediate.dense.bias\n",
      "model.electra.encoder.layer.3.output.dense.weight\n",
      "model.electra.encoder.layer.3.output.dense.bias\n",
      "model.electra.encoder.layer.3.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.3.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.4.attention.self.query.weight\n",
      "model.electra.encoder.layer.4.attention.self.query.bias\n",
      "model.electra.encoder.layer.4.attention.self.key.weight\n",
      "model.electra.encoder.layer.4.attention.self.key.bias\n",
      "model.electra.encoder.layer.4.attention.self.value.weight\n",
      "model.electra.encoder.layer.4.attention.self.value.bias\n",
      "model.electra.encoder.layer.4.attention.output.dense.weight\n",
      "model.electra.encoder.layer.4.attention.output.dense.bias\n",
      "model.electra.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.4.intermediate.dense.weight\n",
      "model.electra.encoder.layer.4.intermediate.dense.bias\n",
      "model.electra.encoder.layer.4.output.dense.weight\n",
      "model.electra.encoder.layer.4.output.dense.bias\n",
      "model.electra.encoder.layer.4.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.4.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.5.attention.self.query.weight\n",
      "model.electra.encoder.layer.5.attention.self.query.bias\n",
      "model.electra.encoder.layer.5.attention.self.key.weight\n",
      "model.electra.encoder.layer.5.attention.self.key.bias\n",
      "model.electra.encoder.layer.5.attention.self.value.weight\n",
      "model.electra.encoder.layer.5.attention.self.value.bias\n",
      "model.electra.encoder.layer.5.attention.output.dense.weight\n",
      "model.electra.encoder.layer.5.attention.output.dense.bias\n",
      "model.electra.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.5.intermediate.dense.weight\n",
      "model.electra.encoder.layer.5.intermediate.dense.bias\n",
      "model.electra.encoder.layer.5.output.dense.weight\n",
      "model.electra.encoder.layer.5.output.dense.bias\n",
      "model.electra.encoder.layer.5.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.5.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.6.attention.self.query.weight\n",
      "model.electra.encoder.layer.6.attention.self.query.bias\n",
      "model.electra.encoder.layer.6.attention.self.key.weight\n",
      "model.electra.encoder.layer.6.attention.self.key.bias\n",
      "model.electra.encoder.layer.6.attention.self.value.weight\n",
      "model.electra.encoder.layer.6.attention.self.value.bias\n",
      "model.electra.encoder.layer.6.attention.output.dense.weight\n",
      "model.electra.encoder.layer.6.attention.output.dense.bias\n",
      "model.electra.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.6.intermediate.dense.weight\n",
      "model.electra.encoder.layer.6.intermediate.dense.bias\n",
      "model.electra.encoder.layer.6.output.dense.weight\n",
      "model.electra.encoder.layer.6.output.dense.bias\n",
      "model.electra.encoder.layer.6.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.6.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.7.attention.self.query.weight\n",
      "model.electra.encoder.layer.7.attention.self.query.bias\n",
      "model.electra.encoder.layer.7.attention.self.key.weight\n",
      "model.electra.encoder.layer.7.attention.self.key.bias\n",
      "model.electra.encoder.layer.7.attention.self.value.weight\n",
      "model.electra.encoder.layer.7.attention.self.value.bias\n",
      "model.electra.encoder.layer.7.attention.output.dense.weight\n",
      "model.electra.encoder.layer.7.attention.output.dense.bias\n",
      "model.electra.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.7.intermediate.dense.weight\n",
      "model.electra.encoder.layer.7.intermediate.dense.bias\n",
      "model.electra.encoder.layer.7.output.dense.weight\n",
      "model.electra.encoder.layer.7.output.dense.bias\n",
      "model.electra.encoder.layer.7.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.7.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.8.attention.self.query.weight\n",
      "model.electra.encoder.layer.8.attention.self.query.bias\n",
      "model.electra.encoder.layer.8.attention.self.key.weight\n",
      "model.electra.encoder.layer.8.attention.self.key.bias\n",
      "model.electra.encoder.layer.8.attention.self.value.weight\n",
      "model.electra.encoder.layer.8.attention.self.value.bias\n",
      "model.electra.encoder.layer.8.attention.output.dense.weight\n",
      "model.electra.encoder.layer.8.attention.output.dense.bias\n",
      "model.electra.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.8.intermediate.dense.weight\n",
      "model.electra.encoder.layer.8.intermediate.dense.bias\n",
      "model.electra.encoder.layer.8.output.dense.weight\n",
      "model.electra.encoder.layer.8.output.dense.bias\n",
      "model.electra.encoder.layer.8.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.8.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.9.attention.self.query.weight\n",
      "model.electra.encoder.layer.9.attention.self.query.bias\n",
      "model.electra.encoder.layer.9.attention.self.key.weight\n",
      "model.electra.encoder.layer.9.attention.self.key.bias\n",
      "model.electra.encoder.layer.9.attention.self.value.weight\n",
      "model.electra.encoder.layer.9.attention.self.value.bias\n",
      "model.electra.encoder.layer.9.attention.output.dense.weight\n",
      "model.electra.encoder.layer.9.attention.output.dense.bias\n",
      "model.electra.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.9.intermediate.dense.weight\n",
      "model.electra.encoder.layer.9.intermediate.dense.bias\n",
      "model.electra.encoder.layer.9.output.dense.weight\n",
      "model.electra.encoder.layer.9.output.dense.bias\n",
      "model.electra.encoder.layer.9.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.9.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.10.attention.self.query.weight\n",
      "model.electra.encoder.layer.10.attention.self.query.bias\n",
      "model.electra.encoder.layer.10.attention.self.key.weight\n",
      "model.electra.encoder.layer.10.attention.self.key.bias\n",
      "model.electra.encoder.layer.10.attention.self.value.weight\n",
      "model.electra.encoder.layer.10.attention.self.value.bias\n",
      "model.electra.encoder.layer.10.attention.output.dense.weight\n",
      "model.electra.encoder.layer.10.attention.output.dense.bias\n",
      "model.electra.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.10.intermediate.dense.weight\n",
      "model.electra.encoder.layer.10.intermediate.dense.bias\n",
      "model.electra.encoder.layer.10.output.dense.weight\n",
      "model.electra.encoder.layer.10.output.dense.bias\n",
      "model.electra.encoder.layer.10.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.10.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.11.attention.self.query.weight\n",
      "model.electra.encoder.layer.11.attention.self.query.bias\n",
      "model.electra.encoder.layer.11.attention.self.key.weight\n",
      "model.electra.encoder.layer.11.attention.self.key.bias\n",
      "model.electra.encoder.layer.11.attention.self.value.weight\n",
      "model.electra.encoder.layer.11.attention.self.value.bias\n",
      "model.electra.encoder.layer.11.attention.output.dense.weight\n",
      "model.electra.encoder.layer.11.attention.output.dense.bias\n",
      "model.electra.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model.electra.encoder.layer.11.intermediate.dense.weight\n",
      "model.electra.encoder.layer.11.intermediate.dense.bias\n",
      "model.electra.encoder.layer.11.output.dense.weight\n",
      "model.electra.encoder.layer.11.output.dense.bias\n",
      "model.electra.encoder.layer.11.output.LayerNorm.weight\n",
      "model.electra.encoder.layer.11.output.LayerNorm.bias\n",
      "model.qa_outputs.weight\n",
      "model.qa_outputs.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f344f4c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
